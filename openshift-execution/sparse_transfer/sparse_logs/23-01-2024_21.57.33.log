Logging all SparseML modifier-level logs to sparse_logs/23-01-2024_21.57.33.log
python DistillationModifier [0 - 1706047064.020954]: Calling loss_update with:
args: 0.6932349801063538| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [0 - 1706047065.9187348]: 
Returned: 1.577850341796875| 

python LearningRateFunctionModifier [0 - 1706047068.4722266]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [0 - 1706047068.4723816]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [0 - 1706047068.4724295]: 0.00015
python LearningRateFunctionModifier/ParamGroup1 [0 - 1706047068.472456]: 0.00015
python DistillationModifier/task_loss [0 - 1706047068.4724987]: tensor(0.6932, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [0 - 1706047068.4733248]: tensor(1.5779, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [0 - 1706047068.4736025]: tensor(1.5779, grad_fn=<AddBackward0>)
python ConstantPruningModifier [0 - 1706047068.4738576]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.0| steps_per_epoch: 63| 
python ConstantPruningModifier [0 - 1706047068.8229423]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [0 - 1706047068.8234196]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [0 - 1706047068.8237343]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [0 - 1706047068.8239942]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [0 - 1706047068.8242483]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [0 - 1706047068.8250573]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [0 - 1706047068.8256736]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [0 - 1706047068.8259132]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [0 - 1706047068.8261414]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [0 - 1706047068.8263292]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [0 - 1706047068.826511]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [0 - 1706047068.8270714]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [0 - 1706047068.827637]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [0 - 1706047068.8278656]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [0 - 1706047068.8280592]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [0 - 1706047068.8282595]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [0 - 1706047068.8284476]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [0 - 1706047068.8290267]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [0 - 1706047068.8295875]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [0 - 1706047068.8297994]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [0 - 1706047068.829994]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [0 - 1706047068.8301904]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [0 - 1706047068.8304422]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [0 - 1706047068.8310096]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [0 - 1706047068.8315508]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [0 - 1706047068.831775]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [0 - 1706047068.8319383]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [0 - 1706047068.8321314]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [0 - 1706047068.8323243]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [0 - 1706047068.8328807]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [0 - 1706047068.8334205]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [0 - 1706047068.8336368]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [0 - 1706047068.8338356]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [0 - 1706047068.834025]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [0 - 1706047068.834202]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [0 - 1706047068.8347604]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [0 - 1706047068.835299]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [0 - 1706047068.8354983]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [0 - 1706047068.835709]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [0 - 1706047068.835899]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [0 - 1706047068.8360722]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [0 - 1706047068.836639]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [0 - 1706047068.8371828]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [0 - 1706047068.8373775]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [0 - 1706047068.8375585]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [0 - 1706047068.8377733]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [0 - 1706047068.8379533]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [0 - 1706047068.8384883]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [0 - 1706047068.8390467]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [0 - 1706047068.8392446]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [0 - 1706047068.839428]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [0 - 1706047068.839621]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [0 - 1706047068.8397968]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [0 - 1706047068.8403285]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [0 - 1706047068.8408976]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [0 - 1706047068.8411036]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [0 - 1706047068.8412845]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [0 - 1706047068.841447]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [0 - 1706047068.8416436]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [0 - 1706047068.8421896]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [0 - 1706047068.8427503]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [0 - 1706047068.8429534]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [0 - 1706047068.8431346]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [0 - 1706047068.8433168]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [0 - 1706047068.843498]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [0 - 1706047068.8440578]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [0 - 1706047068.8446307]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [0 - 1706047068.8448298]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [0 - 1706047068.845019]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [0 - 1706047068.8452094]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [0 - 1706047068.8453712]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [0 - 1706047068.8459225]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [0 - 1706047068.8464622]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [0 - 1706047068.846679]: 0.0
python ParamPruning/classifier.weight [0 - 1706047068.8467503]: 0.0
python DistillationModifier [7 - 1706047107.3277247]: Calling loss_update with:
args: 0.7696167826652527| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 0.1111111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [7 - 1706047108.544008]: 
Returned: 1.533712387084961| 

python LearningRateFunctionModifier [7 - 1706047111.1685002]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.1111111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [7 - 1706047111.1686823]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [7 - 1706047111.1687298]: 0.00014871794871794872
python LearningRateFunctionModifier/ParamGroup1 [7 - 1706047111.1687555]: 0.00014871794871794872
python DistillationModifier/task_loss [7 - 1706047111.168797]: tensor(0.7696, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [7 - 1706047111.1693132]: tensor(1.5337, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [7 - 1706047111.1695552]: tensor(1.5337, grad_fn=<AddBackward0>)
python ConstantPruningModifier [7 - 1706047111.1698253]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.1111111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [7 - 1706047111.5185807]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [7 - 1706047111.5190842]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [7 - 1706047111.5193079]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [7 - 1706047111.5194867]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [7 - 1706047111.5196784]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [7 - 1706047111.5204332]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [7 - 1706047111.521274]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [7 - 1706047111.521526]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [7 - 1706047111.5217214]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [7 - 1706047111.5218923]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [7 - 1706047111.5220587]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [7 - 1706047111.5226817]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [7 - 1706047111.5232353]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [7 - 1706047111.5234551]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [7 - 1706047111.5236368]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [7 - 1706047111.5238068]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [7 - 1706047111.5239716]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [7 - 1706047111.5244894]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [7 - 1706047111.5251746]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [7 - 1706047111.525392]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [7 - 1706047111.5255558]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [7 - 1706047111.5257516]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [7 - 1706047111.525914]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [7 - 1706047111.5264397]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [7 - 1706047111.527043]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [7 - 1706047111.5272563]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [7 - 1706047111.527422]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [7 - 1706047111.5275996]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [7 - 1706047111.527773]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [7 - 1706047111.528309]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [7 - 1706047111.5288777]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [7 - 1706047111.5290952]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [7 - 1706047111.5292606]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [7 - 1706047111.5294275]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [7 - 1706047111.5296078]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [7 - 1706047111.5301256]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [7 - 1706047111.530801]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [7 - 1706047111.531012]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [7 - 1706047111.531176]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [7 - 1706047111.5313356]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [7 - 1706047111.5314953]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [7 - 1706047111.5320504]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [7 - 1706047111.5326529]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [7 - 1706047111.5328822]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [7 - 1706047111.5330484]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [7 - 1706047111.5332115]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [7 - 1706047111.5333722]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [7 - 1706047111.533923]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [7 - 1706047111.5344577]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [7 - 1706047111.5346818]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [7 - 1706047111.53485]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [7 - 1706047111.5350127]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [7 - 1706047111.5351741]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [7 - 1706047111.5356994]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [7 - 1706047111.53623]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [7 - 1706047111.5364306]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [7 - 1706047111.536624]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [7 - 1706047111.5367975]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [7 - 1706047111.536957]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [7 - 1706047111.5374596]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [7 - 1706047111.5380054]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [7 - 1706047111.538201]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [7 - 1706047111.5383654]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [7 - 1706047111.5385256]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [7 - 1706047111.5387135]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [7 - 1706047111.5392196]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [7 - 1706047111.5399396]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [7 - 1706047111.540156]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [7 - 1706047111.5403183]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [7 - 1706047111.5404782]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [7 - 1706047111.5406861]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [7 - 1706047111.541222]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [7 - 1706047111.5417736]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [7 - 1706047111.5419738]: 0.0
python ParamPruning/classifier.weight [7 - 1706047111.542043]: 0.0
python DistillationModifier [14 - 1706047152.942481]: Calling loss_update with:
args: 0.753770112991333| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 0.2222222222222222| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [14 - 1706047154.2240236]: 
Returned: 1.6643567085266113| 

python LearningRateFunctionModifier [14 - 1706047156.4754007]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.2222222222222222| steps_per_epoch: 63| 
python LearningRateFunctionModifier [14 - 1706047156.4755871]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [14 - 1706047156.4756355]: 0.00014743589743589742
python LearningRateFunctionModifier/ParamGroup1 [14 - 1706047156.4756615]: 0.00014743589743589742
python DistillationModifier/task_loss [14 - 1706047156.4757037]: tensor(0.7538, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [14 - 1706047156.4761994]: tensor(1.6644, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [14 - 1706047156.4764423]: tensor(1.6644, grad_fn=<AddBackward0>)
python ConstantPruningModifier [14 - 1706047156.4767187]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.2222222222222222| steps_per_epoch: 63| 
python ConstantPruningModifier [14 - 1706047156.7230766]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [14 - 1706047156.7235594]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [14 - 1706047156.7238796]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [14 - 1706047156.7241275]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [14 - 1706047156.7243774]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [14 - 1706047156.7251978]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [14 - 1706047156.725796]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [14 - 1706047156.7260294]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [14 - 1706047156.7262473]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [14 - 1706047156.726426]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [14 - 1706047156.726618]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [14 - 1706047156.7271512]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [14 - 1706047156.72793]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [14 - 1706047156.728147]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [14 - 1706047156.7283516]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [14 - 1706047156.7285378]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [14 - 1706047156.7288675]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [14 - 1706047156.7294495]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [14 - 1706047156.7301333]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [14 - 1706047156.7303472]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [14 - 1706047156.7305532]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [14 - 1706047156.7307634]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [14 - 1706047156.7309961]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [14 - 1706047156.7316265]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [14 - 1706047156.7322488]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [14 - 1706047156.7324538]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [14 - 1706047156.7326927]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [14 - 1706047156.7329426]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [14 - 1706047156.7331724]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [14 - 1706047156.7337763]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [14 - 1706047156.734373]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [14 - 1706047156.73461]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [14 - 1706047156.7348359]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [14 - 1706047156.7350576]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [14 - 1706047156.7352364]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [14 - 1706047156.7358313]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [14 - 1706047156.736428]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [14 - 1706047156.7366889]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [14 - 1706047156.736902]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [14 - 1706047156.737116]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [14 - 1706047156.7373383]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [14 - 1706047156.737959]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [14 - 1706047156.738555]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [14 - 1706047156.7388158]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [14 - 1706047156.7390177]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [14 - 1706047156.7392328]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [14 - 1706047156.7394507]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [14 - 1706047156.740064]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [14 - 1706047156.740695]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [14 - 1706047156.7409263]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [14 - 1706047156.7411313]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [14 - 1706047156.7413483]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [14 - 1706047156.7415805]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [14 - 1706047156.7422106]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [14 - 1706047156.742835]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [14 - 1706047156.7430615]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [14 - 1706047156.7432683]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [14 - 1706047156.7434547]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [14 - 1706047156.7436483]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [14 - 1706047156.7441933]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [14 - 1706047156.744793]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [14 - 1706047156.745012]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [14 - 1706047156.7452035]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [14 - 1706047156.7454526]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [14 - 1706047156.7456958]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [14 - 1706047156.7463276]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [14 - 1706047156.746958]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [14 - 1706047156.7471778]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [14 - 1706047156.7473783]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [14 - 1706047156.74762]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [14 - 1706047156.7478423]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [14 - 1706047156.7484176]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [14 - 1706047156.749036]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [14 - 1706047156.7492588]: 0.0
python ParamPruning/classifier.weight [14 - 1706047156.7493289]: 0.0
python DistillationModifier [21 - 1706047198.0448751]: Calling loss_update with:
args: 0.6486025452613831| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 0.3333333333333333| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [21 - 1706047199.2272313]: 
Returned: 1.6965540647506714| 

python LearningRateFunctionModifier [21 - 1706047201.5700586]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.3333333333333333| steps_per_epoch: 63| 
python LearningRateFunctionModifier [21 - 1706047201.5702128]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [21 - 1706047201.570261]: 0.00014615384615384615
python LearningRateFunctionModifier/ParamGroup1 [21 - 1706047201.5702863]: 0.00014615384615384615
python DistillationModifier/task_loss [21 - 1706047201.5703275]: tensor(0.6486, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [21 - 1706047201.5708766]: tensor(1.6966, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [21 - 1706047201.571134]: tensor(1.6966, grad_fn=<AddBackward0>)
python ConstantPruningModifier [21 - 1706047201.571372]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.3333333333333333| steps_per_epoch: 63| 
python ConstantPruningModifier [21 - 1706047201.8394494]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [21 - 1706047201.8399582]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [21 - 1706047201.8402271]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [21 - 1706047201.840418]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [21 - 1706047201.840642]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [21 - 1706047201.841248]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [21 - 1706047201.8417435]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [21 - 1706047201.8419595]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [21 - 1706047201.8421526]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [21 - 1706047201.8423011]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [21 - 1706047201.84245]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [21 - 1706047201.8430543]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [21 - 1706047201.8437142]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [21 - 1706047201.8439243]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [21 - 1706047201.844122]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [21 - 1706047201.844276]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [21 - 1706047201.8444326]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [21 - 1706047201.8449607]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [21 - 1706047201.8453965]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [21 - 1706047201.8456283]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [21 - 1706047201.8457968]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [21 - 1706047201.846082]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [21 - 1706047201.8462448]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [21 - 1706047201.8466408]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [21 - 1706047201.8470087]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [21 - 1706047201.8471882]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [21 - 1706047201.8473275]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [21 - 1706047201.8475046]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [21 - 1706047201.847683]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [21 - 1706047201.848056]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [21 - 1706047201.8486986]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [21 - 1706047201.8488812]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [21 - 1706047201.8490326]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [21 - 1706047201.8492186]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [21 - 1706047201.849369]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [21 - 1706047201.849797]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [21 - 1706047201.8501737]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [21 - 1706047201.8503532]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [21 - 1706047201.8504987]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [21 - 1706047201.850696]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [21 - 1706047201.8508892]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [21 - 1706047201.8512418]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [21 - 1706047201.8516345]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [21 - 1706047201.8518271]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [21 - 1706047201.8520064]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [21 - 1706047201.852159]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [21 - 1706047201.8523443]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [21 - 1706047201.9167888]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [21 - 1706047201.9171927]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [21 - 1706047201.9173825]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [21 - 1706047201.917522]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [21 - 1706047201.917739]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [21 - 1706047201.9178908]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [21 - 1706047201.9183996]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [21 - 1706047201.9190092]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [21 - 1706047201.9192176]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [21 - 1706047201.9194078]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [21 - 1706047201.9196022]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [21 - 1706047201.9197793]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [21 - 1706047201.9202278]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [21 - 1706047201.9206748]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [21 - 1706047201.9208667]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [21 - 1706047201.9210584]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [21 - 1706047201.9212441]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [21 - 1706047201.9213982]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [21 - 1706047201.9218016]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [21 - 1706047201.9221816]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [21 - 1706047201.9223776]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [21 - 1706047201.9225373]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [21 - 1706047201.9227035]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [21 - 1706047201.9228795]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [21 - 1706047201.92323]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [21 - 1706047201.923608]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [21 - 1706047201.923799]: 0.0
python ParamPruning/classifier.weight [21 - 1706047201.923868]: 0.0
python DistillationModifier [28 - 1706047239.928954]: Calling loss_update with:
args: 0.8467352986335754| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 0.4444444444444444| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [28 - 1706047241.7375274]: 
Returned: 1.784904956817627| 

python LearningRateFunctionModifier [28 - 1706047245.2173405]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.4444444444444444| steps_per_epoch: 63| 
python LearningRateFunctionModifier [28 - 1706047245.217516]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [28 - 1706047245.2175794]: 0.00014487179487179485
python LearningRateFunctionModifier/ParamGroup1 [28 - 1706047245.2176204]: 0.00014487179487179485
python DistillationModifier/task_loss [28 - 1706047245.217665]: tensor(0.8467, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [28 - 1706047245.2181737]: tensor(1.7849, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [28 - 1706047245.2184153]: tensor(1.7849, grad_fn=<AddBackward0>)
python ConstantPruningModifier [28 - 1706047245.2186675]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.4444444444444444| steps_per_epoch: 63| 
python ConstantPruningModifier [28 - 1706047245.526495]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [28 - 1706047245.5270128]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [28 - 1706047245.5272436]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [28 - 1706047245.52748]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [28 - 1706047245.5276773]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [28 - 1706047245.528445]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [28 - 1706047245.5292819]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [28 - 1706047245.5295267]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [28 - 1706047245.529801]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [28 - 1706047245.529973]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [28 - 1706047245.5301611]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [28 - 1706047245.5307798]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [28 - 1706047245.5314279]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [28 - 1706047245.5316885]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [28 - 1706047245.5318575]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [28 - 1706047245.5320861]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [28 - 1706047245.5323176]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [28 - 1706047245.532916]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [28 - 1706047245.5337164]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [28 - 1706047245.5339499]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [28 - 1706047245.534178]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [28 - 1706047245.5344164]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [28 - 1706047245.5346735]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [28 - 1706047245.5352573]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [28 - 1706047245.536013]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [28 - 1706047245.5362473]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [28 - 1706047245.5364695]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [28 - 1706047245.5367446]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [28 - 1706047245.5369852]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [28 - 1706047245.5376074]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [28 - 1706047245.5383008]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [28 - 1706047245.5385325]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [28 - 1706047245.5387866]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [28 - 1706047245.53898]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [28 - 1706047245.5391665]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [28 - 1706047245.539742]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [28 - 1706047245.5405197]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [28 - 1706047245.5407844]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [28 - 1706047245.5410051]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [28 - 1706047245.5412445]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [28 - 1706047245.541427]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [28 - 1706047245.5420713]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [28 - 1706047245.542811]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [28 - 1706047245.5430474]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [28 - 1706047245.5432825]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [28 - 1706047245.5434716]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [28 - 1706047245.5436745]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [28 - 1706047245.5442514]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [28 - 1706047245.5450282]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [28 - 1706047245.5452616]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [28 - 1706047245.5454934]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [28 - 1706047245.545731]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [28 - 1706047245.545952]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [28 - 1706047245.5466933]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [28 - 1706047245.5473626]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [28 - 1706047245.5476117]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [28 - 1706047245.5478356]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [28 - 1706047245.5480573]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [28 - 1706047245.548221]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [28 - 1706047245.5488093]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [28 - 1706047245.5494568]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [28 - 1706047245.5497015]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [28 - 1706047245.5499241]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [28 - 1706047245.5501072]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [28 - 1706047245.550327]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [28 - 1706047245.5510867]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [28 - 1706047245.5519006]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [28 - 1706047245.5521479]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [28 - 1706047245.5523617]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [28 - 1706047245.6165109]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [28 - 1706047245.6168196]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [28 - 1706047245.6175828]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [28 - 1706047245.618356]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [28 - 1706047245.6185997]: 0.0
python ParamPruning/classifier.weight [28 - 1706047245.618677]: 0.0
python DistillationModifier [35 - 1706047291.5424423]: Calling loss_update with:
args: 0.6801326870918274| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 0.5555555555555556| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [35 - 1706047292.820632]: 
Returned: 1.6131408214569092| 

python LearningRateFunctionModifier [35 - 1706047295.060978]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.5555555555555556| steps_per_epoch: 63| 
python LearningRateFunctionModifier [35 - 1706047295.0611274]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [35 - 1706047295.0611756]: 0.00014358974358974358
python LearningRateFunctionModifier/ParamGroup1 [35 - 1706047295.0612016]: 0.00014358974358974358
python DistillationModifier/task_loss [35 - 1706047295.0612419]: tensor(0.6801, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [35 - 1706047295.061766]: tensor(1.6131, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [35 - 1706047295.0620244]: tensor(1.6131, grad_fn=<AddBackward0>)
python ConstantPruningModifier [35 - 1706047295.0622656]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.5555555555555556| steps_per_epoch: 63| 
python ConstantPruningModifier [35 - 1706047295.337353]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [35 - 1706047295.3378816]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [35 - 1706047295.3381808]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [35 - 1706047295.3383644]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [35 - 1706047295.3386269]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [35 - 1706047295.3394063]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [35 - 1706047295.3400528]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [35 - 1706047295.3402965]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [35 - 1706047295.3404596]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [35 - 1706047295.3407118]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [35 - 1706047295.3409405]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [35 - 1706047295.3414774]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [35 - 1706047295.3420515]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [35 - 1706047295.3422585]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [35 - 1706047295.342449]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [35 - 1706047295.3426676]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [35 - 1706047295.3428478]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [35 - 1706047295.343377]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [35 - 1706047295.3439362]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [35 - 1706047295.3441584]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [35 - 1706047295.344322]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [35 - 1706047295.3444977]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [35 - 1706047295.3447223]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [35 - 1706047295.345261]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [35 - 1706047295.345828]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [35 - 1706047295.3460212]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [35 - 1706047295.346183]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [35 - 1706047295.3463826]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [35 - 1706047295.346556]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [35 - 1706047295.3471215]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [35 - 1706047295.3476872]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [35 - 1706047295.3479016]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [35 - 1706047295.3480778]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [35 - 1706047295.3482397]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [35 - 1706047295.3484125]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [35 - 1706047295.348907]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [35 - 1706047295.3492556]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [35 - 1706047295.349429]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [35 - 1706047295.3496082]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [35 - 1706047295.349755]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [35 - 1706047295.3499286]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [35 - 1706047295.3502855]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [35 - 1706047295.3506758]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [35 - 1706047295.3508554]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [35 - 1706047295.3510165]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [35 - 1706047295.3511562]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [35 - 1706047295.3513258]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [35 - 1706047295.351692]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [35 - 1706047295.352047]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [35 - 1706047295.3522108]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [35 - 1706047295.3523595]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [35 - 1706047295.3525321]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [35 - 1706047295.4167757]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [35 - 1706047295.4173224]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [35 - 1706047295.4177754]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [35 - 1706047295.4179857]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [35 - 1706047295.4181743]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [35 - 1706047295.4183264]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [35 - 1706047295.418513]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [35 - 1706047295.4188998]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [35 - 1706047295.4192615]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [35 - 1706047295.4194396]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [35 - 1706047295.4196198]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [35 - 1706047295.4197736]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [35 - 1706047295.4199219]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [35 - 1706047295.4202733]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [35 - 1706047295.4206796]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [35 - 1706047295.4208539]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [35 - 1706047295.42104]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [35 - 1706047295.4211795]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [35 - 1706047295.4213562]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [35 - 1706047295.4217398]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [35 - 1706047295.4221134]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [35 - 1706047295.4223163]: 0.0
python ParamPruning/classifier.weight [35 - 1706047295.4223852]: 0.0
python DistillationModifier [42 - 1706047332.8260195]: Calling loss_update with:
args: 0.5663852691650391| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 0.6666666666666666| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [42 - 1706047333.9464579]: 
Returned: 1.342790126800537| 

python LearningRateFunctionModifier [42 - 1706047336.2577515]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.6666666666666666| steps_per_epoch: 63| 
python LearningRateFunctionModifier [42 - 1706047336.2579026]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [42 - 1706047336.2579489]: 0.00014230769230769228
python LearningRateFunctionModifier/ParamGroup1 [42 - 1706047336.257975]: 0.00014230769230769228
python DistillationModifier/task_loss [42 - 1706047336.258017]: tensor(0.5664, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [42 - 1706047336.2585087]: tensor(1.3428, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [42 - 1706047336.2587972]: tensor(1.3428, grad_fn=<AddBackward0>)
python ConstantPruningModifier [42 - 1706047336.2590425]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.6666666666666666| steps_per_epoch: 63| 
python ConstantPruningModifier [42 - 1706047336.4357347]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [42 - 1706047336.4361987]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [42 - 1706047336.43641]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [42 - 1706047336.4366646]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [42 - 1706047336.4368322]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [42 - 1706047336.43739]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [42 - 1706047336.4380171]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [42 - 1706047336.4382436]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [42 - 1706047336.4383981]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [42 - 1706047336.4386187]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [42 - 1706047336.4388015]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [42 - 1706047336.4392881]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [42 - 1706047336.4397748]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [42 - 1706047336.4399674]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [42 - 1706047336.4401143]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [42 - 1706047336.4402523]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [42 - 1706047336.4403985]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [42 - 1706047336.440865]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [42 - 1706047336.441289]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [42 - 1706047336.4414797]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [42 - 1706047336.4416559]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [42 - 1706047336.441854]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [42 - 1706047336.4420447]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [42 - 1706047336.4424393]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [42 - 1706047336.4428566]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [42 - 1706047336.4430523]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [42 - 1706047336.4431946]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [42 - 1706047336.443378]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [42 - 1706047336.443579]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [42 - 1706047336.4439554]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [42 - 1706047336.4444654]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [42 - 1706047336.4446874]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [42 - 1706047336.4448416]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [42 - 1706047336.4449873]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [42 - 1706047336.4451494]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [42 - 1706047336.4455383]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [42 - 1706047336.4459925]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [42 - 1706047336.446195]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [42 - 1706047336.446378]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [42 - 1706047336.4465156]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [42 - 1706047336.446674]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [42 - 1706047336.4470546]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [42 - 1706047336.4474442]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [42 - 1706047336.4476466]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [42 - 1706047336.4477954]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [42 - 1706047336.4479759]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [42 - 1706047336.4481292]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [42 - 1706047336.4485478]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [42 - 1706047336.4490707]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [42 - 1706047336.4492488]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [42 - 1706047336.4493864]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [42 - 1706047336.4495203]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [42 - 1706047336.4497285]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [42 - 1706047336.4501276]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [42 - 1706047336.4505594]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [42 - 1706047336.4507787]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [42 - 1706047336.4509532]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [42 - 1706047336.4510944]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [42 - 1706047336.4512544]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [42 - 1706047336.4516516]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [42 - 1706047336.4520495]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [42 - 1706047336.452221]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [42 - 1706047336.4523556]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [42 - 1706047336.4525473]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [42 - 1706047336.5167227]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [42 - 1706047336.5172718]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [42 - 1706047336.5177188]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [42 - 1706047336.5179403]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [42 - 1706047336.5181344]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [42 - 1706047336.5182786]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [42 - 1706047336.518465]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [42 - 1706047336.519014]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [42 - 1706047336.519445]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [42 - 1706047336.5196698]: 0.0
python ParamPruning/classifier.weight [42 - 1706047336.5197432]: 0.0
python DistillationModifier [49 - 1706047376.6465409]: Calling loss_update with:
args: 0.5626958012580872| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 0.7777777777777778| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [49 - 1706047378.4277294]: 
Returned: 1.350040316581726| 

python LearningRateFunctionModifier [49 - 1706047381.6783047]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.7777777777777778| steps_per_epoch: 63| 
python LearningRateFunctionModifier [49 - 1706047381.6784887]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [49 - 1706047381.6790628]: 0.00014102564102564101
python LearningRateFunctionModifier/ParamGroup1 [49 - 1706047381.6790986]: 0.00014102564102564101
python DistillationModifier/task_loss [49 - 1706047381.6791499]: tensor(0.5627, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [49 - 1706047381.6797276]: tensor(1.3500, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [49 - 1706047381.6800025]: tensor(1.3500, grad_fn=<AddBackward0>)
python ConstantPruningModifier [49 - 1706047381.680255]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.7777777777777778| steps_per_epoch: 63| 
python ConstantPruningModifier [49 - 1706047381.9350302]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [49 - 1706047381.9355268]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [49 - 1706047381.9357746]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [49 - 1706047381.9359422]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [49 - 1706047381.9360921]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [49 - 1706047381.936709]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [49 - 1706047381.9373105]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [49 - 1706047381.9375312]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [49 - 1706047381.9377084]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [49 - 1706047381.9378548]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [49 - 1706047381.9380043]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [49 - 1706047381.9384313]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [49 - 1706047381.9389422]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [49 - 1706047381.9391327]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [49 - 1706047381.9392788]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [49 - 1706047381.9394212]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [49 - 1706047381.9395769]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [49 - 1706047381.9400058]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [49 - 1706047381.940421]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [49 - 1706047381.9406419]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [49 - 1706047381.9407995]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [49 - 1706047381.940946]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [49 - 1706047381.941081]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [49 - 1706047381.9414759]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [49 - 1706047381.941926]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [49 - 1706047381.9421089]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [49 - 1706047381.9422665]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [49 - 1706047381.9424276]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [49 - 1706047381.9426303]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [49 - 1706047381.9430647]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [49 - 1706047381.94352]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [49 - 1706047381.94373]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [49 - 1706047381.9438813]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [49 - 1706047381.9440272]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [49 - 1706047381.9441605]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [49 - 1706047381.9445972]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [49 - 1706047381.9450166]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [49 - 1706047381.9451985]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [49 - 1706047381.9453492]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [49 - 1706047381.945485]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [49 - 1706047381.9456427]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [49 - 1706047381.9460385]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [49 - 1706047381.946434]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [49 - 1706047381.946641]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [49 - 1706047381.946792]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [49 - 1706047381.946937]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [49 - 1706047381.947073]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [49 - 1706047381.947458]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [49 - 1706047381.947924]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [49 - 1706047381.9481056]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [49 - 1706047381.9482415]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [49 - 1706047381.9483764]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [49 - 1706047381.948525]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [49 - 1706047381.948962]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [49 - 1706047381.9493854]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [49 - 1706047381.9495857]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [49 - 1706047381.9497466]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [49 - 1706047381.9498913]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [49 - 1706047381.9500437]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [49 - 1706047381.9504187]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [49 - 1706047381.950849]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [49 - 1706047381.9510343]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [49 - 1706047381.9511714]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [49 - 1706047381.9513159]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [49 - 1706047381.9514546]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [49 - 1706047381.9518802]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [49 - 1706047381.9522874]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [49 - 1706047381.95247]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [49 - 1706047382.0166776]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [49 - 1706047382.0168855]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [49 - 1706047382.0170393]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [49 - 1706047382.0175893]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [49 - 1706047382.0181456]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [49 - 1706047382.0183542]: 0.0
python ParamPruning/classifier.weight [49 - 1706047382.0184252]: 0.0
python DistillationModifier [56 - 1706047421.7338574]: Calling loss_update with:
args: 0.814497709274292| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 0.8888888888888888| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [56 - 1706047423.1369245]: 
Returned: 1.4369127750396729| 

python LearningRateFunctionModifier [56 - 1706047425.4169605]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.8888888888888888| steps_per_epoch: 63| 
python LearningRateFunctionModifier [56 - 1706047425.4171135]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [56 - 1706047425.41716]: 0.00013974358974358974
python LearningRateFunctionModifier/ParamGroup1 [56 - 1706047425.4171853]: 0.00013974358974358974
python DistillationModifier/task_loss [56 - 1706047425.4172266]: tensor(0.8145, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [56 - 1706047425.4177573]: tensor(1.4369, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [56 - 1706047425.4180224]: tensor(1.4369, grad_fn=<AddBackward0>)
python ConstantPruningModifier [56 - 1706047425.4182618]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.8888888888888888| steps_per_epoch: 63| 
python ConstantPruningModifier [56 - 1706047425.6389391]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [56 - 1706047425.6394527]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [56 - 1706047425.6397066]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [56 - 1706047425.6398973]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [56 - 1706047425.6400764]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [56 - 1706047425.6408727]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [56 - 1706047425.6416864]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [56 - 1706047425.641931]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [56 - 1706047425.6421008]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [56 - 1706047425.642267]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [56 - 1706047425.642433]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [56 - 1706047425.6430416]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [56 - 1706047425.6436586]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [56 - 1706047425.6439056]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [56 - 1706047425.6440718]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [56 - 1706047425.6442347]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [56 - 1706047425.6443942]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [56 - 1706047425.644956]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [56 - 1706047425.645749]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [56 - 1706047425.6459696]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [56 - 1706047425.646136]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [56 - 1706047425.6462977]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [56 - 1706047425.6464608]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [56 - 1706047425.647038]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [56 - 1706047425.6477323]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [56 - 1706047425.647952]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [56 - 1706047425.648115]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [56 - 1706047425.6482773]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [56 - 1706047425.6484401]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [56 - 1706047425.649002]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [56 - 1706047425.6496274]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [56 - 1706047425.6498406]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [56 - 1706047425.650005]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [56 - 1706047425.6501653]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [56 - 1706047425.6503282]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [56 - 1706047425.6508734]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [56 - 1706047425.6514418]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [56 - 1706047425.651733]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [56 - 1706047425.651902]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [56 - 1706047425.6520634]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [56 - 1706047425.6522243]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [56 - 1706047425.7172916]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [56 - 1706047425.718167]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [56 - 1706047425.7184634]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [56 - 1706047425.7186604]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [56 - 1706047425.7188363]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [56 - 1706047425.7190027]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [56 - 1706047425.7196283]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [56 - 1706047425.7204123]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [56 - 1706047425.720696]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [56 - 1706047425.7208753]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [56 - 1706047425.7210395]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [56 - 1706047425.721203]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [56 - 1706047425.7219028]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [56 - 1706047425.7227108]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [56 - 1706047425.7229514]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [56 - 1706047425.7231166]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [56 - 1706047425.7232785]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [56 - 1706047425.723443]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [56 - 1706047425.7241426]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [56 - 1706047425.724903]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [56 - 1706047425.7251391]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [56 - 1706047425.725304]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [56 - 1706047425.7254655]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [56 - 1706047425.7256477]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [56 - 1706047425.7262273]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [56 - 1706047425.7268524]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [56 - 1706047425.7270727]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [56 - 1706047425.7272375]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [56 - 1706047425.7273993]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [56 - 1706047425.7275798]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [56 - 1706047425.7281547]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [56 - 1706047425.7289686]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [56 - 1706047425.729206]: 0.0
python ParamPruning/classifier.weight [56 - 1706047425.7292783]: 0.0
python DistillationModifier [63 - 1706047469.821371]: Calling loss_update with:
args: 1.070671558380127| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047471.2172272]: 
Returned: 2.195180654525757| 

python DistillationModifier [63 - 1706047472.4423761]: Calling loss_update with:
args: 1.0518358945846558| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047473.628319]: 
Returned: 2.1044092178344727| 

python DistillationModifier [63 - 1706047474.8355174]: Calling loss_update with:
args: 1.090422511100769| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047476.028784]: 
Returned: 2.128584384918213| 

python DistillationModifier [63 - 1706047477.645498]: Calling loss_update with:
args: 1.0037897825241089| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047479.5304506]: 
Returned: 1.7887356281280518| 

python DistillationModifier [63 - 1706047481.3446174]: Calling loss_update with:
args: 0.8890010714530945| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047482.7433617]: 
Returned: 1.5796338319778442| 

python DistillationModifier [63 - 1706047483.9430053]: Calling loss_update with:
args: 0.6886889338493347| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047485.5167675]: 
Returned: 1.5562191009521484| 

python DistillationModifier [63 - 1706047487.328221]: Calling loss_update with:
args: 0.7283934354782104| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047489.1385884]: 
Returned: 1.9166587591171265| 

python DistillationModifier [63 - 1706047491.0317833]: Calling loss_update with:
args: 0.9390285611152649| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047492.843407]: 
Returned: 1.8930028676986694| 

python DistillationModifier [63 - 1706047494.7281075]: Calling loss_update with:
args: 0.6413961052894592| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047496.2431314]: 
Returned: 1.3219693899154663| 

python DistillationModifier [63 - 1706047497.4512627]: Calling loss_update with:
args: 0.6690317392349243| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047498.6511204]: 
Returned: 1.6419240236282349| 

python DistillationModifier [63 - 1706047499.9179707]: Calling loss_update with:
args: 0.8772715330123901| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047501.1179392]: 
Returned: 1.7065987586975098| 

python DistillationModifier [63 - 1706047502.546452]: Calling loss_update with:
args: 0.8694005608558655| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047503.729853]: 
Returned: 1.6898249387741089| 

python DistillationModifier [63 - 1706047504.9347403]: Calling loss_update with:
args: 0.9421157836914062| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047506.1273367]: 
Returned: 2.2832844257354736| 

python DistillationModifier [63 - 1706047507.4184933]: Calling loss_update with:
args: 0.9929445385932922| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047508.6205974]: 
Returned: 1.5079137086868286| 

python DistillationModifier [63 - 1706047509.9228976]: Calling loss_update with:
args: 1.0606324672698975| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047511.0467331]: 
Returned: 2.0102596282958984| 

python DistillationModifier [63 - 1706047512.2472486]: Calling loss_update with:
args: 0.901318371295929| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047513.4407713]: 
Returned: 2.0140457153320312| 

python DistillationModifier [63 - 1706047514.64275]: Calling loss_update with:
args: 0.7045557498931885| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047515.8266919]: 
Returned: 1.4970521926879883| 

python DistillationModifier [63 - 1706047517.0350244]: Calling loss_update with:
args: 0.8554231524467468| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047518.2393975]: 
Returned: 1.9279334545135498| 

python DistillationModifier [63 - 1706047519.7280016]: Calling loss_update with:
args: 1.2882254123687744| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047521.527424]: 
Returned: 2.1408815383911133| 

python DistillationModifier [63 - 1706047522.9503691]: Calling loss_update with:
args: 1.0679763555526733| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047524.82708]: 
Returned: 1.968595266342163| 

python DistillationModifier [63 - 1706047526.6460829]: Calling loss_update with:
args: 0.7025992274284363| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047527.9331205]: 
Returned: 1.2864892482757568| 

python DistillationModifier [63 - 1706047529.1356363]: Calling loss_update with:
args: 0.739509642124176| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047530.330422]: 
Returned: 1.4908533096313477| 

python DistillationModifier [63 - 1706047531.6181266]: Calling loss_update with:
args: 0.8690637946128845| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047532.746439]: 
Returned: 1.8962137699127197| 

python DistillationModifier [63 - 1706047533.9484606]: Calling loss_update with:
args: 0.6619169116020203| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047535.1358848]: 
Returned: 1.802865743637085| 

python DistillationModifier [63 - 1706047536.3391335]: Calling loss_update with:
args: 1.1385358572006226| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047537.9208832]: 
Returned: 2.0592710971832275| 

python DistillationModifier [63 - 1706047539.7387793]: Calling loss_update with:
args: 0.7809295058250427| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047541.5260375]: 
Returned: 1.614746332168579| 

python DistillationModifier [63 - 1706047542.7357235]: Calling loss_update with:
args: 0.7798887491226196| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047543.9178727]: 
Returned: 1.5931109189987183| 

python DistillationModifier [63 - 1706047545.1216059]: Calling loss_update with:
args: 0.8757654428482056| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047546.2434077]: 
Returned: 1.8130542039871216| 

python DistillationModifier [63 - 1706047547.7279289]: Calling loss_update with:
args: 1.0183848142623901| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047549.443095]: 
Returned: 2.1426901817321777| 

python DistillationModifier [63 - 1706047550.644129]: Calling loss_update with:
args: 0.626970112323761| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047551.8330839]: 
Returned: 1.6326000690460205| 

python DistillationModifier [63 - 1706047553.1231675]: Calling loss_update with:
args: 0.8902204632759094| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047554.6432865]: 
Returned: 2.0078110694885254| 

python DistillationModifier [63 - 1706047556.5294933]: Calling loss_update with:
args: 0.707090437412262| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047558.2513]: 
Returned: 1.756759762763977| 

python DistillationModifier [63 - 1706047559.7294767]: Calling loss_update with:
args: 1.1614601612091064| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047560.9307978]: 
Returned: 2.378634452819824| 

python DistillationModifier [63 - 1706047562.3282514]: Calling loss_update with:
args: 0.9852141737937927| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047563.4478767]: 
Returned: 1.8892232179641724| 

python DistillationModifier [63 - 1706047564.7214413]: Calling loss_update with:
args: 0.9745169878005981| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047565.844321]: 
Returned: 2.030712604522705| 

python DistillationModifier [63 - 1706047567.6450534]: Calling loss_update with:
args: 1.1875394582748413| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047569.443173]: 
Returned: 2.461777448654175| 

python DistillationModifier [63 - 1706047571.3280823]: Calling loss_update with:
args: 1.084507942199707| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047572.820213]: 
Returned: 1.9324073791503906| 

python DistillationModifier [63 - 1706047574.0283737]: Calling loss_update with:
args: 0.8347921967506409| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047575.2229319]: 
Returned: 1.7898391485214233| 

python DistillationModifier [63 - 1706047576.4230125]: Calling loss_update with:
args: 1.0466768741607666| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047578.0434046]: 
Returned: 2.160860061645508| 

python DistillationModifier [63 - 1706047579.92825]: Calling loss_update with:
args: 0.9203625321388245| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047581.7315538]: 
Returned: 2.12239146232605| 

python DistillationModifier [63 - 1706047583.5460567]: Calling loss_update with:
args: 0.677432119846344| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047585.1483648]: 
Returned: 1.827901005744934| 

python DistillationModifier [63 - 1706047586.4285612]: Calling loss_update with:
args: 0.6905881762504578| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047587.5486932]: 
Returned: 1.7401468753814697| 

python DistillationModifier [63 - 1706047588.751343]: Calling loss_update with:
args: 0.8611193895339966| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047590.1213362]: 
Returned: 1.6848387718200684| 

python DistillationModifier [63 - 1706047591.9436178]: Calling loss_update with:
args: 0.688805341720581| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047593.1523821]: 
Returned: 1.6691913604736328| 

python DistillationModifier [63 - 1706047594.4223888]: Calling loss_update with:
args: 0.994803786277771| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047595.5407639]: 
Returned: 2.366173028945923| 

python DistillationModifier [63 - 1706047597.1365032]: Calling loss_update with:
args: 0.9313010573387146| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047598.4377146]: 
Returned: 1.7758675813674927| 

python DistillationModifier [63 - 1706047600.1314278]: Calling loss_update with:
args: 0.8650285601615906| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047601.449732]: 
Returned: 1.9073609113693237| 

python DistillationModifier [63 - 1706047602.7234135]: Calling loss_update with:
args: 0.7406609058380127| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047603.848111]: 
Returned: 1.930131435394287| 

python DistillationModifier [63 - 1706047605.7287993]: Calling loss_update with:
args: 0.9479968547821045| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047607.5330517]: 
Returned: 1.8978999853134155| 

python DistillationModifier [63 - 1706047609.4181044]: Calling loss_update with:
args: 0.6588315367698669| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047611.226669]: 
Returned: 1.6115286350250244| 

python DistillationModifier [63 - 1706047613.044427]: Calling loss_update with:
args: 0.6253320574760437| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047614.8415554]: 
Returned: 1.3561393022537231| 

python DistillationModifier [63 - 1706047616.645795]: Calling loss_update with:
args: 1.0743199586868286| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047618.5171397]: 
Returned: 1.7180205583572388| 

python DistillationModifier [63 - 1706047619.7411335]: Calling loss_update with:
args: 0.9432462453842163| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047620.9362004]: 
Returned: 2.049278497695923| 

python DistillationModifier [63 - 1706047622.7435942]: Calling loss_update with:
args: 1.0230164527893066| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047623.9228234]: 
Returned: 1.8772392272949219| 

python DistillationModifier [63 - 1706047625.1313505]: Calling loss_update with:
args: 0.7026175260543823| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047626.438903]: 
Returned: 1.6783950328826904| 

python DistillationModifier [63 - 1706047627.9253738]: Calling loss_update with:
args: 0.8543845415115356| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047629.1492305]: 
Returned: 1.7307603359222412| 

python DistillationModifier [63 - 1706047630.4223228]: Calling loss_update with:
args: 0.9033994078636169| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047632.1198082]: 
Returned: 1.3701218366622925| 

python DistillationModifier [63 - 1706047633.429545]: Calling loss_update with:
args: 0.8780303597450256| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047634.550493]: 
Returned: 1.456220030784607| 

python DistillationModifier [63 - 1706047635.9179695]: Calling loss_update with:
args: 1.179666519165039| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047637.0404139]: 
Returned: 2.0808663368225098| 

python DistillationModifier [63 - 1706047638.340632]: Calling loss_update with:
args: 0.8145129680633545| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047640.1434844]: 
Returned: 1.9192696809768677| 

python DistillationModifier [63 - 1706047641.5222697]: Calling loss_update with:
args: 0.8991153836250305| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047642.6394482]: 
Returned: 2.0064806938171387| 

python DistillationModifier [63 - 1706047644.02255]: Calling loss_update with:
args: 1.1359155178070068| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047645.3265557]: 
Returned: 2.216010570526123| 

python DistillationModifier [63 - 1706047646.3386405]: Calling loss_update with:
args: 0.528659462928772| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047647.3347]: 
Returned: 1.23564875125885| 

python DistillationModifier [63 - 1706047649.6362503]: Calling loss_update with:
args: 0.5474267601966858| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1706047650.8477142]: 
Returned: 1.2152202129364014| 

python LearningRateFunctionModifier [63 - 1706047653.1691556]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [63 - 1706047653.1693044]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [63 - 1706047653.1693525]: 0.00013846153846153845
python LearningRateFunctionModifier/ParamGroup1 [63 - 1706047653.1693783]: 0.00013846153846153845
python DistillationModifier/task_loss [63 - 1706047653.1694224]: tensor(0.5474, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [63 - 1706047653.1699507]: tensor(1.2152, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [63 - 1706047653.1701975]: tensor(1.2152, grad_fn=<AddBackward0>)
python ConstantPruningModifier [63 - 1706047653.1704369]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| 
python ConstantPruningModifier [63 - 1706047653.3481743]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [63 - 1706047653.348657]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [63 - 1706047653.3488665]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [63 - 1706047653.3490746]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [63 - 1706047653.3492408]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [63 - 1706047653.3498323]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [63 - 1706047653.3502383]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [63 - 1706047653.3504446]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [63 - 1706047653.350601]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [63 - 1706047653.350747]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [63 - 1706047653.3509421]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [63 - 1706047653.3512976]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [63 - 1706047653.3516872]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [63 - 1706047653.35188]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [63 - 1706047653.3520203]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [63 - 1706047653.3521507]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [63 - 1706047653.352309]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [63 - 1706047653.4167056]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [63 - 1706047653.4170709]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [63 - 1706047653.417267]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [63 - 1706047653.4174109]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [63 - 1706047653.4175487]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [63 - 1706047653.417712]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [63 - 1706047653.418078]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [63 - 1706047653.418612]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [63 - 1706047653.418817]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [63 - 1706047653.4189935]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [63 - 1706047653.4191337]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [63 - 1706047653.4192724]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [63 - 1706047653.4197283]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [63 - 1706047653.4202814]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [63 - 1706047653.4204767]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [63 - 1706047653.4206626]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [63 - 1706047653.4208236]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [63 - 1706047653.4210017]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [63 - 1706047653.4213867]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [63 - 1706047653.4217722]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [63 - 1706047653.4219441]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [63 - 1706047653.4220903]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [63 - 1706047653.4222202]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [63 - 1706047653.4223533]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [63 - 1706047653.4227247]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [63 - 1706047653.4232044]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [63 - 1706047653.423372]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [63 - 1706047653.4235113]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [63 - 1706047653.4236648]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [63 - 1706047653.4238138]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [63 - 1706047653.424197]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [63 - 1706047653.4246]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [63 - 1706047653.4247959]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [63 - 1706047653.4249444]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [63 - 1706047653.425075]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [63 - 1706047653.425205]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [63 - 1706047653.4255579]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [63 - 1706047653.42605]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [63 - 1706047653.4262137]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [63 - 1706047653.4264023]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [63 - 1706047653.4265516]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [63 - 1706047653.4267278]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [63 - 1706047653.427114]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [63 - 1706047653.4274778]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [63 - 1706047653.427684]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [63 - 1706047653.4278424]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [63 - 1706047653.4280376]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [63 - 1706047653.428207]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [63 - 1706047653.4286659]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [63 - 1706047653.4291568]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [63 - 1706047653.4293473]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [63 - 1706047653.4294903]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [63 - 1706047653.429694]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [63 - 1706047653.4298375]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [63 - 1706047653.4302094]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [63 - 1706047653.430641]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [63 - 1706047653.4308124]: 0.0
python ParamPruning/classifier.weight [63 - 1706047653.4308782]: 0.0
python DistillationModifier [70 - 1706047693.7333872]: Calling loss_update with:
args: 0.6422213315963745| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.1111111111111112| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [70 - 1706047695.023547]: 
Returned: 1.3951914310455322| 

python LearningRateFunctionModifier [70 - 1706047697.2593288]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.1111111111111112| steps_per_epoch: 63| 
python LearningRateFunctionModifier [70 - 1706047697.2595057]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [70 - 1706047697.25956]: 0.00013717948717948718
python LearningRateFunctionModifier/ParamGroup1 [70 - 1706047697.259611]: 0.00013717948717948718
python DistillationModifier/task_loss [70 - 1706047697.2596612]: tensor(0.6422, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [70 - 1706047697.260216]: tensor(1.3952, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [70 - 1706047697.2604756]: tensor(1.3952, grad_fn=<AddBackward0>)
python ConstantPruningModifier [70 - 1706047697.2607777]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.1111111111111112| steps_per_epoch: 63| 
python ConstantPruningModifier [70 - 1706047697.5453694]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [70 - 1706047697.5459485]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [70 - 1706047697.5462456]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [70 - 1706047697.5464885]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [70 - 1706047697.5467596]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [70 - 1706047697.5475948]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [70 - 1706047697.548453]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [70 - 1706047697.548759]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [70 - 1706047697.5489783]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [70 - 1706047697.549192]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [70 - 1706047697.5494013]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [70 - 1706047697.5500994]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [70 - 1706047697.5507355]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [70 - 1706047697.550999]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [70 - 1706047697.5512292]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [70 - 1706047697.551434]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [70 - 1706047697.5516634]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [70 - 1706047697.552385]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [70 - 1706047697.6171982]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [70 - 1706047697.6174877]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [70 - 1706047697.6177325]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [70 - 1706047697.6179523]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [70 - 1706047697.6181612]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [70 - 1706047697.6189215]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [70 - 1706047697.619658]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [70 - 1706047697.6199095]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [70 - 1706047697.6201196]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [70 - 1706047697.620359]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [70 - 1706047697.620599]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [70 - 1706047697.6213741]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [70 - 1706047697.6220953]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [70 - 1706047697.6223457]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [70 - 1706047697.6226025]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [70 - 1706047697.6228197]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [70 - 1706047697.6230223]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [70 - 1706047697.6237292]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [70 - 1706047697.624479]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [70 - 1706047697.6247697]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [70 - 1706047697.6250556]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [70 - 1706047697.6252897]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [70 - 1706047697.6254976]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [70 - 1706047697.6262205]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [70 - 1706047697.626947]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [70 - 1706047697.6272154]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [70 - 1706047697.6274538]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [70 - 1706047697.6276903]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [70 - 1706047697.6278954]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [70 - 1706047697.6285412]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [70 - 1706047697.629147]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [70 - 1706047697.6293783]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [70 - 1706047697.6296003]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [70 - 1706047697.6298165]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [70 - 1706047697.6300166]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [70 - 1706047697.6306117]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [70 - 1706047697.6313326]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [70 - 1706047697.6315918]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [70 - 1706047697.6318662]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [70 - 1706047697.6320941]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [70 - 1706047697.6323013]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [70 - 1706047697.632954]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [70 - 1706047697.6336553]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [70 - 1706047697.6339078]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [70 - 1706047697.6341724]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [70 - 1706047697.634382]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [70 - 1706047697.6346664]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [70 - 1706047697.635276]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [70 - 1706047697.6359348]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [70 - 1706047697.6361687]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [70 - 1706047697.6364052]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [70 - 1706047697.636656]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [70 - 1706047697.6368637]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [70 - 1706047697.6374853]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [70 - 1706047697.6381695]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [70 - 1706047697.638415]: 0.0
python ParamPruning/classifier.weight [70 - 1706047697.638513]: 0.0
python DistillationModifier [77 - 1706047734.5465102]: Calling loss_update with:
args: 0.3126729130744934| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.2222222222222223| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [77 - 1706047736.4470024]: 
Returned: 0.958981990814209| 

python LearningRateFunctionModifier [77 - 1706047739.9173636]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.2222222222222223| steps_per_epoch: 63| 
python LearningRateFunctionModifier [77 - 1706047739.9175365]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [77 - 1706047739.9180946]: 0.00013589743589743588
python LearningRateFunctionModifier/ParamGroup1 [77 - 1706047739.918127]: 0.00013589743589743588
python DistillationModifier/task_loss [77 - 1706047739.9181774]: tensor(0.3127, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [77 - 1706047739.918744]: tensor(0.9590, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [77 - 1706047739.9190016]: tensor(0.9590, grad_fn=<AddBackward0>)
python ConstantPruningModifier [77 - 1706047739.9192603]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.2222222222222223| steps_per_epoch: 63| 
python ConstantPruningModifier [77 - 1706047740.2354805]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [77 - 1706047740.2360563]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [77 - 1706047740.2363513]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [77 - 1706047740.2366264]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [77 - 1706047740.2368634]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [77 - 1706047740.237688]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [77 - 1706047740.2385185]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [77 - 1706047740.2388134]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [77 - 1706047740.2390237]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [77 - 1706047740.2392316]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [77 - 1706047740.2394361]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [77 - 1706047740.2401068]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [77 - 1706047740.2408276]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [77 - 1706047740.2410805]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [77 - 1706047740.2412844]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [77 - 1706047740.2414913]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [77 - 1706047740.2417147]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [77 - 1706047740.2422907]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [77 - 1706047740.2431045]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [77 - 1706047740.2433496]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [77 - 1706047740.2435565]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [77 - 1706047740.243818]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [77 - 1706047740.244017]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [77 - 1706047740.2447808]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [77 - 1706047740.2454455]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [77 - 1706047740.2457097]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [77 - 1706047740.245935]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [77 - 1706047740.24614]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [77 - 1706047740.246339]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [77 - 1706047740.246989]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [77 - 1706047740.247771]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [77 - 1706047740.2480118]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [77 - 1706047740.248215]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [77 - 1706047740.2484186]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [77 - 1706047740.2486546]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [77 - 1706047740.2492583]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [77 - 1706047740.2500224]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [77 - 1706047740.250263]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [77 - 1706047740.2504647]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [77 - 1706047740.2506883]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [77 - 1706047740.2508912]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [77 - 1706047740.2514832]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [77 - 1706047740.2521644]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [77 - 1706047740.2524014]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [77 - 1706047740.3166637]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [77 - 1706047740.3168821]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [77 - 1706047740.317084]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [77 - 1706047740.3177743]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [77 - 1706047740.31855]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [77 - 1706047740.3188045]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [77 - 1706047740.319005]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [77 - 1706047740.3192077]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [77 - 1706047740.319409]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [77 - 1706047740.3200214]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [77 - 1706047740.3207197]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [77 - 1706047740.3209474]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [77 - 1706047740.321151]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [77 - 1706047740.321353]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [77 - 1706047740.3215544]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [77 - 1706047740.3221602]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [77 - 1706047740.3229647]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [77 - 1706047740.323204]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [77 - 1706047740.3234222]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [77 - 1706047740.3236573]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [77 - 1706047740.3238585]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [77 - 1706047740.3245237]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [77 - 1706047740.3252156]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [77 - 1706047740.3254483]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [77 - 1706047740.325674]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [77 - 1706047740.3258784]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [77 - 1706047740.3260758]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [77 - 1706047740.3266869]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [77 - 1706047740.3273385]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [77 - 1706047740.327587]: 0.0
python ParamPruning/classifier.weight [77 - 1706047740.3276882]: 0.0
python DistillationModifier [84 - 1706047782.0287435]: Calling loss_update with:
args: 0.4499419331550598| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.3333333333333333| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [84 - 1706047783.8498318]: 
Returned: 1.1035302877426147| 

python LearningRateFunctionModifier [84 - 1706047787.3231018]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.3333333333333333| steps_per_epoch: 63| 
python LearningRateFunctionModifier [84 - 1706047787.3232636]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [84 - 1706047787.3239658]: 0.0001346153846153846
python LearningRateFunctionModifier/ParamGroup1 [84 - 1706047787.3239937]: 0.0001346153846153846
python DistillationModifier/task_loss [84 - 1706047787.3240345]: tensor(0.4499, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [84 - 1706047787.324543]: tensor(1.1035, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [84 - 1706047787.3248177]: tensor(1.1035, grad_fn=<AddBackward0>)
python ConstantPruningModifier [84 - 1706047787.3250494]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.3333333333333333| steps_per_epoch: 63| 
python ConstantPruningModifier [84 - 1706047787.6327875]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [84 - 1706047787.6332836]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [84 - 1706047787.6335046]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [84 - 1706047787.6337743]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [84 - 1706047787.6340365]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [84 - 1706047787.6348414]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [84 - 1706047787.6356573]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [84 - 1706047787.635907]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [84 - 1706047787.636075]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [84 - 1706047787.6362967]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [84 - 1706047787.6365218]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [84 - 1706047787.637188]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [84 - 1706047787.637872]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [84 - 1706047787.638103]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [84 - 1706047787.6382716]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [84 - 1706047787.638488]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [84 - 1706047787.63874]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [84 - 1706047787.639335]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [84 - 1706047787.6399353]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [84 - 1706047787.6401596]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [84 - 1706047787.6403255]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [84 - 1706047787.640553]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [84 - 1706047787.6407697]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [84 - 1706047787.64136]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [84 - 1706047787.641994]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [84 - 1706047787.642208]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [84 - 1706047787.6423724]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [84 - 1706047787.6425996]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [84 - 1706047787.642837]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [84 - 1706047787.6433926]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [84 - 1706047787.643997]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [84 - 1706047787.6442137]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [84 - 1706047787.6443765]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [84 - 1706047787.644616]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [84 - 1706047787.6448095]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [84 - 1706047787.6453972]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [84 - 1706047787.646022]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [84 - 1706047787.6462338]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [84 - 1706047787.6463969]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [84 - 1706047787.6465547]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [84 - 1706047787.6467493]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [84 - 1706047787.6472907]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [84 - 1706047787.6479075]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [84 - 1706047787.648125]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [84 - 1706047787.6482892]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [84 - 1706047787.64852]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [84 - 1706047787.6487508]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [84 - 1706047787.6493335]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [84 - 1706047787.6499465]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [84 - 1706047787.6501584]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [84 - 1706047787.6503203]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [84 - 1706047787.6505313]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [84 - 1706047787.6507323]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [84 - 1706047787.6512988]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [84 - 1706047787.6519103]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [84 - 1706047787.6521256]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [84 - 1706047787.6522899]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [84 - 1706047787.6525166]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [84 - 1706047787.716744]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [84 - 1706047787.7173564]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [84 - 1706047787.717915]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [84 - 1706047787.7181284]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [84 - 1706047787.718293]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [84 - 1706047787.718455]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [84 - 1706047787.7186885]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [84 - 1706047787.7192132]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [84 - 1706047787.7199216]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [84 - 1706047787.7201388]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [84 - 1706047787.7203002]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [84 - 1706047787.7204733]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [84 - 1706047787.7207134]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [84 - 1706047787.721278]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [84 - 1706047787.7219203]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [84 - 1706047787.7221453]: 0.0
python ParamPruning/classifier.weight [84 - 1706047787.7222126]: 0.0
python DistillationModifier [91 - 1706047824.2389538]: Calling loss_update with:
args: 0.6774072051048279| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.4444444444444444| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [91 - 1706047825.4397926]: 
Returned: 1.104785680770874| 

python LearningRateFunctionModifier [91 - 1706047827.671852]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.4444444444444444| steps_per_epoch: 63| 
python LearningRateFunctionModifier [91 - 1706047827.6719987]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [91 - 1706047827.6720583]: 0.0001333333333333333
python LearningRateFunctionModifier/ParamGroup1 [91 - 1706047827.672085]: 0.0001333333333333333
python DistillationModifier/task_loss [91 - 1706047827.6721282]: tensor(0.6774, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [91 - 1706047827.6726587]: tensor(1.1048, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [91 - 1706047827.672918]: tensor(1.1048, grad_fn=<AddBackward0>)
python ConstantPruningModifier [91 - 1706047827.6731505]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.4444444444444444| steps_per_epoch: 63| 
python ConstantPruningModifier [91 - 1706047827.9194815]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [91 - 1706047827.9199562]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [91 - 1706047827.9201593]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [91 - 1706047827.9203231]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [91 - 1706047827.9204824]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [91 - 1706047827.9211411]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [91 - 1706047827.9215994]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [91 - 1706047827.9217954]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [91 - 1706047827.9219413]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [91 - 1706047827.9220767]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [91 - 1706047827.9222093]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [91 - 1706047827.922581]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [91 - 1706047827.922955]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [91 - 1706047827.9231246]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [91 - 1706047827.9232605]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [91 - 1706047827.9233937]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [91 - 1706047827.923525]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [91 - 1706047827.923892]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [91 - 1706047827.9242673]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [91 - 1706047827.9244382]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [91 - 1706047827.924624]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [91 - 1706047827.9248486]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [91 - 1706047827.9250429]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [91 - 1706047827.9253955]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [91 - 1706047827.9257638]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [91 - 1706047827.925956]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [91 - 1706047827.926108]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [91 - 1706047827.9262404]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [91 - 1706047827.9263694]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [91 - 1706047827.9267523]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [91 - 1706047827.9271204]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [91 - 1706047827.9273043]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [91 - 1706047827.927506]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [91 - 1706047827.9277124]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [91 - 1706047827.9278533]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [91 - 1706047827.9282334]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [91 - 1706047827.9286103]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [91 - 1706047827.928788]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [91 - 1706047827.9289355]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [91 - 1706047827.929076]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [91 - 1706047827.929208]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [91 - 1706047827.9295433]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [91 - 1706047827.9299517]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [91 - 1706047827.9301512]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [91 - 1706047827.9303539]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [91 - 1706047827.9305084]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [91 - 1706047827.9306638]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [91 - 1706047827.93102]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [91 - 1706047827.9313738]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [91 - 1706047827.9315498]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [91 - 1706047827.9317684]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [91 - 1706047827.931955]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [91 - 1706047827.9321365]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [91 - 1706047827.9326165]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [91 - 1706047827.933149]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [91 - 1706047827.9333231]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [91 - 1706047827.9334679]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [91 - 1706047827.9336255]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [91 - 1706047827.9337783]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [91 - 1706047827.9341471]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [91 - 1706047827.934545]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [91 - 1706047827.9347734]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [91 - 1706047827.9349282]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [91 - 1706047827.93506]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [91 - 1706047827.935194]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [91 - 1706047827.935672]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [91 - 1706047827.9360738]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [91 - 1706047827.936249]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [91 - 1706047827.9363873]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [91 - 1706047827.9365375]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [91 - 1706047827.936719]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [91 - 1706047827.937096]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [91 - 1706047827.9374564]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [91 - 1706047827.937648]: 0.0
python ParamPruning/classifier.weight [91 - 1706047827.9377203]: 0.0
python DistillationModifier [98 - 1706047864.6369648]: Calling loss_update with:
args: 0.4491332173347473| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.5555555555555556| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [98 - 1706047866.3262038]: 
Returned: 1.2596728801727295| 

python LearningRateFunctionModifier [98 - 1706047869.0759258]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.5555555555555556| steps_per_epoch: 63| 
python LearningRateFunctionModifier [98 - 1706047869.076078]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [98 - 1706047869.076124]: 0.00013205128205128204
python LearningRateFunctionModifier/ParamGroup1 [98 - 1706047869.0761502]: 0.00013205128205128204
python DistillationModifier/task_loss [98 - 1706047869.0761936]: tensor(0.4491, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [98 - 1706047869.0767462]: tensor(1.2597, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [98 - 1706047869.0770085]: tensor(1.2597, grad_fn=<AddBackward0>)
python ConstantPruningModifier [98 - 1706047869.0772452]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.5555555555555556| steps_per_epoch: 63| 
python ConstantPruningModifier [98 - 1706047869.419969]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [98 - 1706047869.4204812]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [98 - 1706047869.4208105]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [98 - 1706047869.4210336]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [98 - 1706047869.4212828]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [98 - 1706047869.4220698]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [98 - 1706047869.4228895]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [98 - 1706047869.4231327]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [98 - 1706047869.4233549]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [98 - 1706047869.4235342]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [98 - 1706047869.4237509]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [98 - 1706047869.4243836]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [98 - 1706047869.4252048]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [98 - 1706047869.425475]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [98 - 1706047869.4257367]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [98 - 1706047869.425961]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [98 - 1706047869.4261317]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [98 - 1706047869.4267032]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [98 - 1706047869.427404]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [98 - 1706047869.4276586]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [98 - 1706047869.4278805]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [98 - 1706047869.4280674]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [98 - 1706047869.4282646]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [98 - 1706047869.4288423]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [98 - 1706047869.4294572]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [98 - 1706047869.4296935]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [98 - 1706047869.4298904]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [98 - 1706047869.4300551]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [98 - 1706047869.4302824]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [98 - 1706047869.4308782]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [98 - 1706047869.4314141]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [98 - 1706047869.4316335]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [98 - 1706047869.431801]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [98 - 1706047869.4320133]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [98 - 1706047869.4321942]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [98 - 1706047869.432741]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [98 - 1706047869.433449]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [98 - 1706047869.4336848]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [98 - 1706047869.4339218]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [98 - 1706047869.434103]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [98 - 1706047869.4342823]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [98 - 1706047869.4348593]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [98 - 1706047869.4356375]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [98 - 1706047869.4358482]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [98 - 1706047869.4360611]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [98 - 1706047869.4362364]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [98 - 1706047869.43641]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [98 - 1706047869.4371219]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [98 - 1706047869.4376955]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [98 - 1706047869.4379234]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [98 - 1706047869.438087]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [98 - 1706047869.4382873]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [98 - 1706047869.43846]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [98 - 1706047869.4390128]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [98 - 1706047869.4397335]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [98 - 1706047869.4399524]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [98 - 1706047869.4401746]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [98 - 1706047869.4404023]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [98 - 1706047869.4406195]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [98 - 1706047869.4412324]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [98 - 1706047869.4418643]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [98 - 1706047869.44207]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [98 - 1706047869.442292]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [98 - 1706047869.4424658]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [98 - 1706047869.4426935]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [98 - 1706047869.4432616]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [98 - 1706047869.4438682]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [98 - 1706047869.4440758]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [98 - 1706047869.4442787]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [98 - 1706047869.4445179]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [98 - 1706047869.444725]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [98 - 1706047869.4453306]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [98 - 1706047869.4459486]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [98 - 1706047869.4461815]: 0.0
python ParamPruning/classifier.weight [98 - 1706047869.446251]: 0.0
python DistillationModifier [105 - 1706047911.840041]: Calling loss_update with:
args: 0.5112848281860352| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.6666666666666665| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [105 - 1706047913.7277172]: 
Returned: 1.0662062168121338| 

python LearningRateFunctionModifier [105 - 1706047917.1682878]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.6666666666666665| steps_per_epoch: 63| 
python LearningRateFunctionModifier [105 - 1706047917.1684592]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [105 - 1706047917.1691096]: 0.00013076923076923075
python LearningRateFunctionModifier/ParamGroup1 [105 - 1706047917.1691372]: 0.00013076923076923075
python DistillationModifier/task_loss [105 - 1706047917.1691794]: tensor(0.5113, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [105 - 1706047917.169688]: tensor(1.0662, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [105 - 1706047917.1699572]: tensor(1.0662, grad_fn=<AddBackward0>)
python ConstantPruningModifier [105 - 1706047917.170205]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.6666666666666665| steps_per_epoch: 63| 
python ConstantPruningModifier [105 - 1706047917.4507544]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [105 - 1706047917.4512503]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [105 - 1706047917.4514687]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [105 - 1706047917.451745]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [105 - 1706047917.4520035]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [105 - 1706047917.5167742]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [105 - 1706047917.517595]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [105 - 1706047917.5178642]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [105 - 1706047917.518093]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [105 - 1706047917.518389]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [105 - 1706047917.5186934]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [105 - 1706047917.5193477]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [105 - 1706047917.5200284]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [105 - 1706047917.5202596]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [105 - 1706047917.520493]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [105 - 1706047917.5207145]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [105 - 1706047917.520902]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [105 - 1706047917.521546]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [105 - 1706047917.5223138]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [105 - 1706047917.5226023]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [105 - 1706047917.5228097]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [105 - 1706047917.522986]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [105 - 1706047917.5232246]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [105 - 1706047917.5239413]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [105 - 1706047917.5247138]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [105 - 1706047917.524989]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [105 - 1706047917.525278]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [105 - 1706047917.5255435]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [105 - 1706047917.5258157]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [105 - 1706047917.5265117]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [105 - 1706047917.5272539]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [105 - 1706047917.5274904]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [105 - 1706047917.5277443]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [105 - 1706047917.5279872]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [105 - 1706047917.5282297]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [105 - 1706047917.5289636]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [105 - 1706047917.5297015]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [105 - 1706047917.5299518]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [105 - 1706047917.5301971]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [105 - 1706047917.530481]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [105 - 1706047917.5308068]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [105 - 1706047917.5314703]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [105 - 1706047917.5321488]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [105 - 1706047917.5323987]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [105 - 1706047917.53266]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [105 - 1706047917.5328376]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [105 - 1706047917.5330431]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [105 - 1706047917.533632]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [105 - 1706047917.5341911]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [105 - 1706047917.5344095]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [105 - 1706047917.5346062]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [105 - 1706047917.5348303]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [105 - 1706047917.5350099]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [105 - 1706047917.5356662]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [105 - 1706047917.5362768]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [105 - 1706047917.5365021]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [105 - 1706047917.5367417]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [105 - 1706047917.536936]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [105 - 1706047917.5371144]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [105 - 1706047917.5377557]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [105 - 1706047917.5383158]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [105 - 1706047917.538528]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [105 - 1706047917.538732]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [105 - 1706047917.538935]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [105 - 1706047917.5391603]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [105 - 1706047917.5397332]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [105 - 1706047917.5404203]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [105 - 1706047917.5406911]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [105 - 1706047917.5409184]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [105 - 1706047917.5411417]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [105 - 1706047917.541326]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [105 - 1706047917.5419428]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [105 - 1706047917.5425856]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [105 - 1706047917.542823]: 0.0
python ParamPruning/classifier.weight [105 - 1706047917.5428958]: 0.0
python DistillationModifier [112 - 1706047960.7176776]: Calling loss_update with:
args: 0.43586981296539307| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.7777777777777777| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [112 - 1706047962.6181765]: 
Returned: 0.9478427767753601| 

python LearningRateFunctionModifier [112 - 1706047966.0230513]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.7777777777777777| steps_per_epoch: 63| 
python LearningRateFunctionModifier [112 - 1706047966.0231996]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [112 - 1706047966.0232456]: 0.00012948717948717948
python LearningRateFunctionModifier/ParamGroup1 [112 - 1706047966.0232718]: 0.00012948717948717948
python DistillationModifier/task_loss [112 - 1706047966.0233147]: tensor(0.4359, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [112 - 1706047966.0238137]: tensor(0.9478, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [112 - 1706047966.0240545]: tensor(0.9478, grad_fn=<AddBackward0>)
python ConstantPruningModifier [112 - 1706047966.0242834]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.7777777777777777| steps_per_epoch: 63| 
python ConstantPruningModifier [112 - 1706047966.3315797]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [112 - 1706047966.3320556]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [112 - 1706047966.3323379]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [112 - 1706047966.3326273]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [112 - 1706047966.3328915]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [112 - 1706047966.3336666]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [112 - 1706047966.334442]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [112 - 1706047966.334696]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [112 - 1706047966.334867]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [112 - 1706047966.3350768]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [112 - 1706047966.3352406]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [112 - 1706047966.3358622]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [112 - 1706047966.3364916]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [112 - 1706047966.3367314]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [112 - 1706047966.3369598]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [112 - 1706047966.337161]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [112 - 1706047966.3373704]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [112 - 1706047966.3379202]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [112 - 1706047966.3385196]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [112 - 1706047966.3387585]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [112 - 1706047966.3389251]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [112 - 1706047966.3391218]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [112 - 1706047966.3392942]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [112 - 1706047966.3399138]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [112 - 1706047966.3405774]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [112 - 1706047966.3407938]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [112 - 1706047966.3409874]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [112 - 1706047966.341154]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [112 - 1706047966.3413234]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [112 - 1706047966.3418636]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [112 - 1706047966.3423872]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [112 - 1706047966.3425987]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [112 - 1706047966.3427947]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [112 - 1706047966.3429565]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [112 - 1706047966.3431234]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [112 - 1706047966.3436434]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [112 - 1706047966.3443263]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [112 - 1706047966.3445432]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [112 - 1706047966.3447647]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [112 - 1706047966.3449397]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [112 - 1706047966.3451028]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [112 - 1706047966.3456676]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [112 - 1706047966.3462744]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [112 - 1706047966.3464758]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [112 - 1706047966.3467114]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [112 - 1706047966.3468893]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [112 - 1706047966.3470783]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [112 - 1706047966.3476937]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [112 - 1706047966.34832]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [112 - 1706047966.348533]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [112 - 1706047966.3487465]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [112 - 1706047966.3489404]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [112 - 1706047966.349112]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [112 - 1706047966.3496974]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [112 - 1706047966.350237]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [112 - 1706047966.3504348]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [112 - 1706047966.3506324]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [112 - 1706047966.3507993]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [112 - 1706047966.350966]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [112 - 1706047966.3514717]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [112 - 1706047966.3520072]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [112 - 1706047966.3521981]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [112 - 1706047966.352371]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [112 - 1706047966.3525522]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [112 - 1706047966.4187422]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [112 - 1706047966.4192963]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [112 - 1706047966.4200308]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [112 - 1706047966.4202278]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [112 - 1706047966.420425]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [112 - 1706047966.4206336]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [112 - 1706047966.4208026]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [112 - 1706047966.4213626]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [112 - 1706047966.4220679]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [112 - 1706047966.422286]: 0.0
python ParamPruning/classifier.weight [112 - 1706047966.4223552]: 0.0
python DistillationModifier [119 - 1706048007.3245046]: Calling loss_update with:
args: 0.3045923709869385| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.8888888888888888| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [119 - 1706048008.541358]: 
Returned: 0.9975287914276123| 

python LearningRateFunctionModifier [119 - 1706048010.7818935]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.8888888888888888| steps_per_epoch: 63| 
python LearningRateFunctionModifier [119 - 1706048010.782038]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [119 - 1706048010.7820854]: 0.0001282051282051282
python LearningRateFunctionModifier/ParamGroup1 [119 - 1706048010.782111]: 0.0001282051282051282
python DistillationModifier/task_loss [119 - 1706048010.7821522]: tensor(0.3046, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [119 - 1706048010.81665]: tensor(0.9975, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [119 - 1706048010.8169765]: tensor(0.9975, grad_fn=<AddBackward0>)
python ConstantPruningModifier [119 - 1706048010.8172495]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.8888888888888888| steps_per_epoch: 63| 
python ConstantPruningModifier [119 - 1706048011.027153]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [119 - 1706048011.0276291]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [119 - 1706048011.0279036]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [119 - 1706048011.0280814]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [119 - 1706048011.0282876]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [119 - 1706048011.0289571]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [119 - 1706048011.0296175]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [119 - 1706048011.029855]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [119 - 1706048011.0300024]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [119 - 1706048011.0302157]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [119 - 1706048011.0304153]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [119 - 1706048011.0309975]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [119 - 1706048011.0315487]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [119 - 1706048011.0318034]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [119 - 1706048011.0320108]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [119 - 1706048011.0322495]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [119 - 1706048011.0324259]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [119 - 1706048011.0329065]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [119 - 1706048011.0333304]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [119 - 1706048011.0335238]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [119 - 1706048011.0336998]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [119 - 1706048011.033892]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [119 - 1706048011.0340817]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [119 - 1706048011.0344694]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [119 - 1706048011.0349174]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [119 - 1706048011.0351052]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [119 - 1706048011.0352466]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [119 - 1706048011.0354233]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [119 - 1706048011.0355933]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [119 - 1706048011.0359874]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [119 - 1706048011.0364065]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [119 - 1706048011.0366285]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [119 - 1706048011.0368235]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [119 - 1706048011.0370207]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [119 - 1706048011.0371828]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [119 - 1706048011.0375962]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [119 - 1706048011.0380094]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [119 - 1706048011.0381932]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [119 - 1706048011.03834]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [119 - 1706048011.0385199]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [119 - 1706048011.0386927]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [119 - 1706048011.0390804]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [119 - 1706048011.03946]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [119 - 1706048011.039664]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [119 - 1706048011.0398095]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [119 - 1706048011.0399892]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [119 - 1706048011.0401313]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [119 - 1706048011.040599]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [119 - 1706048011.041017]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [119 - 1706048011.0411932]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [119 - 1706048011.0413346]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [119 - 1706048011.0415077]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [119 - 1706048011.0416722]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [119 - 1706048011.0420618]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [119 - 1706048011.042518]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [119 - 1706048011.0427265]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [119 - 1706048011.0429113]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [119 - 1706048011.0430613]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [119 - 1706048011.0432093]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [119 - 1706048011.0435927]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [119 - 1706048011.0440042]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [119 - 1706048011.044179]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [119 - 1706048011.0443149]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [119 - 1706048011.0445023]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [119 - 1706048011.044681]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [119 - 1706048011.0450797]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [119 - 1706048011.0455093]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [119 - 1706048011.0457058]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [119 - 1706048011.0458837]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [119 - 1706048011.0460296]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [119 - 1706048011.046179]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [119 - 1706048011.0465796]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [119 - 1706048011.0469992]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [119 - 1706048011.0471823]: 0.0
python ParamPruning/classifier.weight [119 - 1706048011.0472515]: 0.0
python DistillationModifier [126 - 1706048045.1440046]: Calling loss_update with:
args: 0.5289445519447327| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048046.9365819]: 
Returned: 1.179065227508545| 

python DistillationModifier [126 - 1706048048.5445206]: Calling loss_update with:
args: 0.8923551440238953| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048050.316601]: 
Returned: 1.6522172689437866| 

python DistillationModifier [126 - 1706048051.7388709]: Calling loss_update with:
args: 0.7087405920028687| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048052.9337115]: 
Returned: 1.0609530210494995| 

python DistillationModifier [126 - 1706048054.1361024]: Calling loss_update with:
args: 0.5595294833183289| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048055.3233078]: 
Returned: 0.855109453201294| 

python DistillationModifier [126 - 1706048056.5243022]: Calling loss_update with:
args: 0.7151913642883301| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048057.7242484]: 
Returned: 1.168209433555603| 

python DistillationModifier [126 - 1706048059.353546]: Calling loss_update with:
args: 0.585665762424469| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048061.217516]: 
Returned: 1.0923261642456055| 

python DistillationModifier [126 - 1706048063.0408528]: Calling loss_update with:
args: 0.3842101991176605| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048064.8431413]: 
Returned: 1.0692058801651| 

python DistillationModifier [126 - 1706048066.7276914]: Calling loss_update with:
args: 0.5771883130073547| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048068.3413467]: 
Returned: 1.1019634008407593| 

python DistillationModifier [126 - 1706048069.5413826]: Calling loss_update with:
args: 0.3788345754146576| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048070.7373803]: 
Returned: 0.4498884081840515| 

python DistillationModifier [126 - 1706048072.2435267]: Calling loss_update with:
args: 0.5999921560287476| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048074.0420525]: 
Returned: 1.1718674898147583| 

python DistillationModifier [126 - 1706048075.632258]: Calling loss_update with:
args: 0.4122237265110016| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048076.8287392]: 
Returned: 0.8555492162704468| 

python DistillationModifier [126 - 1706048078.0422697]: Calling loss_update with:
args: 0.8526001572608948| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048079.6361609]: 
Returned: 1.287133812904358| 

python DistillationModifier [126 - 1706048080.9532053]: Calling loss_update with:
args: 0.6280209422111511| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048082.736649]: 
Returned: 1.364129662513733| 

python DistillationModifier [126 - 1706048084.5449479]: Calling loss_update with:
args: 0.4913882315158844| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048086.2546213]: 
Returned: 0.7139630913734436| 

python DistillationModifier [126 - 1706048087.63131]: Calling loss_update with:
args: 0.8486789464950562| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048088.9218524]: 
Returned: 1.4294160604476929| 

python DistillationModifier [126 - 1706048090.1261613]: Calling loss_update with:
args: 0.5184247493743896| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048091.251461]: 
Returned: 1.0678706169128418| 

python DistillationModifier [126 - 1706048092.5214481]: Calling loss_update with:
args: 0.46420547366142273| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048094.0370958]: 
Returned: 0.8561695218086243| 

python DistillationModifier [126 - 1706048095.9281044]: Calling loss_update with:
args: 0.5190929770469666| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048097.726594]: 
Returned: 1.1037211418151855| 

python DistillationModifier [126 - 1706048099.5342202]: Calling loss_update with:
args: 0.60201495885849| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048101.336653]: 
Returned: 0.8961633443832397| 

python DistillationModifier [126 - 1706048102.6367612]: Calling loss_update with:
args: 0.44776448607444763| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048103.8240318]: 
Returned: 0.8586317896842957| 

python DistillationModifier [126 - 1706048105.0282762]: Calling loss_update with:
args: 0.4907543361186981| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048106.1515112]: 
Returned: 0.8302597999572754| 

python DistillationModifier [126 - 1706048107.3471332]: Calling loss_update with:
args: 0.6073322296142578| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048109.0311067]: 
Returned: 1.0561951398849487| 

python DistillationModifier [126 - 1706048110.8444219]: Calling loss_update with:
args: 0.45553234219551086| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048112.2472808]: 
Returned: 0.9798212647438049| 

python DistillationModifier [126 - 1706048113.5248497]: Calling loss_update with:
args: 0.2563560903072357| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048114.6463122]: 
Returned: 0.7549977898597717| 

python DistillationModifier [126 - 1706048115.846698]: Calling loss_update with:
args: 0.666278064250946| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048117.5409427]: 
Returned: 0.8521052002906799| 

python DistillationModifier [126 - 1706048119.4171534]: Calling loss_update with:
args: 0.26006194949150085| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048121.22675]: 
Returned: 0.6469954252243042| 

python DistillationModifier [126 - 1706048122.4307947]: Calling loss_update with:
args: 0.25721094012260437| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048123.6225061]: 
Returned: 0.5215252637863159| 

python DistillationModifier [126 - 1706048124.839082]: Calling loss_update with:
args: 0.42281726002693176| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048126.435901]: 
Returned: 0.6447437405586243| 

python DistillationModifier [126 - 1706048127.9299953]: Calling loss_update with:
args: 0.5343896150588989| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048129.1178727]: 
Returned: 0.9114153981208801| 

python DistillationModifier [126 - 1706048130.3240666]: Calling loss_update with:
args: 0.3895227909088135| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048131.5554626]: 
Returned: 0.8290329575538635| 

python DistillationModifier [126 - 1706048132.8363163]: Calling loss_update with:
args: 0.41079142689704895| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048134.0184464]: 
Returned: 1.039424180984497| 

python DistillationModifier [126 - 1706048135.2211978]: Calling loss_update with:
args: 0.7011146545410156| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048136.3482585]: 
Returned: 1.1480696201324463| 

python DistillationModifier [126 - 1706048137.9230235]: Calling loss_update with:
args: 0.7262317538261414| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048139.323763]: 
Returned: 1.3918558359146118| 

python DistillationModifier [126 - 1706048140.5292614]: Calling loss_update with:
args: 0.8350829482078552| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048141.6519425]: 
Returned: 1.4394309520721436| 

python DistillationModifier [126 - 1706048142.9273126]: Calling loss_update with:
args: 0.9137333631515503| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048144.0499399]: 
Returned: 1.2178382873535156| 

python DistillationModifier [126 - 1706048145.3169332]: Calling loss_update with:
args: 0.96490478515625| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048146.93489]: 
Returned: 1.8226407766342163| 

python DistillationModifier [126 - 1706048148.3243465]: Calling loss_update with:
args: 0.5739552974700928| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048149.4475899]: 
Returned: 0.7836244106292725| 

python DistillationModifier [126 - 1706048150.9174654]: Calling loss_update with:
args: 0.7110685110092163| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048152.1400914]: 
Returned: 1.0942730903625488| 

python DistillationModifier [126 - 1706048153.344743]: Calling loss_update with:
args: 0.6985266208648682| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048155.1311965]: 
Returned: 1.4456121921539307| 

python DistillationModifier [126 - 1706048156.9444308]: Calling loss_update with:
args: 0.6013142466545105| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048158.434953]: 
Returned: 1.251375675201416| 

python DistillationModifier [126 - 1706048159.6352847]: Calling loss_update with:
args: 0.4381205439567566| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048160.8387663]: 
Returned: 0.9729635715484619| 

python DistillationModifier [126 - 1706048162.44463]: Calling loss_update with:
args: 0.43132293224334717| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048164.2430964]: 
Returned: 0.8745399713516235| 

python DistillationModifier [126 - 1706048165.5372353]: Calling loss_update with:
args: 0.5677539706230164| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048167.0367064]: 
Returned: 0.7919610142707825| 

python DistillationModifier [126 - 1706048168.3187764]: Calling loss_update with:
args: 0.4511672258377075| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048169.733117]: 
Returned: 1.100422978401184| 

python DistillationModifier [126 - 1706048171.1349614]: Calling loss_update with:
args: 0.6097444891929626| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048172.836734]: 
Returned: 1.4187285900115967| 

python DistillationModifier [126 - 1706048174.034459]: Calling loss_update with:
args: 0.6184592247009277| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048175.2270513]: 
Returned: 1.0594342947006226| 

python DistillationModifier [126 - 1706048176.4340541]: Calling loss_update with:
args: 0.4009074866771698| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048177.6331165]: 
Returned: 0.774619460105896| 

python DistillationModifier [126 - 1706048178.8357153]: Calling loss_update with:
args: 0.276656836271286| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048180.0272665]: 
Returned: 0.8127920031547546| 

python DistillationModifier [126 - 1706048181.3380136]: Calling loss_update with:
args: 0.7938209176063538| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048182.5273082]: 
Returned: 1.629570722579956| 

python DistillationModifier [126 - 1706048184.3443778]: Calling loss_update with:
args: 0.7531921863555908| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048186.1418836]: 
Returned: 1.3862673044204712| 

python DistillationModifier [126 - 1706048187.9511442]: Calling loss_update with:
args: 0.3289504647254944| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048189.3416212]: 
Returned: 0.6359557509422302| 

python DistillationModifier [126 - 1706048190.839771]: Calling loss_update with:
args: 1.379996418952942| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048192.6366737]: 
Returned: 1.5146753787994385| 

python DistillationModifier [126 - 1706048194.419583]: Calling loss_update with:
args: 0.3858465552330017| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048195.537463]: 
Returned: 0.8618113398551941| 

python DistillationModifier [126 - 1706048196.9181914]: Calling loss_update with:
args: 0.6834487915039062| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048198.049676]: 
Returned: 1.1552740335464478| 

python DistillationModifier [126 - 1706048199.6344213]: Calling loss_update with:
args: 0.40240195393562317| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048201.435048]: 
Returned: 1.0849148035049438| 

python DistillationModifier [126 - 1706048202.9180026]: Calling loss_update with:
args: 0.5556938052177429| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048204.1318283]: 
Returned: 1.1318132877349854| 

python DistillationModifier [126 - 1706048205.3378658]: Calling loss_update with:
args: 0.6773797273635864| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048206.5248587]: 
Returned: 0.6460726857185364| 

python DistillationModifier [126 - 1706048207.7369473]: Calling loss_update with:
args: 0.518549382686615| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048208.9290192]: 
Returned: 0.8344277143478394| 

python DistillationModifier [126 - 1706048210.1389616]: Calling loss_update with:
args: 0.6177417635917664| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048211.429964]: 
Returned: 0.85988849401474| 

python DistillationModifier [126 - 1706048212.6323338]: Calling loss_update with:
args: 0.4672030508518219| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048213.836372]: 
Returned: 0.8535747528076172| 

python DistillationModifier [126 - 1706048215.532556]: Calling loss_update with:
args: 0.4022026062011719| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048216.7236564]: 
Returned: 1.1023187637329102| 

python DistillationModifier [126 - 1706048217.9284194]: Calling loss_update with:
args: 0.5836231112480164| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048219.117874]: 
Returned: 1.2310560941696167| 

python DistillationModifier [126 - 1706048219.730822]: Calling loss_update with:
args: 0.5666625499725342| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048220.3414521]: 
Returned: 1.1051466464996338| 

python DistillationModifier [126 - 1706048222.042204]: Calling loss_update with:
args: 0.5390937924385071| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1706048223.2306552]: 
Returned: 0.9191344976425171| 

python LearningRateFunctionModifier [126 - 1706048225.7718184]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [126 - 1706048225.7719975]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [126 - 1706048225.7730124]: 0.0001269230769230769
python LearningRateFunctionModifier/ParamGroup1 [126 - 1706048225.7730436]: 0.0001269230769230769
python DistillationModifier/task_loss [126 - 1706048225.7730944]: tensor(0.5391, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [126 - 1706048225.773647]: tensor(0.9191, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [126 - 1706048225.7739012]: tensor(0.9191, grad_fn=<AddBackward0>)
python ConstantPruningModifier [126 - 1706048225.7741516]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| 
python ConstantPruningModifier [126 - 1706048226.1212437]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [126 - 1706048226.121798]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [126 - 1706048226.1221478]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [126 - 1706048226.1224155]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [126 - 1706048226.12266]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [126 - 1706048226.1234596]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [126 - 1706048226.1241112]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [126 - 1706048226.1243646]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [126 - 1706048226.1246684]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [126 - 1706048226.124922]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [126 - 1706048226.1251454]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [126 - 1706048226.1257343]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [126 - 1706048226.1263013]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [126 - 1706048226.1265357]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [126 - 1706048226.1267793]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [126 - 1706048226.1269963]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [126 - 1706048226.1272373]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [126 - 1706048226.1278062]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [126 - 1706048226.128388]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [126 - 1706048226.1286557]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [126 - 1706048226.128857]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [126 - 1706048226.129071]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [126 - 1706048226.1292777]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [126 - 1706048226.1298625]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [126 - 1706048226.1304219]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [126 - 1706048226.1306863]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [126 - 1706048226.1309094]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [126 - 1706048226.1311326]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [126 - 1706048226.1313329]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [126 - 1706048226.1319067]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [126 - 1706048226.132464]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [126 - 1706048226.1327136]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [126 - 1706048226.1329312]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [126 - 1706048226.1331263]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [126 - 1706048226.133325]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [126 - 1706048226.133903]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [126 - 1706048226.1344543]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [126 - 1706048226.1347156]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [126 - 1706048226.134925]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [126 - 1706048226.1351404]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [126 - 1706048226.1353486]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [126 - 1706048226.1359155]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [126 - 1706048226.136487]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [126 - 1706048226.136736]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [126 - 1706048226.1369505]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [126 - 1706048226.1371613]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [126 - 1706048226.1373618]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [126 - 1706048226.1379395]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [126 - 1706048226.138495]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [126 - 1706048226.1387382]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [126 - 1706048226.1389477]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [126 - 1706048226.1391659]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [126 - 1706048226.1393692]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [126 - 1706048226.1399536]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [126 - 1706048226.1405885]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [126 - 1706048226.1408179]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [126 - 1706048226.1410136]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [126 - 1706048226.1412647]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [126 - 1706048226.141474]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [126 - 1706048226.1420496]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [126 - 1706048226.1426246]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [126 - 1706048226.1428695]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [126 - 1706048226.1430664]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [126 - 1706048226.1432812]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [126 - 1706048226.1434937]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [126 - 1706048226.1440694]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [126 - 1706048226.1446686]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [126 - 1706048226.1448963]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [126 - 1706048226.1450884]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [126 - 1706048226.1452978]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [126 - 1706048226.145502]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [126 - 1706048226.1460793]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [126 - 1706048226.1466546]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [126 - 1706048226.1468728]: 0.0
python ParamPruning/classifier.weight [126 - 1706048226.146964]: 0.0
python DistillationModifier [133 - 1706048266.2368894]: Calling loss_update with:
args: 0.3727319836616516| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.111111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [133 - 1706048267.4323597]: 
Returned: 0.8014795184135437| 

python LearningRateFunctionModifier [133 - 1706048269.6734643]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.111111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [133 - 1706048269.6736295]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [133 - 1706048269.6736772]: 0.00012564102564102564
python LearningRateFunctionModifier/ParamGroup1 [133 - 1706048269.6737034]: 0.00012564102564102564
python DistillationModifier/task_loss [133 - 1706048269.6737442]: tensor(0.3727, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [133 - 1706048269.6742291]: tensor(0.8015, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [133 - 1706048269.6744666]: tensor(0.8015, grad_fn=<AddBackward0>)
python ConstantPruningModifier [133 - 1706048269.674725]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.111111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [133 - 1706048269.9234295]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [133 - 1706048269.9239368]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [133 - 1706048269.9242294]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [133 - 1706048269.924455]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [133 - 1706048269.9246714]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [133 - 1706048269.9252722]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [133 - 1706048269.9257169]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [133 - 1706048269.9259205]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [133 - 1706048269.9260578]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [133 - 1706048269.9262404]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [133 - 1706048269.9263947]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [133 - 1706048269.9267795]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [133 - 1706048269.9273386]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [133 - 1706048269.927533]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [133 - 1706048269.9277427]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [133 - 1706048269.927951]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [133 - 1706048269.9281173]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [133 - 1706048269.9285183]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [133 - 1706048269.9290133]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [133 - 1706048269.929196]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [133 - 1706048269.9293606]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [133 - 1706048269.9295118]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [133 - 1706048269.9296794]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [133 - 1706048269.9300942]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [133 - 1706048269.9304771]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [133 - 1706048269.9306738]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [133 - 1706048269.9308448]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [133 - 1706048269.9310315]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [133 - 1706048269.9311783]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [133 - 1706048269.9315307]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [133 - 1706048269.9320393]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [133 - 1706048269.932211]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [133 - 1706048269.9323733]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [133 - 1706048269.9325354]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [133 - 1706048269.9327495]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [133 - 1706048269.9331448]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [133 - 1706048269.9336271]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [133 - 1706048269.9338307]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [133 - 1706048269.9339764]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [133 - 1706048269.9341533]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [133 - 1706048269.9343383]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [133 - 1706048269.934805]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [133 - 1706048269.9353192]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [133 - 1706048269.9355145]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [133 - 1706048269.9357328]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [133 - 1706048269.9358804]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [133 - 1706048269.936067]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [133 - 1706048269.936609]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [133 - 1706048269.937113]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [133 - 1706048269.9373093]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [133 - 1706048269.9374807]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [133 - 1706048269.9376566]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [133 - 1706048269.937857]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [133 - 1706048269.938333]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [133 - 1706048269.9387877]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [133 - 1706048269.9389749]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [133 - 1706048269.9391108]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [133 - 1706048269.9392734]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [133 - 1706048269.9394135]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [133 - 1706048269.939808]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [133 - 1706048269.9402845]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [133 - 1706048269.9404652]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [133 - 1706048269.940681]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [133 - 1706048269.9408765]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [133 - 1706048269.9410164]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [133 - 1706048269.941398]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [133 - 1706048269.9419022]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [133 - 1706048269.9421065]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [133 - 1706048269.9422457]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [133 - 1706048269.9423928]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [133 - 1706048269.9425223]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [133 - 1706048269.942924]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [133 - 1706048269.9433289]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [133 - 1706048269.943521]: 0.0
python ParamPruning/classifier.weight [133 - 1706048269.9436126]: 0.0
python DistillationModifier [140 - 1706048307.2367773]: Calling loss_update with:
args: 0.1660955548286438| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.2222222222222223| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [140 - 1706048308.4600275]: 
Returned: 0.41517558693885803| 

python LearningRateFunctionModifier [140 - 1706048310.7645304]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.2222222222222223| steps_per_epoch: 63| 
python LearningRateFunctionModifier [140 - 1706048310.7646906]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [140 - 1706048310.7647378]: 0.00012435897435897434
python LearningRateFunctionModifier/ParamGroup1 [140 - 1706048310.7647643]: 0.00012435897435897434
python DistillationModifier/task_loss [140 - 1706048310.764806]: tensor(0.1661, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [140 - 1706048310.7652917]: tensor(0.4152, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [140 - 1706048310.7655845]: tensor(0.4152, grad_fn=<AddBackward0>)
python ConstantPruningModifier [140 - 1706048310.7658393]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.2222222222222223| steps_per_epoch: 63| 
python ConstantPruningModifier [140 - 1706048311.044311]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [140 - 1706048311.044831]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [140 - 1706048311.045124]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [140 - 1706048311.0453758]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [140 - 1706048311.0456192]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [140 - 1706048311.0464032]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [140 - 1706048311.0472283]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [140 - 1706048311.047482]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [140 - 1706048311.0477142]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [140 - 1706048311.0479395]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [140 - 1706048311.0481076]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [140 - 1706048311.0487416]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [140 - 1706048311.0494075]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [140 - 1706048311.0496495]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [140 - 1706048311.049866]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [140 - 1706048311.0500824]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [140 - 1706048311.05026]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [140 - 1706048311.0508351]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [140 - 1706048311.051361]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [140 - 1706048311.0515904]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [140 - 1706048311.0518177]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [140 - 1706048311.0519977]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [140 - 1706048311.052224]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [140 - 1706048311.1168587]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [140 - 1706048311.117434]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [140 - 1706048311.1176775]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [140 - 1706048311.1179023]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [140 - 1706048311.1180825]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [140 - 1706048311.1182914]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [140 - 1706048311.118829]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [140 - 1706048311.1196]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [140 - 1706048311.1198385]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [140 - 1706048311.1200316]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [140 - 1706048311.120242]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [140 - 1706048311.120482]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [140 - 1706048311.121102]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [140 - 1706048311.1217902]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [140 - 1706048311.1220148]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [140 - 1706048311.1222055]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [140 - 1706048311.1223698]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [140 - 1706048311.1225939]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [140 - 1706048311.1231773]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [140 - 1706048311.1238115]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [140 - 1706048311.1240335]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [140 - 1706048311.1241946]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [140 - 1706048311.1243825]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [140 - 1706048311.1245787]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [140 - 1706048311.1251752]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [140 - 1706048311.125722]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [140 - 1706048311.1259265]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [140 - 1706048311.1261444]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [140 - 1706048311.1263654]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [140 - 1706048311.1266246]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [140 - 1706048311.127145]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [140 - 1706048311.127879]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [140 - 1706048311.1281035]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [140 - 1706048311.1283188]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [140 - 1706048311.1285572]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [140 - 1706048311.12877]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [140 - 1706048311.1293488]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [140 - 1706048311.1299915]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [140 - 1706048311.1302133]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [140 - 1706048311.1303756]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [140 - 1706048311.1305568]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [140 - 1706048311.130797]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [140 - 1706048311.1313512]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [140 - 1706048311.1319714]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [140 - 1706048311.1321695]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [140 - 1706048311.1323314]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [140 - 1706048311.1325512]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [140 - 1706048311.1328003]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [140 - 1706048311.1333883]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [140 - 1706048311.134009]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [140 - 1706048311.1342316]: 0.0
python ParamPruning/classifier.weight [140 - 1706048311.134299]: 0.0
python DistillationModifier [147 - 1706048347.222746]: Calling loss_update with:
args: 0.6027346253395081| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.3333333333333335| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [147 - 1706048349.0175524]: 
Returned: 1.1595284938812256| 

python LearningRateFunctionModifier [147 - 1706048351.561185]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.3333333333333335| steps_per_epoch: 63| 
python LearningRateFunctionModifier [147 - 1706048351.5613267]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [147 - 1706048351.561374]: 0.00012307692307692307
python LearningRateFunctionModifier/ParamGroup1 [147 - 1706048351.5614002]: 0.00012307692307692307
python DistillationModifier/task_loss [147 - 1706048351.561442]: tensor(0.6027, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [147 - 1706048351.5619595]: tensor(1.1595, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [147 - 1706048351.5622113]: tensor(1.1595, grad_fn=<AddBackward0>)
python ConstantPruningModifier [147 - 1706048351.5624435]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.3333333333333335| steps_per_epoch: 63| 
python ConstantPruningModifier [147 - 1706048351.7434652]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [147 - 1706048351.7439477]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [147 - 1706048351.7441688]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [147 - 1706048351.7444193]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [147 - 1706048351.7446814]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [147 - 1706048351.745276]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [147 - 1706048351.7457297]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [147 - 1706048351.7459419]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [147 - 1706048351.7460833]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [147 - 1706048351.746269]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [147 - 1706048351.7464526]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [147 - 1706048351.7468438]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [147 - 1706048351.7472057]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [147 - 1706048351.7473955]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [147 - 1706048351.7475374]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [147 - 1706048351.747698]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [147 - 1706048351.7478368]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [147 - 1706048351.7481866]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [147 - 1706048351.7485895]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [147 - 1706048351.7487905]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [147 - 1706048351.7489288]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [147 - 1706048351.7491004]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [147 - 1706048351.7492723]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [147 - 1706048351.7496638]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [147 - 1706048351.7500045]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [147 - 1706048351.7501838]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [147 - 1706048351.7503278]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [147 - 1706048351.7504988]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [147 - 1706048351.7506742]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [147 - 1706048351.7511811]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [147 - 1706048351.751765]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [147 - 1706048351.7519462]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [147 - 1706048351.7520866]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [147 - 1706048351.7522676]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [147 - 1706048351.752411]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [147 - 1706048351.8168995]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [147 - 1706048351.8173132]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [147 - 1706048351.8174992]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [147 - 1706048351.8177135]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [147 - 1706048351.8178818]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [147 - 1706048351.818026]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [147 - 1706048351.8183923]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [147 - 1706048351.8188179]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [147 - 1706048351.8189907]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [147 - 1706048351.8191292]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [147 - 1706048351.8192647]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [147 - 1706048351.819397]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [147 - 1706048351.819786]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [147 - 1706048351.8201349]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [147 - 1706048351.820309]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [147 - 1706048351.8204446]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [147 - 1706048351.8206408]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [147 - 1706048351.8207955]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [147 - 1706048351.82117]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [147 - 1706048351.8215175]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [147 - 1706048351.8217118]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [147 - 1706048351.8218486]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [147 - 1706048351.8219793]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [147 - 1706048351.8221564]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [147 - 1706048351.8224983]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [147 - 1706048351.822856]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [147 - 1706048351.8230925]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [147 - 1706048351.8232336]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [147 - 1706048351.8234031]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [147 - 1706048351.8235433]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [147 - 1706048351.8239315]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [147 - 1706048351.8242989]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [147 - 1706048351.8244815]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [147 - 1706048351.824657]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [147 - 1706048351.8248394]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [147 - 1706048351.8249764]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [147 - 1706048351.8253312]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [147 - 1706048351.825691]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [147 - 1706048351.8258648]: 0.0
python ParamPruning/classifier.weight [147 - 1706048351.8259346]: 0.0
python DistillationModifier [154 - 1706048391.5371943]: Calling loss_update with:
args: 0.35358789563179016| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.4444444444444446| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [154 - 1706048392.73231]: 
Returned: 0.8574471473693848| 

python LearningRateFunctionModifier [154 - 1706048395.3169918]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.4444444444444446| steps_per_epoch: 63| 
python LearningRateFunctionModifier [154 - 1706048395.317162]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [154 - 1706048395.317214]: 0.00012179487179487177
python LearningRateFunctionModifier/ParamGroup1 [154 - 1706048395.3172429]: 0.00012179487179487177
python DistillationModifier/task_loss [154 - 1706048395.3172903]: tensor(0.3536, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [154 - 1706048395.3178577]: tensor(0.8574, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [154 - 1706048395.3181179]: tensor(0.8574, grad_fn=<AddBackward0>)
python ConstantPruningModifier [154 - 1706048395.3183708]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.4444444444444446| steps_per_epoch: 63| 
python ConstantPruningModifier [154 - 1706048395.632203]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [154 - 1706048395.6328015]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [154 - 1706048395.633151]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [154 - 1706048395.6334133]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [154 - 1706048395.6336725]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [154 - 1706048395.634471]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [154 - 1706048395.6351216]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [154 - 1706048395.6354017]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [154 - 1706048395.6356754]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [154 - 1706048395.6358848]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [154 - 1706048395.6361291]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [154 - 1706048395.6368098]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [154 - 1706048395.6376436]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [154 - 1706048395.6379082]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [154 - 1706048395.6381454]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [154 - 1706048395.6383796]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [154 - 1706048395.6386166]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [154 - 1706048395.639206]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [154 - 1706048395.639925]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [154 - 1706048395.64019]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [154 - 1706048395.6404176]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [154 - 1706048395.640659]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [154 - 1706048395.6409333]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [154 - 1706048395.6415288]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [154 - 1706048395.6421082]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [154 - 1706048395.6423576]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [154 - 1706048395.6426048]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [154 - 1706048395.6428447]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [154 - 1706048395.643115]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [154 - 1706048395.6436963]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [154 - 1706048395.644252]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [154 - 1706048395.6445062]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [154 - 1706048395.6447399]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [154 - 1706048395.6449623]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [154 - 1706048395.6452131]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [154 - 1706048395.6458058]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [154 - 1706048395.6463652]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [154 - 1706048395.6466217]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [154 - 1706048395.646846]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [154 - 1706048395.6470447]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [154 - 1706048395.6472833]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [154 - 1706048395.6478655]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [154 - 1706048395.6484344]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [154 - 1706048395.648698]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [154 - 1706048395.64892]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [154 - 1706048395.6491463]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [154 - 1706048395.6494052]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [154 - 1706048395.6499832]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [154 - 1706048395.6508121]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [154 - 1706048395.6510606]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [154 - 1706048395.6512935]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [154 - 1706048395.6515386]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [154 - 1706048395.6517732]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [154 - 1706048395.6523628]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [154 - 1706048395.7171016]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [154 - 1706048395.7173507]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [154 - 1706048395.7176278]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [154 - 1706048395.7179022]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [154 - 1706048395.7181365]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [154 - 1706048395.7188108]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [154 - 1706048395.7194452]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [154 - 1706048395.7197347]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [154 - 1706048395.7199733]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [154 - 1706048395.7202148]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [154 - 1706048395.7204278]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [154 - 1706048395.7210686]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [154 - 1706048395.7216978]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [154 - 1706048395.7219656]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [154 - 1706048395.722195]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [154 - 1706048395.7224627]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [154 - 1706048395.7226894]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [154 - 1706048395.7232935]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [154 - 1706048395.7239776]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [154 - 1706048395.7242362]: 0.0
python ParamPruning/classifier.weight [154 - 1706048395.7243302]: 0.0
python DistillationModifier [161 - 1706048431.9321663]: Calling loss_update with:
args: 0.3378779888153076| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.5555555555555554| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [161 - 1706048432.9477432]: 
Returned: 0.6525954008102417| 

python LearningRateFunctionModifier [161 - 1706048434.8803616]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.5555555555555554| steps_per_epoch: 63| 
python LearningRateFunctionModifier [161 - 1706048434.8805153]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [161 - 1706048434.880561]: 0.0001205128205128205
python LearningRateFunctionModifier/ParamGroup1 [161 - 1706048434.8806055]: 0.0001205128205128205
python DistillationModifier/task_loss [161 - 1706048434.8806455]: tensor(0.3379, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [161 - 1706048434.8811204]: tensor(0.6526, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [161 - 1706048434.8813565]: tensor(0.6526, grad_fn=<AddBackward0>)
python ConstantPruningModifier [161 - 1706048434.8815928]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.5555555555555554| steps_per_epoch: 63| 
python ConstantPruningModifier [161 - 1706048435.1230147]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [161 - 1706048435.123491]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [161 - 1706048435.1237826]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [161 - 1706048435.1239831]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [161 - 1706048435.1241503]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [161 - 1706048435.124783]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [161 - 1706048435.1253686]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [161 - 1706048435.125592]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [161 - 1706048435.1258104]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [161 - 1706048435.125957]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [161 - 1706048435.1261022]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [161 - 1706048435.1265602]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [161 - 1706048435.1271389]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [161 - 1706048435.1273437]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [161 - 1706048435.1274872]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [161 - 1706048435.1277008]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [161 - 1706048435.1278453]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [161 - 1706048435.128243]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [161 - 1706048435.128725]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [161 - 1706048435.1289184]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [161 - 1706048435.129102]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [161 - 1706048435.1292455]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [161 - 1706048435.1293993]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [161 - 1706048435.1298358]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [161 - 1706048435.130215]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [161 - 1706048435.1303945]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [161 - 1706048435.1305985]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [161 - 1706048435.1307914]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [161 - 1706048435.1309428]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [161 - 1706048435.1313136]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [161 - 1706048435.1317248]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [161 - 1706048435.1319096]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [161 - 1706048435.1320817]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [161 - 1706048435.1322134]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [161 - 1706048435.132402]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [161 - 1706048435.1328454]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [161 - 1706048435.13325]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [161 - 1706048435.1334312]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [161 - 1706048435.1336272]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [161 - 1706048435.133773]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [161 - 1706048435.133955]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [161 - 1706048435.1343615]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [161 - 1706048435.1348114]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [161 - 1706048435.134999]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [161 - 1706048435.1351643]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [161 - 1706048435.135311]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [161 - 1706048435.1354609]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [161 - 1706048435.1359143]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [161 - 1706048435.1363254]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [161 - 1706048435.136522]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [161 - 1706048435.1366866]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [161 - 1706048435.1368308]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [161 - 1706048435.1370099]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [161 - 1706048435.137383]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [161 - 1706048435.1377811]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [161 - 1706048435.137981]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [161 - 1706048435.138149]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [161 - 1706048435.138292]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [161 - 1706048435.138447]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [161 - 1706048435.1388514]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [161 - 1706048435.1392004]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [161 - 1706048435.1393857]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [161 - 1706048435.1395574]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [161 - 1706048435.1397202]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [161 - 1706048435.1398997]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [161 - 1706048435.1402545]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [161 - 1706048435.1407971]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [161 - 1706048435.1409917]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [161 - 1706048435.1411257]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [161 - 1706048435.1413143]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [161 - 1706048435.1414568]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [161 - 1706048435.1419039]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [161 - 1706048435.142297]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [161 - 1706048435.1424875]: 0.0
python ParamPruning/classifier.weight [161 - 1706048435.1425557]: 0.0
python DistillationModifier [168 - 1706048474.721798]: Calling loss_update with:
args: 0.2890065014362335| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.6666666666666665| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [168 - 1706048476.1508691]: 
Returned: 0.7170284986495972| 

python LearningRateFunctionModifier [168 - 1706048478.1718771]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.6666666666666665| steps_per_epoch: 63| 
python LearningRateFunctionModifier [168 - 1706048478.1720262]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [168 - 1706048478.172073]: 0.00011923076923076922
python LearningRateFunctionModifier/ParamGroup1 [168 - 1706048478.1720994]: 0.00011923076923076922
python DistillationModifier/task_loss [168 - 1706048478.17214]: tensor(0.2890, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [168 - 1706048478.1726615]: tensor(0.7170, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [168 - 1706048478.172912]: tensor(0.7170, grad_fn=<AddBackward0>)
python ConstantPruningModifier [168 - 1706048478.173142]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.6666666666666665| steps_per_epoch: 63| 
python ConstantPruningModifier [168 - 1706048478.424163]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [168 - 1706048478.4246502]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [168 - 1706048478.424923]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [168 - 1706048478.4251394]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [168 - 1706048478.425317]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [168 - 1706048478.4259315]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [168 - 1706048478.426364]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [168 - 1706048478.426582]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [168 - 1706048478.426792]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [168 - 1706048478.4269292]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [168 - 1706048478.4271038]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [168 - 1706048478.427485]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [168 - 1706048478.4278822]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [168 - 1706048478.4280713]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [168 - 1706048478.4282484]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [168 - 1706048478.4283967]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [168 - 1706048478.4285457]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [168 - 1706048478.4289405]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [168 - 1706048478.4293]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [168 - 1706048478.429491]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [168 - 1706048478.42967]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [168 - 1706048478.4298186]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [168 - 1706048478.4299567]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [168 - 1706048478.4303367]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [168 - 1706048478.4307582]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [168 - 1706048478.4309404]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [168 - 1706048478.4311028]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [168 - 1706048478.4312437]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [168 - 1706048478.43142]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [168 - 1706048478.4317946]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [168 - 1706048478.4321346]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [168 - 1706048478.4323063]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [168 - 1706048478.432481]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [168 - 1706048478.4326584]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [168 - 1706048478.432829]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [168 - 1706048478.4333737]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [168 - 1706048478.433973]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [168 - 1706048478.4341836]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [168 - 1706048478.4343607]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [168 - 1706048478.4345195]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [168 - 1706048478.434709]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [168 - 1706048478.4351213]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [168 - 1706048478.4355214]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [168 - 1706048478.4357197]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [168 - 1706048478.4359016]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [168 - 1706048478.4360645]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [168 - 1706048478.4362133]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [168 - 1706048478.4366252]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [168 - 1706048478.4369853]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [168 - 1706048478.4371638]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [168 - 1706048478.4373338]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [168 - 1706048478.4374988]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [168 - 1706048478.4376647]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [168 - 1706048478.4380295]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [168 - 1706048478.4383724]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [168 - 1706048478.4385424]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [168 - 1706048478.438727]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [168 - 1706048478.4388611]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [168 - 1706048478.4390118]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [168 - 1706048478.439386]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [168 - 1706048478.4397607]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [168 - 1706048478.4399388]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [168 - 1706048478.4400988]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [168 - 1706048478.4402535]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [168 - 1706048478.440401]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [168 - 1706048478.4408073]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [168 - 1706048478.441174]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [168 - 1706048478.4413488]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [168 - 1706048478.4414995]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [168 - 1706048478.441671]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [168 - 1706048478.441811]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [168 - 1706048478.4421701]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [168 - 1706048478.4425275]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [168 - 1706048478.4427154]: 0.0
python ParamPruning/classifier.weight [168 - 1706048478.4427881]: 0.0
python DistillationModifier [175 - 1706048512.539662]: Calling loss_update with:
args: 0.3732898533344269| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.7777777777777777| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [175 - 1706048513.6202936]: 
Returned: 0.656950056552887| 

python LearningRateFunctionModifier [175 - 1706048515.8564887]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.7777777777777777| steps_per_epoch: 63| 
python LearningRateFunctionModifier [175 - 1706048515.8566601]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [175 - 1706048515.8567088]: 0.00011794871794871794
python LearningRateFunctionModifier/ParamGroup1 [175 - 1706048515.8567343]: 0.00011794871794871794
python DistillationModifier/task_loss [175 - 1706048515.856774]: tensor(0.3733, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [175 - 1706048515.857261]: tensor(0.6570, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [175 - 1706048515.8575015]: tensor(0.6570, grad_fn=<AddBackward0>)
python ConstantPruningModifier [175 - 1706048515.8577626]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.7777777777777777| steps_per_epoch: 63| 
python ConstantPruningModifier [175 - 1706048516.1376944]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [175 - 1706048516.1381793]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [175 - 1706048516.1384087]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [175 - 1706048516.1386166]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [175 - 1706048516.1388042]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [175 - 1706048516.1395988]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [175 - 1706048516.140413]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [175 - 1706048516.1407003]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [175 - 1706048516.1408834]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [175 - 1706048516.141051]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [175 - 1706048516.1412137]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [175 - 1706048516.141831]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [175 - 1706048516.1424613]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [175 - 1706048516.1427062]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [175 - 1706048516.1428773]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [175 - 1706048516.1430402]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [175 - 1706048516.1432002]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [175 - 1706048516.143775]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [175 - 1706048516.144369]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [175 - 1706048516.1446192]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [175 - 1706048516.1447983]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [175 - 1706048516.1449623]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [175 - 1706048516.1451247]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [175 - 1706048516.1456852]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [175 - 1706048516.1462166]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [175 - 1706048516.1464336]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [175 - 1706048516.1466172]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [175 - 1706048516.1467881]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [175 - 1706048516.1469505]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [175 - 1706048516.1477087]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [175 - 1706048516.148345]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [175 - 1706048516.1485934]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [175 - 1706048516.1487782]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [175 - 1706048516.1489418]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [175 - 1706048516.1491032]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [175 - 1706048516.149641]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [175 - 1706048516.1501713]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [175 - 1706048516.150384]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [175 - 1706048516.1505451]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [175 - 1706048516.150738]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [175 - 1706048516.1509018]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [175 - 1706048516.1514163]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [175 - 1706048516.151996]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [175 - 1706048516.1522102]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [175 - 1706048516.152373]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [175 - 1706048516.152531]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [175 - 1706048516.2167413]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [175 - 1706048516.2172792]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [175 - 1706048516.2178261]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [175 - 1706048516.218031]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [175 - 1706048516.2181964]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [175 - 1706048516.218358]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [175 - 1706048516.2185159]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [175 - 1706048516.2190557]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [175 - 1706048516.2196102]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [175 - 1706048516.2198257]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [175 - 1706048516.2199886]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [175 - 1706048516.220149]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [175 - 1706048516.2203083]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [175 - 1706048516.2208495]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [175 - 1706048516.2213829]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [175 - 1706048516.2216117]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [175 - 1706048516.2217846]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [175 - 1706048516.2219455]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [175 - 1706048516.2221055]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [175 - 1706048516.2226377]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [175 - 1706048516.2231665]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [175 - 1706048516.2234235]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [175 - 1706048516.223684]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [175 - 1706048516.2239125]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [175 - 1706048516.2241197]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [175 - 1706048516.2249103]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [175 - 1706048516.2255838]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [175 - 1706048516.225863]: 0.0
python ParamPruning/classifier.weight [175 - 1706048516.2259552]: 0.0
python DistillationModifier [182 - 1706048552.048855]: Calling loss_update with:
args: 0.23850159347057343| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.888888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [182 - 1706048553.1407442]: 
Returned: 0.4755186438560486| 

python LearningRateFunctionModifier [182 - 1706048555.0821497]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.888888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [182 - 1706048555.082295]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [182 - 1706048555.0823412]: 0.00011666666666666667
python LearningRateFunctionModifier/ParamGroup1 [182 - 1706048555.0823672]: 0.00011666666666666667
python DistillationModifier/task_loss [182 - 1706048555.0824084]: tensor(0.2385, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [182 - 1706048555.116901]: tensor(0.4755, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [182 - 1706048555.1171606]: tensor(0.4755, grad_fn=<AddBackward0>)
python ConstantPruningModifier [182 - 1706048555.1173995]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.888888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [182 - 1706048555.3250372]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [182 - 1706048555.3255339]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [182 - 1706048555.3257754]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [182 - 1706048555.3259382]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [182 - 1706048555.3260815]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [182 - 1706048555.3266535]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [182 - 1706048555.3270879]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [182 - 1706048555.3272963]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [182 - 1706048555.3274379]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [182 - 1706048555.3275917]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [182 - 1706048555.3277457]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [182 - 1706048555.328313]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [182 - 1706048555.3289125]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [182 - 1706048555.329126]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [182 - 1706048555.3292744]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [182 - 1706048555.3294153]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [182 - 1706048555.329556]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [182 - 1706048555.3300567]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [182 - 1706048555.3305562]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [182 - 1706048555.3307836]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [182 - 1706048555.3309212]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [182 - 1706048555.3310592]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [182 - 1706048555.3311996]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [182 - 1706048555.3316245]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [182 - 1706048555.3320858]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [182 - 1706048555.3323007]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [182 - 1706048555.3324513]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [182 - 1706048555.3326223]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [182 - 1706048555.332772]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [182 - 1706048555.333145]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [182 - 1706048555.3336084]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [182 - 1706048555.3337932]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [182 - 1706048555.3339305]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [182 - 1706048555.334063]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [182 - 1706048555.3341963]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [182 - 1706048555.3347392]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [182 - 1706048555.335156]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [182 - 1706048555.3353484]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [182 - 1706048555.33549]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [182 - 1706048555.335651]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [182 - 1706048555.335795]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [182 - 1706048555.336249]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [182 - 1706048555.3367074]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [182 - 1706048555.3368914]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [182 - 1706048555.3370311]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [182 - 1706048555.3371623]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [182 - 1706048555.3372924]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [182 - 1706048555.3378322]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [182 - 1706048555.3382208]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [182 - 1706048555.338406]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [182 - 1706048555.338547]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [182 - 1706048555.338708]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [182 - 1706048555.338848]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [182 - 1706048555.3392975]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [182 - 1706048555.3397338]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [182 - 1706048555.339927]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [182 - 1706048555.340063]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [182 - 1706048555.340198]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [182 - 1706048555.3403263]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [182 - 1706048555.340788]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [182 - 1706048555.3411853]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [182 - 1706048555.3413947]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [182 - 1706048555.3415368]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [182 - 1706048555.3416975]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [182 - 1706048555.3418403]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [182 - 1706048555.3422241]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [182 - 1706048555.3426588]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [182 - 1706048555.3428528]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [182 - 1706048555.342988]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [182 - 1706048555.343119]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [182 - 1706048555.3432512]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [182 - 1706048555.343649]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [182 - 1706048555.3440228]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [182 - 1706048555.344209]: 0.0
python ParamPruning/classifier.weight [182 - 1706048555.344279]: 0.0
python DistillationModifier [189 - 1706048587.149288]: Calling loss_update with:
args: 0.6128049492835999| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048588.823781]: 
Returned: 1.3353991508483887| 

python DistillationModifier [189 - 1706048590.4285038]: Calling loss_update with:
args: 1.108898401260376| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048591.4377434]: 
Returned: 1.8830664157867432| 

python DistillationModifier [189 - 1706048592.526881]: Calling loss_update with:
args: 1.035094141960144| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048593.5339644]: 
Returned: 1.3008129596710205| 

python DistillationModifier [189 - 1706048594.6187708]: Calling loss_update with:
args: 0.6474282145500183| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048595.627254]: 
Returned: 0.8676927089691162| 

python DistillationModifier [189 - 1706048596.6496036]: Calling loss_update with:
args: 0.8689509630203247| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048597.730807]: 
Returned: 1.2148507833480835| 

python DistillationModifier [189 - 1706048598.8220992]: Calling loss_update with:
args: 0.7087401747703552| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048599.8350685]: 
Returned: 1.3588749170303345| 

python DistillationModifier [189 - 1706048601.4482844]: Calling loss_update with:
args: 0.4662785530090332| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048602.6464448]: 
Returned: 1.181280255317688| 

python DistillationModifier [189 - 1706048603.821019]: Calling loss_update with:
args: 0.7176204919815063| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048604.8285193]: 
Returned: 1.0737214088439941| 

python DistillationModifier [189 - 1706048606.2248626]: Calling loss_update with:
args: 0.590979278087616| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048607.6240091]: 
Returned: 0.5463376641273499| 

python DistillationModifier [189 - 1706048609.2272103]: Calling loss_update with:
args: 0.8286155462265015| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048610.4487097]: 
Returned: 1.4222358465194702| 

python DistillationModifier [189 - 1706048611.6358962]: Calling loss_update with:
args: 0.6297274231910706| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048613.2303011]: 
Returned: 1.0782095193862915| 

python DistillationModifier [189 - 1706048614.8492134]: Calling loss_update with:
args: 1.0198917388916016| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048616.4458413]: 
Returned: 1.5165554285049438| 

python DistillationModifier [189 - 1706048618.1192791]: Calling loss_update with:
args: 0.7961357831954956| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048619.430491]: 
Returned: 1.6513563394546509| 

python DistillationModifier [189 - 1706048620.5421975]: Calling loss_update with:
args: 0.5586491823196411| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048622.1360347]: 
Returned: 0.533140242099762| 

python DistillationModifier [189 - 1706048623.342156]: Calling loss_update with:
args: 1.0253255367279053| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048624.3517122]: 
Returned: 1.7036038637161255| 

python DistillationModifier [189 - 1706048625.4389071]: Calling loss_update with:
args: 0.7313395738601685| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048626.4501364]: 
Returned: 1.3540332317352295| 

python DistillationModifier [189 - 1706048627.5437868]: Calling loss_update with:
args: 0.6726441383361816| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048628.6189156]: 
Returned: 1.052677035331726| 

python DistillationModifier [189 - 1706048629.6452303]: Calling loss_update with:
args: 0.7672756314277649| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048630.7214897]: 
Returned: 1.4088748693466187| 

python DistillationModifier [189 - 1706048632.3269958]: Calling loss_update with:
args: 0.8764539361000061| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048633.747598]: 
Returned: 1.1043099164962769| 

python DistillationModifier [189 - 1706048635.4254398]: Calling loss_update with:
args: 0.5752565860748291| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048636.744213]: 
Returned: 1.0636948347091675| 

python DistillationModifier [189 - 1706048637.8364785]: Calling loss_update with:
args: 0.5424740314483643| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048638.843099]: 
Returned: 0.8834537863731384| 

python DistillationModifier [189 - 1706048639.928993]: Calling loss_update with:
args: 0.5792293548583984| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048640.951243]: 
Returned: 0.8883676528930664| 

python DistillationModifier [189 - 1706048642.119945]: Calling loss_update with:
args: 0.6072909235954285| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048643.1314266]: 
Returned: 1.0744211673736572| 

python DistillationModifier [189 - 1706048644.3486717]: Calling loss_update with:
args: 0.33137911558151245| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048645.4226]: 
Returned: 0.8762747645378113| 

python DistillationModifier [189 - 1706048646.93754]: Calling loss_update with:
args: 0.8266245126724243| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048648.5367885]: 
Returned: 0.9369046092033386| 

python DistillationModifier [189 - 1706048650.1205294]: Calling loss_update with:
args: 0.4231570065021515| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048651.724157]: 
Returned: 0.8387819528579712| 

python DistillationModifier [189 - 1706048653.229452]: Calling loss_update with:
args: 0.46146997809410095| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048654.2345488]: 
Returned: 0.7040925621986389| 

python DistillationModifier [189 - 1706048655.321501]: Calling loss_update with:
args: 0.6116499900817871| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048656.8310566]: 
Returned: 0.8894758224487305| 

python DistillationModifier [189 - 1706048658.3199203]: Calling loss_update with:
args: 0.5343025326728821| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048659.3276656]: 
Returned: 0.9455270171165466| 

python DistillationModifier [189 - 1706048660.935156]: Calling loss_update with:
args: 0.5250125527381897| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048662.528825]: 
Returned: 1.059523582458496| 

python DistillationModifier [189 - 1706048664.1325502]: Calling loss_update with:
args: 0.47663357853889465| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048665.5287285]: 
Returned: 1.115078330039978| 

python DistillationModifier [189 - 1706048666.6247027]: Calling loss_update with:
args: 0.8219772577285767| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048667.7311504]: 
Returned: 1.2569983005523682| 

python DistillationModifier [189 - 1706048668.7498758]: Calling loss_update with:
args: 0.8405166268348694| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048669.8202107]: 
Returned: 1.4804667234420776| 

python DistillationModifier [189 - 1706048670.8521068]: Calling loss_update with:
args: 1.0336949825286865| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048671.9432154]: 
Returned: 1.6120619773864746| 

python DistillationModifier [189 - 1706048673.0261707]: Calling loss_update with:
args: 1.1091824769973755| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048674.4242322]: 
Returned: 1.5664207935333252| 

python DistillationModifier [189 - 1706048676.0361147]: Calling loss_update with:
args: 1.1462432146072388| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048677.6338303]: 
Returned: 2.109497547149658| 

python DistillationModifier [189 - 1706048679.2500167]: Calling loss_update with:
args: 0.7228068113327026| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048680.847385]: 
Returned: 0.9101837277412415| 

python DistillationModifier [189 - 1706048682.2250073]: Calling loss_update with:
args: 0.6345009803771973| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048683.2416742]: 
Returned: 1.0103240013122559| 

python DistillationModifier [189 - 1706048684.3235965]: Calling loss_update with:
args: 0.881572425365448| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048685.3303862]: 
Returned: 1.7474535703659058| 

python DistillationModifier [189 - 1706048686.524135]: Calling loss_update with:
args: 0.8912596106529236| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048687.6396532]: 
Returned: 1.7362706661224365| 

python DistillationModifier [189 - 1706048688.7257626]: Calling loss_update with:
args: 0.36822187900543213| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048689.736628]: 
Returned: 1.0139464139938354| 

python DistillationModifier [189 - 1706048690.8228388]: Calling loss_update with:
args: 0.5653649568557739| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048691.9190705]: 
Returned: 1.143994927406311| 

python DistillationModifier [189 - 1706048692.9339914]: Calling loss_update with:
args: 0.6847392320632935| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048694.1503592]: 
Returned: 0.7764294147491455| 

python DistillationModifier [189 - 1706048695.8268702]: Calling loss_update with:
args: 0.5489843487739563| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048697.4303002]: 
Returned: 1.313103199005127| 

python DistillationModifier [189 - 1706048698.6237595]: Calling loss_update with:
args: 0.8258200287818909| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048699.6369262]: 
Returned: 1.8424227237701416| 

python DistillationModifier [189 - 1706048700.7282267]: Calling loss_update with:
args: 0.8319105505943298| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048701.9410696]: 
Returned: 1.2160139083862305| 

python DistillationModifier [189 - 1706048703.5480785]: Calling loss_update with:
args: 0.5137581825256348| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048705.141157]: 
Returned: 0.838132381439209| 

python DistillationModifier [189 - 1706048706.8207552]: Calling loss_update with:
args: 0.2851913571357727| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048708.4227495]: 
Returned: 0.7372551560401917| 

python DistillationModifier [189 - 1706048710.0255888]: Calling loss_update with:
args: 1.032804012298584| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048711.623196]: 
Returned: 1.982848882675171| 

python DistillationModifier [189 - 1706048713.2319837]: Calling loss_update with:
args: 0.866904616355896| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048714.3447697]: 
Returned: 1.5633381605148315| 

python DistillationModifier [189 - 1706048715.7520108]: Calling loss_update with:
args: 0.3154812455177307| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048716.8295393]: 
Returned: 0.6574703454971313| 

python DistillationModifier [189 - 1706048717.9247413]: Calling loss_update with:
args: 1.6405836343765259| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048719.030708]: 
Returned: 1.7860133647918701| 

python DistillationModifier [189 - 1706048720.1241155]: Calling loss_update with:
args: 0.5040555000305176| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048721.1387181]: 
Returned: 0.9917232990264893| 

python DistillationModifier [189 - 1706048722.5259697]: Calling loss_update with:
args: 0.8348031044006348| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048724.1245456]: 
Returned: 1.339802622795105| 

python DistillationModifier [189 - 1706048725.6505196]: Calling loss_update with:
args: 0.44715601205825806| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048727.325492]: 
Returned: 1.105823040008545| 

python DistillationModifier [189 - 1706048728.934109]: Calling loss_update with:
args: 0.5239805579185486| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048730.5301416]: 
Returned: 1.0053608417510986| 

python DistillationModifier [189 - 1706048732.1489706]: Calling loss_update with:
args: 0.9262862205505371| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048733.7298105]: 
Returned: 0.8150013089179993| 

python DistillationModifier [189 - 1706048735.0490417]: Calling loss_update with:
args: 0.7239272594451904| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048736.1289098]: 
Returned: 1.0167315006256104| 

python DistillationModifier [189 - 1706048737.1488621]: Calling loss_update with:
args: 0.9254955053329468| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048738.2362204]: 
Returned: 1.1243271827697754| 

python DistillationModifier [189 - 1706048739.4224243]: Calling loss_update with:
args: 0.7618529200553894| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048740.4306507]: 
Returned: 1.3522969484329224| 

python DistillationModifier [189 - 1706048741.5330708]: Calling loss_update with:
args: 0.5539329051971436| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048742.543746]: 
Returned: 1.2442529201507568| 

python DistillationModifier [189 - 1706048743.627081]: Calling loss_update with:
args: 0.7669434547424316| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048744.6364374]: 
Returned: 1.45773184299469| 

python DistillationModifier [189 - 1706048745.2288465]: Calling loss_update with:
args: 0.5610778331756592| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048745.741876]: 
Returned: 1.1083641052246094| 

python DistillationModifier [189 - 1706048747.318963]: Calling loss_update with:
args: 0.27407997846603394| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1706048748.337787]: 
Returned: 0.5729258060455322| 

python LearningRateFunctionModifier [189 - 1706048750.3224385]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [189 - 1706048750.3226037]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [189 - 1706048750.3226504]: 0.00011538461538461537
python LearningRateFunctionModifier/ParamGroup1 [189 - 1706048750.3226757]: 0.00011538461538461537
python DistillationModifier/task_loss [189 - 1706048750.3227167]: tensor(0.2741, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [189 - 1706048750.323211]: tensor(0.5729, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [189 - 1706048750.3234525]: tensor(0.5729, grad_fn=<AddBackward0>)
python ConstantPruningModifier [189 - 1706048750.323707]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| 
python ConstantPruningModifier [189 - 1706048750.532682]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [189 - 1706048750.5331526]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [189 - 1706048750.533411]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [189 - 1706048750.5336235]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [189 - 1706048750.533799]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [189 - 1706048750.5344129]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [189 - 1706048750.5349705]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [189 - 1706048750.5352256]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [189 - 1706048750.5354128]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [189 - 1706048750.5356655]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [189 - 1706048750.5358746]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [189 - 1706048750.5363636]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [189 - 1706048750.5368955]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [189 - 1706048750.5371482]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [189 - 1706048750.537332]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [189 - 1706048750.5375068]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [189 - 1706048750.5376747]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [189 - 1706048750.538049]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [189 - 1706048750.5384088]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [189 - 1706048750.5386045]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [189 - 1706048750.5387537]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [189 - 1706048750.538888]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [189 - 1706048750.5390198]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [189 - 1706048750.5394049]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [189 - 1706048750.539776]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [189 - 1706048750.5399473]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [189 - 1706048750.5400906]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [189 - 1706048750.5402246]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [189 - 1706048750.5403569]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [189 - 1706048750.5407588]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [189 - 1706048750.5411103]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [189 - 1706048750.5413055]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [189 - 1706048750.5414364]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [189 - 1706048750.5415895]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [189 - 1706048750.5417552]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [189 - 1706048750.542111]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [189 - 1706048750.542645]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [189 - 1706048750.5428371]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [189 - 1706048750.5429778]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [189 - 1706048750.5431178]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [189 - 1706048750.5432599]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [189 - 1706048750.5436704]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [189 - 1706048750.544121]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [189 - 1706048750.5442967]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [189 - 1706048750.544432]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [189 - 1706048750.5446308]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [189 - 1706048750.544788]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [189 - 1706048750.5451856]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [189 - 1706048750.545636]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [189 - 1706048750.5458188]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [189 - 1706048750.545961]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [189 - 1706048750.546098]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [189 - 1706048750.5462313]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [189 - 1706048750.5466473]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [189 - 1706048750.5470061]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [189 - 1706048750.5471902]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [189 - 1706048750.547323]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [189 - 1706048750.5474565]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [189 - 1706048750.5476534]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [189 - 1706048750.5480127]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [189 - 1706048750.5484676]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [189 - 1706048750.548685]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [189 - 1706048750.5488312]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [189 - 1706048750.5489757]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [189 - 1706048750.5491147]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [189 - 1706048750.5494778]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [189 - 1706048750.5498753]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [189 - 1706048750.5500553]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [189 - 1706048750.550189]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [189 - 1706048750.5503218]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [189 - 1706048750.550454]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [189 - 1706048750.550852]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [189 - 1706048750.5512035]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [189 - 1706048750.5513785]: 0.0
python ParamPruning/classifier.weight [189 - 1706048750.5514464]: 0.0
python DistillationModifier [196 - 1706048787.6290681]: Calling loss_update with:
args: 0.20223933458328247| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.111111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [196 - 1706048788.7206786]: 
Returned: 0.3811314105987549| 

python LearningRateFunctionModifier [196 - 1706048790.756198]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.111111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [196 - 1706048790.7564137]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [196 - 1706048790.7564697]: 0.0001141025641025641
python LearningRateFunctionModifier/ParamGroup1 [196 - 1706048790.7565007]: 0.0001141025641025641
python DistillationModifier/task_loss [196 - 1706048790.7565513]: tensor(0.2022, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [196 - 1706048790.7571104]: tensor(0.3811, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [196 - 1706048790.7573578]: tensor(0.3811, grad_fn=<AddBackward0>)
python ConstantPruningModifier [196 - 1706048790.7576268]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.111111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [196 - 1706048791.0417142]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [196 - 1706048791.042251]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [196 - 1706048791.0426168]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [196 - 1706048791.0428767]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [196 - 1706048791.0430896]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [196 - 1706048791.0439005]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [196 - 1706048791.0447428]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [196 - 1706048791.0450132]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [196 - 1706048791.0452292]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [196 - 1706048791.045482]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [196 - 1706048791.045726]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [196 - 1706048791.046364]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [196 - 1706048791.0470285]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [196 - 1706048791.0472693]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [196 - 1706048791.0475092]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [196 - 1706048791.0477366]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [196 - 1706048791.0479362]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [196 - 1706048791.0485277]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [196 - 1706048791.0491216]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [196 - 1706048791.0493639]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [196 - 1706048791.04961]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [196 - 1706048791.0498166]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [196 - 1706048791.0500128]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [196 - 1706048791.0505836]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [196 - 1706048791.0512633]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [196 - 1706048791.0515294]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [196 - 1706048791.0517921]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [196 - 1706048791.052019]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [196 - 1706048791.0522418]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [196 - 1706048791.1169262]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [196 - 1706048791.1175027]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [196 - 1706048791.1177561]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [196 - 1706048791.1179621]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [196 - 1706048791.11816]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [196 - 1706048791.1183558]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [196 - 1706048791.1189349]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [196 - 1706048791.1196551]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [196 - 1706048791.1198897]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [196 - 1706048791.1200922]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [196 - 1706048791.120291]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [196 - 1706048791.1204844]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [196 - 1706048791.1211097]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [196 - 1706048791.1217585]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [196 - 1706048791.1219878]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [196 - 1706048791.122182]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [196 - 1706048791.1223757]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [196 - 1706048791.1225917]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [196 - 1706048791.1231918]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [196 - 1706048791.123801]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [196 - 1706048791.1240325]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [196 - 1706048791.1242313]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [196 - 1706048791.124429]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [196 - 1706048791.1246576]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [196 - 1706048791.1252599]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [196 - 1706048791.1258593]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [196 - 1706048791.1260865]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [196 - 1706048791.1263423]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [196 - 1706048791.1265516]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [196 - 1706048791.1267745]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [196 - 1706048791.1273565]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [196 - 1706048791.1279635]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [196 - 1706048791.1282265]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [196 - 1706048791.1284502]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [196 - 1706048791.1287024]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [196 - 1706048791.1289268]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [196 - 1706048791.1295347]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [196 - 1706048791.1301339]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [196 - 1706048791.1304042]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [196 - 1706048791.1306722]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [196 - 1706048791.130893]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [196 - 1706048791.1310873]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [196 - 1706048791.1316833]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [196 - 1706048791.132282]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [196 - 1706048791.1325376]: 0.0
python ParamPruning/classifier.weight [196 - 1706048791.1326654]: 0.0
python DistillationModifier [203 - 1706048827.4370303]: Calling loss_update with:
args: 0.2729588747024536| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.2222222222222223| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [203 - 1706048829.1359396]: 
Returned: 0.5714983344078064| 

python LearningRateFunctionModifier [203 - 1706048831.9707375]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.2222222222222223| steps_per_epoch: 63| 
python LearningRateFunctionModifier [203 - 1706048831.9708874]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [203 - 1706048831.9709356]: 0.0001128205128205128
python LearningRateFunctionModifier/ParamGroup1 [203 - 1706048831.9709625]: 0.0001128205128205128
python DistillationModifier/task_loss [203 - 1706048831.9710047]: tensor(0.2730, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [203 - 1706048831.9714882]: tensor(0.5715, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [203 - 1706048831.9717643]: tensor(0.5715, grad_fn=<AddBackward0>)
python ConstantPruningModifier [203 - 1706048831.9720004]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.2222222222222223| steps_per_epoch: 63| 
python ConstantPruningModifier [203 - 1706048832.1496518]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [203 - 1706048832.1501384]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [203 - 1706048832.1503582]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [203 - 1706048832.1505191]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [203 - 1706048832.1506932]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [203 - 1706048832.1512575]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [203 - 1706048832.1518672]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [203 - 1706048832.1520839]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [203 - 1706048832.152221]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [203 - 1706048832.152365]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [203 - 1706048832.1525013]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [203 - 1706048832.2169735]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [203 - 1706048832.2175007]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [203 - 1706048832.2177408]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [203 - 1706048832.2178903]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [203 - 1706048832.218035]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [203 - 1706048832.2181833]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [203 - 1706048832.218637]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [203 - 1706048832.219096]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [203 - 1706048832.2193046]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [203 - 1706048832.2194426]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [203 - 1706048832.2196038]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [203 - 1706048832.219752]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [203 - 1706048832.2202125]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [203 - 1706048832.2206655]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [203 - 1706048832.2208672]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [203 - 1706048832.2210143]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [203 - 1706048832.2211518]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [203 - 1706048832.2212877]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [203 - 1706048832.2216861]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [203 - 1706048832.222161]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [203 - 1706048832.2223635]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [203 - 1706048832.2224953]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [203 - 1706048832.22265]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [203 - 1706048832.222801]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [203 - 1706048832.2232912]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [203 - 1706048832.2237558]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [203 - 1706048832.2239602]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [203 - 1706048832.224099]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [203 - 1706048832.224238]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [203 - 1706048832.2243752]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [203 - 1706048832.2248359]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [203 - 1706048832.2253146]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [203 - 1706048832.225514]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [203 - 1706048832.2256722]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [203 - 1706048832.2258127]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [203 - 1706048832.225946]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [203 - 1706048832.226422]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [203 - 1706048832.226887]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [203 - 1706048832.2270916]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [203 - 1706048832.2272375]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [203 - 1706048832.227379]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [203 - 1706048832.2275167]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [203 - 1706048832.227966]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [203 - 1706048832.2284057]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [203 - 1706048832.2286363]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [203 - 1706048832.2287896]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [203 - 1706048832.2289343]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [203 - 1706048832.2290678]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [203 - 1706048832.2294927]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [203 - 1706048832.2299495]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [203 - 1706048832.2301576]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [203 - 1706048832.2302995]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [203 - 1706048832.2304363]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [203 - 1706048832.2305906]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [203 - 1706048832.2310555]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [203 - 1706048832.231679]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [203 - 1706048832.2319531]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [203 - 1706048832.2321832]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [203 - 1706048832.232387]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [203 - 1706048832.2326121]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [203 - 1706048832.233198]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [203 - 1706048832.2338684]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [203 - 1706048832.2341347]: 0.0
python ParamPruning/classifier.weight [203 - 1706048832.2342494]: 0.0
python DistillationModifier [210 - 1706048867.437941]: Calling loss_update with:
args: 0.19299578666687012| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.3333333333333335| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [210 - 1706048868.5243938]: 
Returned: 0.47183120250701904| 

python LearningRateFunctionModifier [210 - 1706048870.4670837]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.3333333333333335| steps_per_epoch: 63| 
python LearningRateFunctionModifier [210 - 1706048870.4672298]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [210 - 1706048870.4672792]: 0.00011153846153846153
python LearningRateFunctionModifier/ParamGroup1 [210 - 1706048870.4673245]: 0.00011153846153846153
python DistillationModifier/task_loss [210 - 1706048870.4673705]: tensor(0.1930, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [210 - 1706048870.4678817]: tensor(0.4718, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [210 - 1706048870.4681282]: tensor(0.4718, grad_fn=<AddBackward0>)
python ConstantPruningModifier [210 - 1706048870.468373]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.3333333333333335| steps_per_epoch: 63| 
python ConstantPruningModifier [210 - 1706048870.6489828]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [210 - 1706048870.6494403]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [210 - 1706048870.6496668]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [210 - 1706048870.649838]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [210 - 1706048870.6499867]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [210 - 1706048870.6505456]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [210 - 1706048870.6510348]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [210 - 1706048870.651237]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [210 - 1706048870.6513793]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [210 - 1706048870.6515145]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [210 - 1706048870.6516805]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [210 - 1706048870.6520505]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [210 - 1706048870.6524196]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [210 - 1706048870.7166011]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [210 - 1706048870.7167764]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [210 - 1706048870.7169173]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [210 - 1706048870.7170522]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [210 - 1706048870.7174003]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [210 - 1706048870.7178662]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [210 - 1706048870.7180812]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [210 - 1706048870.7182262]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [210 - 1706048870.71837]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [210 - 1706048870.7185214]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [210 - 1706048870.7190278]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [210 - 1706048870.7194898]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [210 - 1706048870.7196922]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [210 - 1706048870.7198532]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [210 - 1706048870.7200096]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [210 - 1706048870.7201614]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [210 - 1706048870.7205837]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [210 - 1706048870.7210643]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [210 - 1706048870.7212274]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [210 - 1706048870.7213652]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [210 - 1706048870.7215176]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [210 - 1706048870.7216702]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [210 - 1706048870.7220283]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [210 - 1706048870.722366]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [210 - 1706048870.7225277]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [210 - 1706048870.7226825]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [210 - 1706048870.722822]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [210 - 1706048870.7229567]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [210 - 1706048870.7233388]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [210 - 1706048870.7237449]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [210 - 1706048870.7239065]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [210 - 1706048870.7240503]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [210 - 1706048870.7241871]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [210 - 1706048870.724326]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [210 - 1706048870.724801]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [210 - 1706048870.725215]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [210 - 1706048870.7253916]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [210 - 1706048870.725525]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [210 - 1706048870.725687]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [210 - 1706048870.7258317]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [210 - 1706048870.7262638]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [210 - 1706048870.7266867]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [210 - 1706048870.7268653]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [210 - 1706048870.7269957]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [210 - 1706048870.7271235]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [210 - 1706048870.7272525]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [210 - 1706048870.7276134]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [210 - 1706048870.7279568]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [210 - 1706048870.7281177]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [210 - 1706048870.7282498]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [210 - 1706048870.7283843]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [210 - 1706048870.7285147]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [210 - 1706048870.728902]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [210 - 1706048870.7292938]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [210 - 1706048870.7294645]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [210 - 1706048870.72961]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [210 - 1706048870.7297509]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [210 - 1706048870.7298834]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [210 - 1706048870.73022]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [210 - 1706048870.730607]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [210 - 1706048870.7307851]: 0.0
python ParamPruning/classifier.weight [210 - 1706048870.7308517]: 0.0
python DistillationModifier [217 - 1706048904.617954]: Calling loss_update with:
args: 0.20465096831321716| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.4444444444444446| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [217 - 1706048906.1180236]: 
Returned: 0.4387374222278595| 

python LearningRateFunctionModifier [217 - 1706048908.0553842]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.4444444444444446| steps_per_epoch: 63| 
python LearningRateFunctionModifier [217 - 1706048908.0555239]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [217 - 1706048908.0555875]: 0.00011025641025641025
python LearningRateFunctionModifier/ParamGroup1 [217 - 1706048908.0556152]: 0.00011025641025641025
python DistillationModifier/task_loss [217 - 1706048908.0556574]: tensor(0.2047, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [217 - 1706048908.0561337]: tensor(0.4387, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [217 - 1706048908.0563824]: tensor(0.4387, grad_fn=<AddBackward0>)
python ConstantPruningModifier [217 - 1706048908.0566638]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.4444444444444446| steps_per_epoch: 63| 
python ConstantPruningModifier [217 - 1706048908.2378259]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [217 - 1706048908.2382882]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [217 - 1706048908.2385488]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [217 - 1706048908.2387648]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [217 - 1706048908.238932]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [217 - 1706048908.2395382]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [217 - 1706048908.2401664]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [217 - 1706048908.240378]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [217 - 1706048908.2405992]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [217 - 1706048908.2408288]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [217 - 1706048908.241022]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [217 - 1706048908.2415004]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [217 - 1706048908.2420282]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [217 - 1706048908.2422597]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [217 - 1706048908.2424474]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [217 - 1706048908.2426517]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [217 - 1706048908.2428553]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [217 - 1706048908.2432396]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [217 - 1706048908.2437146]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [217 - 1706048908.243914]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [217 - 1706048908.2441034]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [217 - 1706048908.244286]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [217 - 1706048908.244466]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [217 - 1706048908.2449389]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [217 - 1706048908.2453923]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [217 - 1706048908.245603]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [217 - 1706048908.245795]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [217 - 1706048908.2459786]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [217 - 1706048908.2461782]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [217 - 1706048908.2466009]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [217 - 1706048908.2470531]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [217 - 1706048908.2472532]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [217 - 1706048908.2474298]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [217 - 1706048908.2476392]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [217 - 1706048908.2478468]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [217 - 1706048908.2482915]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [217 - 1706048908.2487695]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [217 - 1706048908.248975]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [217 - 1706048908.249159]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [217 - 1706048908.2493465]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [217 - 1706048908.2495434]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [217 - 1706048908.250005]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [217 - 1706048908.2504537]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [217 - 1706048908.2506654]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [217 - 1706048908.2508073]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [217 - 1706048908.2509952]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [217 - 1706048908.251147]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [217 - 1706048908.2516026]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [217 - 1706048908.2520545]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [217 - 1706048908.2522871]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [217 - 1706048908.2524903]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [217 - 1706048908.3167279]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [217 - 1706048908.3169873]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [217 - 1706048908.3175423]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [217 - 1706048908.3181136]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [217 - 1706048908.3183227]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [217 - 1706048908.3185134]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [217 - 1706048908.3187032]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [217 - 1706048908.318865]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [217 - 1706048908.3192947]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [217 - 1706048908.3197396]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [217 - 1706048908.3199446]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [217 - 1706048908.3201287]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [217 - 1706048908.3202858]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [217 - 1706048908.3204713]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [217 - 1706048908.3209314]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [217 - 1706048908.3213782]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [217 - 1706048908.3215997]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [217 - 1706048908.3218052]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [217 - 1706048908.3219652]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [217 - 1706048908.3221505]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [217 - 1706048908.322554]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [217 - 1706048908.3229978]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [217 - 1706048908.3232305]: 0.0
python ParamPruning/classifier.weight [217 - 1706048908.3233378]: 0.0
python DistillationModifier [224 - 1706048942.4317408]: Calling loss_update with:
args: 0.5801306962966919| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.5555555555555554| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [224 - 1706048943.823588]: 
Returned: 0.6817061305046082| 

python LearningRateFunctionModifier [224 - 1706048945.7702513]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.5555555555555554| steps_per_epoch: 63| 
python LearningRateFunctionModifier [224 - 1706048945.770402]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [224 - 1706048945.7704506]: 0.00010897435897435896
python LearningRateFunctionModifier/ParamGroup1 [224 - 1706048945.7704766]: 0.00010897435897435896
python DistillationModifier/task_loss [224 - 1706048945.7705166]: tensor(0.5801, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [224 - 1706048945.771051]: tensor(0.6817, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [224 - 1706048945.7712965]: tensor(0.6817, grad_fn=<AddBackward0>)
python ConstantPruningModifier [224 - 1706048945.7715263]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.5555555555555554| steps_per_epoch: 63| 
python ConstantPruningModifier [224 - 1706048945.9504993]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [224 - 1706048945.9509816]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [224 - 1706048945.9512444]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [224 - 1706048945.9514744]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [224 - 1706048945.9516747]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [224 - 1706048945.952266]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [224 - 1706048946.0174332]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [224 - 1706048946.0177355]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [224 - 1706048946.017962]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [224 - 1706048946.0181732]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [224 - 1706048946.0183513]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [224 - 1706048946.018872]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [224 - 1706048946.0193555]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [224 - 1706048946.0195966]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [224 - 1706048946.0197995]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [224 - 1706048946.0199926]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [224 - 1706048946.0202034]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [224 - 1706048946.0207086]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [224 - 1706048946.021171]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [224 - 1706048946.0213776]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [224 - 1706048946.0215604]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [224 - 1706048946.0217593]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [224 - 1706048946.0219376]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [224 - 1706048946.0223022]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [224 - 1706048946.0226898]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [224 - 1706048946.022872]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [224 - 1706048946.0230472]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [224 - 1706048946.0232024]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [224 - 1706048946.0233483]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [224 - 1706048946.0237255]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [224 - 1706048946.0240896]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [224 - 1706048946.0242567]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [224 - 1706048946.0244377]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [224 - 1706048946.0246031]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [224 - 1706048946.0247762]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [224 - 1706048946.025142]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [224 - 1706048946.025492]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [224 - 1706048946.0257]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [224 - 1706048946.0258708]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [224 - 1706048946.0260394]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [224 - 1706048946.0262313]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [224 - 1706048946.026593]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [224 - 1706048946.0269637]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [224 - 1706048946.0271435]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [224 - 1706048946.0272932]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [224 - 1706048946.0274534]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [224 - 1706048946.027614]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [224 - 1706048946.0279956]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [224 - 1706048946.0283477]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [224 - 1706048946.0285275]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [224 - 1706048946.0287154]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [224 - 1706048946.0289114]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [224 - 1706048946.0290945]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [224 - 1706048946.029447]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [224 - 1706048946.0298495]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [224 - 1706048946.030027]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [224 - 1706048946.0301805]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [224 - 1706048946.0303116]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [224 - 1706048946.030471]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [224 - 1706048946.0308566]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [224 - 1706048946.0312135]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [224 - 1706048946.0313895]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [224 - 1706048946.0315492]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [224 - 1706048946.0317404]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [224 - 1706048946.0318902]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [224 - 1706048946.0322344]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [224 - 1706048946.0326314]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [224 - 1706048946.0328166]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [224 - 1706048946.0329902]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [224 - 1706048946.0331264]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [224 - 1706048946.0332792]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [224 - 1706048946.033685]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [224 - 1706048946.0340397]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [224 - 1706048946.0342088]: 0.0
python ParamPruning/classifier.weight [224 - 1706048946.0342755]: 0.0
python DistillationModifier [231 - 1706048982.2172177]: Calling loss_update with:
args: 0.49832412600517273| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.6666666666666665| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [231 - 1706048983.327358]: 
Returned: 0.8873710036277771| 

python LearningRateFunctionModifier [231 - 1706048985.2656598]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.6666666666666665| steps_per_epoch: 63| 
python LearningRateFunctionModifier [231 - 1706048985.2658017]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [231 - 1706048985.2658482]: 0.0001076923076923077
python LearningRateFunctionModifier/ParamGroup1 [231 - 1706048985.2658744]: 0.0001076923076923077
python DistillationModifier/task_loss [231 - 1706048985.2659156]: tensor(0.4983, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [231 - 1706048985.2663982]: tensor(0.8874, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [231 - 1706048985.2666588]: tensor(0.8874, grad_fn=<AddBackward0>)
python ConstantPruningModifier [231 - 1706048985.266897]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.6666666666666665| steps_per_epoch: 63| 
python ConstantPruningModifier [231 - 1706048985.4484115]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [231 - 1706048985.4489164]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [231 - 1706048985.4491236]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [231 - 1706048985.4492838]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [231 - 1706048985.4494267]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [231 - 1706048985.4500222]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [231 - 1706048985.4506488]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [231 - 1706048985.450857]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [231 - 1706048985.4510536]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [231 - 1706048985.451196]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [231 - 1706048985.4513829]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [231 - 1706048985.4518628]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [231 - 1706048985.4523149]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [231 - 1706048985.452516]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [231 - 1706048985.516724]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [231 - 1706048985.5169904]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [231 - 1706048985.5171614]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [231 - 1706048985.517706]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [231 - 1706048985.5182319]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [231 - 1706048985.5184603]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [231 - 1706048985.5186908]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [231 - 1706048985.5188491]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [231 - 1706048985.51903]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [231 - 1706048985.5194306]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [231 - 1706048985.5198326]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [231 - 1706048985.5200214]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [231 - 1706048985.520163]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [231 - 1706048985.52034]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [231 - 1706048985.5204873]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [231 - 1706048985.5208733]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [231 - 1706048985.5212297]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [231 - 1706048985.5213993]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [231 - 1706048985.5215251]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [231 - 1706048985.521722]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [231 - 1706048985.5218644]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [231 - 1706048985.5222237]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [231 - 1706048985.5227437]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [231 - 1706048985.5229225]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [231 - 1706048985.5230613]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [231 - 1706048985.5232344]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [231 - 1706048985.5233738]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [231 - 1706048985.523778]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [231 - 1706048985.5241666]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [231 - 1706048985.5243273]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [231 - 1706048985.5244584]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [231 - 1706048985.524653]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [231 - 1706048985.5248036]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [231 - 1706048985.5251615]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [231 - 1706048985.5256495]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [231 - 1706048985.5258236]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [231 - 1706048985.5259774]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [231 - 1706048985.5261776]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [231 - 1706048985.5263178]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [231 - 1706048985.526721]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [231 - 1706048985.527127]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [231 - 1706048985.5273001]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [231 - 1706048985.52744]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [231 - 1706048985.5276105]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [231 - 1706048985.527807]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [231 - 1706048985.5282383]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [231 - 1706048985.5286772]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [231 - 1706048985.5288813]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [231 - 1706048985.529057]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [231 - 1706048985.5292888]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [231 - 1706048985.52945]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [231 - 1706048985.5298688]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [231 - 1706048985.530362]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [231 - 1706048985.530536]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [231 - 1706048985.5307014]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [231 - 1706048985.5308828]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [231 - 1706048985.5310218]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [231 - 1706048985.5314064]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [231 - 1706048985.5318494]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [231 - 1706048985.5320199]: 0.0
python ParamPruning/classifier.weight [231 - 1706048985.532088]: 0.0
python DistillationModifier [238 - 1706049021.5319393]: Calling loss_update with:
args: 0.13842368125915527| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.7777777777777777| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [238 - 1706049023.1318097]: 
Returned: 0.3532165586948395| 

python LearningRateFunctionModifier [238 - 1706049026.1732688]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.7777777777777777| steps_per_epoch: 63| 
python LearningRateFunctionModifier [238 - 1706049026.173412]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [238 - 1706049026.1734583]: 0.0001064102564102564
python LearningRateFunctionModifier/ParamGroup1 [238 - 1706049026.1734843]: 0.0001064102564102564
python DistillationModifier/task_loss [238 - 1706049026.173525]: tensor(0.1384, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [238 - 1706049026.1740267]: tensor(0.3532, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [238 - 1706049026.1742685]: tensor(0.3532, grad_fn=<AddBackward0>)
python ConstantPruningModifier [238 - 1706049026.1745117]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.7777777777777777| steps_per_epoch: 63| 
python ConstantPruningModifier [238 - 1706049026.5196674]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [238 - 1706049026.520153]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [238 - 1706049026.5203702]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [238 - 1706049026.5206544]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [238 - 1706049026.520875]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [238 - 1706049026.5216565]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [238 - 1706049026.5222375]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [238 - 1706049026.5224683]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [238 - 1706049026.5227165]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [238 - 1706049026.5228887]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [238 - 1706049026.523049]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [238 - 1706049026.5235949]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [238 - 1706049026.5241528]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [238 - 1706049026.5243747]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [238 - 1706049026.5245364]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [238 - 1706049026.5247328]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [238 - 1706049026.5248969]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [238 - 1706049026.525413]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [238 - 1706049026.5259585]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [238 - 1706049026.526163]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [238 - 1706049026.5263276]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [238 - 1706049026.5264935]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [238 - 1706049026.526673]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [238 - 1706049026.5271943]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [238 - 1706049026.5277371]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [238 - 1706049026.527937]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [238 - 1706049026.5280995]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [238 - 1706049026.5282578]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [238 - 1706049026.5284152]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [238 - 1706049026.528962]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [238 - 1706049026.5294936]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [238 - 1706049026.5297058]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [238 - 1706049026.5299566]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [238 - 1706049026.5301363]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [238 - 1706049026.5302958]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [238 - 1706049026.530836]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [238 - 1706049026.5313606]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [238 - 1706049026.5315537]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [238 - 1706049026.5317445]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [238 - 1706049026.531905]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [238 - 1706049026.5320632]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [238 - 1706049026.532829]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [238 - 1706049026.5334375]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [238 - 1706049026.5336566]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [238 - 1706049026.5338235]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [238 - 1706049026.5339837]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [238 - 1706049026.5341408]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [238 - 1706049026.5346968]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [238 - 1706049026.5352252]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [238 - 1706049026.5354195]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [238 - 1706049026.5355964]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [238 - 1706049026.535767]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [238 - 1706049026.5359337]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [238 - 1706049026.5364547]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [238 - 1706049026.5370076]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [238 - 1706049026.5372043]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [238 - 1706049026.5373652]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [238 - 1706049026.5375233]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [238 - 1706049026.537709]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [238 - 1706049026.5382328]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [238 - 1706049026.5387733]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [238 - 1706049026.5389643]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [238 - 1706049026.5391266]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [238 - 1706049026.5392866]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [238 - 1706049026.5394454]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [238 - 1706049026.5399795]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [238 - 1706049026.5405016]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [238 - 1706049026.5407176]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [238 - 1706049026.540884]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [238 - 1706049026.541059]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [238 - 1706049026.5412185]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [238 - 1706049026.5417533]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [238 - 1706049026.5422795]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [238 - 1706049026.5424664]: 0.0
python ParamPruning/classifier.weight [238 - 1706049026.5425336]: 0.0
python DistillationModifier [245 - 1706049061.3180099]: Calling loss_update with:
args: 0.1175975352525711| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.888888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [245 - 1706049062.6311412]: 
Returned: 0.3550012409687042| 

python LearningRateFunctionModifier [245 - 1706049064.676307]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.888888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [245 - 1706049064.6764562]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [245 - 1706049064.6765025]: 0.00010512820512820513
python LearningRateFunctionModifier/ParamGroup1 [245 - 1706049064.6765285]: 0.00010512820512820513
python DistillationModifier/task_loss [245 - 1706049064.6765885]: tensor(0.1176, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [245 - 1706049064.6770837]: tensor(0.3550, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [245 - 1706049064.6773245]: tensor(0.3550, grad_fn=<AddBackward0>)
python ConstantPruningModifier [245 - 1706049064.6775575]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.888888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [245 - 1706049064.919591]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [245 - 1706049064.9200609]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [245 - 1706049064.9203322]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [245 - 1706049064.920507]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [245 - 1706049064.9207242]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [245 - 1706049064.9212856]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [245 - 1706049064.9219258]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [245 - 1706049064.9221473]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [245 - 1706049064.9223416]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [245 - 1706049064.9224882]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [245 - 1706049064.9226673]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [245 - 1706049064.9231071]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [245 - 1706049064.923708]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [245 - 1706049064.923904]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [245 - 1706049064.9240935]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [245 - 1706049064.9242585]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [245 - 1706049064.9244406]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [245 - 1706049064.9249146]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [245 - 1706049064.9254732]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [245 - 1706049064.925712]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [245 - 1706049064.9259079]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [245 - 1706049064.926054]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [245 - 1706049064.9262083]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [245 - 1706049064.9267364]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [245 - 1706049064.9272304]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [245 - 1706049064.9274325]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [245 - 1706049064.9276483]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [245 - 1706049064.9278133]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [245 - 1706049064.9279664]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [245 - 1706049064.9284234]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [245 - 1706049064.9289117]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [245 - 1706049064.929109]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [245 - 1706049064.9292815]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [245 - 1706049064.9294262]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [245 - 1706049064.9295921]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [245 - 1706049064.930028]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [245 - 1706049064.9304497]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [245 - 1706049064.9306645]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [245 - 1706049064.930867]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [245 - 1706049064.9310484]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [245 - 1706049064.931202]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [245 - 1706049064.931637]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [245 - 1706049064.932089]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [245 - 1706049064.9322808]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [245 - 1706049064.9324496]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [245 - 1706049064.932648]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [245 - 1706049064.932834]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [245 - 1706049064.9332354]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [245 - 1706049064.9336848]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [245 - 1706049064.9338906]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [245 - 1706049064.934062]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [245 - 1706049064.9342055]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [245 - 1706049064.9343688]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [245 - 1706049064.9347925]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [245 - 1706049064.9353347]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [245 - 1706049064.9355383]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [245 - 1706049064.9357502]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [245 - 1706049064.9358997]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [245 - 1706049064.9360447]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [245 - 1706049064.936449]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [245 - 1706049064.9370062]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [245 - 1706049064.9372144]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [245 - 1706049064.9374]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [245 - 1706049064.9375477]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [245 - 1706049064.937743]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [245 - 1706049064.9381218]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [245 - 1706049064.9386191]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [245 - 1706049064.9388232]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [245 - 1706049064.9389997]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [245 - 1706049064.939139]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [245 - 1706049064.9392834]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [245 - 1706049064.9397323]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [245 - 1706049064.940174]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [245 - 1706049064.940368]: 0.0
python ParamPruning/classifier.weight [245 - 1706049064.9404361]: 0.0
python DistillationModifier [252 - 1706049094.8500335]: Calling loss_update with:
args: 0.6050013303756714| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049095.9175193]: 
Returned: 1.1548269987106323| 

python DistillationModifier [252 - 1706049097.222442]: Calling loss_update with:
args: 1.205971121788025| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049098.549703]: 
Returned: 1.9505252838134766| 

python DistillationModifier [252 - 1706049100.221233]: Calling loss_update with:
args: 1.0240800380706787| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049101.8189988]: 
Returned: 1.1495054960250854| 

python DistillationModifier [252 - 1706049103.4252377]: Calling loss_update with:
args: 0.6273388266563416| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049105.0167696]: 
Returned: 0.8016630411148071| 

python DistillationModifier [252 - 1706049106.2334108]: Calling loss_update with:
args: 1.046608328819275| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049107.2468324]: 
Returned: 1.5668693780899048| 

python DistillationModifier [252 - 1706049108.344029]: Calling loss_update with:
args: 0.7995400428771973| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049109.4208713]: 
Returned: 1.4960826635360718| 

python DistillationModifier [252 - 1706049110.5178537]: Calling loss_update with:
args: 0.3206050992012024| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049111.5312984]: 
Returned: 0.8560421466827393| 

python DistillationModifier [252 - 1706049112.5506258]: Calling loss_update with:
args: 0.8472772240638733| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049113.6352465]: 
Returned: 0.9662404656410217| 

python DistillationModifier [252 - 1706049114.7272341]: Calling loss_update with:
args: 0.6437462568283081| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049115.9179168]: 
Returned: 0.5260140895843506| 

python DistillationModifier [252 - 1706049116.9429467]: Calling loss_update with:
args: 0.8594127297401428| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049118.1336725]: 
Returned: 1.3956393003463745| 

python DistillationModifier [252 - 1706049119.3382072]: Calling loss_update with:
args: 0.49742141366004944| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049120.3520484]: 
Returned: 0.9408214092254639| 

python DistillationModifier [252 - 1706049121.6493933]: Calling loss_update with:
args: 1.1002346277236938| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049123.126608]: 
Returned: 1.5421569347381592| 

python DistillationModifier [252 - 1706049124.217782]: Calling loss_update with:
args: 0.7527725100517273| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049125.41696]: 
Returned: 1.5277758836746216| 

python DistillationModifier [252 - 1706049126.71898]: Calling loss_update with:
args: 0.703146755695343| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049127.7316117]: 
Returned: 0.5669589638710022| 

python DistillationModifier [252 - 1706049128.9240804]: Calling loss_update with:
args: 1.217122197151184| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049130.02175]: 
Returned: 1.8139595985412598| 

python DistillationModifier [252 - 1706049131.0457358]: Calling loss_update with:
args: 0.6741860508918762| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049132.1182506]: 
Returned: 1.1640558242797852| 

python DistillationModifier [252 - 1706049133.1449277]: Calling loss_update with:
args: 0.610007643699646| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049134.2196922]: 
Returned: 0.856849730014801| 

python DistillationModifier [252 - 1706049135.2392404]: Calling loss_update with:
args: 0.8916324377059937| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049136.2485046]: 
Returned: 1.5005227327346802| 

python DistillationModifier [252 - 1706049137.3452785]: Calling loss_update with:
args: 0.7183521389961243| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049138.4272327]: 
Returned: 0.9376974701881409| 

python DistillationModifier [252 - 1706049139.4519064]: Calling loss_update with:
args: 0.5395550727844238| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049140.5329454]: 
Returned: 1.1102499961853027| 

python DistillationModifier [252 - 1706049141.621829]: Calling loss_update with:
args: 0.683012068271637| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049143.1477196]: 
Returned: 1.1202486753463745| 

python DistillationModifier [252 - 1706049144.8257108]: Calling loss_update with:
args: 0.6656230688095093| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049146.2349243]: 
Returned: 0.989879846572876| 

python DistillationModifier [252 - 1706049147.3238363]: Calling loss_update with:
args: 0.5449032187461853| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049148.337473]: 
Returned: 1.1738786697387695| 

python DistillationModifier [252 - 1706049149.423972]: Calling loss_update with:
args: 0.45252367854118347| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049150.4366696]: 
Returned: 0.9621732234954834| 

python DistillationModifier [252 - 1706049151.6321912]: Calling loss_update with:
args: 0.8267127871513367| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049153.2356493]: 
Returned: 0.8269286155700684| 

python DistillationModifier [252 - 1706049154.5181603]: Calling loss_update with:
args: 0.29029253125190735| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049155.5300767]: 
Returned: 0.5318343043327332| 

python DistillationModifier [252 - 1706049156.6185422]: Calling loss_update with:
args: 0.307609885931015| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049157.8172426]: 
Returned: 0.4007275700569153| 

python DistillationModifier [252 - 1706049159.1307003]: Calling loss_update with:
args: 0.6140893697738647| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049160.7347934]: 
Returned: 0.9071716070175171| 

python DistillationModifier [252 - 1706049161.8249738]: Calling loss_update with:
args: 0.36179134249687195| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049163.2258852]: 
Returned: 0.7214788198471069| 

python DistillationModifier [252 - 1706049164.5220468]: Calling loss_update with:
args: 0.41826778650283813| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049165.5337296]: 
Returned: 0.8560754656791687| 

python DistillationModifier [252 - 1706049167.1408298]: Calling loss_update with:
args: 0.5999274253845215| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049168.8236518]: 
Returned: 1.280644416809082| 

python DistillationModifier [252 - 1706049170.4307144]: Calling loss_update with:
args: 0.9128183126449585| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049172.0228276]: 
Returned: 1.134509801864624| 

python DistillationModifier [252 - 1706049173.1517026]: Calling loss_update with:
args: 0.8496803045272827| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049174.33051]: 
Returned: 1.4118669033050537| 

python DistillationModifier [252 - 1706049175.4375086]: Calling loss_update with:
args: 1.0981391668319702| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049176.520136]: 
Returned: 1.7036817073822021| 

python DistillationModifier [252 - 1706049177.5516474]: Calling loss_update with:
args: 1.351175308227539| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049179.1470952]: 
Returned: 1.7136865854263306| 

python DistillationModifier [252 - 1706049180.8255253]: Calling loss_update with:
args: 1.13606595993042| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049182.4166098]: 
Returned: 1.9909604787826538| 

python DistillationModifier [252 - 1706049184.0243008]: Calling loss_update with:
args: 0.9748287796974182| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049185.6245005]: 
Returned: 1.025128722190857| 

python DistillationModifier [252 - 1706049186.6461506]: Calling loss_update with:
args: 0.5828293561935425| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049187.7239153]: 
Returned: 0.7685450911521912| 

python DistillationModifier [252 - 1706049189.0194917]: Calling loss_update with:
args: 0.7830621600151062| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049190.44031]: 
Returned: 1.4834840297698975| 

python DistillationModifier [252 - 1706049191.525732]: Calling loss_update with:
args: 1.0735557079315186| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049192.5375104]: 
Returned: 1.974369764328003| 

python DistillationModifier [252 - 1706049193.626005]: Calling loss_update with:
args: 0.49235042929649353| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049194.6365163]: 
Returned: 0.6996102333068848| 

python DistillationModifier [252 - 1706049196.1303513]: Calling loss_update with:
args: 0.5695379972457886| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049197.6544003]: 
Returned: 0.9645800590515137| 

python DistillationModifier [252 - 1706049199.3176174]: Calling loss_update with:
args: 0.555476725101471| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049200.847342]: 
Returned: 0.6528000831604004| 

python DistillationModifier [252 - 1706049202.2510583]: Calling loss_update with:
args: 0.5197668671607971| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049203.3392982]: 
Returned: 1.2614728212356567| 

python DistillationModifier [252 - 1706049205.017014]: Calling loss_update with:
args: 0.6627714037895203| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049206.623645]: 
Returned: 1.6176501512527466| 

python DistillationModifier [252 - 1706049208.2471843]: Calling loss_update with:
args: 0.6840326189994812| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049209.8476462]: 
Returned: 0.8730979561805725| 

python DistillationModifier [252 - 1706049211.5214417]: Calling loss_update with:
args: 0.5677445530891418| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049213.124498]: 
Returned: 0.9938933849334717| 

python DistillationModifier [252 - 1706049214.7359383]: Calling loss_update with:
args: 0.17984101176261902| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049216.3240418]: 
Returned: 0.4913203716278076| 

python DistillationModifier [252 - 1706049217.9323528]: Calling loss_update with:
args: 0.9688323736190796| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049219.334958]: 
Returned: 1.877174735069275| 

python DistillationModifier [252 - 1706049220.5339613]: Calling loss_update with:
args: 1.0098310708999634| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049221.547599]: 
Returned: 1.7339855432510376| 

python DistillationModifier [252 - 1706049222.6493847]: Calling loss_update with:
args: 0.2871834337711334| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049223.7192447]: 
Returned: 0.5682808756828308| 

python DistillationModifier [252 - 1706049225.3360903]: Calling loss_update with:
args: 1.9490801095962524| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049226.8378453]: 
Returned: 1.9745802879333496| 

python DistillationModifier [252 - 1706049228.1396418]: Calling loss_update with:
args: 0.6360749006271362| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049229.420796]: 
Returned: 1.0208728313446045| 

python DistillationModifier [252 - 1706049230.4462364]: Calling loss_update with:
args: 0.5894965529441833| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049231.5270538]: 
Returned: 0.8807077407836914| 

python DistillationModifier [252 - 1706049232.6197648]: Calling loss_update with:
args: 0.5882112979888916| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049233.737374]: 
Returned: 1.2544622421264648| 

python DistillationModifier [252 - 1706049235.4266067]: Calling loss_update with:
args: 0.48966890573501587| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049237.0232456]: 
Returned: 0.9986566305160522| 

python DistillationModifier [252 - 1706049238.6365952]: Calling loss_update with:
args: 0.8483664989471436| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049240.2343376]: 
Returned: 0.603334903717041| 

python DistillationModifier [252 - 1706049241.9214451]: Calling loss_update with:
args: 0.869240939617157| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049243.4479609]: 
Returned: 1.187870979309082| 

python DistillationModifier [252 - 1706049245.1179075]: Calling loss_update with:
args: 0.7319855093955994| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049246.7243977]: 
Returned: 0.9464197158813477| 

python DistillationModifier [252 - 1706049248.334507]: Calling loss_update with:
args: 0.4709804058074951| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049249.7477617]: 
Returned: 0.8270054459571838| 

python DistillationModifier [252 - 1706049250.928616]: Calling loss_update with:
args: 0.7026299834251404| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049251.9408247]: 
Returned: 1.3217058181762695| 

python DistillationModifier [252 - 1706049253.0361495]: Calling loss_update with:
args: 0.5544902086257935| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049254.131707]: 
Returned: 1.0172843933105469| 

python DistillationModifier [252 - 1706049255.022768]: Calling loss_update with:
args: 0.5748381018638611| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049255.841309]: 
Returned: 1.1348743438720703| 

python DistillationModifier [252 - 1706049258.018274]: Calling loss_update with:
args: 0.1596507579088211| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1706049259.5472314]: 
Returned: 0.11486003547906876| 

python LearningRateFunctionModifier [252 - 1706049262.5679116]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [252 - 1706049262.568063]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [252 - 1706049262.5681093]: 0.00010384615384615383
python LearningRateFunctionModifier/ParamGroup1 [252 - 1706049262.5681343]: 0.00010384615384615383
python DistillationModifier/task_loss [252 - 1706049262.5681753]: tensor(0.1597, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [252 - 1706049262.5686915]: tensor(0.1149, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [252 - 1706049262.568944]: tensor(0.1149, grad_fn=<AddBackward0>)
python ConstantPruningModifier [252 - 1706049262.5691812]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| 
python ConstantPruningModifier [252 - 1706049262.8484278]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [252 - 1706049262.8489332]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [252 - 1706049262.849231]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [252 - 1706049262.8494127]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [252 - 1706049262.8496275]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [252 - 1706049262.8504138]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [252 - 1706049262.851219]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [252 - 1706049262.8514526]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [252 - 1706049262.8516874]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [252 - 1706049262.8518672]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [252 - 1706049262.852031]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [252 - 1706049262.9166114]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [252 - 1706049262.9173043]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [252 - 1706049262.9175243]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [252 - 1706049262.9177625]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [252 - 1706049262.9179416]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [252 - 1706049262.9181137]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [252 - 1706049262.918704]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [252 - 1706049262.9192991]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [252 - 1706049262.9195106]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [252 - 1706049262.9196935]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [252 - 1706049262.9198577]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [252 - 1706049262.9200163]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [252 - 1706049262.9205563]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [252 - 1706049262.9211497]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [252 - 1706049262.9213521]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [252 - 1706049262.9215147]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [252 - 1706049262.921742]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [252 - 1706049262.921917]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [252 - 1706049262.9224806]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [252 - 1706049262.9230866]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [252 - 1706049262.9232922]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [252 - 1706049262.9234548]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [252 - 1706049262.9236314]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [252 - 1706049262.9237924]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [252 - 1706049262.9243402]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [252 - 1706049262.924961]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [252 - 1706049262.925167]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [252 - 1706049262.9253726]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [252 - 1706049262.9255357]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [252 - 1706049262.9257207]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [252 - 1706049262.9262767]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [252 - 1706049262.9268734]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [252 - 1706049262.9270897]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [252 - 1706049262.9272537]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [252 - 1706049262.927445]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [252 - 1706049262.9276338]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [252 - 1706049262.9282382]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [252 - 1706049262.9288676]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [252 - 1706049262.9290876]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [252 - 1706049262.9292867]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [252 - 1706049262.9294612]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [252 - 1706049262.9296486]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [252 - 1706049262.9302678]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [252 - 1706049262.9308996]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [252 - 1706049262.9311166]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [252 - 1706049262.9312782]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [252 - 1706049262.9314718]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [252 - 1706049262.9316626]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [252 - 1706049262.9322526]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [252 - 1706049262.9328177]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [252 - 1706049262.9330254]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [252 - 1706049262.933222]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [252 - 1706049262.933394]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [252 - 1706049262.9335814]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [252 - 1706049262.9341273]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [252 - 1706049262.9348226]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [252 - 1706049262.9350328]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [252 - 1706049262.935194]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [252 - 1706049262.9353886]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [252 - 1706049262.9355495]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [252 - 1706049262.9361632]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [252 - 1706049262.936747]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [252 - 1706049262.9369519]: 0.0
python ParamPruning/classifier.weight [252 - 1706049262.9370217]: 0.0
python DistillationModifier [259 - 1706049298.0180418]: Calling loss_update with:
args: 0.45864155888557434| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.111111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [259 - 1706049299.6352475]: 
Returned: 0.6457877159118652| 

python LearningRateFunctionModifier [259 - 1706049301.9558992]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.111111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [259 - 1706049301.9560394]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [259 - 1706049301.956086]: 0.00010256410256410256
python LearningRateFunctionModifier/ParamGroup1 [259 - 1706049301.9561112]: 0.00010256410256410256
python DistillationModifier/task_loss [259 - 1706049301.9561524]: tensor(0.4586, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [259 - 1706049301.9566605]: tensor(0.6458, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [259 - 1706049301.9569106]: tensor(0.6458, grad_fn=<AddBackward0>)
python ConstantPruningModifier [259 - 1706049301.957134]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.111111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [259 - 1706049302.236692]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [259 - 1706049302.2371676]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [259 - 1706049302.2374604]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [259 - 1706049302.237664]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [259 - 1706049302.2379067]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [259 - 1706049302.2386982]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [259 - 1706049302.2395008]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [259 - 1706049302.2397685]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [259 - 1706049302.2399375]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [259 - 1706049302.2402074]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [259 - 1706049302.2403688]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [259 - 1706049302.2410264]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [259 - 1706049302.2417283]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [259 - 1706049302.2419603]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [259 - 1706049302.2421894]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [259 - 1706049302.242375]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [259 - 1706049302.2425554]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [259 - 1706049302.243205]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [259 - 1706049302.2438347]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [259 - 1706049302.2440877]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [259 - 1706049302.244249]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [259 - 1706049302.2444603]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [259 - 1706049302.244694]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [259 - 1706049302.245329]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [259 - 1706049302.2459588]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [259 - 1706049302.2461696]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [259 - 1706049302.2463834]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [259 - 1706049302.2465606]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [259 - 1706049302.2467527]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [259 - 1706049302.2473369]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [259 - 1706049302.2478945]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [259 - 1706049302.2480927]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [259 - 1706049302.2482953]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [259 - 1706049302.2484653]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [259 - 1706049302.2486835]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [259 - 1706049302.2492526]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [259 - 1706049302.2499266]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [259 - 1706049302.2501304]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [259 - 1706049302.2503111]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [259 - 1706049302.250484]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [259 - 1706049302.2507265]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [259 - 1706049302.251338]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [259 - 1706049302.2519038]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [259 - 1706049302.2520988]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [259 - 1706049302.2522824]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [259 - 1706049302.252454]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [259 - 1706049302.3168304]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [259 - 1706049302.3174968]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [259 - 1706049302.3182418]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [259 - 1706049302.3184457]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [259 - 1706049302.3186624]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [259 - 1706049302.3188334]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [259 - 1706049302.319051]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [259 - 1706049302.3196902]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [259 - 1706049302.3203223]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [259 - 1706049302.3205466]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [259 - 1706049302.3207693]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [259 - 1706049302.3209465]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [259 - 1706049302.3211687]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [259 - 1706049302.3217974]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [259 - 1706049302.3224216]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [259 - 1706049302.322631]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [259 - 1706049302.322822]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [259 - 1706049302.3230088]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [259 - 1706049302.3232243]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [259 - 1706049302.3238583]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [259 - 1706049302.3244069]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [259 - 1706049302.324633]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [259 - 1706049302.3248281]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [259 - 1706049302.3250017]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [259 - 1706049302.32522]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [259 - 1706049302.3259041]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [259 - 1706049302.3265212]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [259 - 1706049302.3267422]: 0.0
python ParamPruning/classifier.weight [259 - 1706049302.326814]: 0.0
python DistillationModifier [266 - 1706049340.037044]: Calling loss_update with:
args: 0.36396971344947815| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.222222222222222| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [266 - 1706049341.4221833]: 
Returned: 0.562690794467926| 

python LearningRateFunctionModifier [266 - 1706049343.4220955]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.222222222222222| steps_per_epoch: 63| 
python LearningRateFunctionModifier [266 - 1706049343.4222648]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [266 - 1706049343.422312]: 0.00010128205128205126
python LearningRateFunctionModifier/ParamGroup1 [266 - 1706049343.422338]: 0.00010128205128205126
python DistillationModifier/task_loss [266 - 1706049343.4223797]: tensor(0.3640, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [266 - 1706049343.4228742]: tensor(0.5627, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [266 - 1706049343.4231126]: tensor(0.5627, grad_fn=<AddBackward0>)
python ConstantPruningModifier [266 - 1706049343.42334]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.222222222222222| steps_per_epoch: 63| 
python ConstantPruningModifier [266 - 1706049343.6350088]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [266 - 1706049343.6354613]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [266 - 1706049343.6357691]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [266 - 1706049343.6359456]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [266 - 1706049343.6360927]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [266 - 1706049343.636732]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [266 - 1706049343.637329]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [266 - 1706049343.6375365]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [266 - 1706049343.6377115]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [266 - 1706049343.6379144]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [266 - 1706049343.6380658]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [266 - 1706049343.6385157]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [266 - 1706049343.6389363]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [266 - 1706049343.6391256]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [266 - 1706049343.639269]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [266 - 1706049343.6394093]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [266 - 1706049343.6395423]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [266 - 1706049343.6399426]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [266 - 1706049343.6404295]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [266 - 1706049343.640647]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [266 - 1706049343.64084]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [266 - 1706049343.641027]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [266 - 1706049343.6411755]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [266 - 1706049343.6415882]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [266 - 1706049343.6420224]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [266 - 1706049343.6421957]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [266 - 1706049343.642339]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [266 - 1706049343.6425176]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [266 - 1706049343.642676]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [266 - 1706049343.643043]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [266 - 1706049343.6434145]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [266 - 1706049343.643598]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [266 - 1706049343.6437435]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [266 - 1706049343.6438785]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [266 - 1706049343.6440039]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [266 - 1706049343.644382]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [266 - 1706049343.6447945]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [266 - 1706049343.6449666]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [266 - 1706049343.645121]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [266 - 1706049343.6452935]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [266 - 1706049343.6454296]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [266 - 1706049343.645821]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [266 - 1706049343.6463187]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [266 - 1706049343.6464794]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [266 - 1706049343.6466293]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [266 - 1706049343.646821]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [266 - 1706049343.6469576]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [266 - 1706049343.6473415]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [266 - 1706049343.6477818]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [266 - 1706049343.6479502]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [266 - 1706049343.6480875]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [266 - 1706049343.6482234]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [266 - 1706049343.6483924]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [266 - 1706049343.6488347]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [266 - 1706049343.649258]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [266 - 1706049343.6494272]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [266 - 1706049343.6495788]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [266 - 1706049343.6497679]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [266 - 1706049343.649904]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [266 - 1706049343.6502774]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [266 - 1706049343.6507094]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [266 - 1706049343.6508842]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [266 - 1706049343.6510253]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [266 - 1706049343.6511576]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [266 - 1706049343.6513307]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [266 - 1706049343.6517239]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [266 - 1706049343.6521392]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [266 - 1706049343.6523118]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [266 - 1706049343.6524417]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [266 - 1706049343.7166128]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [266 - 1706049343.7168157]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [266 - 1706049343.7172003]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [266 - 1706049343.7176511]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [266 - 1706049343.7178364]: 0.0
python ParamPruning/classifier.weight [266 - 1706049343.7179224]: 0.0
python DistillationModifier [273 - 1706049377.020163]: Calling loss_update with:
args: 0.331262469291687| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.333333333333333| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [273 - 1706049378.1390178]: 
Returned: 0.3637324571609497| 

python LearningRateFunctionModifier [273 - 1706049380.1408534]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.333333333333333| steps_per_epoch: 63| 
python LearningRateFunctionModifier [273 - 1706049380.1410189]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [273 - 1706049380.1416347]: 9.999999999999999e-05
python LearningRateFunctionModifier/ParamGroup1 [273 - 1706049380.1416702]: 9.999999999999999e-05
python DistillationModifier/task_loss [273 - 1706049380.1417139]: tensor(0.3313, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [273 - 1706049380.142216]: tensor(0.3637, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [273 - 1706049380.142469]: tensor(0.3637, grad_fn=<AddBackward0>)
python ConstantPruningModifier [273 - 1706049380.1427257]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.333333333333333| steps_per_epoch: 63| 
python ConstantPruningModifier [273 - 1706049380.3328693]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [273 - 1706049380.3333197]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [273 - 1706049380.3336139]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [273 - 1706049380.3338192]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [273 - 1706049380.3340206]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [273 - 1706049380.3346763]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [273 - 1706049380.3353026]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [273 - 1706049380.335513]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [273 - 1706049380.3357432]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [273 - 1706049380.3359385]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [273 - 1706049380.3361328]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [273 - 1706049380.3366246]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [273 - 1706049380.3370595]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [273 - 1706049380.3372607]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [273 - 1706049380.3374383]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [273 - 1706049380.3376079]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [273 - 1706049380.337847]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [273 - 1706049380.3382504]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [273 - 1706049380.3387806]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [273 - 1706049380.3389843]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [273 - 1706049380.3392024]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [273 - 1706049380.339375]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [273 - 1706049380.3395157]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [273 - 1706049380.3399515]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [273 - 1706049380.3403986]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [273 - 1706049380.3406286]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [273 - 1706049380.3408265]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [273 - 1706049380.340981]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [273 - 1706049380.3411663]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [273 - 1706049380.3415515]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [273 - 1706049380.3419871]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [273 - 1706049380.3421757]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [273 - 1706049380.342335]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [273 - 1706049380.3424764]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [273 - 1706049380.3426723]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [273 - 1706049380.3430748]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [273 - 1706049380.3434298]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [273 - 1706049380.3436391]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [273 - 1706049380.343866]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [273 - 1706049380.344102]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [273 - 1706049380.3443055]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [273 - 1706049380.344772]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [273 - 1706049380.3453312]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [273 - 1706049380.3455212]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [273 - 1706049380.3457308]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [273 - 1706049380.345872]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [273 - 1706049380.346071]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [273 - 1706049380.3465083]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [273 - 1706049380.3470063]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [273 - 1706049380.3472219]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [273 - 1706049380.3474708]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [273 - 1706049380.3476605]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [273 - 1706049380.3478248]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [273 - 1706049380.3482506]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [273 - 1706049380.3486748]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [273 - 1706049380.348883]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [273 - 1706049380.3490608]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [273 - 1706049380.349232]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [273 - 1706049380.3493805]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [273 - 1706049380.3497705]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [273 - 1706049380.3502264]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [273 - 1706049380.3504245]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [273 - 1706049380.3506334]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [273 - 1706049380.350829]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [273 - 1706049380.350985]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [273 - 1706049380.3513722]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [273 - 1706049380.3518527]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [273 - 1706049380.3520534]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [273 - 1706049380.3522406]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [273 - 1706049380.352386]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [273 - 1706049380.416593]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [273 - 1706049380.417066]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [273 - 1706049380.4175298]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [273 - 1706049380.4177654]: 0.0
python ParamPruning/classifier.weight [273 - 1706049380.4178393]: 0.0
python DistillationModifier [280 - 1706049414.9366274]: Calling loss_update with:
args: 0.2997902035713196| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.444444444444445| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [280 - 1706049416.5294194]: 
Returned: 0.486746221780777| 

python LearningRateFunctionModifier [280 - 1706049419.2226741]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.444444444444445| steps_per_epoch: 63| 
python LearningRateFunctionModifier [280 - 1706049419.2228255]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [280 - 1706049419.222874]: 9.871794871794871e-05
python LearningRateFunctionModifier/ParamGroup1 [280 - 1706049419.2229064]: 9.871794871794871e-05
python DistillationModifier/task_loss [280 - 1706049419.2229488]: tensor(0.2998, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [280 - 1706049419.2234373]: tensor(0.4867, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [280 - 1706049419.2237186]: tensor(0.4867, grad_fn=<AddBackward0>)
python ConstantPruningModifier [280 - 1706049419.2239664]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.444444444444445| steps_per_epoch: 63| 
python ConstantPruningModifier [280 - 1706049419.4304297]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [280 - 1706049419.4309204]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [280 - 1706049419.4311886]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [280 - 1706049419.4314196]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [280 - 1706049419.4316523]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [280 - 1706049419.432246]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [280 - 1706049419.4328957]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [280 - 1706049419.433111]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [280 - 1706049419.4333055]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [280 - 1706049419.4335084]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [280 - 1706049419.433693]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [280 - 1706049419.434164]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [280 - 1706049419.4346766]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [280 - 1706049419.4348862]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [280 - 1706049419.4350588]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [280 - 1706049419.4352434]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [280 - 1706049419.4354343]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [280 - 1706049419.4359055]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [280 - 1706049419.4363952]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [280 - 1706049419.4366322]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [280 - 1706049419.4368148]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [280 - 1706049419.4369986]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [280 - 1706049419.437183]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [280 - 1706049419.4376664]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [280 - 1706049419.4380963]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [280 - 1706049419.4383037]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [280 - 1706049419.438495]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [280 - 1706049419.4387052]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [280 - 1706049419.4388897]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [280 - 1706049419.439289]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [280 - 1706049419.4397259]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [280 - 1706049419.4399076]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [280 - 1706049419.4400866]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [280 - 1706049419.4402735]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [280 - 1706049419.4404209]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [280 - 1706049419.4409983]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [280 - 1706049419.4414146]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [280 - 1706049419.441639]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [280 - 1706049419.441816]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [280 - 1706049419.4420028]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [280 - 1706049419.4421668]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [280 - 1706049419.4426038]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [280 - 1706049419.4431431]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [280 - 1706049419.4433465]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [280 - 1706049419.4435372]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [280 - 1706049419.4437456]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [280 - 1706049419.4439332]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [280 - 1706049419.4443862]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [280 - 1706049419.4448876]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [280 - 1706049419.4450963]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [280 - 1706049419.4452736]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [280 - 1706049419.4454596]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [280 - 1706049419.4456723]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [280 - 1706049419.446061]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [280 - 1706049419.4465146]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [280 - 1706049419.4467473]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [280 - 1706049419.4469216]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [280 - 1706049419.447101]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [280 - 1706049419.4472992]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [280 - 1706049419.4477184]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [280 - 1706049419.4481738]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [280 - 1706049419.4483745]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [280 - 1706049419.448599]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [280 - 1706049419.4487865]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [280 - 1706049419.4489372]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [280 - 1706049419.4493482]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [280 - 1706049419.4498334]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [280 - 1706049419.45004]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [280 - 1706049419.450211]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [280 - 1706049419.4503925]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [280 - 1706049419.4506018]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [280 - 1706049419.4510403]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [280 - 1706049419.4514802]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [280 - 1706049419.4516976]: 0.0
python ParamPruning/classifier.weight [280 - 1706049419.4517667]: 0.0
python DistillationModifier [287 - 1706049454.328264]: Calling loss_update with:
args: 0.07451923191547394| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.555555555555555| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [287 - 1706049455.4279656]: 
Returned: 0.20639511942863464| 

python LearningRateFunctionModifier [287 - 1706049458.1229875]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.555555555555555| steps_per_epoch: 63| 
python LearningRateFunctionModifier [287 - 1706049458.1231534]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [287 - 1706049458.123208]: 9.743589743589742e-05
python LearningRateFunctionModifier/ParamGroup1 [287 - 1706049458.1232445]: 9.743589743589742e-05
python DistillationModifier/task_loss [287 - 1706049458.123294]: tensor(0.0745, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [287 - 1706049458.123874]: tensor(0.2064, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [287 - 1706049458.1241355]: tensor(0.2064, grad_fn=<AddBackward0>)
python ConstantPruningModifier [287 - 1706049458.124389]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.555555555555555| steps_per_epoch: 63| 
python ConstantPruningModifier [287 - 1706049458.4365065]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [287 - 1706049458.4370856]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [287 - 1706049458.4374404]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [287 - 1706049458.4377558]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [287 - 1706049458.4380395]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [287 - 1706049458.4388685]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [287 - 1706049458.4394765]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [287 - 1706049458.439758]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [287 - 1706049458.4400203]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [287 - 1706049458.4402297]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [287 - 1706049458.4404814]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [287 - 1706049458.4410763]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [287 - 1706049458.441645]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [287 - 1706049458.4419057]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [287 - 1706049458.4421582]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [287 - 1706049458.442392]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [287 - 1706049458.4426286]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [287 - 1706049458.4431708]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [287 - 1706049458.4437332]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [287 - 1706049458.4439576]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [287 - 1706049458.4441943]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [287 - 1706049458.4444177]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [287 - 1706049458.4446502]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [287 - 1706049458.4453068]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [287 - 1706049458.4459164]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [287 - 1706049458.4461427]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [287 - 1706049458.4463408]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [287 - 1706049458.4465563]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [287 - 1706049458.4467835]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [287 - 1706049458.4473178]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [287 - 1706049458.4478736]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [287 - 1706049458.4480937]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [287 - 1706049458.4483004]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [287 - 1706049458.4484892]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [287 - 1706049458.4487703]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [287 - 1706049458.449311]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [287 - 1706049458.4498694]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [287 - 1706049458.4500906]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [287 - 1706049458.4503007]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [287 - 1706049458.450493]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [287 - 1706049458.4507422]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [287 - 1706049458.4512823]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [287 - 1706049458.451846]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [287 - 1706049458.4520683]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [287 - 1706049458.4522738]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [287 - 1706049458.4524622]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [287 - 1706049458.516671]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [287 - 1706049458.517214]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [287 - 1706049458.5177708]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [287 - 1706049458.5179892]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [287 - 1706049458.5182]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [287 - 1706049458.5184033]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [287 - 1706049458.5186238]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [287 - 1706049458.5191636]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [287 - 1706049458.5197246]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [287 - 1706049458.5199385]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [287 - 1706049458.520141]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [287 - 1706049458.52034]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [287 - 1706049458.5205503]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [287 - 1706049458.5211277]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [287 - 1706049458.5216854]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [287 - 1706049458.5219035]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [287 - 1706049458.5221114]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [287 - 1706049458.5223086]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [287 - 1706049458.5225039]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [287 - 1706049458.523058]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [287 - 1706049458.5238512]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [287 - 1706049458.5240963]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [287 - 1706049458.5243084]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [287 - 1706049458.5245097]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [287 - 1706049458.5247424]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [287 - 1706049458.5253208]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [287 - 1706049458.5258827]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [287 - 1706049458.5261]: 0.0
python ParamPruning/classifier.weight [287 - 1706049458.5261884]: 0.0
python DistillationModifier [294 - 1706049493.138462]: Calling loss_update with:
args: 0.06222117319703102| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.666666666666667| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [294 - 1706049494.736136]: 
Returned: 0.27779656648635864| 

python LearningRateFunctionModifier [294 - 1706049496.677752]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.666666666666667| steps_per_epoch: 63| 
python LearningRateFunctionModifier [294 - 1706049496.6779008]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [294 - 1706049496.6779473]: 9.615384615384615e-05
python LearningRateFunctionModifier/ParamGroup1 [294 - 1706049496.6779783]: 9.615384615384615e-05
python DistillationModifier/task_loss [294 - 1706049496.6780212]: tensor(0.0622, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [294 - 1706049496.6784966]: tensor(0.2778, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [294 - 1706049496.6787696]: tensor(0.2778, grad_fn=<AddBackward0>)
python ConstantPruningModifier [294 - 1706049496.678999]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.666666666666667| steps_per_epoch: 63| 
python ConstantPruningModifier [294 - 1706049496.9213588]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [294 - 1706049496.921838]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [294 - 1706049496.9220917]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [294 - 1706049496.9222775]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [294 - 1706049496.9224377]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [294 - 1706049496.9230173]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [294 - 1706049496.923665]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [294 - 1706049496.9238844]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [294 - 1706049496.9240248]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [294 - 1706049496.9241645]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [294 - 1706049496.9243045]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [294 - 1706049496.9247582]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [294 - 1706049496.925346]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [294 - 1706049496.9255466]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [294 - 1706049496.9257212]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [294 - 1706049496.9258633]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [294 - 1706049496.9260123]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [294 - 1706049496.9265149]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [294 - 1706049496.926951]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [294 - 1706049496.9271493]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [294 - 1706049496.9272847]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [294 - 1706049496.9274187]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [294 - 1706049496.927619]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [294 - 1706049496.9279988]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [294 - 1706049496.928502]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [294 - 1706049496.928723]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [294 - 1706049496.9288774]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [294 - 1706049496.929018]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [294 - 1706049496.9291587]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [294 - 1706049496.929531]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [294 - 1706049496.9300282]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [294 - 1706049496.9302208]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [294 - 1706049496.9303598]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [294 - 1706049496.9304936]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [294 - 1706049496.9306545]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [294 - 1706049496.9310732]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [294 - 1706049496.9315178]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [294 - 1706049496.931738]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [294 - 1706049496.9318833]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [294 - 1706049496.9320278]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [294 - 1706049496.9321678]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [294 - 1706049496.9326258]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [294 - 1706049496.9331436]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [294 - 1706049496.9333386]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [294 - 1706049496.9334748]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [294 - 1706049496.933632]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [294 - 1706049496.9337852]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [294 - 1706049496.934222]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [294 - 1706049496.9346778]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [294 - 1706049496.9348733]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [294 - 1706049496.9350147]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [294 - 1706049496.935151]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [294 - 1706049496.9352884]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [294 - 1706049496.9357202]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [294 - 1706049496.9360938]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [294 - 1706049496.9362826]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [294 - 1706049496.936416]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [294 - 1706049496.9365788]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [294 - 1706049496.936734]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [294 - 1706049496.9372077]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [294 - 1706049496.9377863]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [294 - 1706049496.9379807]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [294 - 1706049496.9381223]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [294 - 1706049496.938265]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [294 - 1706049496.938407]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [294 - 1706049496.9389052]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [294 - 1706049496.9394097]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [294 - 1706049496.9396296]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [294 - 1706049496.9397802]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [294 - 1706049496.939914]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [294 - 1706049496.9400501]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [294 - 1706049496.9405072]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [294 - 1706049496.941011]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [294 - 1706049496.9412127]: 0.0
python ParamPruning/classifier.weight [294 - 1706049496.9412808]: 0.0
python DistillationModifier [301 - 1706049530.820262]: Calling loss_update with:
args: 0.0345698744058609| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.777777777777778| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [301 - 1706049532.3229556]: 
Returned: 0.16792073845863342| 

python LearningRateFunctionModifier [301 - 1706049535.3673265]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.777777777777778| steps_per_epoch: 63| 
python LearningRateFunctionModifier [301 - 1706049535.3674777]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [301 - 1706049535.367526]: 9.487179487179487e-05
python LearningRateFunctionModifier/ParamGroup1 [301 - 1706049535.3675582]: 9.487179487179487e-05
python DistillationModifier/task_loss [301 - 1706049535.36762]: tensor(0.0346, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [301 - 1706049535.368116]: tensor(0.1679, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [301 - 1706049535.368354]: tensor(0.1679, grad_fn=<AddBackward0>)
python ConstantPruningModifier [301 - 1706049535.3686032]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.777777777777778| steps_per_epoch: 63| 
python ConstantPruningModifier [301 - 1706049535.6510136]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [301 - 1706049535.6515346]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [301 - 1706049535.6518557]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [301 - 1706049535.6521127]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [301 - 1706049535.652336]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [301 - 1706049535.7170959]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [301 - 1706049535.7179325]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [301 - 1706049535.7181776]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [301 - 1706049535.7184064]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [301 - 1706049535.718664]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [301 - 1706049535.7188654]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [301 - 1706049535.7194874]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [301 - 1706049535.7201426]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [301 - 1706049535.7203586]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [301 - 1706049535.7205942]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [301 - 1706049535.720826]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [301 - 1706049535.721014]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [301 - 1706049535.7216225]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [301 - 1706049535.7222393]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [301 - 1706049535.7224603]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [301 - 1706049535.7226667]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [301 - 1706049535.7228532]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [301 - 1706049535.7230306]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [301 - 1706049535.7236202]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [301 - 1706049535.7241976]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [301 - 1706049535.724413]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [301 - 1706049535.7246459]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [301 - 1706049535.7248294]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [301 - 1706049535.7250106]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [301 - 1706049535.7256002]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [301 - 1706049535.7261455]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [301 - 1706049535.7263436]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [301 - 1706049535.726522]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [301 - 1706049535.726769]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [301 - 1706049535.7269433]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [301 - 1706049535.7274756]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [301 - 1706049535.7281485]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [301 - 1706049535.7283607]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [301 - 1706049535.7285419]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [301 - 1706049535.728762]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [301 - 1706049535.7289338]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [301 - 1706049535.729509]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [301 - 1706049535.7301419]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [301 - 1706049535.7303314]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [301 - 1706049535.7304916]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [301 - 1706049535.7307034]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [301 - 1706049535.730878]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [301 - 1706049535.731442]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [301 - 1706049535.7319992]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [301 - 1706049535.7321868]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [301 - 1706049535.7323859]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [301 - 1706049535.7325773]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [301 - 1706049535.732771]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [301 - 1706049535.7332962]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [301 - 1706049535.7339768]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [301 - 1706049535.734186]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [301 - 1706049535.7343688]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [301 - 1706049535.734535]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [301 - 1706049535.7347407]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [301 - 1706049535.735332]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [301 - 1706049535.735958]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [301 - 1706049535.7361686]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [301 - 1706049535.7363641]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [301 - 1706049535.7366247]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [301 - 1706049535.7368147]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [301 - 1706049535.7374115]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [301 - 1706049535.7380614]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [301 - 1706049535.738267]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [301 - 1706049535.7384593]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [301 - 1706049535.738698]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [301 - 1706049535.7388744]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [301 - 1706049535.739492]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [301 - 1706049535.7400517]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [301 - 1706049535.7402613]: 0.0
python ParamPruning/classifier.weight [301 - 1706049535.7403307]: 0.0
python DistillationModifier [308 - 1706049573.8413513]: Calling loss_update with:
args: 0.10921555757522583| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.888888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [308 - 1706049575.2394133]: 
Returned: 0.1362675428390503| 

python LearningRateFunctionModifier [308 - 1706049577.2236764]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.888888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [308 - 1706049577.2238472]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [308 - 1706049577.223902]: 9.358974358974357e-05
python LearningRateFunctionModifier/ParamGroup1 [308 - 1706049577.223938]: 9.358974358974357e-05
python DistillationModifier/task_loss [308 - 1706049577.223987]: tensor(0.1092, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [308 - 1706049577.224537]: tensor(0.1363, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [308 - 1706049577.2248356]: tensor(0.1363, grad_fn=<AddBackward0>)
python ConstantPruningModifier [308 - 1706049577.225086]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.888888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [308 - 1706049577.516806]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [308 - 1706049577.5172575]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [308 - 1706049577.5175223]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [308 - 1706049577.5177248]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [308 - 1706049577.517872]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [308 - 1706049577.5184665]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [308 - 1706049577.518906]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [308 - 1706049577.5191138]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [308 - 1706049577.519316]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [308 - 1706049577.519462]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [308 - 1706049577.519613]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [308 - 1706049577.5199802]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [308 - 1706049577.520337]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [308 - 1706049577.520507]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [308 - 1706049577.520729]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [308 - 1706049577.520875]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [308 - 1706049577.521075]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [308 - 1706049577.5214443]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [308 - 1706049577.5218267]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [308 - 1706049577.5220315]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [308 - 1706049577.5221853]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [308 - 1706049577.522316]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [308 - 1706049577.522461]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [308 - 1706049577.5228653]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [308 - 1706049577.5233767]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [308 - 1706049577.523597]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [308 - 1706049577.523783]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [308 - 1706049577.5239408]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [308 - 1706049577.5240765]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [308 - 1706049577.5245192]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [308 - 1706049577.5249362]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [308 - 1706049577.5251224]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [308 - 1706049577.5252829]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [308 - 1706049577.5254288]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [308 - 1706049577.525595]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [308 - 1706049577.5259855]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [308 - 1706049577.5263388]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [308 - 1706049577.5265374]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [308 - 1706049577.5267274]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [308 - 1706049577.5268686]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [308 - 1706049577.5270572]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [308 - 1706049577.5275419]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [308 - 1706049577.527989]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [308 - 1706049577.528202]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [308 - 1706049577.528389]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [308 - 1706049577.5285418]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [308 - 1706049577.528721]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [308 - 1706049577.5290952]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [308 - 1706049577.529605]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [308 - 1706049577.5297875]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [308 - 1706049577.52998]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [308 - 1706049577.5301294]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [308 - 1706049577.530273]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [308 - 1706049577.5306842]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [308 - 1706049577.5311153]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [308 - 1706049577.5313184]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [308 - 1706049577.5314846]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [308 - 1706049577.5316947]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [308 - 1706049577.5318422]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [308 - 1706049577.5322566]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [308 - 1706049577.5326586]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [308 - 1706049577.532839]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [308 - 1706049577.5330193]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [308 - 1706049577.5331674]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [308 - 1706049577.5333169]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [308 - 1706049577.5336785]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [308 - 1706049577.5340328]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [308 - 1706049577.5342102]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [308 - 1706049577.5343442]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [308 - 1706049577.53451]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [308 - 1706049577.5346682]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [308 - 1706049577.5350277]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [308 - 1706049577.535554]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [308 - 1706049577.5357606]: 0.0
python ParamPruning/classifier.weight [308 - 1706049577.535828]: 0.0
python DistillationModifier [315 - 1706049612.7184498]: Calling loss_update with:
args: 0.6465344429016113| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049613.7254481]: 
Returned: 1.5263174772262573| 

python DistillationModifier [315 - 1706049614.7418246]: Calling loss_update with:
args: 1.3216185569763184| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049615.847411]: 
Returned: 2.1376705169677734| 

python DistillationModifier [315 - 1706049617.5183625]: Calling loss_update with:
args: 0.9963333010673523| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049618.5321786]: 
Returned: 1.1594724655151367| 

python DistillationModifier [315 - 1706049619.6237247]: Calling loss_update with:
args: 0.6415947079658508| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049620.6350124]: 
Returned: 0.7679246068000793| 

python DistillationModifier [315 - 1706049621.7191346]: Calling loss_update with:
args: 0.8953068852424622| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049622.72222]: 
Returned: 1.3308508396148682| 

python DistillationModifier [315 - 1706049624.330511]: Calling loss_update with:
args: 0.8563936352729797| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049625.8535259]: 
Returned: 1.6484934091567993| 

python DistillationModifier [315 - 1706049627.5247283]: Calling loss_update with:
args: 0.48066696524620056| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049629.0394013]: 
Returned: 1.095428228378296| 

python DistillationModifier [315 - 1706049630.6497738]: Calling loss_update with:
args: 1.1213252544403076| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049632.233225]: 
Returned: 1.3748750686645508| 

python DistillationModifier [315 - 1706049633.8416126]: Calling loss_update with:
args: 0.506433367729187| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049635.4237113]: 
Returned: 0.4144146144390106| 

python DistillationModifier [315 - 1706049637.0218456]: Calling loss_update with:
args: 0.9392543435096741| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049638.5397284]: 
Returned: 1.5681201219558716| 

python DistillationModifier [315 - 1706049640.1481826]: Calling loss_update with:
args: 0.8344127535820007| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049641.7299755]: 
Returned: 1.1929545402526855| 

python DistillationModifier [315 - 1706049643.2511928]: Calling loss_update with:
args: 0.8934337496757507| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049644.724213]: 
Returned: 1.5704301595687866| 

python DistillationModifier [315 - 1706049646.331436]: Calling loss_update with:
args: 1.0084717273712158| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049647.8474948]: 
Returned: 2.0029091835021973| 

python DistillationModifier [315 - 1706049649.5251968]: Calling loss_update with:
args: 0.658373236656189| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049650.7399774]: 
Returned: 0.7738189697265625| 

python DistillationModifier [315 - 1706049651.8243928]: Calling loss_update with:
args: 1.2518500089645386| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049652.8275328]: 
Returned: 2.0601558685302734| 

python DistillationModifier [315 - 1706049653.9246833]: Calling loss_update with:
args: 0.8965185284614563| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049654.9293985]: 
Returned: 1.5722246170043945| 

python DistillationModifier [315 - 1706049656.5351727]: Calling loss_update with:
args: 0.5989292860031128| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049658.1260836]: 
Returned: 0.870460569858551| 

python DistillationModifier [315 - 1706049659.7297077]: Calling loss_update with:
args: 0.9025397896766663| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049661.2182162]: 
Returned: 1.6877939701080322| 

python DistillationModifier [315 - 1706049662.5214128]: Calling loss_update with:
args: 1.054038166999817| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049663.5329187]: 
Returned: 1.1719365119934082| 

python DistillationModifier [315 - 1706049664.6227283]: Calling loss_update with:
args: 0.7920867204666138| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049665.6297855]: 
Returned: 1.3414186239242554| 

python DistillationModifier [315 - 1706049667.2365236]: Calling loss_update with:
args: 0.8472009301185608| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049668.8339462]: 
Returned: 1.5017417669296265| 

python DistillationModifier [315 - 1706049670.4234421]: Calling loss_update with:
args: 0.6172224283218384| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049671.4306834]: 
Returned: 0.9564427137374878| 

python DistillationModifier [315 - 1706049672.519586]: Calling loss_update with:
args: 0.7230933308601379| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049673.5270112]: 
Returned: 1.070433259010315| 

python DistillationModifier [315 - 1706049674.5481188]: Calling loss_update with:
args: 0.5226061344146729| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049676.0487633]: 
Returned: 1.23047935962677| 

python DistillationModifier [315 - 1706049677.7167914]: Calling loss_update with:
args: 1.0937392711639404| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049679.2367291]: 
Returned: 1.231112003326416| 

python DistillationModifier [315 - 1706049680.8401103]: Calling loss_update with:
args: 0.5400416851043701| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049682.4217026]: 
Returned: 1.0527894496917725| 

python DistillationModifier [315 - 1706049684.0248885]: Calling loss_update with:
args: 0.8178158402442932| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049685.5474298]: 
Returned: 1.1146584749221802| 

python DistillationModifier [315 - 1706049687.2222753]: Calling loss_update with:
args: 0.6272327899932861| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049688.7522097]: 
Returned: 0.942232072353363| 

python DistillationModifier [315 - 1706049690.4170182]: Calling loss_update with:
args: 0.5157666802406311| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049692.017195]: 
Returned: 0.9851967692375183| 

python DistillationModifier [315 - 1706049693.6247838]: Calling loss_update with:
args: 0.5069730281829834| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049694.7201772]: 
Returned: 1.1255815029144287| 

python DistillationModifier [315 - 1706049696.0420709]: Calling loss_update with:
args: 1.040087342262268| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049697.0497162]: 
Returned: 1.9682860374450684| 

python DistillationModifier [315 - 1706049698.2183104]: Calling loss_update with:
args: 0.8928877115249634| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049699.317743]: 
Returned: 1.268759846687317| 

python DistillationModifier [315 - 1706049700.6227548]: Calling loss_update with:
args: 1.0872572660446167| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049701.6273406]: 
Returned: 1.8453021049499512| 

python DistillationModifier [315 - 1706049702.651366]: Calling loss_update with:
args: 1.2886794805526733| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049703.7178159]: 
Returned: 2.043951988220215| 

python DistillationModifier [315 - 1706049704.7460206]: Calling loss_update with:
args: 1.4342849254608154| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049705.74618]: 
Returned: 1.9209091663360596| 

python DistillationModifier [315 - 1706049707.4166763]: Calling loss_update with:
args: 1.2704110145568848| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049708.9359183]: 
Returned: 2.3179047107696533| 

python DistillationModifier [315 - 1706049710.3370903]: Calling loss_update with:
args: 1.019953727722168| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049711.342936]: 
Returned: 1.0519835948944092| 

python DistillationModifier [315 - 1706049712.4240515]: Calling loss_update with:
args: 0.6084586381912231| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049713.4366798]: 
Returned: 1.1286522150039673| 

python DistillationModifier [315 - 1706049714.948325]: Calling loss_update with:
args: 0.8972588181495667| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049716.2458196]: 
Returned: 1.7005695104599| 

python DistillationModifier [315 - 1706049717.6481717]: Calling loss_update with:
args: 1.1073025465011597| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049719.234408]: 
Returned: 2.1598870754241943| 

python DistillationModifier [315 - 1706049720.8478856]: Calling loss_update with:
args: 0.47587597370147705| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049722.019064]: 
Returned: 0.9874440431594849| 

python DistillationModifier [315 - 1706049723.0411296]: Calling loss_update with:
args: 0.558345377445221| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049724.0461245]: 
Returned: 1.1506398916244507| 

python DistillationModifier [315 - 1706049725.6480377]: Calling loss_update with:
args: 0.9778384566307068| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049727.2485414]: 
Returned: 0.9310269355773926| 

python DistillationModifier [315 - 1706049728.9403765]: Calling loss_update with:
args: 0.7264134287834167| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049730.325691]: 
Returned: 1.739595651626587| 

python DistillationModifier [315 - 1706049731.3402934]: Calling loss_update with:
args: 0.9276362657546997| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049732.3454316]: 
Returned: 2.2015902996063232| 

python DistillationModifier [315 - 1706049733.4343636]: Calling loss_update with:
args: 1.123589277267456| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049734.442717]: 
Returned: 1.5099791288375854| 

python DistillationModifier [315 - 1706049735.5224736]: Calling loss_update with:
args: 0.6861618757247925| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049736.52854]: 
Returned: 1.2383557558059692| 

python DistillationModifier [315 - 1706049737.618346]: Calling loss_update with:
args: 0.33370453119277954| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049738.6274729]: 
Returned: 0.8386772871017456| 

python DistillationModifier [315 - 1706049739.6459215]: Calling loss_update with:
args: 1.0072784423828125| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049741.0476072]: 
Returned: 1.9963858127593994| 

python DistillationModifier [315 - 1706049742.7174947]: Calling loss_update with:
args: 0.9162856936454773| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049744.3167439]: 
Returned: 1.5728216171264648| 

python DistillationModifier [315 - 1706049745.5225437]: Calling loss_update with:
args: 0.32782313227653503| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049746.93183]: 
Returned: 0.7353464961051941| 

python DistillationModifier [315 - 1706049748.439705]: Calling loss_update with:
args: 1.6325628757476807| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049749.4439006]: 
Returned: 1.7372328042984009| 

python DistillationModifier [315 - 1706049750.53552]: Calling loss_update with:
args: 0.6736727952957153| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049751.6247497]: 
Returned: 1.0065139532089233| 

python DistillationModifier [315 - 1706049752.7210424]: Calling loss_update with:
args: 0.8715749382972717| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049754.23544]: 
Returned: 1.427686333656311| 

python DistillationModifier [315 - 1706049755.8485594]: Calling loss_update with:
args: 0.5862462520599365| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049757.447481]: 
Returned: 1.529811978340149| 

python DistillationModifier [315 - 1706049759.1183472]: Calling loss_update with:
args: 0.8097481727600098| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049760.7235389]: 
Returned: 1.2869569063186646| 

python DistillationModifier [315 - 1706049762.3223968]: Calling loss_update with:
args: 0.9005360007286072| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049763.6190152]: 
Returned: 0.7734477519989014| 

python DistillationModifier [315 - 1706049765.2214139]: Calling loss_update with:
args: 0.8318725228309631| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049766.3194728]: 
Returned: 1.097451090812683| 

python DistillationModifier [315 - 1706049767.35041]: Calling loss_update with:
args: 1.1742875576019287| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049768.4310386]: 
Returned: 1.4312371015548706| 

python DistillationModifier [315 - 1706049770.0336306]: Calling loss_update with:
args: 0.8172614574432373| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049771.2311523]: 
Returned: 1.5501528978347778| 

python DistillationModifier [315 - 1706049772.4503257]: Calling loss_update with:
args: 0.7787272334098816| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049773.6432052]: 
Returned: 1.487086534500122| 

python DistillationModifier [315 - 1706049775.138131]: Calling loss_update with:
args: 0.9176256656646729| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049776.4216127]: 
Returned: 1.6987650394439697| 

python DistillationModifier [315 - 1706049776.951604]: Calling loss_update with:
args: 0.5536968111991882| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049777.530206]: 
Returned: 1.1400915384292603| 

python DistillationModifier [315 - 1706049779.0452533]: Calling loss_update with:
args: 0.06934399157762527| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1706049780.1180933]: 
Returned: 0.12989608943462372| 

python LearningRateFunctionModifier [315 - 1706049782.0789783]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [315 - 1706049782.079142]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [315 - 1706049782.0791888]: 9.230769230769229e-05
python LearningRateFunctionModifier/ParamGroup1 [315 - 1706049782.079219]: 9.230769230769229e-05
python DistillationModifier/task_loss [315 - 1706049782.0792615]: tensor(0.0693, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [315 - 1706049782.0797658]: tensor(0.1299, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [315 - 1706049782.0800097]: tensor(0.1299, grad_fn=<AddBackward0>)
python ConstantPruningModifier [315 - 1706049782.0802374]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| 
python ConstantPruningModifier [315 - 1706049782.3281114]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [315 - 1706049782.3285606]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [315 - 1706049782.3288715]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [315 - 1706049782.3290424]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [315 - 1706049782.3292077]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [315 - 1706049782.3298125]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [315 - 1706049782.3302786]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [315 - 1706049782.330483]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [315 - 1706049782.3307087]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [315 - 1706049782.3308625]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [315 - 1706049782.3310046]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [315 - 1706049782.331375]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [315 - 1706049782.3317795]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [315 - 1706049782.3319626]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [315 - 1706049782.3321052]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [315 - 1706049782.3322444]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [315 - 1706049782.332419]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [315 - 1706049782.332957]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [315 - 1706049782.333408]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [315 - 1706049782.3336062]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [315 - 1706049782.3337548]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [315 - 1706049782.3338945]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [315 - 1706049782.3340347]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [315 - 1706049782.334501]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [315 - 1706049782.334909]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [315 - 1706049782.3350844]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [315 - 1706049782.3352299]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [315 - 1706049782.3354206]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [315 - 1706049782.3356023]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [315 - 1706049782.335968]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [315 - 1706049782.33632]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [315 - 1706049782.336482]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [315 - 1706049782.3367078]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [315 - 1706049782.336898]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [315 - 1706049782.337044]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [315 - 1706049782.3374734]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [315 - 1706049782.3378937]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [315 - 1706049782.3380935]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [315 - 1706049782.3382423]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [315 - 1706049782.3383808]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [315 - 1706049782.338522]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [315 - 1706049782.3389075]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [315 - 1706049782.3392901]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [315 - 1706049782.3394504]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [315 - 1706049782.33964]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [315 - 1706049782.339788]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [315 - 1706049782.33992]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [315 - 1706049782.340297]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [315 - 1706049782.3406801]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [315 - 1706049782.3408453]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [315 - 1706049782.3409805]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [315 - 1706049782.3411143]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [315 - 1706049782.3412473]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [315 - 1706049782.3416088]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [315 - 1706049782.3419507]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [315 - 1706049782.342115]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [315 - 1706049782.3422873]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [315 - 1706049782.342423]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [315 - 1706049782.3426425]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [315 - 1706049782.3429978]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [315 - 1706049782.3433268]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [315 - 1706049782.343486]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [315 - 1706049782.3436375]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [315 - 1706049782.3438172]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [315 - 1706049782.3439932]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [315 - 1706049782.3443239]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [315 - 1706049782.344705]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [315 - 1706049782.344877]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [315 - 1706049782.3450403]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [315 - 1706049782.3451788]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [315 - 1706049782.345314]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [315 - 1706049782.3456948]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [315 - 1706049782.3460474]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [315 - 1706049782.3462222]: 0.0
python ParamPruning/classifier.weight [315 - 1706049782.3462894]: 0.0
python DistillationModifier [322 - 1706049820.120873]: Calling loss_update with:
args: 0.035365618765354156| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.111111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [322 - 1706049821.5171943]: 
Returned: 0.10856235772371292| 

python LearningRateFunctionModifier [322 - 1706049824.5545979]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.111111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [322 - 1706049824.5547457]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [322 - 1706049824.5547926]: 9.102564102564102e-05
python LearningRateFunctionModifier/ParamGroup1 [322 - 1706049824.554825]: 9.102564102564102e-05
python DistillationModifier/task_loss [322 - 1706049824.5548682]: tensor(0.0354, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [322 - 1706049824.5553474]: tensor(0.1086, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [322 - 1706049824.555618]: tensor(0.1086, grad_fn=<AddBackward0>)
python ConstantPruningModifier [322 - 1706049824.5558739]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.111111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [322 - 1706049824.836392]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [322 - 1706049824.8369257]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [322 - 1706049824.8372202]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [322 - 1706049824.8374717]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [322 - 1706049824.837712]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [322 - 1706049824.8384573]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [322 - 1706049824.8392725]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [322 - 1706049824.8395038]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [322 - 1706049824.8397477]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [322 - 1706049824.8399286]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [322 - 1706049824.84011]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [322 - 1706049824.8407547]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [322 - 1706049824.8413985]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [322 - 1706049824.841632]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [322 - 1706049824.8418403]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [322 - 1706049824.8420684]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [322 - 1706049824.8422964]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [322 - 1706049824.842897]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [322 - 1706049824.8434834]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [322 - 1706049824.8437092]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [322 - 1706049824.8439217]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [322 - 1706049824.8440962]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [322 - 1706049824.844261]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [322 - 1706049824.844862]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [322 - 1706049824.845644]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [322 - 1706049824.8458521]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [322 - 1706049824.846074]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [322 - 1706049824.8462594]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [322 - 1706049824.846422]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [322 - 1706049824.8470848]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [322 - 1706049824.8477373]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [322 - 1706049824.847941]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [322 - 1706049824.8481004]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [322 - 1706049824.8483124]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [322 - 1706049824.8484805]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [322 - 1706049824.8490946]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [322 - 1706049824.8496501]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [322 - 1706049824.8498464]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [322 - 1706049824.850051]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [322 - 1706049824.8502154]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [322 - 1706049824.8503778]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [322 - 1706049824.8510919]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [322 - 1706049824.8516748]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [322 - 1706049824.8518717]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [322 - 1706049824.852078]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [322 - 1706049824.852246]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [322 - 1706049824.8524415]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [322 - 1706049824.916976]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [322 - 1706049824.9177032]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [322 - 1706049824.917933]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [322 - 1706049824.918204]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [322 - 1706049824.9184258]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [322 - 1706049824.9187648]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [322 - 1706049824.9194186]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [322 - 1706049824.920076]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [322 - 1706049824.920287]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [322 - 1706049824.9204943]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [322 - 1706049824.9207468]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [322 - 1706049824.9209359]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [322 - 1706049824.9215136]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [322 - 1706049824.9221573]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [322 - 1706049824.9223592]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [322 - 1706049824.9225843]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [322 - 1706049824.9227772]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [322 - 1706049824.9229555]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [322 - 1706049824.9235253]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [322 - 1706049824.9241614]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [322 - 1706049824.924365]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [322 - 1706049824.9245942]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [322 - 1706049824.9247792]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [322 - 1706049824.9249525]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [322 - 1706049824.925507]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [322 - 1706049824.9260993]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [322 - 1706049824.9263055]: 0.0
python ParamPruning/classifier.weight [322 - 1706049824.9263725]: 0.0
python DistillationModifier [329 - 1706049861.1507792]: Calling loss_update with:
args: 0.016235006973147392| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.222222222222222| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [329 - 1706049862.2229548]: 
Returned: 0.1042405515909195| 

python LearningRateFunctionModifier [329 - 1706049864.2222502]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.222222222222222| steps_per_epoch: 63| 
python LearningRateFunctionModifier [329 - 1706049864.2223923]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [329 - 1706049864.2224543]: 8.974358974358974e-05
python LearningRateFunctionModifier/ParamGroup1 [329 - 1706049864.2224858]: 8.974358974358974e-05
python DistillationModifier/task_loss [329 - 1706049864.2225282]: tensor(0.0162, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [329 - 1706049864.223048]: tensor(0.1042, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [329 - 1706049864.223294]: tensor(0.1042, grad_fn=<AddBackward0>)
python ConstantPruningModifier [329 - 1706049864.223529]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.222222222222222| steps_per_epoch: 63| 
python ConstantPruningModifier [329 - 1706049864.4274397]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [329 - 1706049864.4279017]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [329 - 1706049864.4281688]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [329 - 1706049864.4283576]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [329 - 1706049864.4285052]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [329 - 1706049864.4291382]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [329 - 1706049864.4297848]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [329 - 1706049864.4300022]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [329 - 1706049864.4302042]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [329 - 1706049864.4303675]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [329 - 1706049864.4305775]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [329 - 1706049864.4310865]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [329 - 1706049864.4316063]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [329 - 1706049864.4318016]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [329 - 1706049864.4319417]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [329 - 1706049864.4321182]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [329 - 1706049864.4322686]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [329 - 1706049864.4327629]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [329 - 1706049864.43321]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [329 - 1706049864.4334242]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [329 - 1706049864.4336133]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [329 - 1706049864.4337962]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [329 - 1706049864.4339433]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [329 - 1706049864.43442]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [329 - 1706049864.4348702]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [329 - 1706049864.435065]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [329 - 1706049864.435253]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [329 - 1706049864.4354084]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [329 - 1706049864.4355505]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [329 - 1706049864.4360175]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [329 - 1706049864.4364352]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [329 - 1706049864.4366794]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [329 - 1706049864.436862]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [329 - 1706049864.4370246]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [329 - 1706049864.437169]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [329 - 1706049864.437645]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [329 - 1706049864.4380245]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [329 - 1706049864.4381986]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [329 - 1706049864.4383347]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [329 - 1706049864.4384675]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [329 - 1706049864.4386573]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [329 - 1706049864.4390726]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [329 - 1706049864.4395258]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [329 - 1706049864.4397311]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [329 - 1706049864.4398785]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [329 - 1706049864.4400225]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [329 - 1706049864.4401884]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [329 - 1706049864.4406471]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [329 - 1706049864.441007]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [329 - 1706049864.4411745]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [329 - 1706049864.4413118]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [329 - 1706049864.4414613]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [329 - 1706049864.4416177]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [329 - 1706049864.4419832]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [329 - 1706049864.4424753]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [329 - 1706049864.4427254]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [329 - 1706049864.4428952]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [329 - 1706049864.4430497]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [329 - 1706049864.4431903]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [329 - 1706049864.4436343]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [329 - 1706049864.4439826]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [329 - 1706049864.4441452]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [329 - 1706049864.4442964]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [329 - 1706049864.4444375]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [329 - 1706049864.4446213]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [329 - 1706049864.4450092]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [329 - 1706049864.445499]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [329 - 1706049864.445699]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [329 - 1706049864.4458652]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [329 - 1706049864.4460087]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [329 - 1706049864.4461436]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [329 - 1706049864.4465494]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [329 - 1706049864.4469748]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [329 - 1706049864.4471421]: 0.0
python ParamPruning/classifier.weight [329 - 1706049864.4472115]: 0.0
python DistillationModifier [336 - 1706049897.0172377]: Calling loss_update with:
args: 0.06407389044761658| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.333333333333333| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [336 - 1706049898.1244028]: 
Returned: 0.1887885332107544| 

python LearningRateFunctionModifier [336 - 1706049900.0789936]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.333333333333333| steps_per_epoch: 63| 
python LearningRateFunctionModifier [336 - 1706049900.0791376]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [336 - 1706049900.0791836]: 8.846153846153845e-05
python LearningRateFunctionModifier/ParamGroup1 [336 - 1706049900.0792153]: 8.846153846153845e-05
python DistillationModifier/task_loss [336 - 1706049900.0792582]: tensor(0.0641, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [336 - 1706049900.0798006]: tensor(0.1888, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [336 - 1706049900.0800567]: tensor(0.1888, grad_fn=<AddBackward0>)
python ConstantPruningModifier [336 - 1706049900.080286]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.333333333333333| steps_per_epoch: 63| 
python ConstantPruningModifier [336 - 1706049900.329115]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [336 - 1706049900.3295925]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [336 - 1706049900.3298707]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [336 - 1706049900.3300884]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [336 - 1706049900.3302996]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [336 - 1706049900.330908]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [336 - 1706049900.3313804]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [336 - 1706049900.3316078]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [336 - 1706049900.3318362]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [336 - 1706049900.3320234]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [336 - 1706049900.332176]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [336 - 1706049900.332545]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [336 - 1706049900.3329728]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [336 - 1706049900.333159]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [336 - 1706049900.3333237]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [336 - 1706049900.3334901]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [336 - 1706049900.3336651]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [336 - 1706049900.3340037]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [336 - 1706049900.3343542]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [336 - 1706049900.334531]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [336 - 1706049900.3347127]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [336 - 1706049900.3348644]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [336 - 1706049900.3350458]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [336 - 1706049900.335407]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [336 - 1706049900.3357658]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [336 - 1706049900.335934]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [336 - 1706049900.336096]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [336 - 1706049900.3362532]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [336 - 1706049900.336401]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [336 - 1706049900.3367922]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [336 - 1706049900.3371394]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [336 - 1706049900.3373048]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [336 - 1706049900.3374484]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [336 - 1706049900.3376143]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [336 - 1706049900.337801]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [336 - 1706049900.3381722]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [336 - 1706049900.3385122]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [336 - 1706049900.3386984]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [336 - 1706049900.3388577]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [336 - 1706049900.3390129]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [336 - 1706049900.3391461]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [336 - 1706049900.3394868]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [336 - 1706049900.3398635]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [336 - 1706049900.3400276]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [336 - 1706049900.3401725]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [336 - 1706049900.3403308]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [336 - 1706049900.3404965]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [336 - 1706049900.3408945]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [336 - 1706049900.341268]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [336 - 1706049900.341433]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [336 - 1706049900.3416274]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [336 - 1706049900.3418202]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [336 - 1706049900.341971]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [336 - 1706049900.3423624]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [336 - 1706049900.3427095]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [336 - 1706049900.3428838]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [336 - 1706049900.3430364]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [336 - 1706049900.343178]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [336 - 1706049900.343317]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [336 - 1706049900.343696]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [336 - 1706049900.3440385]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [336 - 1706049900.3442068]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [336 - 1706049900.344367]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [336 - 1706049900.3445187]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [336 - 1706049900.3447037]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [336 - 1706049900.3450437]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [336 - 1706049900.3454094]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [336 - 1706049900.3455982]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [336 - 1706049900.3457603]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [336 - 1706049900.3459144]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [336 - 1706049900.3460877]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [336 - 1706049900.3464622]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [336 - 1706049900.3468487]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [336 - 1706049900.3470173]: 0.0
python ParamPruning/classifier.weight [336 - 1706049900.347085]: 0.0
python DistillationModifier [343 - 1706049936.9382586]: Calling loss_update with:
args: 0.1395263373851776| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.444444444444445| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [343 - 1706049938.2269006]: 
Returned: 0.11891181021928787| 

python LearningRateFunctionModifier [343 - 1706049941.2616365]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.444444444444445| steps_per_epoch: 63| 
python LearningRateFunctionModifier [343 - 1706049941.2617793]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [343 - 1706049941.2618248]: 8.717948717948717e-05
python LearningRateFunctionModifier/ParamGroup1 [343 - 1706049941.261855]: 8.717948717948717e-05
python DistillationModifier/task_loss [343 - 1706049941.2619119]: tensor(0.1395, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [343 - 1706049941.2623951]: tensor(0.1189, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [343 - 1706049941.2626646]: tensor(0.1189, grad_fn=<AddBackward0>)
python ConstantPruningModifier [343 - 1706049941.2629015]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.444444444444445| steps_per_epoch: 63| 
python ConstantPruningModifier [343 - 1706049941.544144]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [343 - 1706049941.5446653]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [343 - 1706049941.5449033]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [343 - 1706049941.5451465]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [343 - 1706049941.545347]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [343 - 1706049941.5461273]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [343 - 1706049941.546737]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [343 - 1706049941.5469909]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [343 - 1706049941.5472138]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [343 - 1706049941.5473914]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [343 - 1706049941.5475545]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [343 - 1706049941.5481024]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [343 - 1706049941.5486674]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [343 - 1706049941.5488975]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [343 - 1706049941.5490997]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [343 - 1706049941.5492644]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [343 - 1706049941.5494246]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [343 - 1706049941.5499666]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [343 - 1706049941.5505002]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [343 - 1706049941.55074]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [343 - 1706049941.5509188]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [343 - 1706049941.5511394]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [343 - 1706049941.5513685]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [343 - 1706049941.5519192]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [343 - 1706049941.5524533]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [343 - 1706049941.6167223]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [343 - 1706049941.6169827]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [343 - 1706049941.6171737]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [343 - 1706049941.6173573]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [343 - 1706049941.6179209]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [343 - 1706049941.6184607]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [343 - 1706049941.6187067]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [343 - 1706049941.6188807]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [343 - 1706049941.6190946]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [343 - 1706049941.619274]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [343 - 1706049941.6198213]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [343 - 1706049941.6203594]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [343 - 1706049941.6206005]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [343 - 1706049941.6208162]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [343 - 1706049941.621037]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [343 - 1706049941.6212223]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [343 - 1706049941.6217606]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [343 - 1706049941.6222994]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [343 - 1706049941.6225214]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [343 - 1706049941.6227412]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [343 - 1706049941.622915]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [343 - 1706049941.6231117]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [343 - 1706049941.623646]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [343 - 1706049941.6241813]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [343 - 1706049941.624385]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [343 - 1706049941.6246634]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [343 - 1706049941.6248782]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [343 - 1706049941.6250508]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [343 - 1706049941.6255887]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [343 - 1706049941.6261327]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [343 - 1706049941.6263597]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [343 - 1706049941.626552]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [343 - 1706049941.626779]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [343 - 1706049941.626947]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [343 - 1706049941.6274593]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [343 - 1706049941.6280131]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [343 - 1706049941.628218]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [343 - 1706049941.628383]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [343 - 1706049941.628608]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [343 - 1706049941.6288543]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [343 - 1706049941.629373]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [343 - 1706049941.6299293]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [343 - 1706049941.6301868]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [343 - 1706049941.630395]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [343 - 1706049941.6305842]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [343 - 1706049941.630785]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [343 - 1706049941.6313176]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [343 - 1706049941.6319113]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [343 - 1706049941.6321425]: 0.0
python ParamPruning/classifier.weight [343 - 1706049941.6322253]: 0.0
python DistillationModifier [350 - 1706049981.525368]: Calling loss_update with:
args: 0.20854796469211578| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.555555555555555| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [350 - 1706049982.5533643]: 
Returned: 0.2866942584514618| 

python LearningRateFunctionModifier [350 - 1706049985.5174634]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.555555555555555| steps_per_epoch: 63| 
python LearningRateFunctionModifier [350 - 1706049985.5176528]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [350 - 1706049985.517702]: 8.58974358974359e-05
python LearningRateFunctionModifier/ParamGroup1 [350 - 1706049985.5177333]: 8.58974358974359e-05
python DistillationModifier/task_loss [350 - 1706049985.5177758]: tensor(0.2085, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [350 - 1706049985.5182524]: tensor(0.2867, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [350 - 1706049985.5184903]: tensor(0.2867, grad_fn=<AddBackward0>)
python ConstantPruningModifier [350 - 1706049985.518743]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.555555555555555| steps_per_epoch: 63| 
python ConstantPruningModifier [350 - 1706049985.826345]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [350 - 1706049985.8268523]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [350 - 1706049985.8271434]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [350 - 1706049985.8273544]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [350 - 1706049985.8275404]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [350 - 1706049985.828375]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [350 - 1706049985.82899]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [350 - 1706049985.829211]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [350 - 1706049985.8294196]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [350 - 1706049985.8296192]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [350 - 1706049985.829799]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [350 - 1706049985.830313]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [350 - 1706049985.8310835]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [350 - 1706049985.8312838]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [350 - 1706049985.8315392]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [350 - 1706049985.831746]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [350 - 1706049985.8319342]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [350 - 1706049985.8324783]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [350 - 1706049985.8332868]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [350 - 1706049985.833487]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [350 - 1706049985.833757]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [350 - 1706049985.833973]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [350 - 1706049985.8342025]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [350 - 1706049985.8347647]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [350 - 1706049985.835504]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [350 - 1706049985.8357255]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [350 - 1706049985.8359609]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [350 - 1706049985.8361378]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [350 - 1706049985.836342]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [350 - 1706049985.8369374]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [350 - 1706049985.837585]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [350 - 1706049985.8377852]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [350 - 1706049985.8379846]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [350 - 1706049985.83821]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [350 - 1706049985.8383834]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [350 - 1706049985.838943]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [350 - 1706049985.8395183]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [350 - 1706049985.8397393]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [350 - 1706049985.839929]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [350 - 1706049985.8401008]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [350 - 1706049985.8402717]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [350 - 1706049985.840852]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [350 - 1706049985.8413885]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [350 - 1706049985.8415968]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [350 - 1706049985.8417904]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [350 - 1706049985.8420167]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [350 - 1706049985.8421893]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [350 - 1706049985.842722]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [350 - 1706049985.8434057]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [350 - 1706049985.8436122]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [350 - 1706049985.8438203]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [350 - 1706049985.843994]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [350 - 1706049985.8441596]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [350 - 1706049985.8447464]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [350 - 1706049985.8452778]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [350 - 1706049985.8454623]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [350 - 1706049985.845704]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [350 - 1706049985.8459353]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [350 - 1706049985.846108]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [350 - 1706049985.846638]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [350 - 1706049985.847321]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [350 - 1706049985.8475516]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [350 - 1706049985.8477817]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [350 - 1706049985.8479617]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [350 - 1706049985.8481355]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [350 - 1706049985.8487117]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [350 - 1706049985.849336]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [350 - 1706049985.8495512]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [350 - 1706049985.8498068]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [350 - 1706049985.84998]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [350 - 1706049985.8501735]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [350 - 1706049985.850746]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [350 - 1706049985.8513613]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [350 - 1706049985.8516176]: 0.0
python ParamPruning/classifier.weight [350 - 1706049985.8516994]: 0.0
python DistillationModifier [357 - 1706050018.8248613]: Calling loss_update with:
args: 0.187872976064682| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.666666666666667| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [357 - 1706050020.4374998]: 
Returned: 0.15813423693180084| 

python LearningRateFunctionModifier [357 - 1706050023.0634727]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.666666666666667| steps_per_epoch: 63| 
python LearningRateFunctionModifier [357 - 1706050023.0636373]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [357 - 1706050023.0636835]: 8.46153846153846e-05
python LearningRateFunctionModifier/ParamGroup1 [357 - 1706050023.0637145]: 8.46153846153846e-05
python DistillationModifier/task_loss [357 - 1706050023.0637574]: tensor(0.1879, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [357 - 1706050023.0642374]: tensor(0.1581, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [357 - 1706050023.0644877]: tensor(0.1581, grad_fn=<AddBackward0>)
python ConstantPruningModifier [357 - 1706050023.0647628]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.666666666666667| steps_per_epoch: 63| 
python ConstantPruningModifier [357 - 1706050023.3435454]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [357 - 1706050023.3440576]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [357 - 1706050023.3443382]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [357 - 1706050023.3445432]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [357 - 1706050023.3447793]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [357 - 1706050023.3455951]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [357 - 1706050023.3463836]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [357 - 1706050023.346654]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [357 - 1706050023.3468752]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [357 - 1706050023.3470833]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [357 - 1706050023.347262]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [357 - 1706050023.3479095]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [357 - 1706050023.3484528]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [357 - 1706050023.348691]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [357 - 1706050023.348901]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [357 - 1706050023.3490744]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [357 - 1706050023.3492465]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [357 - 1706050023.34978]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [357 - 1706050023.3504581]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [357 - 1706050023.3507109]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [357 - 1706050023.3509195]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [357 - 1706050023.351124]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [357 - 1706050023.3512974]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [357 - 1706050023.3518968]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [357 - 1706050023.352514]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [357 - 1706050023.416826]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [357 - 1706050023.4170604]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [357 - 1706050023.4172766]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [357 - 1706050023.4175117]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [357 - 1706050023.4181721]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [357 - 1706050023.4188035]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [357 - 1706050023.4190311]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [357 - 1706050023.4192355]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [357 - 1706050023.4194653]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [357 - 1706050023.4196649]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [357 - 1706050023.4202363]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [357 - 1706050023.4208317]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [357 - 1706050023.4210591]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [357 - 1706050023.4212625]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [357 - 1706050023.4214928]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [357 - 1706050023.4217396]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [357 - 1706050023.4223394]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [357 - 1706050023.4229496]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [357 - 1706050023.4231677]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [357 - 1706050023.4233887]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [357 - 1706050023.4236197]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [357 - 1706050023.4238114]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [357 - 1706050023.4243796]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [357 - 1706050023.4249938]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [357 - 1706050023.4252179]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [357 - 1706050023.4254212]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [357 - 1706050023.4256744]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [357 - 1706050023.4259086]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [357 - 1706050023.42651]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [357 - 1706050023.4271255]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [357 - 1706050023.4273427]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [357 - 1706050023.4275804]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [357 - 1706050023.4278073]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [357 - 1706050023.4279895]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [357 - 1706050023.4285548]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [357 - 1706050023.4291248]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [357 - 1706050023.429339]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [357 - 1706050023.4295375]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [357 - 1706050023.429791]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [357 - 1706050023.4300194]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [357 - 1706050023.4305506]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [357 - 1706050023.431264]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [357 - 1706050023.4314883]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [357 - 1706050023.4317203]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [357 - 1706050023.4319482]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [357 - 1706050023.4321299]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [357 - 1706050023.4327219]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [357 - 1706050023.4333394]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [357 - 1706050023.4335566]: 0.0
python ParamPruning/classifier.weight [357 - 1706050023.433648]: 0.0
python DistillationModifier [364 - 1706050056.6351213]: Calling loss_update with:
args: 0.2201034277677536| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.777777777777778| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [364 - 1706050057.8425589]: 
Returned: 0.47298023104667664| 

python LearningRateFunctionModifier [364 - 1706050060.8681352]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.777777777777778| steps_per_epoch: 63| 
python LearningRateFunctionModifier [364 - 1706050060.8682847]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [364 - 1706050060.8683317]: 8.333333333333333e-05
python LearningRateFunctionModifier/ParamGroup1 [364 - 1706050060.8683636]: 8.333333333333333e-05
python DistillationModifier/task_loss [364 - 1706050060.8684046]: tensor(0.2201, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [364 - 1706050060.8689303]: tensor(0.4730, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [364 - 1706050060.8691728]: tensor(0.4730, grad_fn=<AddBackward0>)
python ConstantPruningModifier [364 - 1706050060.869403]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.777777777777778| steps_per_epoch: 63| 
python ConstantPruningModifier [364 - 1706050061.150893]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [364 - 1706050061.151373]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [364 - 1706050061.1516774]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [364 - 1706050061.151934]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [364 - 1706050061.1521137]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [364 - 1706050061.2168195]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [364 - 1706050061.217648]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [364 - 1706050061.2179024]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [364 - 1706050061.2181463]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [364 - 1706050061.2183526]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [364 - 1706050061.21854]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [364 - 1706050061.2191942]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [364 - 1706050061.2197714]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [364 - 1706050061.2200146]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [364 - 1706050061.2201817]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [364 - 1706050061.22038]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [364 - 1706050061.2206552]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [364 - 1706050061.2211916]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [364 - 1706050061.2217407]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [364 - 1706050061.22195]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [364 - 1706050061.2221465]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [364 - 1706050061.2223215]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [364 - 1706050061.2224922]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [364 - 1706050061.2230387]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [364 - 1706050061.2237682]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [364 - 1706050061.223974]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [364 - 1706050061.2241728]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [364 - 1706050061.224351]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [364 - 1706050061.2245204]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [364 - 1706050061.225113]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [364 - 1706050061.2257676]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [364 - 1706050061.2259724]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [364 - 1706050061.2261717]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [364 - 1706050061.2263453]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [364 - 1706050061.2265189]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [364 - 1706050061.2271457]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [364 - 1706050061.2277749]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [364 - 1706050061.2279813]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [364 - 1706050061.2281775]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [364 - 1706050061.2283528]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [364 - 1706050061.2285252]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [364 - 1706050061.2291522]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [364 - 1706050061.2297847]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [364 - 1706050061.2299929]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [364 - 1706050061.230187]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [364 - 1706050061.2303624]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [364 - 1706050061.2305353]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [364 - 1706050061.2311552]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [364 - 1706050061.2317472]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [364 - 1706050061.231952]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [364 - 1706050061.232147]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [364 - 1706050061.2323222]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [364 - 1706050061.232585]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [364 - 1706050061.2331371]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [364 - 1706050061.2338526]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [364 - 1706050061.234058]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [364 - 1706050061.2342556]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [364 - 1706050061.2344327]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [364 - 1706050061.2346222]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [364 - 1706050061.2351801]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [364 - 1706050061.2358205]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [364 - 1706050061.2360258]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [364 - 1706050061.2361877]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [364 - 1706050061.2363803]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [364 - 1706050061.2365541]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [364 - 1706050061.237189]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [364 - 1706050061.2378385]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [364 - 1706050061.2380464]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [364 - 1706050061.2382479]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [364 - 1706050061.2384222]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [364 - 1706050061.2386017]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [364 - 1706050061.239178]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [364 - 1706050061.239732]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [364 - 1706050061.2399418]: 0.0
python ParamPruning/classifier.weight [364 - 1706050061.2400107]: 0.0
python DistillationModifier [371 - 1706050098.0186355]: Calling loss_update with:
args: 0.1020946204662323| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.888888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [371 - 1706050099.5479548]: 
Returned: 0.20964932441711426| 

python LearningRateFunctionModifier [371 - 1706050101.9762836]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.888888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [371 - 1706050101.9764314]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [371 - 1706050101.976477]: 8.205128205128203e-05
python LearningRateFunctionModifier/ParamGroup1 [371 - 1706050101.9765077]: 8.205128205128203e-05
python DistillationModifier/task_loss [371 - 1706050101.9765487]: tensor(0.1021, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [371 - 1706050101.985511]: tensor(0.2096, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [371 - 1706050101.9858048]: tensor(0.2096, grad_fn=<AddBackward0>)
python ConstantPruningModifier [371 - 1706050101.9860394]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.888888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [371 - 1706050102.2207978]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [371 - 1706050102.221291]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [371 - 1706050102.2215781]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [371 - 1706050102.221818]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [371 - 1706050102.2220442]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [371 - 1706050102.2226777]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [371 - 1706050102.2231333]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [371 - 1706050102.2233367]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [371 - 1706050102.2235265]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [371 - 1706050102.2236986]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [371 - 1706050102.2238941]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [371 - 1706050102.224256]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [371 - 1706050102.224656]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [371 - 1706050102.2248495]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [371 - 1706050102.2250187]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [371 - 1706050102.2251546]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [371 - 1706050102.2253006]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [371 - 1706050102.2256658]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [371 - 1706050102.2260327]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [371 - 1706050102.2262077]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [371 - 1706050102.2263434]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [371 - 1706050102.2264748]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [371 - 1706050102.2266257]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [371 - 1706050102.2269988]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [371 - 1706050102.2273319]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [371 - 1706050102.227499]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [371 - 1706050102.2276862]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [371 - 1706050102.227828]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [371 - 1706050102.2279708]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [371 - 1706050102.2283263]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [371 - 1706050102.2287161]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [371 - 1706050102.2288764]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [371 - 1706050102.2290297]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [371 - 1706050102.2292094]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [371 - 1706050102.2293468]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [371 - 1706050102.2297459]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [371 - 1706050102.2302527]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [371 - 1706050102.2304196]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [371 - 1706050102.2305782]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [371 - 1706050102.2307234]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [371 - 1706050102.230885]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [371 - 1706050102.2312672]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [371 - 1706050102.231682]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [371 - 1706050102.231845]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [371 - 1706050102.2319832]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [371 - 1706050102.232115]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [371 - 1706050102.2322829]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [371 - 1706050102.2327158]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [371 - 1706050102.2331755]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [371 - 1706050102.2333574]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [371 - 1706050102.2335088]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [371 - 1706050102.2336993]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [371 - 1706050102.2339087]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [371 - 1706050102.2343748]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [371 - 1706050102.2348025]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [371 - 1706050102.234973]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [371 - 1706050102.2351491]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [371 - 1706050102.2353215]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [371 - 1706050102.2354789]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [371 - 1706050102.2359078]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [371 - 1706050102.2364438]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [371 - 1706050102.2366476]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [371 - 1706050102.236825]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [371 - 1706050102.2369714]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [371 - 1706050102.2371228]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [371 - 1706050102.2374995]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [371 - 1706050102.2378826]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [371 - 1706050102.2380536]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [371 - 1706050102.238236]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [371 - 1706050102.2383797]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [371 - 1706050102.2385209]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [371 - 1706050102.239037]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [371 - 1706050102.2394736]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [371 - 1706050102.2396615]: 0.0
python ParamPruning/classifier.weight [371 - 1706050102.2397332]: 0.0
python DistillationModifier [378 - 1706050136.6216898]: Calling loss_update with:
args: 0.4974173605442047| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050138.1463268]: 
Returned: 1.2023807764053345| 

python DistillationModifier [378 - 1706050139.7328641]: Calling loss_update with:
args: 1.3326066732406616| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050140.8167574]: 
Returned: 2.18481183052063| 

python DistillationModifier [378 - 1706050142.1336794]: Calling loss_update with:
args: 1.1588655710220337| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050143.2450447]: 
Returned: 1.2632402181625366| 

python DistillationModifier [378 - 1706050144.3330271]: Calling loss_update with:
args: 0.698570191860199| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050145.343224]: 
Returned: 0.8914503455162048| 

python DistillationModifier [378 - 1706050146.4334195]: Calling loss_update with:
args: 1.179563283920288| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050147.5404503]: 
Returned: 1.7738475799560547| 

python DistillationModifier [378 - 1706050148.6396568]: Calling loss_update with:
args: 0.8462764024734497| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050149.8173773]: 
Returned: 1.6434557437896729| 

python DistillationModifier [378 - 1706050150.8409488]: Calling loss_update with:
args: 0.2789236605167389| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050151.8482134]: 
Returned: 0.7619563937187195| 

python DistillationModifier [378 - 1706050152.9415455]: Calling loss_update with:
args: 1.118726372718811| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050153.9481866]: 
Returned: 1.1116559505462646| 

python DistillationModifier [378 - 1706050155.0479944]: Calling loss_update with:
args: 0.813321590423584| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050156.341696]: 
Returned: 0.6733986139297485| 

python DistillationModifier [378 - 1706050157.4416456]: Calling loss_update with:
args: 0.8491990566253662| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050158.526547]: 
Returned: 1.3475584983825684| 

python DistillationModifier [378 - 1706050159.6201167]: Calling loss_update with:
args: 0.6308839917182922| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050160.6301022]: 
Returned: 1.037684440612793| 

python DistillationModifier [378 - 1706050161.9341545]: Calling loss_update with:
args: 1.063583493232727| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050162.940842]: 
Returned: 1.6323219537734985| 

python DistillationModifier [378 - 1706050164.2363024]: Calling loss_update with:
args: 0.9134078025817871| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050165.6198277]: 
Returned: 1.8352948427200317| 

python DistillationModifier [378 - 1706050166.644518]: Calling loss_update with:
args: 0.6387275457382202| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050167.7175183]: 
Returned: 0.6227970123291016| 

python DistillationModifier [378 - 1706050168.7395]: Calling loss_update with:
args: 1.2646353244781494| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050169.8167036]: 
Returned: 2.049973726272583| 

python DistillationModifier [378 - 1706050170.852247]: Calling loss_update with:
args: 0.7857721447944641| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050171.9463558]: 
Returned: 1.3084814548492432| 

python DistillationModifier [378 - 1706050173.0323575]: Calling loss_update with:
args: 0.6998496651649475| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050174.0391018]: 
Returned: 0.9375443458557129| 

python DistillationModifier [378 - 1706050175.6493928]: Calling loss_update with:
args: 0.9469525218009949| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050176.828486]: 
Returned: 1.6684985160827637| 

python DistillationModifier [378 - 1706050177.927202]: Calling loss_update with:
args: 0.9596917033195496| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050179.1242447]: 
Returned: 1.2181949615478516| 

python DistillationModifier [378 - 1706050180.7353532]: Calling loss_update with:
args: 0.6875466704368591| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050182.3227015]: 
Returned: 1.2458690404891968| 

python DistillationModifier [378 - 1706050183.8289824]: Calling loss_update with:
args: 0.6219781041145325| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050184.839439]: 
Returned: 1.2636523246765137| 

python DistillationModifier [378 - 1706050186.027224]: Calling loss_update with:
args: 0.549430251121521| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050187.0388615]: 
Returned: 0.7275329828262329| 

python DistillationModifier [378 - 1706050188.1397793]: Calling loss_update with:
args: 0.5703279972076416| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050189.2227461]: 
Returned: 1.0429006814956665| 

python DistillationModifier [378 - 1706050190.2466154]: Calling loss_update with:
args: 0.6694209575653076| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050191.3216677]: 
Returned: 1.2616788148880005| 

python DistillationModifier [378 - 1706050192.3407075]: Calling loss_update with:
args: 0.8764914274215698| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050193.4185178]: 
Returned: 0.8148539662361145| 

python DistillationModifier [378 - 1706050194.9493802]: Calling loss_update with:
args: 0.38823777437210083| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050196.3356562]: 
Returned: 0.6219384670257568| 

python DistillationModifier [378 - 1706050197.419455]: Calling loss_update with:
args: 0.6056927442550659| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050198.432532]: 
Returned: 0.7602862119674683| 

python DistillationModifier [378 - 1706050199.5230873]: Calling loss_update with:
args: 0.7087193727493286| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050200.7490633]: 
Returned: 1.0868992805480957| 

python DistillationModifier [378 - 1706050202.1377404]: Calling loss_update with:
args: 0.30338382720947266| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050203.72921]: 
Returned: 0.7209649085998535| 

python DistillationModifier [378 - 1706050205.330511]: Calling loss_update with:
args: 0.428092896938324| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050206.4516082]: 
Returned: 0.9895107746124268| 

python DistillationModifier [378 - 1706050208.0364168]: Calling loss_update with:
args: 0.8324893712997437| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050209.6307871]: 
Returned: 1.640277624130249| 

python DistillationModifier [378 - 1706050211.2352223]: Calling loss_update with:
args: 0.9982948303222656| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050212.8302267]: 
Returned: 1.2329920530319214| 

python DistillationModifier [378 - 1706050214.4357228]: Calling loss_update with:
args: 0.9557772278785706| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050215.625898]: 
Returned: 1.5771384239196777| 

python DistillationModifier [378 - 1706050217.2342715]: Calling loss_update with:
args: 1.3097933530807495| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050218.8337796]: 
Returned: 1.9983396530151367| 

python DistillationModifier [378 - 1706050220.4480736]: Calling loss_update with:
args: 1.4885458946228027| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050221.6327295]: 
Returned: 1.8353159427642822| 

python DistillationModifier [378 - 1706050222.7228837]: Calling loss_update with:
args: 1.1021288633346558| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050223.7469807]: 
Returned: 2.0883700847625732| 

python DistillationModifier [378 - 1706050224.8322756]: Calling loss_update with:
args: 0.9244117140769958| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050225.9230382]: 
Returned: 0.9949327707290649| 

python DistillationModifier [378 - 1706050226.9457452]: Calling loss_update with:
args: 0.5983781814575195| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050228.02335]: 
Returned: 1.024308681488037| 

python DistillationModifier [378 - 1706050229.0470543]: Calling loss_update with:
args: 0.7867780327796936| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050230.123102]: 
Returned: 1.47484290599823| 

python DistillationModifier [378 - 1706050231.2188866]: Calling loss_update with:
args: 1.2696646451950073| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050232.6399763]: 
Returned: 2.3203742504119873| 

python DistillationModifier [378 - 1706050233.9418924]: Calling loss_update with:
args: 0.551866888999939| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050234.9508367]: 
Returned: 0.8474615812301636| 

python DistillationModifier [378 - 1706050236.032959]: Calling loss_update with:
args: 0.6444303393363953| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050237.0426955]: 
Returned: 1.3003968000411987| 

python DistillationModifier [378 - 1706050238.2346785]: Calling loss_update with:
args: 0.8792864680290222| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050239.835955]: 
Returned: 0.7901608943939209| 

python DistillationModifier [378 - 1706050241.448256]: Calling loss_update with:
args: 0.7129641175270081| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050243.0397363]: 
Returned: 1.6014896631240845| 

python DistillationModifier [378 - 1706050244.6492543]: Calling loss_update with:
args: 0.7165770530700684| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050246.1239474]: 
Returned: 1.8618508577346802| 

python DistillationModifier [378 - 1706050247.7361662]: Calling loss_update with:
args: 1.112173318862915| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050249.3368168]: 
Returned: 1.3776582479476929| 

python DistillationModifier [378 - 1706050250.71774]: Calling loss_update with:
args: 0.7234148979187012| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050251.7330463]: 
Returned: 1.3184099197387695| 

python DistillationModifier [378 - 1706050252.821393]: Calling loss_update with:
args: 0.21237342059612274| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050253.9339976]: 
Returned: 0.5132840275764465| 

python DistillationModifier [378 - 1706050255.5424998]: Calling loss_update with:
args: 1.1296355724334717| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050257.141789]: 
Returned: 2.16776704788208| 

python DistillationModifier [378 - 1706050258.8178582]: Calling loss_update with:
args: 0.8308643698692322| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050260.1306407]: 
Returned: 1.4683961868286133| 

python DistillationModifier [378 - 1706050261.432061]: Calling loss_update with:
args: 0.3155498504638672| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050263.0210314]: 
Returned: 0.643677830696106| 

python DistillationModifier [378 - 1706050264.041198]: Calling loss_update with:
args: 2.0626978874206543| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050265.0510206]: 
Returned: 2.019998073577881| 

python DistillationModifier [378 - 1706050266.1377897]: Calling loss_update with:
args: 0.7967185378074646| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050267.2284214]: 
Returned: 1.0300735235214233| 

python DistillationModifier [378 - 1706050268.8486593]: Calling loss_update with:
args: 0.7631784081459045| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050270.2368364]: 
Returned: 1.1634589433670044| 

python DistillationModifier [378 - 1706050271.3204525]: Calling loss_update with:
args: 0.5473234057426453| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050272.327349]: 
Returned: 1.3491668701171875| 

python DistillationModifier [378 - 1706050273.4252944]: Calling loss_update with:
args: 0.7318657040596008| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050274.4333284]: 
Returned: 1.1404558420181274| 

python DistillationModifier [378 - 1706050275.5192523]: Calling loss_update with:
args: 0.9912517666816711| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050276.6260872]: 
Returned: 0.7220553159713745| 

python DistillationModifier [378 - 1706050277.729886]: Calling loss_update with:
args: 1.1270397901535034| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050278.7408397]: 
Returned: 1.3635061979293823| 

python DistillationModifier [378 - 1706050280.3483286]: Calling loss_update with:
args: 0.9535453915596008| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050281.9470634]: 
Returned: 1.3214033842086792| 

python DistillationModifier [378 - 1706050283.6279852]: Calling loss_update with:
args: 0.840544581413269| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050285.224051]: 
Returned: 1.511814832687378| 

python DistillationModifier [378 - 1706050286.8506246]: Calling loss_update with:
args: 0.8220563530921936| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050287.9492896]: 
Returned: 1.301771640777588| 

python DistillationModifier [378 - 1706050289.0337882]: Calling loss_update with:
args: 0.6313296556472778| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050290.0503592]: 
Returned: 1.4412022829055786| 

python DistillationModifier [378 - 1706050290.6471012]: Calling loss_update with:
args: 0.48669686913490295| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050291.2257557]: 
Returned: 0.9961477518081665| 

python DistillationModifier [378 - 1706050293.2387042]: Calling loss_update with:
args: 0.2464440017938614| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1706050294.5191617]: 
Returned: 0.13866879045963287| 

python LearningRateFunctionModifier [378 - 1706050296.472034]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [378 - 1706050296.4721816]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [378 - 1706050296.472228]: 8.076923076923076e-05
python LearningRateFunctionModifier/ParamGroup1 [378 - 1706050296.47226]: 8.076923076923076e-05
python DistillationModifier/task_loss [378 - 1706050296.4723022]: tensor(0.2464, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [378 - 1706050296.4728272]: tensor(0.1387, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [378 - 1706050296.4730766]: tensor(0.1387, grad_fn=<AddBackward0>)
python ConstantPruningModifier [378 - 1706050296.4733086]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| 
python ConstantPruningModifier [378 - 1706050296.724689]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [378 - 1706050296.7251484]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [378 - 1706050296.7253575]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [378 - 1706050296.7255282]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [378 - 1706050296.7257638]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [378 - 1706050296.726349]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [378 - 1706050296.7270238]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [378 - 1706050296.7272527]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [378 - 1706050296.7274532]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [378 - 1706050296.7276473]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [378 - 1706050296.7278297]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [378 - 1706050296.728337]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [378 - 1706050296.728856]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [378 - 1706050296.7290716]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [378 - 1706050296.7292607]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [378 - 1706050296.729419]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [378 - 1706050296.7295854]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [378 - 1706050296.7300165]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [378 - 1706050296.7304373]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [378 - 1706050296.7306936]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [378 - 1706050296.7308807]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [378 - 1706050296.7310266]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [378 - 1706050296.7311745]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [378 - 1706050296.7316067]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [378 - 1706050296.7320259]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [378 - 1706050296.7322218]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [378 - 1706050296.7324078]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [378 - 1706050296.7325573]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [378 - 1706050296.7327495]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [378 - 1706050296.7331936]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [378 - 1706050296.7337432]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [378 - 1706050296.7339559]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [378 - 1706050296.73416]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [378 - 1706050296.7343104]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [378 - 1706050296.7344918]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [378 - 1706050296.7349484]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [378 - 1706050296.735382]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [378 - 1706050296.7356067]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [378 - 1706050296.735801]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [378 - 1706050296.7359447]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [378 - 1706050296.7361336]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [378 - 1706050296.736633]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [378 - 1706050296.7370636]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [378 - 1706050296.7372644]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [378 - 1706050296.737458]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [378 - 1706050296.7376406]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [378 - 1706050296.7377973]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [378 - 1706050296.738275]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [378 - 1706050296.738841]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [378 - 1706050296.7390463]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [378 - 1706050296.7392468]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [378 - 1706050296.7394094]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [378 - 1706050296.7396197]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [378 - 1706050296.7400653]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [378 - 1706050296.740531]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [378 - 1706050296.7407894]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [378 - 1706050296.7409923]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [378 - 1706050296.7411473]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [378 - 1706050296.7413154]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [378 - 1706050296.7418447]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [378 - 1706050296.7423158]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [378 - 1706050296.7425182]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [378 - 1706050296.7427332]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [378 - 1706050296.742928]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [378 - 1706050296.743132]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [378 - 1706050296.7436454]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [378 - 1706050296.744065]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [378 - 1706050296.7442656]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [378 - 1706050296.7444458]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [378 - 1706050296.7446654]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [378 - 1706050296.7448704]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [378 - 1706050296.7452667]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [378 - 1706050296.7456775]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [378 - 1706050296.7458777]: 0.0
python ParamPruning/classifier.weight [378 - 1706050296.7459466]: 0.0
python DistillationModifier [385 - 1706050334.121588]: Calling loss_update with:
args: 0.17903783917427063| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.111111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [385 - 1706050335.742024]: 
Returned: 0.1736038774251938| 

python LearningRateFunctionModifier [385 - 1706050338.7631636]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.111111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [385 - 1706050338.7633116]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [385 - 1706050338.7633564]: 7.948717948717948e-05
python LearningRateFunctionModifier/ParamGroup1 [385 - 1706050338.7633884]: 7.948717948717948e-05
python DistillationModifier/task_loss [385 - 1706050338.7634294]: tensor(0.1790, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [385 - 1706050338.7639287]: tensor(0.1736, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [385 - 1706050338.7641647]: tensor(0.1736, grad_fn=<AddBackward0>)
python ConstantPruningModifier [385 - 1706050338.764388]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.111111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [385 - 1706050339.0435069]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [385 - 1706050339.0440218]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [385 - 1706050339.044327]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [385 - 1706050339.044544]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [385 - 1706050339.0448284]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [385 - 1706050339.0456157]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [385 - 1706050339.0464258]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [385 - 1706050339.0466995]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [385 - 1706050339.0469296]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [385 - 1706050339.0470948]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [385 - 1706050339.0473175]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [385 - 1706050339.04795]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [385 - 1706050339.0485036]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [385 - 1706050339.0487585]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [385 - 1706050339.0489783]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [385 - 1706050339.0491922]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [385 - 1706050339.0493648]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [385 - 1706050339.050039]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [385 - 1706050339.0508451]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [385 - 1706050339.0510652]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [385 - 1706050339.0512812]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [385 - 1706050339.0514567]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [385 - 1706050339.051637]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [385 - 1706050339.052295]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [385 - 1706050339.1170382]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [385 - 1706050339.1173322]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [385 - 1706050339.1175857]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [385 - 1706050339.1178424]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [385 - 1706050339.1180284]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [385 - 1706050339.1187687]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [385 - 1706050339.1194108]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [385 - 1706050339.1196542]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [385 - 1706050339.119879]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [385 - 1706050339.1200964]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [385 - 1706050339.1202798]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [385 - 1706050339.1209142]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [385 - 1706050339.1214814]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [385 - 1706050339.121696]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [385 - 1706050339.1219344]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [385 - 1706050339.1221457]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [385 - 1706050339.1223202]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [385 - 1706050339.1228807]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [385 - 1706050339.1234212]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [385 - 1706050339.1236267]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [385 - 1706050339.1238303]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [385 - 1706050339.124001]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [385 - 1706050339.12416]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [385 - 1706050339.1247182]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [385 - 1706050339.1254375]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [385 - 1706050339.1256611]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [385 - 1706050339.1258745]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [385 - 1706050339.1260502]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [385 - 1706050339.126221]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [385 - 1706050339.1268163]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [385 - 1706050339.1273696]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [385 - 1706050339.127612]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [385 - 1706050339.1278117]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [385 - 1706050339.1280031]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [385 - 1706050339.1282306]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [385 - 1706050339.1287894]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [385 - 1706050339.12948]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [385 - 1706050339.1297162]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [385 - 1706050339.129951]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [385 - 1706050339.1301312]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [385 - 1706050339.1303303]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [385 - 1706050339.1309075]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [385 - 1706050339.1315286]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [385 - 1706050339.131765]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [385 - 1706050339.131964]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [385 - 1706050339.1321394]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [385 - 1706050339.1322985]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [385 - 1706050339.13288]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [385 - 1706050339.13347]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [385 - 1706050339.1336977]: 0.0
python ParamPruning/classifier.weight [385 - 1706050339.1337862]: 0.0
python DistillationModifier [392 - 1706050373.1219275]: Calling loss_update with:
args: 0.2659142315387726| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.222222222222222| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [392 - 1706050374.2479699]: 
Returned: 0.12596580386161804| 

python LearningRateFunctionModifier [392 - 1706050376.7461975]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.222222222222222| steps_per_epoch: 63| 
python LearningRateFunctionModifier [392 - 1706050376.7463484]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [392 - 1706050376.746395]: 7.82051282051282e-05
python LearningRateFunctionModifier/ParamGroup1 [392 - 1706050376.7464256]: 7.82051282051282e-05
python DistillationModifier/task_loss [392 - 1706050376.7464654]: tensor(0.2659, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [392 - 1706050376.7469714]: tensor(0.1260, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [392 - 1706050376.7472122]: tensor(0.1260, grad_fn=<AddBackward0>)
python ConstantPruningModifier [392 - 1706050376.7474394]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.222222222222222| steps_per_epoch: 63| 
python ConstantPruningModifier [392 - 1706050377.0371795]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [392 - 1706050377.0376782]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [392 - 1706050377.0379097]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [392 - 1706050377.0380812]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [392 - 1706050377.0382478]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [392 - 1706050377.0390184]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [392 - 1706050377.0398211]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [392 - 1706050377.0400639]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [392 - 1706050377.040229]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [392 - 1706050377.0403917]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [392 - 1706050377.0405507]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [392 - 1706050377.0411892]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [392 - 1706050377.0417554]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [392 - 1706050377.0419714]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [392 - 1706050377.0421329]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [392 - 1706050377.04229]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [392 - 1706050377.0424464]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [392 - 1706050377.0429935]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [392 - 1706050377.0435271]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [392 - 1706050377.0437562]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [392 - 1706050377.043922]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [392 - 1706050377.0441067]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [392 - 1706050377.044267]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [392 - 1706050377.0448155]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [392 - 1706050377.045511]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [392 - 1706050377.0457392]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [392 - 1706050377.0459075]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [392 - 1706050377.0460672]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [392 - 1706050377.0462248]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [392 - 1706050377.0467956]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [392 - 1706050377.047422]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [392 - 1706050377.0476453]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [392 - 1706050377.0478134]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [392 - 1706050377.0479743]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [392 - 1706050377.048133]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [392 - 1706050377.04871]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [392 - 1706050377.0493073]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [392 - 1706050377.0495074]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [392 - 1706050377.049688]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [392 - 1706050377.0498512]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [392 - 1706050377.0500093]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [392 - 1706050377.0505815]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [392 - 1706050377.051173]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [392 - 1706050377.0513754]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [392 - 1706050377.0515351]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [392 - 1706050377.051725]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [392 - 1706050377.0518868]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [392 - 1706050377.0524576]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [392 - 1706050377.1168869]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [392 - 1706050377.1170704]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [392 - 1706050377.117212]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [392 - 1706050377.1173458]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [392 - 1706050377.1174812]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [392 - 1706050377.1179059]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [392 - 1706050377.1183112]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [392 - 1706050377.1184888]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [392 - 1706050377.118638]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [392 - 1706050377.1188028]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [392 - 1706050377.1189358]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [392 - 1706050377.1193297]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [392 - 1706050377.119768]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [392 - 1706050377.1199439]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [392 - 1706050377.1200814]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [392 - 1706050377.1202178]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [392 - 1706050377.120354]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [392 - 1706050377.120756]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [392 - 1706050377.1211722]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [392 - 1706050377.1213493]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [392 - 1706050377.1214828]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [392 - 1706050377.1216323]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [392 - 1706050377.1217797]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [392 - 1706050377.1221704]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [392 - 1706050377.122607]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [392 - 1706050377.1227818]: 0.0
python ParamPruning/classifier.weight [392 - 1706050377.1228511]: 0.0
python DistillationModifier [399 - 1706050414.340638]: Calling loss_update with:
args: 0.11296314746141434| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.333333333333333| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [399 - 1706050415.3390682]: 
Returned: 0.12642186880111694| 

python LearningRateFunctionModifier [399 - 1706050417.3222342]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.333333333333333| steps_per_epoch: 63| 
python LearningRateFunctionModifier [399 - 1706050417.3224008]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [399 - 1706050417.3224466]: 7.692307692307691e-05
python LearningRateFunctionModifier/ParamGroup1 [399 - 1706050417.3224776]: 7.692307692307691e-05
python DistillationModifier/task_loss [399 - 1706050417.3225195]: tensor(0.1130, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [399 - 1706050417.3230228]: tensor(0.1264, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [399 - 1706050417.3232594]: tensor(0.1264, grad_fn=<AddBackward0>)
python ConstantPruningModifier [399 - 1706050417.3234847]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.333333333333333| steps_per_epoch: 63| 
python ConstantPruningModifier [399 - 1706050417.5287852]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [399 - 1706050417.529258]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [399 - 1706050417.5294774]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [399 - 1706050417.5297208]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [399 - 1706050417.5299244]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [399 - 1706050417.5305235]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [399 - 1706050417.5311427]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [399 - 1706050417.5313542]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [399 - 1706050417.5314932]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [399 - 1706050417.5317152]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [399 - 1706050417.531869]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [399 - 1706050417.5323188]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [399 - 1706050417.532835]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [399 - 1706050417.5330343]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [399 - 1706050417.533177]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [399 - 1706050417.5333145]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [399 - 1706050417.5334527]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [399 - 1706050417.5338671]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [399 - 1706050417.5342839]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [399 - 1706050417.534469]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [399 - 1706050417.534625]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [399 - 1706050417.5347712]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [399 - 1706050417.5349598]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [399 - 1706050417.5353687]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [399 - 1706050417.5357394]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [399 - 1706050417.5359235]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [399 - 1706050417.5360663]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [399 - 1706050417.5362434]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [399 - 1706050417.5363834]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [399 - 1706050417.5367777]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [399 - 1706050417.537128]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [399 - 1706050417.5373]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [399 - 1706050417.537438]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [399 - 1706050417.537621]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [399 - 1706050417.537775]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [399 - 1706050417.5381446]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [399 - 1706050417.5384893]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [399 - 1706050417.5386813]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [399 - 1706050417.538819]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [399 - 1706050417.5389788]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [399 - 1706050417.53913]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [399 - 1706050417.5395086]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [399 - 1706050417.5398948]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [399 - 1706050417.5400574]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [399 - 1706050417.5401902]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [399 - 1706050417.5403447]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [399 - 1706050417.5405204]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [399 - 1706050417.540933]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [399 - 1706050417.541304]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [399 - 1706050417.5414748]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [399 - 1706050417.541628]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [399 - 1706050417.5417967]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [399 - 1706050417.541967]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [399 - 1706050417.5423367]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [399 - 1706050417.5428474]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [399 - 1706050417.5430403]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [399 - 1706050417.5431726]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [399 - 1706050417.5433052]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [399 - 1706050417.5434394]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [399 - 1706050417.5438545]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [399 - 1706050417.544296]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [399 - 1706050417.544475]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [399 - 1706050417.544642]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [399 - 1706050417.5447917]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [399 - 1706050417.5449765]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [399 - 1706050417.545345]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [399 - 1706050417.545735]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [399 - 1706050417.545914]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [399 - 1706050417.5460465]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [399 - 1706050417.5461802]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [399 - 1706050417.546347]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [399 - 1706050417.5467381]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [399 - 1706050417.5472138]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [399 - 1706050417.5473928]: 0.0
python ParamPruning/classifier.weight [399 - 1706050417.5474596]: 0.0
python DistillationModifier [406 - 1706050451.6370828]: Calling loss_update with:
args: 0.09275022894144058| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.444444444444445| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [406 - 1706050452.928794]: 
Returned: 0.15505853295326233| 

python LearningRateFunctionModifier [406 - 1706050454.8752396]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.444444444444445| steps_per_epoch: 63| 
python LearningRateFunctionModifier [406 - 1706050454.8753917]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [406 - 1706050454.8754368]: 7.564102564102563e-05
python LearningRateFunctionModifier/ParamGroup1 [406 - 1706050454.8754685]: 7.564102564102563e-05
python DistillationModifier/task_loss [406 - 1706050454.875509]: tensor(0.0928, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [406 - 1706050454.8760257]: tensor(0.1551, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [406 - 1706050454.8762686]: tensor(0.1551, grad_fn=<AddBackward0>)
python ConstantPruningModifier [406 - 1706050454.8764958]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.444444444444445| steps_per_epoch: 63| 
python ConstantPruningModifier [406 - 1706050455.1195397]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [406 - 1706050455.1200283]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [406 - 1706050455.1203017]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [406 - 1706050455.1204817]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [406 - 1706050455.1207297]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [406 - 1706050455.1213152]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [406 - 1706050455.1219673]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [406 - 1706050455.122188]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [406 - 1706050455.1223314]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [406 - 1706050455.1225312]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [406 - 1706050455.122705]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [406 - 1706050455.1231854]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [406 - 1706050455.1236875]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [406 - 1706050455.1238866]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [406 - 1706050455.124028]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [406 - 1706050455.1241703]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [406 - 1706050455.12434]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [406 - 1706050455.1247756]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [406 - 1706050455.1252046]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [406 - 1706050455.1253946]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [406 - 1706050455.1255834]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [406 - 1706050455.1257536]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [406 - 1706050455.1259031]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [406 - 1706050455.1263335]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [406 - 1706050455.126761]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [406 - 1706050455.1269453]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [406 - 1706050455.1270955]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [406 - 1706050455.127265]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [406 - 1706050455.127412]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [406 - 1706050455.127831]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [406 - 1706050455.1282299]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [406 - 1706050455.128402]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [406 - 1706050455.1285393]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [406 - 1706050455.1287754]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [406 - 1706050455.1289678]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [406 - 1706050455.1293879]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [406 - 1706050455.1298187]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [406 - 1706050455.1300008]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [406 - 1706050455.130142]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [406 - 1706050455.1303241]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [406 - 1706050455.1304708]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [406 - 1706050455.1308959]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [406 - 1706050455.1313338]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [406 - 1706050455.1315024]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [406 - 1706050455.1316965]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [406 - 1706050455.131844]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [406 - 1706050455.131981]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [406 - 1706050455.1323764]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [406 - 1706050455.1328125]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [406 - 1706050455.1329927]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [406 - 1706050455.1331322]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [406 - 1706050455.1333058]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [406 - 1706050455.133449]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [406 - 1706050455.1338737]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [406 - 1706050455.1342676]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [406 - 1706050455.1344461]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [406 - 1706050455.1346004]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [406 - 1706050455.1347466]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [406 - 1706050455.1349285]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [406 - 1706050455.1353312]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [406 - 1706050455.1357648]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [406 - 1706050455.135954]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [406 - 1706050455.1361213]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [406 - 1706050455.1362636]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [406 - 1706050455.136412]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [406 - 1706050455.136844]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [406 - 1706050455.1372893]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [406 - 1706050455.1374836]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [406 - 1706050455.137671]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [406 - 1706050455.1378114]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [406 - 1706050455.1379476]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [406 - 1706050455.1383688]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [406 - 1706050455.1388223]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [406 - 1706050455.1390083]: 0.0
python ParamPruning/classifier.weight [406 - 1706050455.139076]: 0.0
python DistillationModifier [413 - 1706050489.7180648]: Calling loss_update with:
args: 0.2430024892091751| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.555555555555555| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [413 - 1706050491.4299674]: 
Returned: 0.07127142697572708| 

python LearningRateFunctionModifier [413 - 1706050494.4538255]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.555555555555555| steps_per_epoch: 63| 
python LearningRateFunctionModifier [413 - 1706050494.4539704]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [413 - 1706050494.454017]: 7.435897435897436e-05
python LearningRateFunctionModifier/ParamGroup1 [413 - 1706050494.4540484]: 7.435897435897436e-05
python DistillationModifier/task_loss [413 - 1706050494.4540915]: tensor(0.2430, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [413 - 1706050494.4545856]: tensor(0.0713, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [413 - 1706050494.4548311]: tensor(0.0713, grad_fn=<AddBackward0>)
python ConstantPruningModifier [413 - 1706050494.4550579]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.555555555555555| steps_per_epoch: 63| 
python ConstantPruningModifier [413 - 1706050494.7361677]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [413 - 1706050494.736695]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [413 - 1706050494.7371137]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [413 - 1706050494.7372925]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [413 - 1706050494.7374592]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [413 - 1706050494.7382271]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [413 - 1706050494.7390342]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [413 - 1706050494.739281]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [413 - 1706050494.7394454]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [413 - 1706050494.739626]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [413 - 1706050494.7397916]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [413 - 1706050494.7403722]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [413 - 1706050494.7410214]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [413 - 1706050494.7412446]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [413 - 1706050494.7414103]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [413 - 1706050494.7415879]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [413 - 1706050494.7417607]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [413 - 1706050494.7423048]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [413 - 1706050494.74289]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [413 - 1706050494.7430947]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [413 - 1706050494.7432563]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [413 - 1706050494.7434132]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [413 - 1706050494.7435882]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [413 - 1706050494.7441335]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [413 - 1706050494.7447374]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [413 - 1706050494.7449417]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [413 - 1706050494.7451067]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [413 - 1706050494.745265]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [413 - 1706050494.7454216]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [413 - 1706050494.7459908]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [413 - 1706050494.746515]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [413 - 1706050494.7467322]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [413 - 1706050494.7468972]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [413 - 1706050494.7470577]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [413 - 1706050494.7472155]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [413 - 1706050494.7477462]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [413 - 1706050494.7483993]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [413 - 1706050494.7486181]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [413 - 1706050494.7487931]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [413 - 1706050494.7489538]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [413 - 1706050494.749111]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [413 - 1706050494.749666]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [413 - 1706050494.750284]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [413 - 1706050494.750478]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [413 - 1706050494.750658]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [413 - 1706050494.7508218]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [413 - 1706050494.7509792]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [413 - 1706050494.7515259]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [413 - 1706050494.7520673]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [413 - 1706050494.752256]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [413 - 1706050494.752415]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [413 - 1706050494.7525883]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [413 - 1706050494.8167243]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [413 - 1706050494.817263]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [413 - 1706050494.8179727]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [413 - 1706050494.818168]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [413 - 1706050494.818329]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [413 - 1706050494.8184903]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [413 - 1706050494.8186686]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [413 - 1706050494.8192234]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [413 - 1706050494.8198652]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [413 - 1706050494.8200603]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [413 - 1706050494.8202207]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [413 - 1706050494.8203788]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [413 - 1706050494.8205364]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [413 - 1706050494.8211086]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [413 - 1706050494.8216538]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [413 - 1706050494.8218443]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [413 - 1706050494.8220088]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [413 - 1706050494.8221667]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [413 - 1706050494.822323]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [413 - 1706050494.822858]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [413 - 1706050494.8235373]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [413 - 1706050494.8237565]: 0.0
python ParamPruning/classifier.weight [413 - 1706050494.823825]: 0.0
python DistillationModifier [420 - 1706050533.1391504]: Calling loss_update with:
args: 0.21136093139648438| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.666666666666667| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [420 - 1706050534.8340595]: 
Returned: 0.39468997716903687| 

python LearningRateFunctionModifier [420 - 1706050537.1697454]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.666666666666667| steps_per_epoch: 63| 
python LearningRateFunctionModifier [420 - 1706050537.1698942]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [420 - 1706050537.1699405]: 7.307692307692306e-05
python LearningRateFunctionModifier/ParamGroup1 [420 - 1706050537.1699722]: 7.307692307692306e-05
python DistillationModifier/task_loss [420 - 1706050537.170013]: tensor(0.2114, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [420 - 1706050537.1704912]: tensor(0.3947, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [420 - 1706050537.1707606]: tensor(0.3947, grad_fn=<AddBackward0>)
python ConstantPruningModifier [420 - 1706050537.1709907]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.666666666666667| steps_per_epoch: 63| 
python ConstantPruningModifier [420 - 1706050537.4483664]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [420 - 1706050537.4488904]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [420 - 1706050537.4491131]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [420 - 1706050537.4492862]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [420 - 1706050537.4495153]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [420 - 1706050537.4503007]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [420 - 1706050537.4511092]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [420 - 1706050537.4513505]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [420 - 1706050537.4515147]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [420 - 1706050537.4517002]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [420 - 1706050537.4518669]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [420 - 1706050537.4524364]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [420 - 1706050537.5170188]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [420 - 1706050537.517279]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [420 - 1706050537.5174515]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [420 - 1706050537.5176377]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [420 - 1706050537.5178018]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [420 - 1706050537.5183506]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [420 - 1706050537.5189147]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [420 - 1706050537.5191324]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [420 - 1706050537.5192945]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [420 - 1706050537.5194542]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [420 - 1706050537.5196314]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [420 - 1706050537.5201695]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [420 - 1706050537.5207741]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [420 - 1706050537.5209866]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [420 - 1706050537.5211496]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [420 - 1706050537.5213695]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [420 - 1706050537.5215538]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [420 - 1706050537.5222008]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [420 - 1706050537.5227861]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [420 - 1706050537.5230443]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [420 - 1706050537.5232687]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [420 - 1706050537.5234995]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [420 - 1706050537.5237608]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [420 - 1706050537.524368]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [420 - 1706050537.5251224]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [420 - 1706050537.5253782]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [420 - 1706050537.525597]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [420 - 1706050537.5258224]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [420 - 1706050537.5260808]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [420 - 1706050537.5267668]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [420 - 1706050537.5273914]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [420 - 1706050537.5276225]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [420 - 1706050537.527793]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [420 - 1706050537.5279515]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [420 - 1706050537.5281212]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [420 - 1706050537.528712]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [420 - 1706050537.5292528]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [420 - 1706050537.5294552]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [420 - 1706050537.5296361]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [420 - 1706050537.5298073]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [420 - 1706050537.5300221]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [420 - 1706050537.530545]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [420 - 1706050537.5312326]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [420 - 1706050537.5314376]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [420 - 1706050537.5316164]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [420 - 1706050537.5317903]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [420 - 1706050537.5319486]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [420 - 1706050537.532475]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [420 - 1706050537.533053]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [420 - 1706050537.5332541]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [420 - 1706050537.533415]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [420 - 1706050537.5335937]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [420 - 1706050537.533764]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [420 - 1706050537.5342836]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [420 - 1706050537.5349038]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [420 - 1706050537.5351036]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [420 - 1706050537.5352645]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [420 - 1706050537.535421]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [420 - 1706050537.5356033]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [420 - 1706050537.5361419]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [420 - 1706050537.5367262]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [420 - 1706050537.5369308]: 0.0
python ParamPruning/classifier.weight [420 - 1706050537.5369985]: 0.0
python DistillationModifier [427 - 1706050573.137479]: Calling loss_update with:
args: 0.15054620802402496| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.777777777777778| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [427 - 1706050574.230015]: 
Returned: 0.17163445055484772| 

python LearningRateFunctionModifier [427 - 1706050576.4481976]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.777777777777778| steps_per_epoch: 63| 
python LearningRateFunctionModifier [427 - 1706050576.4483416]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [427 - 1706050576.4483883]: 7.179487179487178e-05
python LearningRateFunctionModifier/ParamGroup1 [427 - 1706050576.4484193]: 7.179487179487178e-05
python DistillationModifier/task_loss [427 - 1706050576.4484606]: tensor(0.1505, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [427 - 1706050576.44897]: tensor(0.1716, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [427 - 1706050576.44921]: tensor(0.1716, grad_fn=<AddBackward0>)
python ConstantPruningModifier [427 - 1706050576.449442]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.777777777777778| steps_per_epoch: 63| 
python ConstantPruningModifier [427 - 1706050576.630225]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [427 - 1706050576.6306913]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [427 - 1706050576.630953]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [427 - 1706050576.631117]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [427 - 1706050576.631322]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [427 - 1706050576.6319418]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [427 - 1706050576.632579]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [427 - 1706050576.632884]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [427 - 1706050576.6331346]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [427 - 1706050576.6332862]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [427 - 1706050576.6334817]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [427 - 1706050576.6340199]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [427 - 1706050576.634558]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [427 - 1706050576.6347888]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [427 - 1706050576.6349897]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [427 - 1706050576.6351414]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [427 - 1706050576.6352828]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [427 - 1706050576.6357656]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [427 - 1706050576.6362739]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [427 - 1706050576.6364815]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [427 - 1706050576.6367013]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [427 - 1706050576.6368687]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [427 - 1706050576.637038]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [427 - 1706050576.6375115]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [427 - 1706050576.6380277]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [427 - 1706050576.6382844]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [427 - 1706050576.638536]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [427 - 1706050576.6387866]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [427 - 1706050576.638962]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [427 - 1706050576.6393423]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [427 - 1706050576.6398954]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [427 - 1706050576.6400828]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [427 - 1706050576.6402626]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [427 - 1706050576.6404045]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [427 - 1706050576.6405451]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [427 - 1706050576.6410162]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [427 - 1706050576.6415339]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [427 - 1706050576.6417475]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [427 - 1706050576.6419218]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [427 - 1706050576.6420627]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [427 - 1706050576.6422503]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [427 - 1706050576.642663]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [427 - 1706050576.643135]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [427 - 1706050576.6433306]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [427 - 1706050576.6434965]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [427 - 1706050576.6436546]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [427 - 1706050576.6438282]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [427 - 1706050576.6442194]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [427 - 1706050576.6446438]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [427 - 1706050576.6448345]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [427 - 1706050576.645017]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [427 - 1706050576.6451888]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [427 - 1706050576.6453245]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [427 - 1706050576.6457427]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [427 - 1706050576.6462638]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [427 - 1706050576.6464818]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [427 - 1706050576.6466837]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [427 - 1706050576.6468453]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [427 - 1706050576.6470091]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [427 - 1706050576.6475134]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [427 - 1706050576.6480477]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [427 - 1706050576.6482363]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [427 - 1706050576.648421]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [427 - 1706050576.6485958]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [427 - 1706050576.6487684]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [427 - 1706050576.6492054]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [427 - 1706050576.6496725]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [427 - 1706050576.6498473]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [427 - 1706050576.6500113]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [427 - 1706050576.6501453]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [427 - 1706050576.6503217]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [427 - 1706050576.6507733]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [427 - 1706050576.6511946]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [427 - 1706050576.6513731]: 0.0
python ParamPruning/classifier.weight [427 - 1706050576.6514373]: 0.0
python DistillationModifier [434 - 1706050608.6311858]: Calling loss_update with:
args: 0.009861244820058346| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.888888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [434 - 1706050609.6475031]: 
Returned: 0.08105917274951935| 

python LearningRateFunctionModifier [434 - 1706050612.4169352]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.888888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [434 - 1706050612.4170802]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [434 - 1706050612.4171264]: 7.051282051282051e-05
python LearningRateFunctionModifier/ParamGroup1 [434 - 1706050612.417157]: 7.051282051282051e-05
python DistillationModifier/task_loss [434 - 1706050612.4171975]: tensor(0.0099, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [434 - 1706050612.4177003]: tensor(0.0811, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [434 - 1706050612.4179544]: tensor(0.0811, grad_fn=<AddBackward0>)
python ConstantPruningModifier [434 - 1706050612.4181914]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.888888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [434 - 1706050612.724526]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [434 - 1706050612.7250564]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [434 - 1706050612.7252774]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [434 - 1706050612.7254505]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [434 - 1706050612.7256446]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [434 - 1706050612.7264]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [434 - 1706050612.7270086]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [434 - 1706050612.7272477]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [434 - 1706050612.727415]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [434 - 1706050612.7275975]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [434 - 1706050612.7277746]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [434 - 1706050612.7285447]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [434 - 1706050612.7291281]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [434 - 1706050612.7293544]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [434 - 1706050612.729518]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [434 - 1706050612.7297058]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [434 - 1706050612.7298713]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [434 - 1706050612.7303832]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [434 - 1706050612.7309284]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [434 - 1706050612.7311406]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [434 - 1706050612.7313042]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [434 - 1706050612.7314646]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [434 - 1706050612.7316432]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [434 - 1706050612.732176]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [434 - 1706050612.7327237]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [434 - 1706050612.732932]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [434 - 1706050612.733119]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [434 - 1706050612.7332814]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [434 - 1706050612.7334394]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [434 - 1706050612.7339659]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [434 - 1706050612.7344897]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [434 - 1706050612.7347128]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [434 - 1706050612.7348797]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [434 - 1706050612.7350402]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [434 - 1706050612.735199]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [434 - 1706050612.7357187]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [434 - 1706050612.7362404]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [434 - 1706050612.7364388]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [434 - 1706050612.7366233]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [434 - 1706050612.736792]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [434 - 1706050612.7369502]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [434 - 1706050612.737454]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [434 - 1706050612.7380016]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [434 - 1706050612.7382073]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [434 - 1706050612.738372]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [434 - 1706050612.7385294]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [434 - 1706050612.738713]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [434 - 1706050612.7392201]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [434 - 1706050612.7397559]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [434 - 1706050612.739959]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [434 - 1706050612.740123]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [434 - 1706050612.7402837]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [434 - 1706050612.7404442]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [434 - 1706050612.7409766]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [434 - 1706050612.7415004]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [434 - 1706050612.741723]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [434 - 1706050612.741889]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [434 - 1706050612.742048]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [434 - 1706050612.7422051]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [434 - 1706050612.7427263]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [434 - 1706050612.7432451]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [434 - 1706050612.7434409]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [434 - 1706050612.74364]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [434 - 1706050612.7438083]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [434 - 1706050612.7439673]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [434 - 1706050612.7444696]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [434 - 1706050612.7450228]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [434 - 1706050612.7452247]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [434 - 1706050612.7453868]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [434 - 1706050612.7455454]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [434 - 1706050612.7457333]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [434 - 1706050612.7462401]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [434 - 1706050612.7470157]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [434 - 1706050612.7472181]: 0.0
python ParamPruning/classifier.weight [434 - 1706050612.747287]: 0.0
python DistillationModifier [441 - 1706050645.1447532]: Calling loss_update with:
args: 0.4798835217952728| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050646.7291098]: 
Returned: 1.2220940589904785| 

python DistillationModifier [441 - 1706050647.944523]: Calling loss_update with:
args: 1.2621076107025146| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050649.0211246]: 
Returned: 2.0528130531311035| 

python DistillationModifier [441 - 1706050650.117882]: Calling loss_update with:
args: 1.118582844734192| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050651.138291]: 
Returned: 1.1976795196533203| 

python DistillationModifier [441 - 1706050652.725379]: Calling loss_update with:
args: 0.644332766532898| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050654.2474487]: 
Returned: 0.9448174238204956| 

python DistillationModifier [441 - 1706050655.330586]: Calling loss_update with:
args: 1.0364747047424316| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050656.340799]: 
Returned: 1.6904246807098389| 

python DistillationModifier [441 - 1706050657.6371136]: Calling loss_update with:
args: 0.8676463961601257| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050659.0179822]: 
Returned: 1.5862966775894165| 

python DistillationModifier [441 - 1706050660.118512]: Calling loss_update with:
args: 0.18357796967029572| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050661.128349]: 
Returned: 0.5436209440231323| 

python DistillationModifier [441 - 1706050662.1466584]: Calling loss_update with:
args: 1.2424901723861694| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050663.2252238]: 
Returned: 1.2717068195343018| 

python DistillationModifier [441 - 1706050664.636653]: Calling loss_update with:
args: 0.7861210703849792| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050665.6418436]: 
Returned: 0.614777684211731| 

python DistillationModifier [441 - 1706050667.317311]: Calling loss_update with:
args: 0.7991093993186951| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050668.9246502]: 
Returned: 1.2225382328033447| 

python DistillationModifier [441 - 1706050670.2275863]: Calling loss_update with:
args: 0.5645950436592102| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050671.2377925]: 
Returned: 0.8575119972229004| 

python DistillationModifier [441 - 1706050672.3258204]: Calling loss_update with:
args: 1.0914415121078491| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050673.3346884]: 
Returned: 1.7197471857070923| 

python DistillationModifier [441 - 1706050674.4366667]: Calling loss_update with:
args: 0.953062117099762| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050675.9294395]: 
Returned: 1.8550792932510376| 

python DistillationModifier [441 - 1706050677.4370098]: Calling loss_update with:
args: 0.7287432551383972| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050678.4486344]: 
Returned: 0.8183156251907349| 

python DistillationModifier [441 - 1706050680.117259]: Calling loss_update with:
args: 1.290022611618042| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050681.7250419]: 
Returned: 1.9900139570236206| 

python DistillationModifier [441 - 1706050683.331897]: Calling loss_update with:
args: 0.6886695027351379| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050684.4310877]: 
Returned: 1.1468433141708374| 

python DistillationModifier [441 - 1706050685.447455]: Calling loss_update with:
args: 0.5841172337532043| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050686.5179868]: 
Returned: 0.7486099600791931| 

python DistillationModifier [441 - 1706050687.5341818]: Calling loss_update with:
args: 0.9899725914001465| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050688.6398642]: 
Returned: 1.7235959768295288| 

python DistillationModifier [441 - 1706050690.3330846]: Calling loss_update with:
args: 0.8143843412399292| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050691.9415576]: 
Returned: 0.9930156469345093| 

python DistillationModifier [441 - 1706050693.1344297]: Calling loss_update with:
args: 0.7240830659866333| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050694.1403172]: 
Returned: 1.1543354988098145| 

python DistillationModifier [441 - 1706050695.226802]: Calling loss_update with:
args: 0.6656296253204346| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050696.2366517]: 
Returned: 1.3864129781723022| 

python DistillationModifier [441 - 1706050697.4174805]: Calling loss_update with:
args: 0.5713066458702087| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050698.8513334]: 
Returned: 0.8439541459083557| 

python DistillationModifier [441 - 1706050699.9363542]: Calling loss_update with:
args: 0.5008279085159302| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050700.946285]: 
Returned: 0.9658433198928833| 

python DistillationModifier [441 - 1706050702.231649]: Calling loss_update with:
args: 0.6458920836448669| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050703.830357]: 
Returned: 1.2638663053512573| 

python DistillationModifier [441 - 1706050705.4357896]: Calling loss_update with:
args: 0.932522714138031| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050707.0352058]: 
Returned: 0.8367680907249451| 

python DistillationModifier [441 - 1706050708.6491327]: Calling loss_update with:
args: 0.2589498460292816| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050710.247451]: 
Returned: 0.46343621611595154| 

python DistillationModifier [441 - 1706050711.9264748]: Calling loss_update with:
args: 0.5889818072319031| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050713.5253196]: 
Returned: 0.7195055484771729| 

python DistillationModifier [441 - 1706050714.9446836]: Calling loss_update with:
args: 0.5231300592422485| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050716.2250388]: 
Returned: 0.8284652233123779| 

python DistillationModifier [441 - 1706050717.328599]: Calling loss_update with:
args: 0.32418662309646606| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050718.4302962]: 
Returned: 0.6326475739479065| 

python DistillationModifier [441 - 1706050719.923939]: Calling loss_update with:
args: 0.5168026089668274| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050721.5172017]: 
Returned: 1.0417505502700806| 

python DistillationModifier [441 - 1706050722.6347816]: Calling loss_update with:
args: 0.9280853271484375| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050723.6496487]: 
Returned: 1.760964274406433| 

python DistillationModifier [441 - 1706050724.7348413]: Calling loss_update with:
args: 0.9174721837043762| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050725.7412994]: 
Returned: 1.0332543849945068| 

python DistillationModifier [441 - 1706050726.8237774]: Calling loss_update with:
args: 0.9781898260116577| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050728.2240586]: 
Returned: 1.6546952724456787| 

python DistillationModifier [441 - 1706050729.8303514]: Calling loss_update with:
args: 1.4499554634094238| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050731.4297502]: 
Returned: 2.2070064544677734| 

python DistillationModifier [441 - 1706050733.0351405]: Calling loss_update with:
args: 1.6388821601867676| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050734.623533]: 
Returned: 2.0750832557678223| 

python DistillationModifier [441 - 1706050736.232122]: Calling loss_update with:
args: 1.042862892150879| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050737.5165129]: 
Returned: 1.975051999092102| 

python DistillationModifier [441 - 1706050739.1373413]: Calling loss_update with:
args: 0.9231778979301453| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050740.7404864]: 
Returned: 0.8248685002326965| 

python DistillationModifier [441 - 1706050742.4264758]: Calling loss_update with:
args: 0.5913922786712646| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050744.026392]: 
Returned: 0.998343288898468| 

python DistillationModifier [441 - 1706050745.4272423]: Calling loss_update with:
args: 0.7250756621360779| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050746.8197803]: 
Returned: 1.4000600576400757| 

python DistillationModifier [441 - 1706050747.9269168]: Calling loss_update with:
args: 1.2842738628387451| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050749.019379]: 
Returned: 2.3065171241760254| 

python DistillationModifier [441 - 1706050750.1240172]: Calling loss_update with:
args: 0.6000901460647583| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050751.1329784]: 
Returned: 0.8655744791030884| 

python DistillationModifier [441 - 1706050752.216549]: Calling loss_update with:
args: 0.4367254078388214| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050753.233979]: 
Returned: 0.9111212491989136| 

python DistillationModifier [441 - 1706050754.3229384]: Calling loss_update with:
args: 0.9407919645309448| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050755.4214966]: 
Returned: 0.8506275415420532| 

python DistillationModifier [441 - 1706050756.523595]: Calling loss_update with:
args: 0.7488202452659607| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050757.5492666]: 
Returned: 1.6849334239959717| 

python DistillationModifier [441 - 1706050758.6344688]: Calling loss_update with:
args: 0.727559506893158| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050759.6473322]: 
Returned: 1.6148771047592163| 

python DistillationModifier [441 - 1706050760.7383828]: Calling loss_update with:
args: 1.038764476776123| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050761.8316782]: 
Returned: 1.2112127542495728| 

python DistillationModifier [441 - 1706050763.449437]: Calling loss_update with:
args: 0.5554962754249573| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050765.0535026]: 
Returned: 1.239630103111267| 

python DistillationModifier [441 - 1706050766.7221258]: Calling loss_update with:
args: 0.11507962644100189| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050768.3227165]: 
Returned: 0.3267289698123932| 

python DistillationModifier [441 - 1706050769.9273052]: Calling loss_update with:
args: 1.1238924264907837| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050771.5242574]: 
Returned: 2.207362174987793| 

python DistillationModifier [441 - 1706050772.7238255]: Calling loss_update with:
args: 0.9017960429191589| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050774.0470586]: 
Returned: 1.56987464427948| 

python DistillationModifier [441 - 1706050775.7223685]: Calling loss_update with:
args: 0.3220066726207733| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050777.3240142]: 
Returned: 0.6452598571777344| 

python DistillationModifier [441 - 1706050778.6424973]: Calling loss_update with:
args: 2.370907783508301| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050780.2354817]: 
Returned: 2.289466381072998| 

python DistillationModifier [441 - 1706050781.842398]: Calling loss_update with:
args: 0.6337345838546753| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050783.129601]: 
Returned: 0.667928159236908| 

python DistillationModifier [441 - 1706050784.338435]: Calling loss_update with:
args: 0.6603931784629822| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050785.3464136]: 
Returned: 0.9900338053703308| 

python DistillationModifier [441 - 1706050786.4335587]: Calling loss_update with:
args: 0.7703925967216492| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050787.528702]: 
Returned: 1.5697835683822632| 

python DistillationModifier [441 - 1706050788.620851]: Calling loss_update with:
args: 0.6647369265556335| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050789.7440326]: 
Returned: 1.0653347969055176| 

python DistillationModifier [441 - 1706050791.417034]: Calling loss_update with:
args: 0.9676557183265686| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050793.0247533]: 
Returned: 0.7392379641532898| 

python DistillationModifier [441 - 1706050794.6315439]: Calling loss_update with:
args: 1.2842689752578735| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050796.2365954]: 
Returned: 1.618783950805664| 

python DistillationModifier [441 - 1706050797.327203]: Calling loss_update with:
args: 0.825276792049408| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050798.3432343]: 
Returned: 1.108133316040039| 

python DistillationModifier [441 - 1706050799.4280658]: Calling loss_update with:
args: 0.5829479098320007| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050800.533819]: 
Returned: 1.067995309829712| 

python DistillationModifier [441 - 1706050802.149087]: Calling loss_update with:
args: 0.7028579115867615| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050803.816716]: 
Returned: 1.1205588579177856| 

python DistillationModifier [441 - 1706050805.4353693]: Calling loss_update with:
args: 0.45391231775283813| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050806.728235]: 
Returned: 1.1418399810791016| 

python DistillationModifier [441 - 1706050807.3184028]: Calling loss_update with:
args: 0.6706312298774719| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050807.834924]: 
Returned: 1.2138912677764893| 

python DistillationModifier [441 - 1706050809.541557]: Calling loss_update with:
args: 0.1660812795162201| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1706050810.5426698]: 
Returned: 0.3338198661804199| 

python LearningRateFunctionModifier [441 - 1706050812.6722553]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [441 - 1706050812.672406]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [441 - 1706050812.6724536]: 6.923076923076922e-05
python LearningRateFunctionModifier/ParamGroup1 [441 - 1706050812.6724958]: 6.923076923076922e-05
python DistillationModifier/task_loss [441 - 1706050812.67254]: tensor(0.1661, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [441 - 1706050812.673045]: tensor(0.3338, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [441 - 1706050812.6732855]: tensor(0.3338, grad_fn=<AddBackward0>)
python ConstantPruningModifier [441 - 1706050812.6735194]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| 
python ConstantPruningModifier [441 - 1706050812.9506948]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [441 - 1706050812.9511633]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [441 - 1706050812.9514537]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [441 - 1706050812.9517355]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [441 - 1706050812.9519231]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [441 - 1706050813.0167603]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [441 - 1706050813.0175588]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [441 - 1706050813.0178528]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [441 - 1706050813.018089]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [441 - 1706050813.018284]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [441 - 1706050813.0184894]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [441 - 1706050813.019185]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [441 - 1706050813.0197768]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [441 - 1706050813.0199964]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [441 - 1706050813.0201976]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [441 - 1706050813.0203624]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [441 - 1706050813.020533]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [441 - 1706050813.0210857]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [441 - 1706050813.0218015]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [441 - 1706050813.0220582]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [441 - 1706050813.0222836]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [441 - 1706050813.0225341]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [441 - 1706050813.0228117]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [441 - 1706050813.0234563]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [441 - 1706050813.0240233]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [441 - 1706050813.024243]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [441 - 1706050813.024438]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [441 - 1706050813.0246437]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [441 - 1706050813.024814]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [441 - 1706050813.025487]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [441 - 1706050813.0260985]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [441 - 1706050813.0263104]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [441 - 1706050813.026522]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [441 - 1706050813.0267289]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [441 - 1706050813.02691]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [441 - 1706050813.027433]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [441 - 1706050813.0279882]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [441 - 1706050813.028192]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [441 - 1706050813.0283852]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [441 - 1706050813.0285919]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [441 - 1706050813.0287852]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [441 - 1706050813.029305]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [441 - 1706050813.0298605]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [441 - 1706050813.030059]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [441 - 1706050813.030244]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [441 - 1706050813.0304167]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [441 - 1706050813.0306258]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [441 - 1706050813.0311954]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [441 - 1706050813.0319316]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [441 - 1706050813.0321422]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [441 - 1706050813.032348]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [441 - 1706050813.0325365]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [441 - 1706050813.032753]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [441 - 1706050813.0333405]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [441 - 1706050813.0339963]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [441 - 1706050813.0342066]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [441 - 1706050813.0344114]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [441 - 1706050813.034615]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [441 - 1706050813.0347917]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [441 - 1706050813.0353782]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [441 - 1706050813.0360029]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [441 - 1706050813.036212]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [441 - 1706050813.0364199]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [441 - 1706050813.0366867]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [441 - 1706050813.0368762]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [441 - 1706050813.0374625]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [441 - 1706050813.0380301]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [441 - 1706050813.0382297]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [441 - 1706050813.038393]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [441 - 1706050813.0386207]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [441 - 1706050813.0388038]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [441 - 1706050813.0393293]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [441 - 1706050813.0400262]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [441 - 1706050813.0402281]: 0.0
python ParamPruning/classifier.weight [441 - 1706050813.0402958]: 0.0
python DistillationModifier [448 - 1706050848.0462513]: Calling loss_update with:
args: 0.1612599641084671| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.111111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [448 - 1706050849.1496806]: 
Returned: 0.2554062306880951| 

python LearningRateFunctionModifier [448 - 1706050851.2308133]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.111111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [448 - 1706050851.2309594]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [448 - 1706050851.2310185]: 6.794871794871795e-05
python LearningRateFunctionModifier/ParamGroup1 [448 - 1706050851.2310495]: 6.794871794871795e-05
python DistillationModifier/task_loss [448 - 1706050851.2310905]: tensor(0.1613, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [448 - 1706050851.2315962]: tensor(0.2554, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [448 - 1706050851.23187]: tensor(0.2554, grad_fn=<AddBackward0>)
python ConstantPruningModifier [448 - 1706050851.2320943]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.111111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [448 - 1706050851.5291104]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [448 - 1706050851.529622]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [448 - 1706050851.5299146]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [448 - 1706050851.530103]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [448 - 1706050851.5302615]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [448 - 1706050851.5308797]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [448 - 1706050851.5315113]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [448 - 1706050851.5317714]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [448 - 1706050851.531941]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [448 - 1706050851.532096]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [448 - 1706050851.532324]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [448 - 1706050851.532898]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [448 - 1706050851.5334632]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [448 - 1706050851.5337179]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [448 - 1706050851.5339322]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [448 - 1706050851.5341403]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [448 - 1706050851.5343103]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [448 - 1706050851.5348122]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [448 - 1706050851.5353167]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [448 - 1706050851.5355275]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [448 - 1706050851.5357568]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [448 - 1706050851.5359242]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [448 - 1706050851.5361161]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [448 - 1706050851.5366104]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [448 - 1706050851.537015]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [448 - 1706050851.5372243]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [448 - 1706050851.5374143]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [448 - 1706050851.5375853]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [448 - 1706050851.5377624]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [448 - 1706050851.538303]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [448 - 1706050851.538778]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [448 - 1706050851.538976]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [448 - 1706050851.5391643]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [448 - 1706050851.5393152]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [448 - 1706050851.539479]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [448 - 1706050851.5400226]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [448 - 1706050851.5404854]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [448 - 1706050851.5407126]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [448 - 1706050851.5408607]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [448 - 1706050851.541009]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [448 - 1706050851.541224]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [448 - 1706050851.5416965]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [448 - 1706050851.5421476]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [448 - 1706050851.5423567]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [448 - 1706050851.5425386]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [448 - 1706050851.5427206]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [448 - 1706050851.5429187]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [448 - 1706050851.5434494]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [448 - 1706050851.5439072]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [448 - 1706050851.544112]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [448 - 1706050851.5442936]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [448 - 1706050851.5444438]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [448 - 1706050851.5446732]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [448 - 1706050851.5451849]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [448 - 1706050851.545645]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [448 - 1706050851.5458493]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [448 - 1706050851.5460074]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [448 - 1706050851.546213]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [448 - 1706050851.5463798]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [448 - 1706050851.5469224]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [448 - 1706050851.547383]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [448 - 1706050851.5476131]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [448 - 1706050851.547776]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [448 - 1706050851.5479634]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [448 - 1706050851.5481129]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [448 - 1706050851.5486333]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [448 - 1706050851.549099]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [448 - 1706050851.5492978]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [448 - 1706050851.5494878]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [448 - 1706050851.549669]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [448 - 1706050851.5498462]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [448 - 1706050851.5503273]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [448 - 1706050851.550828]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [448 - 1706050851.5510383]: 0.0
python ParamPruning/classifier.weight [448 - 1706050851.5511055]: 0.0
python DistillationModifier [455 - 1706050887.2250998]: Calling loss_update with:
args: 0.07944412529468536| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.222222222222222| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [455 - 1706050888.3184857]: 
Returned: 0.13105450570583344| 

python LearningRateFunctionModifier [455 - 1706050890.3618612]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.222222222222222| steps_per_epoch: 63| 
python LearningRateFunctionModifier [455 - 1706050890.3620265]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [455 - 1706050890.3620732]: 6.666666666666666e-05
python LearningRateFunctionModifier/ParamGroup1 [455 - 1706050890.362103]: 6.666666666666666e-05
python DistillationModifier/task_loss [455 - 1706050890.3621447]: tensor(0.0794, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [455 - 1706050890.3626463]: tensor(0.1311, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [455 - 1706050890.3628924]: tensor(0.1311, grad_fn=<AddBackward0>)
python ConstantPruningModifier [455 - 1706050890.3631241]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.222222222222222| steps_per_epoch: 63| 
python ConstantPruningModifier [455 - 1706050890.6431649]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [455 - 1706050890.6436834]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [455 - 1706050890.6439784]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [455 - 1706050890.6441855]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [455 - 1706050890.6443589]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [455 - 1706050890.64512]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [455 - 1706050890.6457183]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [455 - 1706050890.6459517]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [455 - 1706050890.6461153]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [455 - 1706050890.646277]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [455 - 1706050890.6465018]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [455 - 1706050890.647072]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [455 - 1706050890.647647]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [455 - 1706050890.6478949]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [455 - 1706050890.6480987]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [455 - 1706050890.6482627]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [455 - 1706050890.6484666]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [455 - 1706050890.649021]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [455 - 1706050890.6495595]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [455 - 1706050890.649785]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [455 - 1706050890.649946]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [455 - 1706050890.650135]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [455 - 1706050890.6503553]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [455 - 1706050890.6508932]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [455 - 1706050890.6514285]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [455 - 1706050890.6516426]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [455 - 1706050890.6518302]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [455 - 1706050890.6519904]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [455 - 1706050890.6521485]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [455 - 1706050890.7168705]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [455 - 1706050890.717674]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [455 - 1706050890.7179155]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [455 - 1706050890.7180812]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [455 - 1706050890.7182753]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [455 - 1706050890.7184343]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [455 - 1706050890.7191255]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [455 - 1706050890.719702]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [455 - 1706050890.7199018]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [455 - 1706050890.7201154]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [455 - 1706050890.7202928]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [455 - 1706050890.7205348]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [455 - 1706050890.721097]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [455 - 1706050890.7216597]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [455 - 1706050890.7218492]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [455 - 1706050890.72201]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [455 - 1706050890.7221692]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [455 - 1706050890.7223678]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [455 - 1706050890.7229135]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [455 - 1706050890.7234643]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [455 - 1706050890.723669]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [455 - 1706050890.7238338]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [455 - 1706050890.7240176]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [455 - 1706050890.7241864]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [455 - 1706050890.7247422]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [455 - 1706050890.7252812]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [455 - 1706050890.7254682]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [455 - 1706050890.7256465]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [455 - 1706050890.7258313]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [455 - 1706050890.7260034]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [455 - 1706050890.7265346]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [455 - 1706050890.72709]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [455 - 1706050890.7272794]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [455 - 1706050890.727439]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [455 - 1706050890.7276351]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [455 - 1706050890.7278085]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [455 - 1706050890.7283363]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [455 - 1706050890.7288935]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [455 - 1706050890.7290862]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [455 - 1706050890.7293117]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [455 - 1706050890.7294834]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [455 - 1706050890.7296762]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [455 - 1706050890.730198]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [455 - 1706050890.730753]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [455 - 1706050890.7309432]: 0.0
python ParamPruning/classifier.weight [455 - 1706050890.7310102]: 0.0
python DistillationModifier [462 - 1706050924.7504566]: Calling loss_update with:
args: 0.07352101057767868| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.333333333333333| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [462 - 1706050925.8510447]: 
Returned: 0.050077516585588455| 

python LearningRateFunctionModifier [462 - 1706050927.8383882]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.333333333333333| steps_per_epoch: 63| 
python LearningRateFunctionModifier [462 - 1706050927.838537]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [462 - 1706050927.8385997]: 6.538461538461539e-05
python LearningRateFunctionModifier/ParamGroup1 [462 - 1706050927.8386314]: 6.538461538461539e-05
python DistillationModifier/task_loss [462 - 1706050927.838693]: tensor(0.0735, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [462 - 1706050927.8391721]: tensor(0.0501, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [462 - 1706050927.839408]: tensor(0.0501, grad_fn=<AddBackward0>)
python ConstantPruningModifier [462 - 1706050927.8396554]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.333333333333333| steps_per_epoch: 63| 
python ConstantPruningModifier [462 - 1706050928.0433118]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [462 - 1706050928.043783]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [462 - 1706050928.044055]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [462 - 1706050928.0442429]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [462 - 1706050928.044394]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [462 - 1706050928.0450077]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [462 - 1706050928.0454752]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [462 - 1706050928.0456927]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [462 - 1706050928.0458415]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [462 - 1706050928.0459769]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [462 - 1706050928.0461125]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [462 - 1706050928.046485]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [462 - 1706050928.0468886]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [462 - 1706050928.0470648]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [462 - 1706050928.0472043]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [462 - 1706050928.0473847]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [462 - 1706050928.0475233]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [462 - 1706050928.0479162]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [462 - 1706050928.0484462]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [462 - 1706050928.048669]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [462 - 1706050928.0488226]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [462 - 1706050928.0490024]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [462 - 1706050928.0491428]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [462 - 1706050928.049602]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [462 - 1706050928.0499804]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [462 - 1706050928.0501544]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [462 - 1706050928.050299]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [462 - 1706050928.0504375]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [462 - 1706050928.0505915]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [462 - 1706050928.0510676]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [462 - 1706050928.0516236]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [462 - 1706050928.0518382]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [462 - 1706050928.0519764]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [462 - 1706050928.052158]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [462 - 1706050928.0522914]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [462 - 1706050928.1167707]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [462 - 1706050928.117237]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [462 - 1706050928.1174185]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [462 - 1706050928.1175554]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [462 - 1706050928.1177597]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [462 - 1706050928.117897]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [462 - 1706050928.118311]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [462 - 1706050928.118768]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [462 - 1706050928.1189442]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [462 - 1706050928.1190763]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [462 - 1706050928.1192381]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [462 - 1706050928.1193764]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [462 - 1706050928.119769]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [462 - 1706050928.1201313]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [462 - 1706050928.1203053]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [462 - 1706050928.1204417]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [462 - 1706050928.1206357]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [462 - 1706050928.1207757]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [462 - 1706050928.1212437]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [462 - 1706050928.12178]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [462 - 1706050928.121981]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [462 - 1706050928.1221144]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [462 - 1706050928.122276]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [462 - 1706050928.1224186]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [462 - 1706050928.1229086]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [462 - 1706050928.1233928]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [462 - 1706050928.1235938]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [462 - 1706050928.1237395]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [462 - 1706050928.123897]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [462 - 1706050928.1240396]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [462 - 1706050928.124457]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [462 - 1706050928.124932]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [462 - 1706050928.1251326]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [462 - 1706050928.1252692]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [462 - 1706050928.1254113]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [462 - 1706050928.1255481]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [462 - 1706050928.1260226]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [462 - 1706050928.1264992]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [462 - 1706050928.1267169]: 0.0
python ParamPruning/classifier.weight [462 - 1706050928.12679]: 0.0
python DistillationModifier [469 - 1706050960.729281]: Calling loss_update with:
args: 0.09233994781970978| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.444444444444445| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [469 - 1706050961.7405446]: 
Returned: 0.17251965403556824| 

python LearningRateFunctionModifier [469 - 1706050963.7222311]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.444444444444445| steps_per_epoch: 63| 
python LearningRateFunctionModifier [469 - 1706050963.722383]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [469 - 1706050963.7224312]: 6.41025641025641e-05
python LearningRateFunctionModifier/ParamGroup1 [469 - 1706050963.7224622]: 6.41025641025641e-05
python DistillationModifier/task_loss [469 - 1706050963.7225056]: tensor(0.0923, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [469 - 1706050963.7230053]: tensor(0.1725, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [469 - 1706050963.7232473]: tensor(0.1725, grad_fn=<AddBackward0>)
python ConstantPruningModifier [469 - 1706050963.7234757]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.444444444444445| steps_per_epoch: 63| 
python ConstantPruningModifier [469 - 1706050963.9299755]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [469 - 1706050963.9304433]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [469 - 1706050963.9306839]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [469 - 1706050963.9309094]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [469 - 1706050963.9310646]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [469 - 1706050963.931658]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [469 - 1706050963.932125]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [469 - 1706050963.9323287]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [469 - 1706050963.9324691]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [469 - 1706050963.932646]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [469 - 1706050963.9327943]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [469 - 1706050963.9332073]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [469 - 1706050963.9337695]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [469 - 1706050963.93396]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [469 - 1706050963.934179]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [469 - 1706050963.934343]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [469 - 1706050963.9344838]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [469 - 1706050963.9348903]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [469 - 1706050963.9353566]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [469 - 1706050963.9355464]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [469 - 1706050963.9357154]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [469 - 1706050963.935855]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [469 - 1706050963.9360273]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [469 - 1706050963.9364324]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [469 - 1706050963.9368958]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [469 - 1706050963.937082]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [469 - 1706050963.937221]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [469 - 1706050963.9373946]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [469 - 1706050963.9375386]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [469 - 1706050963.9379542]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [469 - 1706050963.93837]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [469 - 1706050963.938552]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [469 - 1706050963.9387228]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [469 - 1706050963.9389076]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [469 - 1706050963.9391112]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [469 - 1706050963.939674]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [469 - 1706050963.9402208]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [469 - 1706050963.9404223]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [469 - 1706050963.94063]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [469 - 1706050963.9407828]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [469 - 1706050963.940976]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [469 - 1706050963.9413772]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [469 - 1706050963.941861]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [469 - 1706050963.9420764]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [469 - 1706050963.942252]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [469 - 1706050963.9423954]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [469 - 1706050963.9425597]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [469 - 1706050963.9429977]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [469 - 1706050963.9433694]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [469 - 1706050963.9435813]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [469 - 1706050963.9437544]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [469 - 1706050963.9439201]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [469 - 1706050963.944055]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [469 - 1706050963.9444268]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [469 - 1706050963.9449193]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [469 - 1706050963.9451134]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [469 - 1706050963.9452465]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [469 - 1706050963.9453795]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [469 - 1706050963.9455142]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [469 - 1706050963.9459157]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [469 - 1706050963.9463186]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [469 - 1706050963.9465055]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [469 - 1706050963.946663]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [469 - 1706050963.9468448]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [469 - 1706050963.946982]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [469 - 1706050963.9473612]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [469 - 1706050963.947795]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [469 - 1706050963.9479868]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [469 - 1706050963.9481194]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [469 - 1706050963.948247]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [469 - 1706050963.9484136]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [469 - 1706050963.9488637]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [469 - 1706050963.9492695]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [469 - 1706050963.9494545]: 0.0
python ParamPruning/classifier.weight [469 - 1706050963.9495227]: 0.0
python DistillationModifier [476 - 1706051001.8349795]: Calling loss_update with:
args: 0.22266533970832825| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.555555555555555| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [476 - 1706051003.133117]: 
Returned: 0.18222323060035706| 

python LearningRateFunctionModifier [476 - 1706051006.06231]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.555555555555555| steps_per_epoch: 63| 
python LearningRateFunctionModifier [476 - 1706051006.062459]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [476 - 1706051006.0625048]: 6.282051282051282e-05
python LearningRateFunctionModifier/ParamGroup1 [476 - 1706051006.0625348]: 6.282051282051282e-05
python DistillationModifier/task_loss [476 - 1706051006.0626037]: tensor(0.2227, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [476 - 1706051006.0630853]: tensor(0.1822, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [476 - 1706051006.0633326]: tensor(0.1822, grad_fn=<AddBackward0>)
python ConstantPruningModifier [476 - 1706051006.0635822]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.555555555555555| steps_per_epoch: 63| 
python ConstantPruningModifier [476 - 1706051006.3434007]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [476 - 1706051006.3439093]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [476 - 1706051006.3442051]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [476 - 1706051006.3443854]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [476 - 1706051006.344621]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [476 - 1706051006.3453856]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [476 - 1706051006.3461776]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [476 - 1706051006.3464093]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [476 - 1706051006.3466616]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [476 - 1706051006.3468528]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [476 - 1706051006.3470395]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [476 - 1706051006.3476715]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [476 - 1706051006.3482854]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [476 - 1706051006.3484974]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [476 - 1706051006.3487654]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [476 - 1706051006.348934]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [476 - 1706051006.3491068]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [476 - 1706051006.3496764]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [476 - 1706051006.35022]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [476 - 1706051006.3504207]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [476 - 1706051006.3506503]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [476 - 1706051006.3508158]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [476 - 1706051006.3509758]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [476 - 1706051006.3515077]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [476 - 1706051006.3521693]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [476 - 1706051006.352385]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [476 - 1706051006.3525584]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [476 - 1706051006.4168444]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [476 - 1706051006.417053]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [476 - 1706051006.4176183]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [476 - 1706051006.4182303]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [476 - 1706051006.4184365]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [476 - 1706051006.4186547]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [476 - 1706051006.4188986]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [476 - 1706051006.4190774]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [476 - 1706051006.4196641]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [476 - 1706051006.4202507]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [476 - 1706051006.4204602]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [476 - 1706051006.4206514]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [476 - 1706051006.4208484]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [476 - 1706051006.4210243]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [476 - 1706051006.4215603]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [476 - 1706051006.4221342]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [476 - 1706051006.4223335]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [476 - 1706051006.42255]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [476 - 1706051006.4227686]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [476 - 1706051006.422929]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [476 - 1706051006.4235837]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [476 - 1706051006.4241452]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [476 - 1706051006.4243386]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [476 - 1706051006.4244971]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [476 - 1706051006.42472]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [476 - 1706051006.4248936]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [476 - 1706051006.4254208]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [476 - 1706051006.4259763]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [476 - 1706051006.4261644]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [476 - 1706051006.4263222]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [476 - 1706051006.4264991]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [476 - 1706051006.4266818]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [476 - 1706051006.427307]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [476 - 1706051006.427872]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [476 - 1706051006.4280632]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [476 - 1706051006.4282246]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [476 - 1706051006.4284048]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [476 - 1706051006.4286003]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [476 - 1706051006.4291527]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [476 - 1706051006.4298284]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [476 - 1706051006.4300222]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [476 - 1706051006.430181]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [476 - 1706051006.4303844]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [476 - 1706051006.4305484]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [476 - 1706051006.431109]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [476 - 1706051006.4316797]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [476 - 1706051006.4318728]: 0.0
python ParamPruning/classifier.weight [476 - 1706051006.4319396]: 0.0
python DistillationModifier [483 - 1706051042.1449854]: Calling loss_update with:
args: 0.1385250836610794| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.666666666666667| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [483 - 1706051043.2469203]: 
Returned: 0.13835449516773224| 

python LearningRateFunctionModifier [483 - 1706051045.2491121]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.666666666666667| steps_per_epoch: 63| 
python LearningRateFunctionModifier [483 - 1706051045.249259]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [483 - 1706051045.2493055]: 6.153846153846154e-05
python LearningRateFunctionModifier/ParamGroup1 [483 - 1706051045.2493372]: 6.153846153846154e-05
python DistillationModifier/task_loss [483 - 1706051045.2493799]: tensor(0.1385, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [483 - 1706051045.2498875]: tensor(0.1384, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [483 - 1706051045.2501338]: tensor(0.1384, grad_fn=<AddBackward0>)
python ConstantPruningModifier [483 - 1706051045.2503648]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.666666666666667| steps_per_epoch: 63| 
python ConstantPruningModifier [483 - 1706051045.4293296]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [483 - 1706051045.429796]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [483 - 1706051045.4300077]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [483 - 1706051045.4302251]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [483 - 1706051045.4304261]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [483 - 1706051045.4310427]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [483 - 1706051045.4314697]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [483 - 1706051045.431697]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [483 - 1706051045.4318924]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [483 - 1706051045.4320524]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [483 - 1706051045.4321923]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [483 - 1706051045.4325557]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [483 - 1706051045.4329598]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [483 - 1706051045.4331443]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [483 - 1706051045.433322]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [483 - 1706051045.4335084]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [483 - 1706051045.4337127]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [483 - 1706051045.4340646]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [483 - 1706051045.434429]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [483 - 1706051045.434628]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [483 - 1706051045.4347677]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [483 - 1706051045.4349012]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [483 - 1706051045.4350328]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [483 - 1706051045.4353907]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [483 - 1706051045.435741]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [483 - 1706051045.4359107]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [483 - 1706051045.4360948]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [483 - 1706051045.43624]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [483 - 1706051045.4363763]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [483 - 1706051045.4367654]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [483 - 1706051045.4371219]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [483 - 1706051045.4372869]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [483 - 1706051045.4374185]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [483 - 1706051045.4376013]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [483 - 1706051045.4377556]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [483 - 1706051045.4381044]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [483 - 1706051045.438445]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [483 - 1706051045.438636]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [483 - 1706051045.438778]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [483 - 1706051045.4389422]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [483 - 1706051045.439077]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [483 - 1706051045.4394138]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [483 - 1706051045.4398208]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [483 - 1706051045.439986]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [483 - 1706051045.4401205]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [483 - 1706051045.4402957]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [483 - 1706051045.4404583]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [483 - 1706051045.4408512]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [483 - 1706051045.4411948]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [483 - 1706051045.441361]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [483 - 1706051045.4414983]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [483 - 1706051045.4416747]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [483 - 1706051045.4418316]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [483 - 1706051045.4421794]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [483 - 1706051045.4425237]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [483 - 1706051045.4427176]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [483 - 1706051045.442857]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [483 - 1706051045.4430084]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [483 - 1706051045.4431422]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [483 - 1706051045.4435086]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [483 - 1706051045.443883]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [483 - 1706051045.4440477]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [483 - 1706051045.4442012]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [483 - 1706051045.4443426]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [483 - 1706051045.4444802]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [483 - 1706051045.4450116]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [483 - 1706051045.445458]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [483 - 1706051045.4456558]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [483 - 1706051045.4457955]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [483 - 1706051045.4459596]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [483 - 1706051045.446099]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [483 - 1706051045.4465287]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [483 - 1706051045.4469607]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [483 - 1706051045.447138]: 0.0
python ParamPruning/classifier.weight [483 - 1706051045.4472058]: 0.0
python DistillationModifier [490 - 1706051080.3324184]: Calling loss_update with:
args: 0.019682250916957855| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.777777777777778| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [490 - 1706051081.4301393]: 
Returned: 0.045363567769527435| 

python LearningRateFunctionModifier [490 - 1706051083.379688]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.777777777777778| steps_per_epoch: 63| 
python LearningRateFunctionModifier [490 - 1706051083.3798347]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [490 - 1706051083.379881]: 6.025641025641025e-05
python LearningRateFunctionModifier/ParamGroup1 [490 - 1706051083.3799121]: 6.025641025641025e-05
python DistillationModifier/task_loss [490 - 1706051083.3799553]: tensor(0.0197, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [490 - 1706051083.3804307]: tensor(0.0454, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [490 - 1706051083.3807108]: tensor(0.0454, grad_fn=<AddBackward0>)
python ConstantPruningModifier [490 - 1706051083.3809457]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.777777777777778| steps_per_epoch: 63| 
python ConstantPruningModifier [490 - 1706051083.6233597]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [490 - 1706051083.6238382]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [490 - 1706051083.62411]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [490 - 1706051083.6243312]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [490 - 1706051083.6245089]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [490 - 1706051083.6251721]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [490 - 1706051083.625833]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [490 - 1706051083.6260452]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [490 - 1706051083.6262395]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [490 - 1706051083.6264558]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [490 - 1706051083.6266317]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [490 - 1706051083.6271186]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [490 - 1706051083.6276443]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [490 - 1706051083.627842]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [490 - 1706051083.6280296]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [490 - 1706051083.628216]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [490 - 1706051083.62838]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [490 - 1706051083.6288533]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [490 - 1706051083.6292722]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [490 - 1706051083.6294546]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [490 - 1706051083.6296446]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [490 - 1706051083.6298294]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [490 - 1706051083.6299639]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [490 - 1706051083.6303651]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [490 - 1706051083.6307843]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [490 - 1706051083.6309636]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [490 - 1706051083.6311429]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [490 - 1706051083.631289]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [490 - 1706051083.6314325]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [490 - 1706051083.6318514]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [490 - 1706051083.6322575]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [490 - 1706051083.6324313]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [490 - 1706051083.6326258]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [490 - 1706051083.6327808]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [490 - 1706051083.6329615]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [490 - 1706051083.6333787]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [490 - 1706051083.6337967]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [490 - 1706051083.6339796]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [490 - 1706051083.6341517]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [490 - 1706051083.6342938]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [490 - 1706051083.6344323]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [490 - 1706051083.6348383]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [490 - 1706051083.6352615]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [490 - 1706051083.6354315]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [490 - 1706051083.6356158]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [490 - 1706051083.6357706]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [490 - 1706051083.6359107]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [490 - 1706051083.6362932]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [490 - 1706051083.6368015]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [490 - 1706051083.6369896]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [490 - 1706051083.6371577]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [490 - 1706051083.637296]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [490 - 1706051083.637434]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [490 - 1706051083.637856]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [490 - 1706051083.6383271]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [490 - 1706051083.6385076]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [490 - 1706051083.6387055]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [490 - 1706051083.6388502]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [490 - 1706051083.6389875]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [490 - 1706051083.6393535]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [490 - 1706051083.639789]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [490 - 1706051083.639966]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [490 - 1706051083.640137]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [490 - 1706051083.640286]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [490 - 1706051083.6404305]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [490 - 1706051083.6408627]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [490 - 1706051083.641276]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [490 - 1706051083.6414547]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [490 - 1706051083.6416621]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [490 - 1706051083.6418064]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [490 - 1706051083.641972]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [490 - 1706051083.642375]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [490 - 1706051083.6428053]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [490 - 1706051083.6429915]: 0.0
python ParamPruning/classifier.weight [490 - 1706051083.64306]: 0.0
python DistillationModifier [497 - 1706051119.5308783]: Calling loss_update with:
args: 0.11125661432743073| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.888888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [497 - 1706051120.7435243]: 
Returned: 0.08502354472875595| 

python LearningRateFunctionModifier [497 - 1706051123.1231937]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.888888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [497 - 1706051123.1233413]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [497 - 1706051123.1233873]: 5.8974358974358954e-05
python LearningRateFunctionModifier/ParamGroup1 [497 - 1706051123.1234186]: 5.8974358974358954e-05
python DistillationModifier/task_loss [497 - 1706051123.1234598]: tensor(0.1113, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [497 - 1706051123.1239655]: tensor(0.0850, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [497 - 1706051123.1242085]: tensor(0.0850, grad_fn=<AddBackward0>)
python ConstantPruningModifier [497 - 1706051123.1244388]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.888888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [497 - 1706051123.3397522]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [497 - 1706051123.3402195]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [497 - 1706051123.3404293]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [497 - 1706051123.3406312]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [497 - 1706051123.3407848]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [497 - 1706051123.3413901]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [497 - 1706051123.3420322]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [497 - 1706051123.3422494]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [497 - 1706051123.342396]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [497 - 1706051123.3425474]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [497 - 1706051123.342727]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [497 - 1706051123.343157]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [497 - 1706051123.3436558]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [497 - 1706051123.343851]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [497 - 1706051123.3439887]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [497 - 1706051123.3441265]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [497 - 1706051123.344258]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [497 - 1706051123.344675]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [497 - 1706051123.345218]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [497 - 1706051123.3454144]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [497 - 1706051123.3455477]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [497 - 1706051123.3457143]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [497 - 1706051123.345863]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [497 - 1706051123.3462455]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [497 - 1706051123.3466306]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [497 - 1706051123.346812]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [497 - 1706051123.346955]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [497 - 1706051123.3470905]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [497 - 1706051123.34722]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [497 - 1706051123.3477273]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [497 - 1706051123.3481026]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [497 - 1706051123.348276]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [497 - 1706051123.3484042]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [497 - 1706051123.3485327]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [497 - 1706051123.3487184]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [497 - 1706051123.3491366]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [497 - 1706051123.3495228]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [497 - 1706051123.3497167]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [497 - 1706051123.3498569]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [497 - 1706051123.3499894]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [497 - 1706051123.3501246]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [497 - 1706051123.35053]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [497 - 1706051123.3510814]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [497 - 1706051123.3512638]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [497 - 1706051123.3513947]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [497 - 1706051123.3515246]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [497 - 1706051123.3516874]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [497 - 1706051123.3521566]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [497 - 1706051123.416717]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [497 - 1706051123.4169927]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [497 - 1706051123.417164]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [497 - 1706051123.4173253]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [497 - 1706051123.417483]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [497 - 1706051123.4182143]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [497 - 1706051123.418928]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [497 - 1706051123.4191585]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [497 - 1706051123.4193425]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [497 - 1706051123.419584]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [497 - 1706051123.4197805]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [497 - 1706051123.4204063]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [497 - 1706051123.421008]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [497 - 1706051123.421222]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [497 - 1706051123.421403]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [497 - 1706051123.4215975]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [497 - 1706051123.4217718]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [497 - 1706051123.4222937]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [497 - 1706051123.4230216]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [497 - 1706051123.4232433]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [497 - 1706051123.4234061]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [497 - 1706051123.423586]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [497 - 1706051123.423764]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [497 - 1706051123.4243176]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [497 - 1706051123.4249675]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [497 - 1706051123.4251912]: 0.0
python ParamPruning/classifier.weight [497 - 1706051123.4252653]: 0.0
python DistillationModifier [504 - 1706051156.4346352]: Calling loss_update with:
args: 0.5650910139083862| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051157.4463928]: 
Returned: 1.4254146814346313| 

python DistillationModifier [504 - 1706051158.8300312]: Calling loss_update with:
args: 1.3320863246917725| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051160.017831]: 
Returned: 2.1180498600006104| 

python DistillationModifier [504 - 1706051161.0467257]: Calling loss_update with:
args: 1.2012654542922974| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051162.138935]: 
Returned: 1.3145588636398315| 

python DistillationModifier [504 - 1706051163.4489014]: Calling loss_update with:
args: 0.7920200228691101| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051165.0470772]: 
Returned: 1.2054826021194458| 

python DistillationModifier [504 - 1706051166.648877]: Calling loss_update with:
args: 1.0897789001464844| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051168.2402349]: 
Returned: 1.65265953540802| 

python DistillationModifier [504 - 1706051169.9166777]: Calling loss_update with:
args: 0.8467570543289185| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051171.4458673]: 
Returned: 1.5726149082183838| 

python DistillationModifier [504 - 1706051173.0482779]: Calling loss_update with:
args: 0.2946057617664337| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051174.635713]: 
Returned: 0.7561304569244385| 

python DistillationModifier [504 - 1706051176.2411797]: Calling loss_update with:
args: 1.0537731647491455| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051177.6494575]: 
Returned: 1.0872392654418945| 

python DistillationModifier [504 - 1706051178.732665]: Calling loss_update with:
args: 0.9186637997627258| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051179.743148]: 
Returned: 0.9122751355171204| 

python DistillationModifier [504 - 1706051180.8279896]: Calling loss_update with:
args: 0.9213268756866455| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051181.9302886]: 
Returned: 1.4286179542541504| 

python DistillationModifier [504 - 1706051183.1185262]: Calling loss_update with:
args: 0.47358259558677673| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051184.1508424]: 
Returned: 0.7576692700386047| 

python DistillationModifier [504 - 1706051185.431247]: Calling loss_update with:
args: 1.1988437175750732| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051186.9524472]: 
Returned: 1.7590699195861816| 

python DistillationModifier [504 - 1706051188.5498683]: Calling loss_update with:
args: 1.016907811164856| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051190.13696]: 
Returned: 1.979043960571289| 

python DistillationModifier [504 - 1706051191.7308953]: Calling loss_update with:
args: 0.8106909394264221| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051192.7401724]: 
Returned: 0.892659604549408| 

python DistillationModifier [504 - 1706051193.8221827]: Calling loss_update with:
args: 1.2483569383621216| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051194.8277538]: 
Returned: 1.986263394355774| 

python DistillationModifier [504 - 1706051195.846935]: Calling loss_update with:
args: 0.6001010537147522| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051197.4181561]: 
Returned: 0.9888986945152283| 

python DistillationModifier [504 - 1706051198.9298675]: Calling loss_update with:
args: 0.7290042042732239| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051200.0274606]: 
Returned: 1.0084095001220703| 

python DistillationModifier [504 - 1706051201.0488055]: Calling loss_update with:
args: 0.9941550493240356| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051202.1188643]: 
Returned: 1.704365611076355| 

python DistillationModifier [504 - 1706051203.1487613]: Calling loss_update with:
args: 0.7693940997123718| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051204.224049]: 
Returned: 1.1322134733200073| 

python DistillationModifier [504 - 1706051205.2435503]: Calling loss_update with:
args: 0.6021376252174377| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051206.248612]: 
Returned: 1.233710527420044| 

python DistillationModifier [504 - 1706051207.3420186]: Calling loss_update with:
args: 0.49496158957481384| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051208.428414]: 
Returned: 1.0190482139587402| 

python DistillationModifier [504 - 1706051209.8481987]: Calling loss_update with:
args: 0.6955885887145996| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051211.1363919]: 
Returned: 0.9515929222106934| 

python DistillationModifier [504 - 1706051212.7499955]: Calling loss_update with:
args: 0.46967118978500366| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051214.333819]: 
Returned: 1.0579185485839844| 

python DistillationModifier [504 - 1706051215.9410276]: Calling loss_update with:
args: 0.6315619945526123| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051217.565628]: 
Returned: 1.149901270866394| 

python DistillationModifier [504 - 1706051219.2251468]: Calling loss_update with:
args: 0.829267680644989| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051220.8167415]: 
Returned: 0.7642418742179871| 

python DistillationModifier [504 - 1706051222.2489357]: Calling loss_update with:
args: 0.24732959270477295| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051223.8341525]: 
Returned: 0.4437571167945862| 

python DistillationModifier [504 - 1706051225.3306043]: Calling loss_update with:
args: 0.45438775420188904| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051226.7239106]: 
Returned: 0.552618145942688| 

python DistillationModifier [504 - 1706051228.2253046]: Calling loss_update with:
args: 0.5704073905944824| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051229.8253524]: 
Returned: 0.8111024498939514| 

python DistillationModifier [504 - 1706051230.9497845]: Calling loss_update with:
args: 0.4610161781311035| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051232.0185878]: 
Returned: 0.5973132252693176| 

python DistillationModifier [504 - 1706051233.0469217]: Calling loss_update with:
args: 0.5892471075057983| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051234.119718]: 
Returned: 0.9207201600074768| 

python DistillationModifier [504 - 1706051235.1358664]: Calling loss_update with:
args: 0.8557339906692505| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051236.1421077]: 
Returned: 1.6192322969436646| 

python DistillationModifier [504 - 1706051237.220531]: Calling loss_update with:
args: 0.8995881676673889| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051238.2492933]: 
Returned: 1.0124074220657349| 

python DistillationModifier [504 - 1706051239.3347824]: Calling loss_update with:
args: 0.820479154586792| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051240.3421574]: 
Returned: 1.350069522857666| 

python DistillationModifier [504 - 1706051241.429683]: Calling loss_update with:
args: 1.3207745552062988| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051242.4369376]: 
Returned: 2.0475215911865234| 

python DistillationModifier [504 - 1706051243.5278342]: Calling loss_update with:
args: 1.6135032176971436| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051244.5433862]: 
Returned: 2.0457730293273926| 

python DistillationModifier [504 - 1706051245.640491]: Calling loss_update with:
args: 1.1325558423995972| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051246.6484327]: 
Returned: 2.097898483276367| 

python DistillationModifier [504 - 1706051247.7311292]: Calling loss_update with:
args: 1.0320388078689575| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051248.7352822]: 
Returned: 1.2185933589935303| 

python DistillationModifier [504 - 1706051249.8213713]: Calling loss_update with:
args: 0.750843346118927| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051250.834948]: 
Returned: 0.9858890175819397| 

python DistillationModifier [504 - 1706051252.2376285]: Calling loss_update with:
args: 0.8037796020507812| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051253.3217673]: 
Returned: 1.5390087366104126| 

python DistillationModifier [504 - 1706051254.3407345]: Calling loss_update with:
args: 1.2363487482070923| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051255.3467016]: 
Returned: 2.235593318939209| 

python DistillationModifier [504 - 1706051256.4325454]: Calling loss_update with:
args: 0.6475242376327515| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051257.442055]: 
Returned: 0.8120038509368896| 

python DistillationModifier [504 - 1706051258.545244]: Calling loss_update with:
args: 0.5713744759559631| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051259.7213511]: 
Returned: 1.084688425064087| 

python DistillationModifier [504 - 1706051260.7389934]: Calling loss_update with:
args: 0.9153236746788025| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051261.7489002]: 
Returned: 0.6539874076843262| 

python DistillationModifier [504 - 1706051262.8370757]: Calling loss_update with:
args: 0.7328330874443054| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051264.0183165]: 
Returned: 1.6169860363006592| 

python DistillationModifier [504 - 1706051265.3322814]: Calling loss_update with:
args: 0.8207705020904541| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051266.3386621]: 
Returned: 1.4499218463897705| 

python DistillationModifier [504 - 1706051267.4406788]: Calling loss_update with:
args: 0.9787139892578125| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051268.8468006]: 
Returned: 1.2626516819000244| 

python DistillationModifier [504 - 1706051270.5171912]: Calling loss_update with:
args: 0.6484666466712952| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051272.0402784]: 
Returned: 1.1648445129394531| 

python DistillationModifier [504 - 1706051273.6484509]: Calling loss_update with:
args: 0.21174882352352142| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051275.2471366]: 
Returned: 0.44912031292915344| 

python DistillationModifier [504 - 1706051276.919492]: Calling loss_update with:
args: 1.024618148803711| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051278.5251043]: 
Returned: 1.9918063879013062| 

python DistillationModifier [504 - 1706051280.1316895]: Calling loss_update with:
args: 0.9116016626358032| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051281.7319746]: 
Returned: 1.7091277837753296| 

python DistillationModifier [504 - 1706051283.3344855]: Calling loss_update with:
args: 0.3946261405944824| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051284.9219842]: 
Returned: 0.7891575694084167| 

python DistillationModifier [504 - 1706051286.4224663]: Calling loss_update with:
args: 2.173527956008911| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051287.4248605]: 
Returned: 2.122196674346924| 

python DistillationModifier [504 - 1706051289.0201297]: Calling loss_update with:
args: 0.756308376789093| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051290.5476732]: 
Returned: 1.084025263786316| 

python DistillationModifier [504 - 1706051292.0276413]: Calling loss_update with:
args: 0.7982578873634338| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051293.0447142]: 
Returned: 1.2128593921661377| 

python DistillationModifier [504 - 1706051294.1350229]: Calling loss_update with:
args: 0.6622600555419922| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051295.1425197]: 
Returned: 1.3964451551437378| 

python DistillationModifier [504 - 1706051296.221265]: Calling loss_update with:
args: 0.5527089834213257| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051297.2314663]: 
Returned: 0.8390984535217285| 

python DistillationModifier [504 - 1706051298.3483605]: Calling loss_update with:
args: 1.1085001230239868| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051299.3519309]: 
Returned: 0.8378665447235107| 

python DistillationModifier [504 - 1706051300.4356058]: Calling loss_update with:
args: 1.1559736728668213| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051301.4503658]: 
Returned: 1.4880961179733276| 

python DistillationModifier [504 - 1706051302.5310748]: Calling loss_update with:
args: 1.0149565935134888| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051303.5496933]: 
Returned: 1.273313283920288| 

python DistillationModifier [504 - 1706051304.6416008]: Calling loss_update with:
args: 0.616910994052887| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051305.9236753]: 
Returned: 1.1404682397842407| 

python DistillationModifier [504 - 1706051307.5304945]: Calling loss_update with:
args: 0.878906786441803| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051308.81792]: 
Returned: 1.5878018140792847| 

python DistillationModifier [504 - 1706051309.8383949]: Calling loss_update with:
args: 0.5066512227058411| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051310.9190445]: 
Returned: 1.3545451164245605| 

python DistillationModifier [504 - 1706051311.5465205]: Calling loss_update with:
args: 0.5692721009254456| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051312.3257384]: 
Returned: 1.0890445709228516| 

python DistillationModifier [504 - 1706051313.84934]: Calling loss_update with:
args: 0.15727704763412476| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1706051314.9182096]: 
Returned: 0.06967510282993317| 

python LearningRateFunctionModifier [504 - 1706051316.866133]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [504 - 1706051316.8662772]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [504 - 1706051316.8663235]: 5.7692307692307684e-05
python LearningRateFunctionModifier/ParamGroup1 [504 - 1706051316.8663554]: 5.7692307692307684e-05
python DistillationModifier/task_loss [504 - 1706051316.8663995]: tensor(0.1573, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [504 - 1706051316.8669536]: tensor(0.0697, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [504 - 1706051316.8671978]: tensor(0.0697, grad_fn=<AddBackward0>)
python ConstantPruningModifier [504 - 1706051316.8674304]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| 
python ConstantPruningModifier [504 - 1706051317.0461836]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [504 - 1706051317.0466824]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [504 - 1706051317.0469847]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [504 - 1706051317.0471709]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [504 - 1706051317.047379]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [504 - 1706051317.0479856]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [504 - 1706051317.048643]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [504 - 1706051317.0488567]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [504 - 1706051317.049052]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [504 - 1706051317.0492089]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [504 - 1706051317.0493586]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [504 - 1706051317.0498543]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [504 - 1706051317.0502598]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [504 - 1706051317.05045]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [504 - 1706051317.0506177]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [504 - 1706051317.0508177]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [504 - 1706051317.0509686]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [504 - 1706051317.0513458]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [504 - 1706051317.0518477]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [504 - 1706051317.0520382]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [504 - 1706051317.0522208]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [504 - 1706051317.0523603]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [504 - 1706051317.0525088]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [504 - 1706051317.116956]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [504 - 1706051317.1173468]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [504 - 1706051317.1175346]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [504 - 1706051317.1177502]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [504 - 1706051317.117944]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [504 - 1706051317.1180968]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [504 - 1706051317.1184878]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [504 - 1706051317.1190069]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [504 - 1706051317.1191864]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [504 - 1706051317.119369]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [504 - 1706051317.1195128]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [504 - 1706051317.1196833]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [504 - 1706051317.1201386]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [504 - 1706051317.1205251]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [504 - 1706051317.1207578]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [504 - 1706051317.1209035]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [504 - 1706051317.1210876]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [504 - 1706051317.1212275]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [504 - 1706051317.1216307]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [504 - 1706051317.1221104]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [504 - 1706051317.1222758]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [504 - 1706051317.1224568]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [504 - 1706051317.1226203]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [504 - 1706051317.1227858]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [504 - 1706051317.1232371]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [504 - 1706051317.123627]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [504 - 1706051317.1237965]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [504 - 1706051317.123978]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [504 - 1706051317.1241615]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [504 - 1706051317.1243079]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [504 - 1706051317.124755]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [504 - 1706051317.1252308]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [504 - 1706051317.125405]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [504 - 1706051317.1256022]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [504 - 1706051317.125757]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [504 - 1706051317.1259127]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [504 - 1706051317.126321]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [504 - 1706051317.1267169]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [504 - 1706051317.1268885]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [504 - 1706051317.1270292]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [504 - 1706051317.1272264]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [504 - 1706051317.1273694]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [504 - 1706051317.127772]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [504 - 1706051317.128243]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [504 - 1706051317.1284103]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [504 - 1706051317.1286163]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [504 - 1706051317.1287704]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [504 - 1706051317.1289172]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [504 - 1706051317.1293159]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [504 - 1706051317.1297069]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [504 - 1706051317.1298916]: 0.0
python ParamPruning/classifier.weight [504 - 1706051317.1299589]: 0.0
python DistillationModifier [511 - 1706051367.4243102]: Calling loss_update with:
args: 0.14277726411819458| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.11111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [511 - 1706051368.5370283]: 
Returned: 0.17720448970794678| 

python LearningRateFunctionModifier [511 - 1706051370.8790839]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.11111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [511 - 1706051370.8792288]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [511 - 1706051370.8792746]: 5.64102564102564e-05
python LearningRateFunctionModifier/ParamGroup1 [511 - 1706051370.8793042]: 5.64102564102564e-05
python DistillationModifier/task_loss [511 - 1706051370.8793452]: tensor(0.1428, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [511 - 1706051370.8798673]: tensor(0.1772, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [511 - 1706051370.8801234]: tensor(0.1772, grad_fn=<AddBackward0>)
python ConstantPruningModifier [511 - 1706051370.8803568]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.11111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [511 - 1706051371.128305]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [511 - 1706051371.1288528]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [511 - 1706051371.1291804]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [511 - 1706051371.1293883]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [511 - 1706051371.12955]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [511 - 1706051371.1302764]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [511 - 1706051371.1309266]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [511 - 1706051371.1311548]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [511 - 1706051371.1313803]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [511 - 1706051371.1315417]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [511 - 1706051371.1318092]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [511 - 1706051371.1322792]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [511 - 1706051371.1328886]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [511 - 1706051371.1331332]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [511 - 1706051371.1332917]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [511 - 1706051371.1335077]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [511 - 1706051371.1337042]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [511 - 1706051371.13413]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [511 - 1706051371.1346726]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [511 - 1706051371.1348855]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [511 - 1706051371.1351035]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [511 - 1706051371.1352513]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [511 - 1706051371.135409]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [511 - 1706051371.1358576]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [511 - 1706051371.1363962]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [511 - 1706051371.136669]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [511 - 1706051371.1368566]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [511 - 1706051371.1370585]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [511 - 1706051371.1372492]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [511 - 1706051371.1377044]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [511 - 1706051371.1381943]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [511 - 1706051371.1384077]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [511 - 1706051371.1385508]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [511 - 1706051371.138793]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [511 - 1706051371.1389546]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [511 - 1706051371.1393878]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [511 - 1706051371.1398683]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [511 - 1706051371.1400852]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [511 - 1706051371.1402793]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [511 - 1706051371.140428]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [511 - 1706051371.1406372]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [511 - 1706051371.1411994]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [511 - 1706051371.1418262]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [511 - 1706051371.1420357]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [511 - 1706051371.1421864]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [511 - 1706051371.1423752]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [511 - 1706051371.1425152]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [511 - 1706051371.14294]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [511 - 1706051371.1434357]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [511 - 1706051371.1436675]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [511 - 1706051371.1438632]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [511 - 1706051371.1440752]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [511 - 1706051371.1442325]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [511 - 1706051371.1447175]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [511 - 1706051371.1451778]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [511 - 1706051371.1453846]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [511 - 1706051371.1456137]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [511 - 1706051371.1457996]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [511 - 1706051371.1459587]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [511 - 1706051371.1463377]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [511 - 1706051371.146901]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [511 - 1706051371.147114]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [511 - 1706051371.1473038]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [511 - 1706051371.1474538]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [511 - 1706051371.1477015]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [511 - 1706051371.148116]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [511 - 1706051371.1487138]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [511 - 1706051371.148924]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [511 - 1706051371.149105]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [511 - 1706051371.149246]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [511 - 1706051371.149444]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [511 - 1706051371.1498766]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [511 - 1706051371.1502795]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [511 - 1706051371.1504798]: 0.0
python ParamPruning/classifier.weight [511 - 1706051371.1505516]: 0.0
python DistillationModifier [518 - 1706051418.6204882]: Calling loss_update with:
args: 0.23196472227573395| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.222222222222221| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [518 - 1706051420.2229767]: 
Returned: 0.09859533607959747| 

python LearningRateFunctionModifier [518 - 1706051422.966825]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.222222222222221| steps_per_epoch: 63| 
python LearningRateFunctionModifier [518 - 1706051422.9669714]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [518 - 1706051422.9670172]: 5.512820512820513e-05
python LearningRateFunctionModifier/ParamGroup1 [518 - 1706051422.967048]: 5.512820512820513e-05
python DistillationModifier/task_loss [518 - 1706051422.96709]: tensor(0.2320, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [518 - 1706051422.9675863]: tensor(0.0986, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [518 - 1706051422.9678624]: tensor(0.0986, grad_fn=<AddBackward0>)
python ConstantPruningModifier [518 - 1706051422.968104]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.222222222222221| steps_per_epoch: 63| 
python ConstantPruningModifier [518 - 1706051423.2453427]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [518 - 1706051423.2459333]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [518 - 1706051423.2462432]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [518 - 1706051423.2464645]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [518 - 1706051423.246708]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [518 - 1706051423.247488]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [518 - 1706051423.2481332]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [518 - 1706051423.248401]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [518 - 1706051423.2486672]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [518 - 1706051423.2488678]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [518 - 1706051423.2490623]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [518 - 1706051423.2496428]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [518 - 1706051423.2502127]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [518 - 1706051423.2504213]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [518 - 1706051423.2506454]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [518 - 1706051423.2508295]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [518 - 1706051423.2510045]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [518 - 1706051423.2515492]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [518 - 1706051423.252122]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [518 - 1706051423.2523613]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [518 - 1706051423.3166075]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [518 - 1706051423.317059]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [518 - 1706051423.317315]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [518 - 1706051423.318071]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [518 - 1706051423.3186827]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [518 - 1706051423.3189235]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [518 - 1706051423.3191316]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [518 - 1706051423.3193183]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [518 - 1706051423.3194954]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [518 - 1706051423.3200727]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [518 - 1706051423.3206768]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [518 - 1706051423.3208954]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [518 - 1706051423.3210871]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [518 - 1706051423.3212566]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [518 - 1706051423.3214793]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [518 - 1706051423.3220417]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [518 - 1706051423.3226142]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [518 - 1706051423.322828]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [518 - 1706051423.3230183]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [518 - 1706051423.3232043]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [518 - 1706051423.3233826]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [518 - 1706051423.3239474]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [518 - 1706051423.324506]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [518 - 1706051423.324752]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [518 - 1706051423.324954]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [518 - 1706051423.3251297]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [518 - 1706051423.3253028]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [518 - 1706051423.3258662]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [518 - 1706051423.3264222]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [518 - 1706051423.3266344]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [518 - 1706051423.3268251]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [518 - 1706051423.3269992]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [518 - 1706051423.3272264]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [518 - 1706051423.3277793]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [518 - 1706051423.3283317]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [518 - 1706051423.328533]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [518 - 1706051423.328799]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [518 - 1706051423.3289862]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [518 - 1706051423.3291774]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [518 - 1706051423.32975]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [518 - 1706051423.3303065]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [518 - 1706051423.330507]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [518 - 1706051423.3307102]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [518 - 1706051423.3308918]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [518 - 1706051423.331061]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [518 - 1706051423.331618]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [518 - 1706051423.3321762]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [518 - 1706051423.3323753]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [518 - 1706051423.3325937]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [518 - 1706051423.3328006]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [518 - 1706051423.3329768]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [518 - 1706051423.3335257]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [518 - 1706051423.334111]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [518 - 1706051423.3343108]: 0.0
python ParamPruning/classifier.weight [518 - 1706051423.3343847]: 0.0
python DistillationModifier [525 - 1706051468.7550802]: Calling loss_update with:
args: 0.1615099310874939| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.333333333333334| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [525 - 1706051470.3467367]: 
Returned: 0.12643127143383026| 

python LearningRateFunctionModifier [525 - 1706051473.151984]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.333333333333334| steps_per_epoch: 63| 
python LearningRateFunctionModifier [525 - 1706051473.1521327]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [525 - 1706051473.1521795]: 5.384615384615383e-05
python LearningRateFunctionModifier/ParamGroup1 [525 - 1706051473.1522105]: 5.384615384615383e-05
python DistillationModifier/task_loss [525 - 1706051473.1522515]: tensor(0.1615, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [525 - 1706051473.1527832]: tensor(0.1264, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [525 - 1706051473.1530433]: tensor(0.1264, grad_fn=<AddBackward0>)
python ConstantPruningModifier [525 - 1706051473.15328]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.333333333333334| steps_per_epoch: 63| 
python ConstantPruningModifier [525 - 1706051473.3332877]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [525 - 1706051473.3338614]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [525 - 1706051473.334154]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [525 - 1706051473.3343518]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [525 - 1706051473.334605]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [525 - 1706051473.3352838]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [525 - 1706051473.3359547]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [525 - 1706051473.3361788]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [525 - 1706051473.3363235]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [525 - 1706051473.3364692]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [525 - 1706051473.3366733]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [525 - 1706051473.3371665]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [525 - 1706051473.3376942]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [525 - 1706051473.3379207]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [525 - 1706051473.3381422]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [525 - 1706051473.338313]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [525 - 1706051473.338469]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [525 - 1706051473.338955]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [525 - 1706051473.3395228]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [525 - 1706051473.3397527]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [525 - 1706051473.3399062]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [525 - 1706051473.3401034]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [525 - 1706051473.3402789]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [525 - 1706051473.34076]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [525 - 1706051473.341288]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [525 - 1706051473.3414896]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [525 - 1706051473.341676]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [525 - 1706051473.3418872]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [525 - 1706051473.342092]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [525 - 1706051473.3425329]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [525 - 1706051473.3430722]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [525 - 1706051473.343286]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [525 - 1706051473.3434646]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [525 - 1706051473.3436441]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [525 - 1706051473.3438091]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [525 - 1706051473.344265]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [525 - 1706051473.3447104]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [525 - 1706051473.3449175]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [525 - 1706051473.3450754]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [525 - 1706051473.345265]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [525 - 1706051473.3454256]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [525 - 1706051473.3458314]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [525 - 1706051473.3464603]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [525 - 1706051473.3467624]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [525 - 1706051473.3470192]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [525 - 1706051473.3472188]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [525 - 1706051473.3474264]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [525 - 1706051473.3479106]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [525 - 1706051473.3483973]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [525 - 1706051473.3486311]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [525 - 1706051473.3487968]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [525 - 1706051473.3489904]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [525 - 1706051473.3491812]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [525 - 1706051473.3496673]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [525 - 1706051473.350146]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [525 - 1706051473.3503504]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [525 - 1706051473.3504949]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [525 - 1706051473.3507147]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [525 - 1706051473.3508863]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [525 - 1706051473.3513048]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [525 - 1706051473.351696]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [525 - 1706051473.3519018]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [525 - 1706051473.3520465]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [525 - 1706051473.3522346]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [525 - 1706051473.352413]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [525 - 1706051473.4168603]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [525 - 1706051473.417425]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [525 - 1706051473.4176729]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [525 - 1706051473.4178796]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [525 - 1706051473.418023]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [525 - 1706051473.41823]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [525 - 1706051473.4187143]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [525 - 1706051473.4192917]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [525 - 1706051473.4194973]: 0.0
python ParamPruning/classifier.weight [525 - 1706051473.4195905]: 0.0
python DistillationModifier [532 - 1706051521.4216566]: Calling loss_update with:
args: 0.0874534323811531| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.444444444444445| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [532 - 1706051522.9485579]: 
Returned: 0.20953300595283508| 

python LearningRateFunctionModifier [532 - 1706051526.4171567]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.444444444444445| steps_per_epoch: 63| 
python LearningRateFunctionModifier [532 - 1706051526.4173222]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [532 - 1706051526.4173691]: 5.256410256410255e-05
python LearningRateFunctionModifier/ParamGroup1 [532 - 1706051526.4173996]: 5.256410256410255e-05
python DistillationModifier/task_loss [532 - 1706051526.4174416]: tensor(0.0875, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [532 - 1706051526.417943]: tensor(0.2095, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [532 - 1706051526.4181807]: tensor(0.2095, grad_fn=<AddBackward0>)
python ConstantPruningModifier [532 - 1706051526.4184086]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.444444444444445| steps_per_epoch: 63| 
python ConstantPruningModifier [532 - 1706051526.7257621]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [532 - 1706051526.7263021]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [532 - 1706051526.726521]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [532 - 1706051526.7267962]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [532 - 1706051526.7270129]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [532 - 1706051526.7278273]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [532 - 1706051526.7286577]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [532 - 1706051526.7289166]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [532 - 1706051526.729144]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [532 - 1706051526.729324]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [532 - 1706051526.7295775]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [532 - 1706051526.7302122]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [532 - 1706051526.730876]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [532 - 1706051526.7311027]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [532 - 1706051526.7313235]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [532 - 1706051526.7314882]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [532 - 1706051526.7316868]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [532 - 1706051526.7323024]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [532 - 1706051526.7329075]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [532 - 1706051526.7331202]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [532 - 1706051526.7333376]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [532 - 1706051526.733512]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [532 - 1706051526.7337182]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [532 - 1706051526.7343297]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [532 - 1706051526.7348912]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [532 - 1706051526.7351038]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [532 - 1706051526.7353072]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [532 - 1706051526.7355394]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [532 - 1706051526.7357316]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [532 - 1706051526.736289]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [532 - 1706051526.7369335]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [532 - 1706051526.7371533]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [532 - 1706051526.7373598]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [532 - 1706051526.7376046]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [532 - 1706051526.7377937]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [532 - 1706051526.7384615]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [532 - 1706051526.7390559]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [532 - 1706051526.7392673]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [532 - 1706051526.73948]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [532 - 1706051526.7396612]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [532 - 1706051526.7398727]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [532 - 1706051526.7404962]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [532 - 1706051526.741072]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [532 - 1706051526.7412877]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [532 - 1706051526.741492]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [532 - 1706051526.7416887]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [532 - 1706051526.741867]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [532 - 1706051526.7424507]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [532 - 1706051526.743098]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [532 - 1706051526.7433188]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [532 - 1706051526.7435224]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [532 - 1706051526.7437227]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [532 - 1706051526.7438998]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [532 - 1706051526.7444382]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [532 - 1706051526.7451444]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [532 - 1706051526.7453537]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [532 - 1706051526.7455513]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [532 - 1706051526.7458062]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [532 - 1706051526.745972]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [532 - 1706051526.7465158]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [532 - 1706051526.74718]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [532 - 1706051526.747395]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [532 - 1706051526.7476165]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [532 - 1706051526.7477896]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [532 - 1706051526.7479668]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [532 - 1706051526.748527]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [532 - 1706051526.7491736]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [532 - 1706051526.7493863]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [532 - 1706051526.7496018]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [532 - 1706051526.7498405]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [532 - 1706051526.7500198]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [532 - 1706051526.75065]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [532 - 1706051526.7512436]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [532 - 1706051526.751457]: 0.0
python ParamPruning/classifier.weight [532 - 1706051526.7515278]: 0.0
python DistillationModifier [539 - 1706051581.4238672]: Calling loss_update with:
args: 0.04764401912689209| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.555555555555555| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [539 - 1706051582.73173]: 
Returned: 0.06523144245147705| 

python LearningRateFunctionModifier [539 - 1706051585.9794717]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.555555555555555| steps_per_epoch: 63| 
python LearningRateFunctionModifier [539 - 1706051585.979659]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [539 - 1706051585.9797134]: 5.128205128205128e-05
python LearningRateFunctionModifier/ParamGroup1 [539 - 1706051585.9797504]: 5.128205128205128e-05
python DistillationModifier/task_loss [539 - 1706051585.9798005]: tensor(0.0476, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [539 - 1706051585.9803333]: tensor(0.0652, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [539 - 1706051585.980599]: tensor(0.0652, grad_fn=<AddBackward0>)
python ConstantPruningModifier [539 - 1706051585.9808648]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.555555555555555| steps_per_epoch: 63| 
python ConstantPruningModifier [539 - 1706051586.326804]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [539 - 1706051586.327332]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [539 - 1706051586.327652]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [539 - 1706051586.32792]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [539 - 1706051586.3281248]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [539 - 1706051586.328927]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [539 - 1706051586.3295221]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [539 - 1706051586.3297727]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [539 - 1706051586.330009]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [539 - 1706051586.3301952]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [539 - 1706051586.3303812]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [539 - 1706051586.330956]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [539 - 1706051586.331507]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [539 - 1706051586.3317692]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [539 - 1706051586.3319428]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [539 - 1706051586.3321073]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [539 - 1706051586.3323133]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [539 - 1706051586.3328853]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [539 - 1706051586.3334348]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [539 - 1706051586.3336644]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [539 - 1706051586.3338845]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [539 - 1706051586.3340526]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [539 - 1706051586.3342674]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [539 - 1706051586.3348372]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [539 - 1706051586.3354073]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [539 - 1706051586.33565]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [539 - 1706051586.3358622]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [539 - 1706051586.3360898]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [539 - 1706051586.3362792]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [539 - 1706051586.336874]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [539 - 1706051586.3374255]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [539 - 1706051586.3376539]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [539 - 1706051586.3378246]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [539 - 1706051586.3379872]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [539 - 1706051586.338145]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [539 - 1706051586.3387003]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [539 - 1706051586.339245]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [539 - 1706051586.3394494]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [539 - 1706051586.339679]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [539 - 1706051586.3399055]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [539 - 1706051586.3400896]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [539 - 1706051586.3406687]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [539 - 1706051586.341222]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [539 - 1706051586.3414426]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [539 - 1706051586.3416724]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [539 - 1706051586.341844]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [539 - 1706051586.3420496]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [539 - 1706051586.342615]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [539 - 1706051586.3431737]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [539 - 1706051586.3434021]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [539 - 1706051586.3436277]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [539 - 1706051586.3438473]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [539 - 1706051586.344044]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [539 - 1706051586.3446476]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [539 - 1706051586.3452034]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [539 - 1706051586.345419]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [539 - 1706051586.345637]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [539 - 1706051586.3458567]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [539 - 1706051586.3460424]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [539 - 1706051586.346623]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [539 - 1706051586.3471897]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [539 - 1706051586.3473911]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [539 - 1706051586.3476124]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [539 - 1706051586.3478258]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [539 - 1706051586.3480074]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [539 - 1706051586.348601]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [539 - 1706051586.3491518]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [539 - 1706051586.3493679]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [539 - 1706051586.3495774]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [539 - 1706051586.3497589]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [539 - 1706051586.349966]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [539 - 1706051586.350522]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [539 - 1706051586.3510976]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [539 - 1706051586.3513138]: 0.0
python ParamPruning/classifier.weight [539 - 1706051586.3513834]: 0.0
python DistillationModifier [546 - 1706051637.4475937]: Calling loss_update with:
args: 0.06000332534313202| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.666666666666666| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [546 - 1706051638.5282981]: 
Returned: 0.16109715402126312| 

python LearningRateFunctionModifier [546 - 1706051641.2628396]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.666666666666666| steps_per_epoch: 63| 
python LearningRateFunctionModifier [546 - 1706051641.2630067]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [546 - 1706051641.2630584]: 4.9999999999999996e-05
python LearningRateFunctionModifier/ParamGroup1 [546 - 1706051641.2630944]: 4.9999999999999996e-05
python DistillationModifier/task_loss [546 - 1706051641.263144]: tensor(0.0600, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [546 - 1706051641.2637022]: tensor(0.1611, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [546 - 1706051641.263952]: tensor(0.1611, grad_fn=<AddBackward0>)
python ConstantPruningModifier [546 - 1706051641.2641938]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.666666666666666| steps_per_epoch: 63| 
python ConstantPruningModifier [546 - 1706051641.551592]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [546 - 1706051641.552112]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [546 - 1706051641.552417]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [546 - 1706051641.6166637]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [546 - 1706051641.6168988]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [546 - 1706051641.6176789]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [546 - 1706051641.6182618]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [546 - 1706051641.618502]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [546 - 1706051641.618745]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [546 - 1706051641.6189332]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [546 - 1706051641.6191099]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [546 - 1706051641.6196854]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [546 - 1706051641.6204581]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [546 - 1706051641.620712]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [546 - 1706051641.6209333]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [546 - 1706051641.6211019]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [546 - 1706051641.6212628]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [546 - 1706051641.621861]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [546 - 1706051641.6226757]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [546 - 1706051641.6229076]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [546 - 1706051641.6231253]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [546 - 1706051641.6233177]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [546 - 1706051641.6234999]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [546 - 1706051641.6241734]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [546 - 1706051641.6249185]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [546 - 1706051641.6251373]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [546 - 1706051641.6253476]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [546 - 1706051641.6255157]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [546 - 1706051641.6257112]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [546 - 1706051641.6263108]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [546 - 1706051641.626965]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [546 - 1706051641.6271682]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [546 - 1706051641.6273777]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [546 - 1706051641.627554]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [546 - 1706051641.6277614]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [546 - 1706051641.628321]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [546 - 1706051641.6291387]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [546 - 1706051641.6293454]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [546 - 1706051641.6295426]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [546 - 1706051641.629747]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [546 - 1706051641.6299214]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [546 - 1706051641.6306481]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [546 - 1706051641.6313088]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [546 - 1706051641.6315343]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [546 - 1706051641.6317601]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [546 - 1706051641.6319435]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [546 - 1706051641.6321175]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [546 - 1706051641.632786]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [546 - 1706051641.6333823]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [546 - 1706051641.6336372]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [546 - 1706051641.6339195]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [546 - 1706051641.6341648]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [546 - 1706051641.6343641]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [546 - 1706051641.6349993]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [546 - 1706051641.635604]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [546 - 1706051641.635817]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [546 - 1706051641.636022]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [546 - 1706051641.6361976]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [546 - 1706051641.6363702]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [546 - 1706051641.6369972]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [546 - 1706051641.6375518]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [546 - 1706051641.6378007]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [546 - 1706051641.6380112]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [546 - 1706051641.6381927]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [546 - 1706051641.6383686]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [546 - 1706051641.6389556]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [546 - 1706051641.63965]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [546 - 1706051641.6398718]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [546 - 1706051641.640077]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [546 - 1706051641.6402555]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [546 - 1706051641.640427]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [546 - 1706051641.6410396]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [546 - 1706051641.6418705]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [546 - 1706051641.6420994]: 0.0
python ParamPruning/classifier.weight [546 - 1706051641.6421745]: 0.0
python DistillationModifier [553 - 1706051688.1508594]: Calling loss_update with:
args: 0.09390388429164886| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.777777777777779| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [553 - 1706051689.3261309]: 
Returned: 0.10184616595506668| 

python LearningRateFunctionModifier [553 - 1706051691.6842809]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.777777777777779| steps_per_epoch: 63| 
python LearningRateFunctionModifier [553 - 1706051691.684449]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [553 - 1706051691.6844945]: 4.871794871794871e-05
python LearningRateFunctionModifier/ParamGroup1 [553 - 1706051691.684526]: 4.871794871794871e-05
python DistillationModifier/task_loss [553 - 1706051691.6845858]: tensor(0.0939, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [553 - 1706051691.6850843]: tensor(0.1018, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [553 - 1706051691.6853266]: tensor(0.1018, grad_fn=<AddBackward0>)
python ConstantPruningModifier [553 - 1706051691.6855836]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.777777777777779| steps_per_epoch: 63| 
python ConstantPruningModifier [553 - 1706051691.928103]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [553 - 1706051691.9286463]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [553 - 1706051691.9288986]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [553 - 1706051691.9290733]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [553 - 1706051691.929228]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [553 - 1706051691.9298236]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [553 - 1706051691.930465]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [553 - 1706051691.9307077]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [553 - 1706051691.930869]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [553 - 1706051691.9310143]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [553 - 1706051691.9311688]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [553 - 1706051691.9316692]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [553 - 1706051691.9322493]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [553 - 1706051691.9324613]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [553 - 1706051691.9326494]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [553 - 1706051691.9328082]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [553 - 1706051691.9329543]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [553 - 1706051691.9333434]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [553 - 1706051691.9338875]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [553 - 1706051691.9340816]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [553 - 1706051691.9342268]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [553 - 1706051691.9344]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [553 - 1706051691.9345584]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [553 - 1706051691.9349854]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [553 - 1706051691.9354331]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [553 - 1706051691.9356406]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [553 - 1706051691.9358351]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [553 - 1706051691.9359848]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [553 - 1706051691.9361308]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [553 - 1706051691.9365432]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [553 - 1706051691.937026]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [553 - 1706051691.9372015]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [553 - 1706051691.9373422]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [553 - 1706051691.9374785]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [553 - 1706051691.9376404]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [553 - 1706051691.938041]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [553 - 1706051691.9384215]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [553 - 1706051691.938615]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [553 - 1706051691.9387665]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [553 - 1706051691.9389122]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [553 - 1706051691.9390519]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [553 - 1706051691.9394114]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [553 - 1706051691.9399424]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [553 - 1706051691.940114]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [553 - 1706051691.9402585]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [553 - 1706051691.9403963]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [553 - 1706051691.9405382]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [553 - 1706051691.9409876]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [553 - 1706051691.9415212]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [553 - 1706051691.9417362]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [553 - 1706051691.9418983]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [553 - 1706051691.9420466]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [553 - 1706051691.9421945]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [553 - 1706051691.94261]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [553 - 1706051691.9431887]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [553 - 1706051691.9433749]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [553 - 1706051691.9435186]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [553 - 1706051691.9436934]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [553 - 1706051691.943844]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [553 - 1706051691.9442544]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [553 - 1706051691.9447567]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [553 - 1706051691.9449372]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [553 - 1706051691.9451008]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [553 - 1706051691.9452462]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [553 - 1706051691.945385]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [553 - 1706051691.9458148]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [553 - 1706051691.9462492]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [553 - 1706051691.9464276]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [553 - 1706051691.9465895]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [553 - 1706051691.9467437]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [553 - 1706051691.9468882]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [553 - 1706051691.947298]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [553 - 1706051691.947873]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [553 - 1706051691.9480684]: 0.0
python ParamPruning/classifier.weight [553 - 1706051691.9481406]: 0.0
python DistillationModifier [560 - 1706051740.9405966]: Calling loss_update with:
args: 0.07730211317539215| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.88888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [560 - 1706051742.2274878]: 
Returned: 0.1582922339439392| 

python LearningRateFunctionModifier [560 - 1706051744.9794202]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.88888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [560 - 1706051744.9795861]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [560 - 1706051744.9796338]: 4.743589743589743e-05
python LearningRateFunctionModifier/ParamGroup1 [560 - 1706051744.979665]: 4.743589743589743e-05
python DistillationModifier/task_loss [560 - 1706051744.9797077]: tensor(0.0773, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [560 - 1706051744.9801788]: tensor(0.1583, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [560 - 1706051744.980418]: tensor(0.1583, grad_fn=<AddBackward0>)
python ConstantPruningModifier [560 - 1706051744.9806826]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.88888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [560 - 1706051745.2189093]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [560 - 1706051745.2193718]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [560 - 1706051745.2196553]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [560 - 1706051745.2198358]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [560 - 1706051745.2199934]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [560 - 1706051745.2206411]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [560 - 1706051745.2212336]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [560 - 1706051745.2214527]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [560 - 1706051745.2216647]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [560 - 1706051745.221857]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [560 - 1706051745.222012]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [560 - 1706051745.2224455]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [560 - 1706051745.2229793]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [560 - 1706051745.2231748]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [560 - 1706051745.2233589]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [560 - 1706051745.2235074]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [560 - 1706051745.2236803]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [560 - 1706051745.224073]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [560 - 1706051745.2245147]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [560 - 1706051745.2247212]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [560 - 1706051745.224899]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [560 - 1706051745.2250438]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [560 - 1706051745.2251914]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [560 - 1706051745.225622]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [560 - 1706051745.2259881]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [560 - 1706051745.2261665]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [560 - 1706051745.2263458]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [560 - 1706051745.2264922]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [560 - 1706051745.2266562]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [560 - 1706051745.227004]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [560 - 1706051745.2274587]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [560 - 1706051745.2276611]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [560 - 1706051745.2278285]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [560 - 1706051745.2279632]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [560 - 1706051745.228121]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [560 - 1706051745.228508]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [560 - 1706051745.2290747]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [560 - 1706051745.229257]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [560 - 1706051745.2294393]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [560 - 1706051745.2296124]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [560 - 1706051745.2297735]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [560 - 1706051745.2302136]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [560 - 1706051745.2306712]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [560 - 1706051745.230844]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [560 - 1706051745.231015]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [560 - 1706051745.2311728]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [560 - 1706051745.2313144]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [560 - 1706051745.2318156]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [560 - 1706051745.232204]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [560 - 1706051745.2323763]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [560 - 1706051745.232581]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [560 - 1706051745.232737]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [560 - 1706051745.232885]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [560 - 1706051745.2332435]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [560 - 1706051745.233626]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [560 - 1706051745.233802]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [560 - 1706051745.2339573]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [560 - 1706051745.23409]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [560 - 1706051745.2342205]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [560 - 1706051745.234759]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [560 - 1706051745.2351542]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [560 - 1706051745.2353272]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [560 - 1706051745.2354963]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [560 - 1706051745.235666]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [560 - 1706051745.2358148]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [560 - 1706051745.2361717]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [560 - 1706051745.2367134]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [560 - 1706051745.2368963]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [560 - 1706051745.237063]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [560 - 1706051745.2371974]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [560 - 1706051745.2373326]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [560 - 1706051745.2377474]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [560 - 1706051745.2381406]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [560 - 1706051745.238315]: 0.0
python ParamPruning/classifier.weight [560 - 1706051745.238386]: 0.0
python DistillationModifier [567 - 1706051791.2479146]: Calling loss_update with:
args: 0.7635963559150696| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051792.63299]: 
Returned: 1.723633885383606| 

python DistillationModifier [567 - 1706051794.6418545]: Calling loss_update with:
args: 1.410304307937622| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051795.717368]: 
Returned: 2.2947516441345215| 

python DistillationModifier [567 - 1706051797.7201538]: Calling loss_update with:
args: 1.7687305212020874| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051798.8205142]: 
Returned: 2.3033056259155273| 

python DistillationModifier [567 - 1706051801.1504264]: Calling loss_update with:
args: 0.8329825401306152| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051802.1508234]: 
Returned: 0.9983385801315308| 

python DistillationModifier [567 - 1706051805.2171001]: Calling loss_update with:
args: 0.7859894633293152| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051806.2226734]: 
Returned: 1.4078141450881958| 

python DistillationModifier [567 - 1706051808.4208105]: Calling loss_update with:
args: 0.9146530032157898| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051809.4426322]: 
Returned: 1.747804045677185| 

python DistillationModifier [567 - 1706051811.9510036]: Calling loss_update with:
args: 0.5221982598304749| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051813.6238334]: 
Returned: 1.257944941520691| 

python DistillationModifier [567 - 1706051816.8222315]: Calling loss_update with:
args: 1.4216930866241455| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051817.8319597]: 
Returned: 1.7092758417129517| 

python DistillationModifier [567 - 1706051819.8364298]: Calling loss_update with:
args: 0.7590306997299194| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051820.9191813]: 
Returned: 0.5804119110107422| 

python DistillationModifier [567 - 1706051823.0221496]: Calling loss_update with:
args: 1.007019281387329| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051824.0337958]: 
Returned: 1.7004142999649048| 

python DistillationModifier [567 - 1706051827.3269758]: Calling loss_update with:
args: 0.9466946125030518| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051828.9242733]: 
Returned: 1.2842967510223389| 

python DistillationModifier [567 - 1706051831.9446354]: Calling loss_update with:
args: 1.1439557075500488| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051833.0286896]: 
Returned: 2.0188326835632324| 

python DistillationModifier [567 - 1706051835.0267196]: Calling loss_update with:
args: 1.1677993535995483| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051836.3168993]: 
Returned: 2.3154616355895996| 

python DistillationModifier [567 - 1706051838.6332867]: Calling loss_update with:
args: 0.8085066080093384| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051839.6398048]: 
Returned: 1.1569017171859741| 

python DistillationModifier [567 - 1706051841.6515863]: Calling loss_update with:
args: 1.2590383291244507| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051842.7213225]: 
Returned: 2.5252745151519775| 

python DistillationModifier [567 - 1706051845.123531]: Calling loss_update with:
args: 1.0330461263656616| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051846.145977]: 
Returned: 1.8027753829956055| 

python DistillationModifier [567 - 1706051848.2228377]: Calling loss_update with:
args: 0.8575645685195923| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051849.2303927]: 
Returned: 1.2250515222549438| 

python DistillationModifier [567 - 1706051851.2517147]: Calling loss_update with:
args: 1.2613765001296997| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051852.3424273]: 
Returned: 2.2289011478424072| 

python DistillationModifier [567 - 1706051854.3458936]: Calling loss_update with:
args: 1.3499300479888916| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051855.3514037]: 
Returned: 1.7511582374572754| 

python DistillationModifier [567 - 1706051857.3490458]: Calling loss_update with:
args: 0.8108141422271729| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051858.6477492]: 
Returned: 1.3852912187576294| 

python DistillationModifier [567 - 1706051861.836535]: Calling loss_update with:
args: 0.7875829935073853| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051862.9183197]: 
Returned: 1.3411637544631958| 

python DistillationModifier [567 - 1706051865.7416859]: Calling loss_update with:
args: 0.7818204164505005| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051866.7525237]: 
Returned: 1.1447118520736694| 

python DistillationModifier [567 - 1706051869.9395192]: Calling loss_update with:
args: 0.9036911725997925| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051871.4456978]: 
Returned: 1.113183856010437| 

python DistillationModifier [567 - 1706051874.129472]: Calling loss_update with:
args: 0.934134304523468| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051875.247727]: 
Returned: 2.0287232398986816| 

python DistillationModifier [567 - 1706051878.5410154]: Calling loss_update with:
args: 1.1892180442810059| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051880.1356468]: 
Returned: 1.4534991979599| 

python DistillationModifier [567 - 1706051882.833461]: Calling loss_update with:
args: 0.6554678678512573| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051883.8472376]: 
Returned: 1.1879855394363403| 

python DistillationModifier [567 - 1706051885.8505177]: Calling loss_update with:
args: 1.1006128787994385| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051886.9242573]: 
Returned: 1.5509742498397827| 

python DistillationModifier [567 - 1706051889.742144]: Calling loss_update with:
args: 0.8391155004501343| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051891.124822]: 
Returned: 1.2144960165023804| 

python DistillationModifier [567 - 1706051894.4243557]: Calling loss_update with:
args: 0.7177169919013977| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051896.0166657]: 
Returned: 1.4955263137817383| 

python DistillationModifier [567 - 1706051898.1290336]: Calling loss_update with:
args: 0.9390763640403748| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051899.148028]: 
Returned: 2.111262798309326| 

python DistillationModifier [567 - 1706051901.629133]: Calling loss_update with:
args: 1.0772905349731445| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051903.1524312]: 
Returned: 2.078573703765869| 

python DistillationModifier [567 - 1706051906.5363777]: Calling loss_update with:
args: 1.0537728071212769| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051908.1406972]: 
Returned: 1.5835447311401367| 

python DistillationModifier [567 - 1706051911.1358814]: Calling loss_update with:
args: 1.30140221118927| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051912.221617]: 
Returned: 2.2224509716033936| 

python DistillationModifier [567 - 1706051914.228609]: Calling loss_update with:
args: 1.6274101734161377| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051915.2349746]: 
Returned: 2.5518834590911865| 

python DistillationModifier [567 - 1706051918.049026]: Calling loss_update with:
args: 1.7462214231491089| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051919.4306715]: 
Returned: 2.276113748550415| 

python DistillationModifier [567 - 1706051921.430157]: Calling loss_update with:
args: 1.4219515323638916| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051922.4428937]: 
Returned: 2.562647819519043| 

python DistillationModifier [567 - 1706051924.4432735]: Calling loss_update with:
args: 1.1309349536895752| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051925.7169526]: 
Returned: 1.1621637344360352| 

python DistillationModifier [567 - 1706051927.9346943]: Calling loss_update with:
args: 0.7630061507225037| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051929.5335107]: 
Returned: 1.3454961776733398| 

python DistillationModifier [567 - 1706051931.6517732]: Calling loss_update with:
args: 1.0111571550369263| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051932.723892]: 
Returned: 1.9566630125045776| 

python DistillationModifier [567 - 1706051936.1217349]: Calling loss_update with:
args: 1.630334496498108| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051937.7301137]: 
Returned: 3.055283784866333| 

python DistillationModifier [567 - 1706051941.0410407]: Calling loss_update with:
args: 0.61625736951828| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051942.6183171]: 
Returned: 1.4227477312088013| 

python DistillationModifier [567 - 1706051944.5510807]: Calling loss_update with:
args: 0.9286347031593323| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051945.6177695]: 
Returned: 1.967647671699524| 

python DistillationModifier [567 - 1706051947.616641]: Calling loss_update with:
args: 1.243811011314392| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051948.6374362]: 
Returned: 1.266514539718628| 

python DistillationModifier [567 - 1706051950.9492269]: Calling loss_update with:
args: 0.5918799638748169| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051952.0171666]: 
Returned: 1.5251381397247314| 

python DistillationModifier [567 - 1706051954.7496893]: Calling loss_update with:
args: 1.2183698415756226| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051956.3396006]: 
Returned: 2.881434679031372| 

python DistillationModifier [567 - 1706051959.4263086]: Calling loss_update with:
args: 1.4104535579681396| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051960.630753]: 
Returned: 1.8089157342910767| 

python DistillationModifier [567 - 1706051963.9352365]: Calling loss_update with:
args: 1.1180566549301147| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051965.5349092]: 
Returned: 1.7442123889923096| 

python DistillationModifier [567 - 1706051968.8200219]: Calling loss_update with:
args: 0.533883810043335| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051970.347708]: 
Returned: 1.2019157409667969| 

python DistillationModifier [567 - 1706051973.649021]: Calling loss_update with:
args: 1.0898739099502563| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051975.24751]: 
Returned: 2.113975763320923| 

python DistillationModifier [567 - 1706051977.8249855]: Calling loss_update with:
args: 0.8903048634529114| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051978.8500488]: 
Returned: 1.6350080966949463| 

python DistillationModifier [567 - 1706051981.0190568]: Calling loss_update with:
args: 0.47778594493865967| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051982.02369]: 
Returned: 1.002116084098816| 

python DistillationModifier [567 - 1706051984.0340002]: Calling loss_update with:
args: 2.0356078147888184| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051985.0407512]: 
Returned: 2.039898633956909| 

python DistillationModifier [567 - 1706051987.036546]: Calling loss_update with:
args: 0.9859087467193604| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051988.1333122]: 
Returned: 1.22427237033844| 

python DistillationModifier [567 - 1706051990.1475208]: Calling loss_update with:
args: 0.8725947141647339| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051991.2240634]: 
Returned: 1.3789174556732178| 

python DistillationModifier [567 - 1706051994.534755]: Calling loss_update with:
args: 0.6511275172233582| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706051996.1374645]: 
Returned: 1.6754730939865112| 

python DistillationModifier [567 - 1706051999.523964]: Calling loss_update with:
args: 0.9751842617988586| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706052001.1227293]: 
Returned: 1.4875239133834839| 

python DistillationModifier [567 - 1706052004.423007]: Calling loss_update with:
args: 0.982931911945343| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706052006.0234504]: 
Returned: 0.8844149112701416| 

python DistillationModifier [567 - 1706052009.2528203]: Calling loss_update with:
args: 1.1684893369674683| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706052010.839302]: 
Returned: 1.5320380926132202| 

python DistillationModifier [567 - 1706052013.829084]: Calling loss_update with:
args: 1.4632906913757324| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706052015.4249644]: 
Returned: 1.7816377878189087| 

python DistillationModifier [567 - 1706052018.218444]: Calling loss_update with:
args: 0.953181266784668| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706052019.2319307]: 
Returned: 1.8614190816879272| 

python DistillationModifier [567 - 1706052021.2410872]: Calling loss_update with:
args: 0.9474219083786011| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706052022.2478735]: 
Returned: 1.5771420001983643| 

python DistillationModifier [567 - 1706052024.7497628]: Calling loss_update with:
args: 1.1600176095962524| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706052025.840072]: 
Returned: 2.35085391998291| 

python DistillationModifier [567 - 1706052027.2489598]: Calling loss_update with:
args: 0.5983372330665588| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706052027.829981]: 
Returned: 1.2116740942001343| 

python DistillationModifier [567 - 1706052030.7265208]: Calling loss_update with:
args: 0.03318488970398903| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1706052032.3243768]: 
Returned: 0.12535016238689423| 

python LearningRateFunctionModifier [567 - 1706052035.8237662]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [567 - 1706052035.8239284]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [567 - 1706052035.8239756]: 4.615384615384616e-05
python LearningRateFunctionModifier/ParamGroup1 [567 - 1706052035.8240063]: 4.615384615384616e-05
python DistillationModifier/task_loss [567 - 1706052035.8240502]: tensor(0.0332, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [567 - 1706052035.8245382]: tensor(0.1254, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [567 - 1706052035.8248024]: tensor(0.1254, grad_fn=<AddBackward0>)
python ConstantPruningModifier [567 - 1706052035.8250327]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| 
python ConstantPruningModifier [567 - 1706052036.133498]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [567 - 1706052036.1340277]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [567 - 1706052036.1343107]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [567 - 1706052036.1345782]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [567 - 1706052036.1347713]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [567 - 1706052036.1355212]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [567 - 1706052036.1361299]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [567 - 1706052036.136363]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [567 - 1706052036.1366158]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [567 - 1706052036.1368115]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [567 - 1706052036.1370308]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [567 - 1706052036.1375797]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [567 - 1706052036.13812]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [567 - 1706052036.1383247]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [567 - 1706052036.1385274]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [567 - 1706052036.1387246]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [567 - 1706052036.1389282]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [567 - 1706052036.139436]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [567 - 1706052036.1399775]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [567 - 1706052036.140178]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [567 - 1706052036.140398]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [567 - 1706052036.1406193]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [567 - 1706052036.140806]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [567 - 1706052036.1413178]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [567 - 1706052036.14186]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [567 - 1706052036.1420538]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [567 - 1706052036.142261]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [567 - 1706052036.1424394]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [567 - 1706052036.142627]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [567 - 1706052036.1431637]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [567 - 1706052036.143701]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [567 - 1706052036.1438878]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [567 - 1706052036.144091]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [567 - 1706052036.144259]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [567 - 1706052036.1444411]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [567 - 1706052036.144987]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [567 - 1706052036.145511]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [567 - 1706052036.145728]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [567 - 1706052036.1459367]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [567 - 1706052036.146117]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [567 - 1706052036.146291]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [567 - 1706052036.146825]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [567 - 1706052036.147351]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [567 - 1706052036.1475418]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [567 - 1706052036.1478302]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [567 - 1706052036.148003]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [567 - 1706052036.1482053]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [567 - 1706052036.1487544]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [567 - 1706052036.149306]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [567 - 1706052036.1494997]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [567 - 1706052036.1497157]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [567 - 1706052036.1499097]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [567 - 1706052036.150082]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [567 - 1706052036.150611]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [567 - 1706052036.1511319]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [567 - 1706052036.1513252]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [567 - 1706052036.1515076]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [567 - 1706052036.1517138]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [567 - 1706052036.151889]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [567 - 1706052036.1523995]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [567 - 1706052036.2168548]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [567 - 1706052036.217056]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [567 - 1706052036.2172582]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [567 - 1706052036.2174323]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [567 - 1706052036.2176464]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [567 - 1706052036.2181773]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [567 - 1706052036.2187161]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [567 - 1706052036.2189066]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [567 - 1706052036.2191052]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [567 - 1706052036.2192936]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [567 - 1706052036.2194576]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [567 - 1706052036.219987]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [567 - 1706052036.2205465]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [567 - 1706052036.220764]: 0.0
python ParamPruning/classifier.weight [567 - 1706052036.2208333]: 0.0
python DistillationModifier [574 - 1706052086.3412871]: Calling loss_update with:
args: 0.09775948524475098| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.11111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [574 - 1706052087.4488761]: 
Returned: 0.04561030492186546| 

python LearningRateFunctionModifier [574 - 1706052090.3224156]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.11111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [574 - 1706052090.322598]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [574 - 1706052090.3226466]: 4.4871794871794874e-05
python LearningRateFunctionModifier/ParamGroup1 [574 - 1706052090.3226793]: 4.4871794871794874e-05
python DistillationModifier/task_loss [574 - 1706052090.322723]: tensor(0.0978, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [574 - 1706052090.3231947]: tensor(0.0456, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [574 - 1706052090.3234293]: tensor(0.0456, grad_fn=<AddBackward0>)
python ConstantPruningModifier [574 - 1706052090.3236763]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.11111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [574 - 1706052090.5341907]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [574 - 1706052090.534728]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [574 - 1706052090.535019]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [574 - 1706052090.5352597]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [574 - 1706052090.5354304]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [574 - 1706052090.5361142]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [574 - 1706052090.5367794]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [574 - 1706052090.537011]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [574 - 1706052090.537225]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [574 - 1706052090.5373867]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [574 - 1706052090.5376244]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [574 - 1706052090.5381372]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [574 - 1706052090.5388343]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [574 - 1706052090.5390587]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [574 - 1706052090.5392115]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [574 - 1706052090.5394216]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [574 - 1706052090.5395977]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [574 - 1706052090.54011]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [574 - 1706052090.5407345]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [574 - 1706052090.5409658]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [574 - 1706052090.5411632]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [574 - 1706052090.5413177]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [574 - 1706052090.5414867]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [574 - 1706052090.5420213]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [574 - 1706052090.5425148]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [574 - 1706052090.5427475]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [574 - 1706052090.5429466]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [574 - 1706052090.543132]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [574 - 1706052090.5432744]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [574 - 1706052090.5436897]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [574 - 1706052090.5442197]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [574 - 1706052090.5444112]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [574 - 1706052090.5445776]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [574 - 1706052090.5447931]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [574 - 1706052090.544978]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [574 - 1706052090.545376]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [574 - 1706052090.5458825]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [574 - 1706052090.546086]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [574 - 1706052090.5462782]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [574 - 1706052090.546478]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [574 - 1706052090.5467386]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [574 - 1706052090.5473068]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [574 - 1706052090.547844]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [574 - 1706052090.5480318]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [574 - 1706052090.5481682]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [574 - 1706052090.5483568]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [574 - 1706052090.548586]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [574 - 1706052090.5491014]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [574 - 1706052090.5495422]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [574 - 1706052090.549771]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [574 - 1706052090.549955]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [574 - 1706052090.5501132]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [574 - 1706052090.5502658]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [574 - 1706052090.5507405]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [574 - 1706052090.5512614]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [574 - 1706052090.5514493]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [574 - 1706052090.5516071]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [574 - 1706052090.5518212]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [574 - 1706052090.5520113]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [574 - 1706052090.552495]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [574 - 1706052090.6169987]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [574 - 1706052090.61723]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [574 - 1706052090.6174335]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [574 - 1706052090.6176434]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [574 - 1706052090.6178083]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [574 - 1706052090.6182628]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [574 - 1706052090.6187694]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [574 - 1706052090.618971]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [574 - 1706052090.6191607]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [574 - 1706052090.6193116]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [574 - 1706052090.619467]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [574 - 1706052090.6199036]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [574 - 1706052090.6204443]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [574 - 1706052090.6207228]: 0.0
python ParamPruning/classifier.weight [574 - 1706052090.6208022]: 0.0
python DistillationModifier [581 - 1706052137.1342144]: Calling loss_update with:
args: 0.0978323370218277| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.222222222222221| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [581 - 1706052138.2392547]: 
Returned: 0.17105314135551453| 

python LearningRateFunctionModifier [581 - 1706052140.7643492]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.222222222222221| steps_per_epoch: 63| 
python LearningRateFunctionModifier [581 - 1706052140.7644985]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [581 - 1706052140.764555]: 4.3589743589743604e-05
python LearningRateFunctionModifier/ParamGroup1 [581 - 1706052140.764607]: 4.3589743589743604e-05
python DistillationModifier/task_loss [581 - 1706052140.7646663]: tensor(0.0978, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [581 - 1706052140.7651646]: tensor(0.1711, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [581 - 1706052140.7654176]: tensor(0.1711, grad_fn=<AddBackward0>)
python ConstantPruningModifier [581 - 1706052140.7656677]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.222222222222221| steps_per_epoch: 63| 
python ConstantPruningModifier [581 - 1706052140.9412947]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [581 - 1706052140.9418304]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [581 - 1706052140.9420702]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [581 - 1706052140.9422565]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [581 - 1706052140.9424288]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [581 - 1706052140.9430323]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [581 - 1706052140.9434829]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [581 - 1706052140.9437082]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [581 - 1706052140.9438512]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [581 - 1706052140.9439833]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [581 - 1706052140.9441156]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [581 - 1706052140.944623]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [581 - 1706052140.945058]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [581 - 1706052140.945257]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [581 - 1706052140.9454145]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [581 - 1706052140.945597]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [581 - 1706052140.945777]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [581 - 1706052140.9462504]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [581 - 1706052140.9469922]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [581 - 1706052140.9472163]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [581 - 1706052140.9473648]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [581 - 1706052140.9475193]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [581 - 1706052140.9477158]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [581 - 1706052140.9482536]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [581 - 1706052140.9488945]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [581 - 1706052140.9491343]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [581 - 1706052140.9493294]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [581 - 1706052140.9494925]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [581 - 1706052140.9496758]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [581 - 1706052140.950137]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [581 - 1706052140.950592]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [581 - 1706052140.9507968]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [581 - 1706052140.9509358]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [581 - 1706052140.9510689]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [581 - 1706052140.9511998]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [581 - 1706052140.9517186]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [581 - 1706052140.9521418]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [581 - 1706052140.952328]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [581 - 1706052140.9524727]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [581 - 1706052141.016666]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [581 - 1706052141.0168893]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [581 - 1706052141.017454]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [581 - 1706052141.0180745]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [581 - 1706052141.0182734]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [581 - 1706052141.0184264]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [581 - 1706052141.0186167]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [581 - 1706052141.0187747]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [581 - 1706052141.0192666]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [581 - 1706052141.0197484]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [581 - 1706052141.019937]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [581 - 1706052141.02009]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [581 - 1706052141.0202467]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [581 - 1706052141.0203993]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [581 - 1706052141.0208716]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [581 - 1706052141.0214448]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [581 - 1706052141.0216653]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [581 - 1706052141.021826]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [581 - 1706052141.0219772]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [581 - 1706052141.022128]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [581 - 1706052141.022624]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [581 - 1706052141.0231557]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [581 - 1706052141.0233579]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [581 - 1706052141.0235784]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [581 - 1706052141.0237896]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [581 - 1706052141.023986]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [581 - 1706052141.024478]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [581 - 1706052141.0250647]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [581 - 1706052141.025281]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [581 - 1706052141.0254228]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [581 - 1706052141.0255587]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [581 - 1706052141.0257354]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [581 - 1706052141.0261214]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [581 - 1706052141.0265374]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [581 - 1706052141.0267563]: 0.0
python ParamPruning/classifier.weight [581 - 1706052141.0268302]: 0.0
python DistillationModifier [588 - 1706052185.5371575]: Calling loss_update with:
args: 0.3074958622455597| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.333333333333334| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [588 - 1706052186.5456038]: 
Returned: 0.17518913745880127| 

python LearningRateFunctionModifier [588 - 1706052189.5824373]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.333333333333334| steps_per_epoch: 63| 
python LearningRateFunctionModifier [588 - 1706052189.5826275]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [588 - 1706052189.5826845]: 4.230769230769231e-05
python LearningRateFunctionModifier/ParamGroup1 [588 - 1706052189.5827212]: 4.230769230769231e-05
python DistillationModifier/task_loss [588 - 1706052189.5827718]: tensor(0.3075, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [588 - 1706052189.5833075]: tensor(0.1752, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [588 - 1706052189.5835552]: tensor(0.1752, grad_fn=<AddBackward0>)
python ConstantPruningModifier [588 - 1706052189.5838492]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.333333333333334| steps_per_epoch: 63| 
python ConstantPruningModifier [588 - 1706052189.927063]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [588 - 1706052189.9276938]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [588 - 1706052189.9280477]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [588 - 1706052189.9283578]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [588 - 1706052189.928624]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [588 - 1706052189.929446]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [588 - 1706052189.9300926]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [588 - 1706052189.930365]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [588 - 1706052189.9306011]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [588 - 1706052189.9308887]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [588 - 1706052189.9311597]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [588 - 1706052189.9317596]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [588 - 1706052189.9323664]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [588 - 1706052189.9326696]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [588 - 1706052189.9329672]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [588 - 1706052189.9332378]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [588 - 1706052189.9334707]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [588 - 1706052189.9340715]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [588 - 1706052189.9346743]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [588 - 1706052189.934924]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [588 - 1706052189.9351318]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [588 - 1706052189.935339]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [588 - 1706052189.9355452]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [588 - 1706052189.936144]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [588 - 1706052189.9367554]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [588 - 1706052189.9369948]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [588 - 1706052189.9372606]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [588 - 1706052189.9374802]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [588 - 1706052189.9377277]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [588 - 1706052189.9383018]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [588 - 1706052189.9389045]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [588 - 1706052189.9391308]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [588 - 1706052189.9393942]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [588 - 1706052189.9396288]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [588 - 1706052189.9398472]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [588 - 1706052189.9404285]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [588 - 1706052189.9410422]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [588 - 1706052189.9412704]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [588 - 1706052189.9415205]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [588 - 1706052189.9417586]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [588 - 1706052189.9419656]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [588 - 1706052189.942536]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [588 - 1706052189.9431386]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [588 - 1706052189.9433792]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [588 - 1706052189.9436138]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [588 - 1706052189.943881]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [588 - 1706052189.944094]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [588 - 1706052189.9447134]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [588 - 1706052189.9452903]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [588 - 1706052189.9455376]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [588 - 1706052189.945838]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [588 - 1706052189.9460557]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [588 - 1706052189.9462843]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [588 - 1706052189.9468927]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [588 - 1706052189.9474738]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [588 - 1706052189.9477262]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [588 - 1706052189.9479656]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [588 - 1706052189.9481797]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [588 - 1706052189.948392]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [588 - 1706052189.9490016]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [588 - 1706052189.949595]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [588 - 1706052189.9498353]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [588 - 1706052189.9500394]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [588 - 1706052189.9502442]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [588 - 1706052189.9504454]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [588 - 1706052189.951044]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [588 - 1706052189.9516454]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [588 - 1706052189.951873]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [588 - 1706052189.9521096]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [588 - 1706052189.95232]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [588 - 1706052190.016551]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [588 - 1706052190.0172145]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [588 - 1706052190.0178106]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [588 - 1706052190.018056]: 0.0
python ParamPruning/classifier.weight [588 - 1706052190.018159]: 0.0
python DistillationModifier [595 - 1706052236.939374]: Calling loss_update with:
args: 0.042387764900922775| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.444444444444445| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [595 - 1706052238.646719]: 
Returned: 0.09170400351285934| 

python LearningRateFunctionModifier [595 - 1706052242.1837692]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.444444444444445| steps_per_epoch: 63| 
python LearningRateFunctionModifier [595 - 1706052242.1839235]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [595 - 1706052242.2165365]: 4.1025641025641023e-05
python LearningRateFunctionModifier/ParamGroup1 [595 - 1706052242.216587]: 4.1025641025641023e-05
python DistillationModifier/task_loss [595 - 1706052242.2166321]: tensor(0.0424, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [595 - 1706052242.217113]: tensor(0.0917, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [595 - 1706052242.2173767]: tensor(0.0917, grad_fn=<AddBackward0>)
python ConstantPruningModifier [595 - 1706052242.21764]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.444444444444445| steps_per_epoch: 63| 
python ConstantPruningModifier [595 - 1706052242.5257652]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [595 - 1706052242.5263016]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [595 - 1706052242.5265248]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [595 - 1706052242.5267448]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [595 - 1706052242.526928]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [595 - 1706052242.527737]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [595 - 1706052242.5283182]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [595 - 1706052242.528594]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [595 - 1706052242.528781]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [595 - 1706052242.5289493]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [595 - 1706052242.5291185]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [595 - 1706052242.529887]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [595 - 1706052242.5306718]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [595 - 1706052242.530905]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [595 - 1706052242.531072]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [595 - 1706052242.5312371]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [595 - 1706052242.5314016]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [595 - 1706052242.53207]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [595 - 1706052242.5326521]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [595 - 1706052242.5328743]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [595 - 1706052242.5330422]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [595 - 1706052242.5332084]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [595 - 1706052242.5333717]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [595 - 1706052242.5339081]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [595 - 1706052242.534442]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [595 - 1706052242.534674]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [595 - 1706052242.5348492]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [595 - 1706052242.535013]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [595 - 1706052242.5351753]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [595 - 1706052242.5357225]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [595 - 1706052242.5362587]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [595 - 1706052242.536465]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [595 - 1706052242.5366697]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [595 - 1706052242.5368612]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [595 - 1706052242.5370262]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [595 - 1706052242.537544]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [595 - 1706052242.5381005]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [595 - 1706052242.5383034]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [595 - 1706052242.538503]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [595 - 1706052242.5386853]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [595 - 1706052242.538856]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [595 - 1706052242.5395877]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [595 - 1706052242.5401604]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [595 - 1706052242.540358]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [595 - 1706052242.540537]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [595 - 1706052242.5407367]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [595 - 1706052242.5409026]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [595 - 1706052242.5414195]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [595 - 1706052242.5419734]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [595 - 1706052242.5421774]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [595 - 1706052242.5423453]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [595 - 1706052242.5425105]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [595 - 1706052242.5426955]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [595 - 1706052242.5433586]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [595 - 1706052242.5439386]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [595 - 1706052242.5441413]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [595 - 1706052242.544307]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [595 - 1706052242.5444713]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [595 - 1706052242.544677]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [595 - 1706052242.5452108]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [595 - 1706052242.5459635]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [595 - 1706052242.5461714]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [595 - 1706052242.5463383]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [595 - 1706052242.5465038]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [595 - 1706052242.5466905]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [595 - 1706052242.5472376]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [595 - 1706052242.5480127]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [595 - 1706052242.548217]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [595 - 1706052242.5483823]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [595 - 1706052242.548597]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [595 - 1706052242.548788]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [595 - 1706052242.5493667]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [595 - 1706052242.5501716]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [595 - 1706052242.5503867]: 0.0
python ParamPruning/classifier.weight [595 - 1706052242.5504603]: 0.0
python DistillationModifier [602 - 1706052291.424024]: Calling loss_update with:
args: 0.07008206099271774| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.555555555555555| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [602 - 1706052292.6307178]: 
Returned: 0.1337657868862152| 

python LearningRateFunctionModifier [602 - 1706052296.1176744]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.555555555555555| steps_per_epoch: 63| 
python LearningRateFunctionModifier [602 - 1706052296.1178396]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [602 - 1706052296.117887]: 3.974358974358975e-05
python LearningRateFunctionModifier/ParamGroup1 [602 - 1706052296.117918]: 3.974358974358975e-05
python DistillationModifier/task_loss [602 - 1706052296.1179605]: tensor(0.0701, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [602 - 1706052296.1184316]: tensor(0.1338, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [602 - 1706052296.1187027]: tensor(0.1338, grad_fn=<AddBackward0>)
python ConstantPruningModifier [602 - 1706052296.118948]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.555555555555555| steps_per_epoch: 63| 
python ConstantPruningModifier [602 - 1706052296.425914]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [602 - 1706052296.4264112]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [602 - 1706052296.4267213]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [602 - 1706052296.4269881]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [602 - 1706052296.4272165]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [602 - 1706052296.4280303]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [602 - 1706052296.428643]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [602 - 1706052296.4288971]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [602 - 1706052296.4291143]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [602 - 1706052296.4292905]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [602 - 1706052296.429518]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [602 - 1706052296.4300654]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [602 - 1706052296.4306228]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [602 - 1706052296.4308255]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [602 - 1706052296.4310327]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [602 - 1706052296.431207]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [602 - 1706052296.4314692]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [602 - 1706052296.4320068]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [602 - 1706052296.4325478]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [602 - 1706052296.432772]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [602 - 1706052296.4329808]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [602 - 1706052296.4331536]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [602 - 1706052296.433333]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [602 - 1706052296.4338694]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [602 - 1706052296.4343972]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [602 - 1706052296.4346035]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [602 - 1706052296.4348226]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [602 - 1706052296.4349964]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [602 - 1706052296.4351902]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [602 - 1706052296.4357285]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [602 - 1706052296.4362555]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [602 - 1706052296.4364433]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [602 - 1706052296.43666]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [602 - 1706052296.4368398]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [602 - 1706052296.437015]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [602 - 1706052296.437529]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [602 - 1706052296.4380753]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [602 - 1706052296.4382648]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [602 - 1706052296.4384768]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [602 - 1706052296.4386642]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [602 - 1706052296.438842]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [602 - 1706052296.43936]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [602 - 1706052296.4399025]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [602 - 1706052296.4400907]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [602 - 1706052296.440254]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [602 - 1706052296.4404414]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [602 - 1706052296.4406843]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [602 - 1706052296.441199]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [602 - 1706052296.441744]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [602 - 1706052296.441942]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [602 - 1706052296.4421277]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [602 - 1706052296.4423504]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [602 - 1706052296.4425387]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [602 - 1706052296.4430811]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [602 - 1706052296.4436202]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [602 - 1706052296.4438148]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [602 - 1706052296.443993]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [602 - 1706052296.4441683]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [602 - 1706052296.4443796]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [602 - 1706052296.4449298]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [602 - 1706052296.4454577]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [602 - 1706052296.445665]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [602 - 1706052296.445869]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [602 - 1706052296.4460404]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [602 - 1706052296.4462163]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [602 - 1706052296.4467528]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [602 - 1706052296.4472847]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [602 - 1706052296.447474]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [602 - 1706052296.447679]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [602 - 1706052296.447895]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [602 - 1706052296.448074]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [602 - 1706052296.4486213]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [602 - 1706052296.4491503]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [602 - 1706052296.449344]: 0.0
python ParamPruning/classifier.weight [602 - 1706052296.449415]: 0.0
python DistillationModifier [609 - 1706052348.0472064]: Calling loss_update with:
args: 0.015334182418882847| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.666666666666666| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [609 - 1706052349.729083]: 
Returned: 0.02336098998785019| 

python LearningRateFunctionModifier [609 - 1706052353.0666478]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.666666666666666| steps_per_epoch: 63| 
python LearningRateFunctionModifier [609 - 1706052353.066827]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [609 - 1706052353.0673337]: 3.846153846153847e-05
python LearningRateFunctionModifier/ParamGroup1 [609 - 1706052353.067371]: 3.846153846153847e-05
python DistillationModifier/task_loss [609 - 1706052353.0674202]: tensor(0.0153, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [609 - 1706052353.067993]: tensor(0.0234, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [609 - 1706052353.068248]: tensor(0.0234, grad_fn=<AddBackward0>)
python ConstantPruningModifier [609 - 1706052353.0685039]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.666666666666666| steps_per_epoch: 63| 
python ConstantPruningModifier [609 - 1706052353.351748]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [609 - 1706052353.3523426]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [609 - 1706052353.4167295]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [609 - 1706052353.4170268]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [609 - 1706052353.4173138]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [609 - 1706052353.418121]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [609 - 1706052353.418956]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [609 - 1706052353.419245]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [609 - 1706052353.4194784]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [609 - 1706052353.419774]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [609 - 1706052353.4200106]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [609 - 1706052353.4207072]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [609 - 1706052353.4213662]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [609 - 1706052353.4216847]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [609 - 1706052353.4219398]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [609 - 1706052353.4221647]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [609 - 1706052353.4223716]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [609 - 1706052353.4229846]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [609 - 1706052353.423556]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [609 - 1706052353.423821]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [609 - 1706052353.4240625]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [609 - 1706052353.42429]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [609 - 1706052353.4245393]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [609 - 1706052353.42514]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [609 - 1706052353.4258475]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [609 - 1706052353.4260812]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [609 - 1706052353.4263542]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [609 - 1706052353.426561]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [609 - 1706052353.4268324]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [609 - 1706052353.427482]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [609 - 1706052353.4281301]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [609 - 1706052353.4283633]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [609 - 1706052353.4286416]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [609 - 1706052353.4288492]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [609 - 1706052353.4291136]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [609 - 1706052353.4298272]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [609 - 1706052353.4304402]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [609 - 1706052353.4307086]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [609 - 1706052353.4309177]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [609 - 1706052353.4311635]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [609 - 1706052353.4313786]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [609 - 1706052353.4320521]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [609 - 1706052353.4326618]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [609 - 1706052353.432934]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [609 - 1706052353.433143]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [609 - 1706052353.433341]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [609 - 1706052353.4335988]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [609 - 1706052353.4342127]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [609 - 1706052353.4349413]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [609 - 1706052353.4351833]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [609 - 1706052353.4354281]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [609 - 1706052353.4356592]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [609 - 1706052353.4359455]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [609 - 1706052353.4366286]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [609 - 1706052353.437278]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [609 - 1706052353.4375257]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [609 - 1706052353.4377506]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [609 - 1706052353.4379969]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [609 - 1706052353.4382558]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [609 - 1706052353.438895]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [609 - 1706052353.4395]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [609 - 1706052353.4397945]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [609 - 1706052353.440038]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [609 - 1706052353.4402375]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [609 - 1706052353.4405098]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [609 - 1706052353.441173]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [609 - 1706052353.4418101]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [609 - 1706052353.4420712]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [609 - 1706052353.442281]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [609 - 1706052353.4424899]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [609 - 1706052353.4427185]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [609 - 1706052353.4434016]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [609 - 1706052353.4440067]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [609 - 1706052353.4442484]: 0.0
python ParamPruning/classifier.weight [609 - 1706052353.4443438]: 0.0
python DistillationModifier [616 - 1706052403.3371136]: Calling loss_update with:
args: 0.04343019053339958| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.777777777777779| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [616 - 1706052404.5193672]: 
Returned: 0.15508930385112762| 

python LearningRateFunctionModifier [616 - 1706052406.7790394]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.777777777777779| steps_per_epoch: 63| 
python LearningRateFunctionModifier [616 - 1706052406.7792034]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [616 - 1706052406.7792513]: 3.717948717948716e-05
python LearningRateFunctionModifier/ParamGroup1 [616 - 1706052406.7792835]: 3.717948717948716e-05
python DistillationModifier/task_loss [616 - 1706052406.779325]: tensor(0.0434, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [616 - 1706052406.779817]: tensor(0.1551, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [616 - 1706052406.7800674]: tensor(0.1551, grad_fn=<AddBackward0>)
python ConstantPruningModifier [616 - 1706052406.780294]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.777777777777779| steps_per_epoch: 63| 
python ConstantPruningModifier [616 - 1706052407.0192938]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [616 - 1706052407.0197794]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [616 - 1706052407.020058]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [616 - 1706052407.0202286]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [616 - 1706052407.0203967]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [616 - 1706052407.021016]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [616 - 1706052407.0216606]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [616 - 1706052407.0218759]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [616 - 1706052407.0220723]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [616 - 1706052407.022278]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [616 - 1706052407.022423]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [616 - 1706052407.0229366]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [616 - 1706052407.023403]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [616 - 1706052407.023615]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [616 - 1706052407.0238256]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [616 - 1706052407.023984]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [616 - 1706052407.024142]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [616 - 1706052407.024585]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [616 - 1706052407.0250626]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [616 - 1706052407.0252602]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [616 - 1706052407.025444]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [616 - 1706052407.0256057]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [616 - 1706052407.0257697]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [616 - 1706052407.026196]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [616 - 1706052407.0265584]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [616 - 1706052407.0267656]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [616 - 1706052407.02696]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [616 - 1706052407.0271037]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [616 - 1706052407.0272877]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [616 - 1706052407.0277689]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [616 - 1706052407.0282197]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [616 - 1706052407.0284023]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [616 - 1706052407.0286329]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [616 - 1706052407.0288355]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [616 - 1706052407.029019]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [616 - 1706052407.0294945]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [616 - 1706052407.029948]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [616 - 1706052407.0301423]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [616 - 1706052407.030335]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [616 - 1706052407.030478]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [616 - 1706052407.030655]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [616 - 1706052407.0311408]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [616 - 1706052407.031589]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [616 - 1706052407.0317698]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [616 - 1706052407.031952]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [616 - 1706052407.0320976]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [616 - 1706052407.032246]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [616 - 1706052407.0327382]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [616 - 1706052407.0331302]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [616 - 1706052407.0333161]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [616 - 1706052407.0335004]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [616 - 1706052407.0336685]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [616 - 1706052407.0338583]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [616 - 1706052407.0342286]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [616 - 1706052407.0347474]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [616 - 1706052407.0349317]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [616 - 1706052407.0351114]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [616 - 1706052407.0352523]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [616 - 1706052407.0354378]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [616 - 1706052407.0358486]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [616 - 1706052407.036281]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [616 - 1706052407.0364559]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [616 - 1706052407.036683]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [616 - 1706052407.036871]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [616 - 1706052407.0370624]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [616 - 1706052407.0374544]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [616 - 1706052407.0378757]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [616 - 1706052407.0380588]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [616 - 1706052407.0382318]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [616 - 1706052407.0383642]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [616 - 1706052407.0385137]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [616 - 1706052407.0389128]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [616 - 1706052407.0393827]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [616 - 1706052407.039556]: 0.0
python ParamPruning/classifier.weight [616 - 1706052407.039652]: 0.0
python DistillationModifier [623 - 1706052453.9189112]: Calling loss_update with:
args: 0.204464390873909| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.88888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [623 - 1706052454.925902]: 
Returned: 0.2087721973657608| 

python LearningRateFunctionModifier [623 - 1706052457.2593296]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.88888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [623 - 1706052457.2594771]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [623 - 1706052457.2595253]: 3.589743589743589e-05
python LearningRateFunctionModifier/ParamGroup1 [623 - 1706052457.2595565]: 3.589743589743589e-05
python DistillationModifier/task_loss [623 - 1706052457.2596369]: tensor(0.2045, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [623 - 1706052457.2601225]: tensor(0.2088, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [623 - 1706052457.2603636]: tensor(0.2088, grad_fn=<AddBackward0>)
python ConstantPruningModifier [623 - 1706052457.2606187]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.88888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [623 - 1706052457.435129]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [623 - 1706052457.435663]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [623 - 1706052457.435952]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [623 - 1706052457.4361367]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [623 - 1706052457.4363396]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [623 - 1706052457.4369578]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [623 - 1706052457.4376013]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [623 - 1706052457.4378319]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [623 - 1706052457.4380243]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [623 - 1706052457.4381795]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [623 - 1706052457.4383667]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [623 - 1706052457.4388697]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [623 - 1706052457.4394617]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [623 - 1706052457.4396913]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [623 - 1706052457.4398923]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [623 - 1706052457.4400437]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [623 - 1706052457.4401853]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [623 - 1706052457.440674]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [623 - 1706052457.4410908]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [623 - 1706052457.44128]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [623 - 1706052457.4414186]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [623 - 1706052457.4415493]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [623 - 1706052457.4417222]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [623 - 1706052457.442091]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [623 - 1706052457.4425995]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [623 - 1706052457.4428074]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [623 - 1706052457.4430091]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [623 - 1706052457.4431667]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [623 - 1706052457.4433324]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [623 - 1706052457.4437294]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [623 - 1706052457.4442039]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [623 - 1706052457.4443965]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [623 - 1706052457.4446092]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [623 - 1706052457.4447787]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [623 - 1706052457.4449341]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [623 - 1706052457.4453354]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [623 - 1706052457.44577]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [623 - 1706052457.4459846]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [623 - 1706052457.4461725]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [623 - 1706052457.4463227]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [623 - 1706052457.446513]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [623 - 1706052457.4469268]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [623 - 1706052457.4473934]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [623 - 1706052457.4476042]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [623 - 1706052457.4477603]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [623 - 1706052457.4478996]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [623 - 1706052457.4480317]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [623 - 1706052457.4484255]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [623 - 1706052457.4488356]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [623 - 1706052457.4490256]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [623 - 1706052457.4492075]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [623 - 1706052457.4493992]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [623 - 1706052457.4495566]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [623 - 1706052457.4499567]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [623 - 1706052457.4504254]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [623 - 1706052457.4506376]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [623 - 1706052457.450831]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [623 - 1706052457.4510212]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [623 - 1706052457.4511743]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [623 - 1706052457.451554]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [623 - 1706052457.4520123]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [623 - 1706052457.4522142]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [623 - 1706052457.4524126]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [623 - 1706052457.5165966]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [623 - 1706052457.5168412]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [623 - 1706052457.5172982]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [623 - 1706052457.517901]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [623 - 1706052457.5181065]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [623 - 1706052457.5182915]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [623 - 1706052457.5184538]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [623 - 1706052457.5186114]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [623 - 1706052457.519048]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [623 - 1706052457.5194364]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [623 - 1706052457.51966]: 0.0
python ParamPruning/classifier.weight [623 - 1706052457.519737]: 0.0
python DistillationModifier [630 - 1706052503.2200253]: Calling loss_update with:
args: 0.6554675102233887| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052504.74123]: 
Returned: 1.5053131580352783| 

python DistillationModifier [630 - 1706052507.4351792]: Calling loss_update with:
args: 1.1303843259811401| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052508.4462512]: 
Returned: 1.8485349416732788| 

python DistillationModifier [630 - 1706052510.5223072]: Calling loss_update with:
args: 1.3541587591171265| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052511.6404552]: 
Returned: 1.6025620698928833| 

python DistillationModifier [630 - 1706052513.7227097]: Calling loss_update with:
args: 0.5692947506904602| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052515.2270133]: 
Returned: 0.8727071285247803| 

python DistillationModifier [630 - 1706052517.630906]: Calling loss_update with:
args: 0.7048044204711914| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052518.717463]: 
Returned: 1.2120641469955444| 

python DistillationModifier [630 - 1706052521.04581]: Calling loss_update with:
args: 0.7356138229370117| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052522.0524533]: 
Returned: 1.4009323120117188| 

python DistillationModifier [630 - 1706052525.2343314]: Calling loss_update with:
args: 0.36398327350616455| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052526.2442236]: 
Returned: 0.8633726835250854| 

python DistillationModifier [630 - 1706052528.4243233]: Calling loss_update with:
args: 1.2986981868743896| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052529.434185]: 
Returned: 1.3877334594726562| 

python DistillationModifier [630 - 1706052531.4473326]: Calling loss_update with:
args: 0.6359731554985046| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052532.4518209]: 
Returned: 0.4845926761627197| 

python DistillationModifier [630 - 1706052535.2236824]: Calling loss_update with:
args: 0.8504034280776978| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052536.7511969]: 
Returned: 1.39670729637146| 

python DistillationModifier [630 - 1706052539.937188]: Calling loss_update with:
args: 0.6440851092338562| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052541.0284123]: 
Returned: 0.9654209613800049| 

python DistillationModifier [630 - 1706052543.124908]: Calling loss_update with:
args: 1.067272663116455| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052544.2242103]: 
Returned: 1.8061443567276| 

python DistillationModifier [630 - 1706052546.2497337]: Calling loss_update with:
args: 0.8517629504203796| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052547.2517025]: 
Returned: 1.768997073173523| 

python DistillationModifier [630 - 1706052549.7421744]: Calling loss_update with:
args: 0.6201205849647522| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052551.3337388]: 
Returned: 0.87888503074646| 

python DistillationModifier [630 - 1706052553.7208123]: Calling loss_update with:
args: 1.1479393243789673| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052554.7329843]: 
Returned: 2.0335373878479004| 

python DistillationModifier [630 - 1706052556.7437956]: Calling loss_update with:
args: 0.7016791105270386| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052557.83679]: 
Returned: 1.1987862586975098| 

python DistillationModifier [630 - 1706052559.8480465]: Calling loss_update with:
args: 0.5759797096252441| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052560.9233594]: 
Returned: 0.7788594961166382| 

python DistillationModifier [630 - 1706052562.9189563]: Calling loss_update with:
args: 1.0238022804260254| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052563.939354]: 
Returned: 1.7819335460662842| 

python DistillationModifier [630 - 1706052567.1497679]: Calling loss_update with:
args: 0.8241884708404541| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052568.22997]: 
Returned: 0.9768694043159485| 

python DistillationModifier [630 - 1706052570.8458748]: Calling loss_update with:
args: 0.7059651017189026| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052571.941351]: 
Returned: 1.1584975719451904| 

python DistillationModifier [630 - 1706052573.9396858]: Calling loss_update with:
args: 0.6448454260826111| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052575.0192087]: 
Returned: 1.212113857269287| 

python DistillationModifier [630 - 1706052577.0242748]: Calling loss_update with:
args: 0.6338841319084167| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052578.0415146]: 
Returned: 0.8528627157211304| 

python DistillationModifier [630 - 1706052580.9258344]: Calling loss_update with:
args: 0.6501761674880981| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052582.517635]: 
Returned: 0.8033782839775085| 

python DistillationModifier [630 - 1706052584.9287765]: Calling loss_update with:
args: 0.7440323829650879| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052585.938179]: 
Returned: 1.5765650272369385| 

python DistillationModifier [630 - 1706052588.1232533]: Calling loss_update with:
args: 1.0602954626083374| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052589.1402595]: 
Returned: 1.1601537466049194| 

python DistillationModifier [630 - 1706052591.2233822]: Calling loss_update with:
args: 0.47059908509254456| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052592.2274663]: 
Returned: 0.8425458073616028| 

python DistillationModifier [630 - 1706052594.8241763]: Calling loss_update with:
args: 0.7091004848480225| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052596.251672]: 
Returned: 0.880403995513916| 

python DistillationModifier [630 - 1706052598.4279103]: Calling loss_update with:
args: 0.7315935492515564| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052599.4343486]: 
Returned: 1.113946795463562| 

python DistillationModifier [630 - 1706052602.5272152]: Calling loss_update with:
args: 0.5098612904548645| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052603.5362349]: 
Returned: 1.0376704931259155| 

python DistillationModifier [630 - 1706052606.1349437]: Calling loss_update with:
args: 0.6146649122238159| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052607.1396444]: 
Returned: 1.4055304527282715| 

python DistillationModifier [630 - 1706052609.4280336]: Calling loss_update with:
args: 0.8584176898002625| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052610.4368954]: 
Returned: 1.6452149152755737| 

python DistillationModifier [630 - 1706052612.4446342]: Calling loss_update with:
args: 0.8861654996871948| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052613.4506526]: 
Returned: 1.0466992855072021| 

python DistillationModifier [630 - 1706052615.5245125]: Calling loss_update with:
args: 0.9650782942771912| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052616.537874]: 
Returned: 1.6804821491241455| 

python DistillationModifier [630 - 1706052618.6469874]: Calling loss_update with:
args: 1.2401584386825562| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052619.9482157]: 
Returned: 1.7921721935272217| 

python DistillationModifier [630 - 1706052623.242593]: Calling loss_update with:
args: 1.4648044109344482| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052624.4383836]: 
Returned: 1.8722476959228516| 

python DistillationModifier [630 - 1706052627.227666]: Calling loss_update with:
args: 1.2172729969024658| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052628.3386097]: 
Returned: 2.1449790000915527| 

python DistillationModifier [630 - 1706052630.3455443]: Calling loss_update with:
args: 1.1212472915649414| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052631.9183524]: 
Returned: 1.1282494068145752| 

python DistillationModifier [630 - 1706052633.916678]: Calling loss_update with:
args: 0.8668909072875977| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052634.9303477]: 
Returned: 1.2969367504119873| 

python DistillationModifier [630 - 1706052636.9300601]: Calling loss_update with:
args: 0.8767846822738647| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052637.9445784]: 
Returned: 1.6247459650039673| 

python DistillationModifier [630 - 1706052640.838162]: Calling loss_update with:
args: 1.1894629001617432| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052642.149139]: 
Returned: 2.2024099826812744| 

python DistillationModifier [630 - 1706052644.2284677]: Calling loss_update with:
args: 0.5805277824401855| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052645.2384799]: 
Returned: 1.1567946672439575| 

python DistillationModifier [630 - 1706052647.2469559]: Calling loss_update with:
args: 0.6326941251754761| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052648.342086]: 
Returned: 1.4239014387130737| 

python DistillationModifier [630 - 1706052650.4188132]: Calling loss_update with:
args: 0.8511887788772583| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052651.4332063]: 
Returned: 0.8505340218544006| 

python DistillationModifier [630 - 1706052653.5251005]: Calling loss_update with:
args: 0.5871264338493347| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052655.1259432]: 
Returned: 1.3965473175048828| 

python DistillationModifier [630 - 1706052658.428206]: Calling loss_update with:
args: 0.9120559692382812| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052659.518692]: 
Returned: 2.009153127670288| 

python DistillationModifier [630 - 1706052661.633078]: Calling loss_update with:
args: 1.0227793455123901| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052662.6462512]: 
Returned: 1.217055320739746| 

python DistillationModifier [630 - 1706052665.250952]: Calling loss_update with:
args: 0.8063046932220459| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052666.641986]: 
Returned: 1.286667823791504| 

python DistillationModifier [630 - 1706052669.1412888]: Calling loss_update with:
args: 0.2796550691127777| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052670.6343691]: 
Returned: 0.6871852278709412| 

python DistillationModifier [630 - 1706052674.0336697]: Calling loss_update with:
args: 1.0475521087646484| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052675.6349201]: 
Returned: 1.9925034046173096| 

python DistillationModifier [630 - 1706052678.1295292]: Calling loss_update with:
args: 0.75191730260849| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052679.5546298]: 
Returned: 1.4348137378692627| 

python DistillationModifier [630 - 1706052682.9410222]: Calling loss_update with:
args: 0.283915251493454| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052684.5216827]: 
Returned: 0.5582436919212341| 

python DistillationModifier [630 - 1706052687.1237283]: Calling loss_update with:
args: 1.7877618074417114| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052688.1357722]: 
Returned: 1.803300142288208| 

python DistillationModifier [630 - 1706052690.3407557]: Calling loss_update with:
args: 0.7563873529434204| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052691.4230921]: 
Returned: 0.986700177192688| 

python DistillationModifier [630 - 1706052693.534286]: Calling loss_update with:
args: 0.8495163917541504| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052694.540016]: 
Returned: 1.300143837928772| 

python DistillationModifier [630 - 1706052696.5422583]: Calling loss_update with:
args: 0.5830488204956055| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052697.9521716]: 
Returned: 1.2860180139541626| 

python DistillationModifier [630 - 1706052700.5425153]: Calling loss_update with:
args: 0.761737585067749| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052701.5490384]: 
Returned: 1.1864027976989746| 

python DistillationModifier [630 - 1706052703.634298]: Calling loss_update with:
args: 0.9100641012191772| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052704.720591]: 
Returned: 0.821125328540802| 

python DistillationModifier [630 - 1706052706.723709]: Calling loss_update with:
args: 0.8270432353019714| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052707.8234887]: 
Returned: 1.0431509017944336| 

python DistillationModifier [630 - 1706052709.8304982]: Calling loss_update with:
args: 1.1101090908050537| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052710.8373792]: 
Returned: 1.3549392223358154| 

python DistillationModifier [630 - 1706052712.8288722]: Calling loss_update with:
args: 0.9016663432121277| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052713.9178205]: 
Returned: 1.6117156744003296| 

python DistillationModifier [630 - 1706052717.1278188]: Calling loss_update with:
args: 0.8633440136909485| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052718.7228427]: 
Returned: 1.4506919384002686| 

python DistillationModifier [630 - 1706052722.0199883]: Calling loss_update with:
args: 0.8712881803512573| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052723.5474029]: 
Returned: 1.801637887954712| 

python DistillationModifier [630 - 1706052725.5411036]: Calling loss_update with:
args: 0.4986943006515503| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052726.426266]: 
Returned: 1.009519338607788| 

python DistillationModifier [630 - 1706052730.347673]: Calling loss_update with:
args: 0.014685498550534248| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1706052731.6482465]: 
Returned: 0.2043902575969696| 

python LearningRateFunctionModifier [630 - 1706052734.432515]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [630 - 1706052734.432693]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [630 - 1706052734.4327397]: 3.4615384615384605e-05
python LearningRateFunctionModifier/ParamGroup1 [630 - 1706052734.4327714]: 3.4615384615384605e-05
python DistillationModifier/task_loss [630 - 1706052734.4328144]: tensor(0.0147, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [630 - 1706052734.4332914]: tensor(0.2044, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [630 - 1706052734.433529]: tensor(0.2044, grad_fn=<AddBackward0>)
python ConstantPruningModifier [630 - 1706052734.433784]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| 
python ConstantPruningModifier [630 - 1706052734.7344036]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [630 - 1706052734.7349153]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [630 - 1706052734.7351332]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [630 - 1706052734.7353146]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [630 - 1706052734.7354858]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [630 - 1706052734.7362726]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [630 - 1706052734.7368853]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [630 - 1706052734.7371206]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [630 - 1706052734.7372866]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [630 - 1706052734.737448]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [630 - 1706052734.7376263]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [630 - 1706052734.7381513]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [630 - 1706052734.7389255]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [630 - 1706052734.7391436]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [630 - 1706052734.7393045]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [630 - 1706052734.7394645]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [630 - 1706052734.7396436]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [630 - 1706052734.74018]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [630 - 1706052734.7408154]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [630 - 1706052734.7410307]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [630 - 1706052734.741228]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [630 - 1706052734.7413902]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [630 - 1706052734.7415507]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [630 - 1706052734.7421103]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [630 - 1706052734.742714]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [630 - 1706052734.7429178]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [630 - 1706052734.743082]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [630 - 1706052734.7432396]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [630 - 1706052734.7433965]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [630 - 1706052734.7439482]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [630 - 1706052734.74452]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [630 - 1706052734.7447562]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [630 - 1706052734.7449224]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [630 - 1706052734.7450814]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [630 - 1706052734.745239]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [630 - 1706052734.745784]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [630 - 1706052734.7465508]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [630 - 1706052734.7467904]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [630 - 1706052734.7469568]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [630 - 1706052734.7471166]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [630 - 1706052734.7472744]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [630 - 1706052734.7478414]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [630 - 1706052734.7484736]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [630 - 1706052734.748727]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [630 - 1706052734.7488666]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [630 - 1706052734.7489963]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [630 - 1706052734.7491293]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [630 - 1706052734.749535]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [630 - 1706052734.7499735]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [630 - 1706052734.7501462]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [630 - 1706052734.7502816]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [630 - 1706052734.750417]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [630 - 1706052734.7505534]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [630 - 1706052734.7510946]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [630 - 1706052734.7514846]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [630 - 1706052734.7516842]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [630 - 1706052734.7518413]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [630 - 1706052734.7519765]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [630 - 1706052734.7521148]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [630 - 1706052734.7524946]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [630 - 1706052734.816954]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [630 - 1706052734.817164]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [630 - 1706052734.8173091]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [630 - 1706052734.8174539]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [630 - 1706052734.8176105]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [630 - 1706052734.8180676]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [630 - 1706052734.8185134]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [630 - 1706052734.818709]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [630 - 1706052734.818849]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [630 - 1706052734.8189816]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [630 - 1706052734.8191133]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [630 - 1706052734.8194802]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [630 - 1706052734.819902]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [630 - 1706052734.8200738]: 0.0
python ParamPruning/classifier.weight [630 - 1706052734.8201408]: 0.0
python DistillationModifier [637 - 1706052782.9468927]: Calling loss_update with:
args: 0.202715203166008| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.11111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [637 - 1706052784.0411968]: 
Returned: 0.11102412641048431| 

python LearningRateFunctionModifier [637 - 1706052786.385017]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.11111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [637 - 1706052786.3851855]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [637 - 1706052786.3852327]: 3.3333333333333335e-05
python LearningRateFunctionModifier/ParamGroup1 [637 - 1706052786.3852634]: 3.3333333333333335e-05
python DistillationModifier/task_loss [637 - 1706052786.3853054]: tensor(0.2027, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [637 - 1706052786.3858056]: tensor(0.1110, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [637 - 1706052786.3860602]: tensor(0.1110, grad_fn=<AddBackward0>)
python ConstantPruningModifier [637 - 1706052786.3863003]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.11111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [637 - 1706052786.6255202]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [637 - 1706052786.6260445]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [637 - 1706052786.6263578]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [637 - 1706052786.6265981]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [637 - 1706052786.6268048]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [637 - 1706052786.6275012]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [637 - 1706052786.6281974]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [637 - 1706052786.6284087]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [637 - 1706052786.6286483]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [637 - 1706052786.628829]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [637 - 1706052786.6289806]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [637 - 1706052786.6294441]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [637 - 1706052786.6300488]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [637 - 1706052786.630249]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [637 - 1706052786.6304393]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [637 - 1706052786.630618]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [637 - 1706052786.6307774]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [637 - 1706052786.6311967]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [637 - 1706052786.6317728]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [637 - 1706052786.6319835]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [637 - 1706052786.6321666]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [637 - 1706052786.6323125]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [637 - 1706052786.6324475]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [637 - 1706052786.6330166]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [637 - 1706052786.6335924]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [637 - 1706052786.6338122]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [637 - 1706052786.634017]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [637 - 1706052786.6341784]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [637 - 1706052786.6343205]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [637 - 1706052786.6347406]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [637 - 1706052786.6351907]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [637 - 1706052786.6353748]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [637 - 1706052786.6355486]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [637 - 1706052786.6357377]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [637 - 1706052786.63589]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [637 - 1706052786.6362844]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [637 - 1706052786.6367373]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [637 - 1706052786.6369293]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [637 - 1706052786.6371012]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [637 - 1706052786.6372702]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [637 - 1706052786.6374116]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [637 - 1706052786.6379774]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [637 - 1706052786.63866]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [637 - 1706052786.638861]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [637 - 1706052786.639056]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [637 - 1706052786.6392243]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [637 - 1706052786.6393723]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [637 - 1706052786.6399705]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [637 - 1706052786.6403828]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [637 - 1706052786.6406114]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [637 - 1706052786.6407664]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [637 - 1706052786.6409557]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [637 - 1706052786.6411026]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [637 - 1706052786.641499]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [637 - 1706052786.642058]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [637 - 1706052786.642266]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [637 - 1706052786.6424496]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [637 - 1706052786.6426303]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [637 - 1706052786.6427937]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [637 - 1706052786.6431863]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [637 - 1706052786.6436946]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [637 - 1706052786.6439013]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [637 - 1706052786.6440885]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [637 - 1706052786.6442306]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [637 - 1706052786.6443865]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [637 - 1706052786.644844]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [637 - 1706052786.6454206]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [637 - 1706052786.6456323]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [637 - 1706052786.6458263]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [637 - 1706052786.6459627]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [637 - 1706052786.6461017]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [637 - 1706052786.646658]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [637 - 1706052786.6471584]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [637 - 1706052786.6473525]: 0.0
python ParamPruning/classifier.weight [637 - 1706052786.6474218]: 0.0
python DistillationModifier [644 - 1706052834.6331189]: Calling loss_update with:
args: 0.1750248521566391| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.222222222222221| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [644 - 1706052835.7375178]: 
Returned: 0.1444588303565979| 

python LearningRateFunctionModifier [644 - 1706052838.0740106]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.222222222222221| steps_per_epoch: 63| 
python LearningRateFunctionModifier [644 - 1706052838.0741782]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [644 - 1706052838.0742269]: 3.205128205128205e-05
python LearningRateFunctionModifier/ParamGroup1 [644 - 1706052838.0742576]: 3.205128205128205e-05
python DistillationModifier/task_loss [644 - 1706052838.0742989]: tensor(0.1750, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [644 - 1706052838.0748076]: tensor(0.1445, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [644 - 1706052838.0750635]: tensor(0.1445, grad_fn=<AddBackward0>)
python ConstantPruningModifier [644 - 1706052838.0752993]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.222222222222221| steps_per_epoch: 63| 
python ConstantPruningModifier [644 - 1706052838.3175583]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [644 - 1706052838.3180828]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [644 - 1706052838.318369]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [644 - 1706052838.3186228]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [644 - 1706052838.318813]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [644 - 1706052838.3193824]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [644 - 1706052838.319885]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [644 - 1706052838.3201013]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [644 - 1706052838.3202493]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [644 - 1706052838.3203907]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [644 - 1706052838.3205314]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [644 - 1706052838.3209524]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [644 - 1706052838.3213308]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [644 - 1706052838.3215194]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [644 - 1706052838.321744]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [644 - 1706052838.321915]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [644 - 1706052838.322079]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [644 - 1706052838.3224542]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [644 - 1706052838.3228617]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [644 - 1706052838.3230565]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [644 - 1706052838.3232307]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [644 - 1706052838.323377]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [644 - 1706052838.3235483]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [644 - 1706052838.324098]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [644 - 1706052838.324704]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [644 - 1706052838.3248944]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [644 - 1706052838.3250437]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [644 - 1706052838.3252337]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [644 - 1706052838.3254266]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [644 - 1706052838.3258944]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [644 - 1706052838.326268]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [644 - 1706052838.3264449]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [644 - 1706052838.3266397]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [644 - 1706052838.326792]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [644 - 1706052838.3269267]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [644 - 1706052838.3273065]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [644 - 1706052838.3276863]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [644 - 1706052838.3278685]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [644 - 1706052838.3280094]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [644 - 1706052838.3281746]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [644 - 1706052838.3283577]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [644 - 1706052838.3287334]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [644 - 1706052838.3291202]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [644 - 1706052838.3293152]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [644 - 1706052838.3295038]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [644 - 1706052838.3296916]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [644 - 1706052838.3298528]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [644 - 1706052838.3302479]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [644 - 1706052838.3306196]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [644 - 1706052838.330809]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [644 - 1706052838.3309693]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [644 - 1706052838.3311124]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [644 - 1706052838.3312647]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [644 - 1706052838.3316462]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [644 - 1706052838.3320084]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [644 - 1706052838.332192]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [644 - 1706052838.3323743]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [644 - 1706052838.3325386]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [644 - 1706052838.3327584]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [644 - 1706052838.33333]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [644 - 1706052838.333862]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [644 - 1706052838.3340592]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [644 - 1706052838.3342235]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [644 - 1706052838.3343728]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [644 - 1706052838.3345416]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [644 - 1706052838.335077]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [644 - 1706052838.3355005]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [644 - 1706052838.3357117]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [644 - 1706052838.335888]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [644 - 1706052838.336024]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [644 - 1706052838.336212]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [644 - 1706052838.3367238]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [644 - 1706052838.3372989]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [644 - 1706052838.3375213]: 0.0
python ParamPruning/classifier.weight [644 - 1706052838.3376205]: 0.0
python DistillationModifier [651 - 1706052886.748007]: Calling loss_update with:
args: 0.12966401875019073| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.333333333333334| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [651 - 1706052887.8432896]: 
Returned: 0.09517624229192734| 

python LearningRateFunctionModifier [651 - 1706052890.3806572]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.333333333333334| steps_per_epoch: 63| 
python LearningRateFunctionModifier [651 - 1706052890.3808243]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [651 - 1706052890.3808722]: 3.0769230769230754e-05
python LearningRateFunctionModifier/ParamGroup1 [651 - 1706052890.3809035]: 3.0769230769230754e-05
python DistillationModifier/task_loss [651 - 1706052890.3809452]: tensor(0.1297, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [651 - 1706052890.381434]: tensor(0.0952, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [651 - 1706052890.3817208]: tensor(0.0952, grad_fn=<AddBackward0>)
python ConstantPruningModifier [651 - 1706052890.3819773]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.333333333333334| steps_per_epoch: 63| 
python ConstantPruningModifier [651 - 1706052890.7233143]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [651 - 1706052890.7238548]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [651 - 1706052890.7241566]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [651 - 1706052890.724422]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [651 - 1706052890.7247274]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [651 - 1706052890.72553]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [651 - 1706052890.7263763]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [651 - 1706052890.726661]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [651 - 1706052890.726898]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [651 - 1706052890.7271383]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [651 - 1706052890.7273204]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [651 - 1706052890.728007]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [651 - 1706052890.7286136]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [651 - 1706052890.728866]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [651 - 1706052890.7290854]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [651 - 1706052890.7293239]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [651 - 1706052890.7296023]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [651 - 1706052890.7302136]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [651 - 1706052890.7309637]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [651 - 1706052890.7312016]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [651 - 1706052890.7314298]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [651 - 1706052890.7316203]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [651 - 1706052890.7318048]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [651 - 1706052890.7324553]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [651 - 1706052890.7332761]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [651 - 1706052890.7335224]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [651 - 1706052890.733782]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [651 - 1706052890.7340288]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [651 - 1706052890.7342236]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [651 - 1706052890.7348528]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [651 - 1706052890.7356524]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [651 - 1706052890.7358952]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [651 - 1706052890.7361114]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [651 - 1706052890.7363095]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [651 - 1706052890.7365088]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [651 - 1706052890.7371972]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [651 - 1706052890.7377868]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [651 - 1706052890.7380188]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [651 - 1706052890.7382226]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [651 - 1706052890.7384162]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [651 - 1706052890.7386718]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [651 - 1706052890.7392612]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [651 - 1706052890.7398267]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [651 - 1706052890.7400477]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [651 - 1706052890.7402499]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [651 - 1706052890.74048]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [651 - 1706052890.7406974]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [651 - 1706052890.7414641]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [651 - 1706052890.7421777]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [651 - 1706052890.742465]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [651 - 1706052890.742753]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [651 - 1706052890.7429845]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [651 - 1706052890.7431934]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [651 - 1706052890.7438107]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [651 - 1706052890.7443628]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [651 - 1706052890.7446172]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [651 - 1706052890.744831]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [651 - 1706052890.7450018]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [651 - 1706052890.7451894]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [651 - 1706052890.745774]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [651 - 1706052890.7465312]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [651 - 1706052890.7467926]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [651 - 1706052890.7470067]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [651 - 1706052890.747176]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [651 - 1706052890.7473714]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [651 - 1706052890.7480526]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [651 - 1706052890.7487264]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [651 - 1706052890.7489579]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [651 - 1706052890.7491658]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [651 - 1706052890.7493908]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [651 - 1706052890.7496018]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [651 - 1706052890.7502508]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [651 - 1706052890.7509353]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [651 - 1706052890.7511744]: 0.0
python ParamPruning/classifier.weight [651 - 1706052890.751248]: 0.0
python DistillationModifier [658 - 1706052942.9547772]: Calling loss_update with:
args: 0.2117024064064026| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.444444444444445| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [658 - 1706052944.5489674]: 
Returned: 0.15687081217765808| 

python LearningRateFunctionModifier [658 - 1706052947.7632618]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.444444444444445| steps_per_epoch: 63| 
python LearningRateFunctionModifier [658 - 1706052947.763403]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [658 - 1706052947.7634506]: 2.9487179487179484e-05
python LearningRateFunctionModifier/ParamGroup1 [658 - 1706052947.763482]: 2.9487179487179484e-05
python DistillationModifier/task_loss [658 - 1706052947.7635255]: tensor(0.2117, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [658 - 1706052947.7640436]: tensor(0.1569, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [658 - 1706052947.7642891]: tensor(0.1569, grad_fn=<AddBackward0>)
python ConstantPruningModifier [658 - 1706052947.7645392]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.444444444444445| steps_per_epoch: 63| 
python ConstantPruningModifier [658 - 1706052947.9396517]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [658 - 1706052947.9401407]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [658 - 1706052947.9404216]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [658 - 1706052947.9406493]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [658 - 1706052947.9408746]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [658 - 1706052947.9414759]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [658 - 1706052947.942118]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [658 - 1706052947.942342]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [658 - 1706052947.9425359]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [658 - 1706052947.9427264]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [658 - 1706052947.942938]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [658 - 1706052947.9434211]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [658 - 1706052947.9439611]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [658 - 1706052947.944173]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [658 - 1706052947.9443665]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [658 - 1706052947.944534]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [658 - 1706052947.9447272]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [658 - 1706052947.9451396]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [658 - 1706052947.9457355]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [658 - 1706052947.9459445]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [658 - 1706052947.9461477]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [658 - 1706052947.9463003]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [658 - 1706052947.946462]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [658 - 1706052947.9469078]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [658 - 1706052947.9473252]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [658 - 1706052947.9475172]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [658 - 1706052947.9477456]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [658 - 1706052947.9478958]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [658 - 1706052947.9480855]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [658 - 1706052947.9486778]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [658 - 1706052947.9490838]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [658 - 1706052947.9492788]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [658 - 1706052947.9494693]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [658 - 1706052947.949638]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [658 - 1706052947.9498403]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [658 - 1706052947.950232]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [658 - 1706052947.9507647]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [658 - 1706052947.9509652]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [658 - 1706052947.951156]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [658 - 1706052947.9513047]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [658 - 1706052947.9514558]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [658 - 1706052947.9518673]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [658 - 1706052947.952266]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [658 - 1706052947.9524522]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [658 - 1706052948.0166702]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [658 - 1706052948.016887]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [658 - 1706052948.0173905]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [658 - 1706052948.0178335]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [658 - 1706052948.0183604]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [658 - 1706052948.018552]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [658 - 1706052948.0187776]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [658 - 1706052948.0189292]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [658 - 1706052948.0191123]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [658 - 1706052948.0198014]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [658 - 1706052948.0203824]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [658 - 1706052948.0206535]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [658 - 1706052948.0208714]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [658 - 1706052948.0210147]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [658 - 1706052948.0211508]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [658 - 1706052948.0216663]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [658 - 1706052948.0221949]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [658 - 1706052948.0223978]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [658 - 1706052948.0226305]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [658 - 1706052948.0227907]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [658 - 1706052948.022941]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [658 - 1706052948.0234237]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [658 - 1706052948.0238822]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [658 - 1706052948.0240982]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [658 - 1706052948.024356]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [658 - 1706052948.0246122]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [658 - 1706052948.0248392]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [658 - 1706052948.025243]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [658 - 1706052948.0258667]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [658 - 1706052948.026084]: 0.0
python ParamPruning/classifier.weight [658 - 1706052948.026156]: 0.0
python DistillationModifier [665 - 1706052997.5446637]: Calling loss_update with:
args: 0.1648518443107605| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.555555555555555| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [665 - 1706052999.2288113]: 
Returned: 0.14484746754169464| 

python LearningRateFunctionModifier [665 - 1706053002.0808144]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.555555555555555| steps_per_epoch: 63| 
python LearningRateFunctionModifier [665 - 1706053002.0809805]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [665 - 1706053002.0810285]: 2.82051282051282e-05
python LearningRateFunctionModifier/ParamGroup1 [665 - 1706053002.0810602]: 2.82051282051282e-05
python DistillationModifier/task_loss [665 - 1706053002.0811026]: tensor(0.1649, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [665 - 1706053002.0815928]: tensor(0.1448, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [665 - 1706053002.0818434]: tensor(0.1448, grad_fn=<AddBackward0>)
python ConstantPruningModifier [665 - 1706053002.082073]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.555555555555555| steps_per_epoch: 63| 
python ConstantPruningModifier [665 - 1706053002.3252227]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [665 - 1706053002.3257227]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [665 - 1706053002.3259966]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [665 - 1706053002.3261688]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [665 - 1706053002.3263795]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [665 - 1706053002.3269649]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [665 - 1706053002.3274024]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [665 - 1706053002.32763]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [665 - 1706053002.3278325]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [665 - 1706053002.3279717]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [665 - 1706053002.3281217]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [665 - 1706053002.3285286]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [665 - 1706053002.3289626]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [665 - 1706053002.3291605]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [665 - 1706053002.3293033]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [665 - 1706053002.3294919]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [665 - 1706053002.3296573]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [665 - 1706053002.3301873]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [665 - 1706053002.3307881]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [665 - 1706053002.3310049]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [665 - 1706053002.3311927]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [665 - 1706053002.3313365]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [665 - 1706053002.3315284]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [665 - 1706053002.3319983]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [665 - 1706053002.3325005]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [665 - 1706053002.3327284]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [665 - 1706053002.3329213]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [665 - 1706053002.333064]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [665 - 1706053002.333202]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [665 - 1706053002.3335905]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [665 - 1706053002.3339927]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [665 - 1706053002.334165]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [665 - 1706053002.3342962]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [665 - 1706053002.3344283]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [665 - 1706053002.3346236]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [665 - 1706053002.3349996]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [665 - 1706053002.3353674]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [665 - 1706053002.335549]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [665 - 1706053002.3357403]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [665 - 1706053002.335881]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [665 - 1706053002.3360605]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [665 - 1706053002.3366055]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [665 - 1706053002.337069]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [665 - 1706053002.3372443]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [665 - 1706053002.3373792]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [665 - 1706053002.3375564]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [665 - 1706053002.3377354]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [665 - 1706053002.3381205]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [665 - 1706053002.3384948]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [665 - 1706053002.3387215]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [665 - 1706053002.3389268]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [665 - 1706053002.3390856]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [665 - 1706053002.339226]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [665 - 1706053002.3396425]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [665 - 1706053002.3399923]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [665 - 1706053002.3401802]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [665 - 1706053002.3403144]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [665 - 1706053002.3404982]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [665 - 1706053002.340699]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [665 - 1706053002.3410785]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [665 - 1706053002.3414295]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [665 - 1706053002.3416255]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [665 - 1706053002.3418078]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [665 - 1706053002.3419583]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [665 - 1706053002.342106]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [665 - 1706053002.3426332]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [665 - 1706053002.3430724]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [665 - 1706053002.343265]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [665 - 1706053002.343445]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [665 - 1706053002.3436055]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [665 - 1706053002.3437712]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [665 - 1706053002.3441415]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [665 - 1706053002.344538]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [665 - 1706053002.344764]: 0.0
python ParamPruning/classifier.weight [665 - 1706053002.344838]: 0.0
python DistillationModifier [672 - 1706053049.7416384]: Calling loss_update with:
args: 0.12911348044872284| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.666666666666666| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [672 - 1706053051.341548]: 
Returned: 0.11268565058708191| 

python LearningRateFunctionModifier [672 - 1706053053.7809985]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.666666666666666| steps_per_epoch: 63| 
python LearningRateFunctionModifier [672 - 1706053053.7811406]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [672 - 1706053053.781187]: 2.6923076923076917e-05
python LearningRateFunctionModifier/ParamGroup1 [672 - 1706053053.781218]: 2.6923076923076917e-05
python DistillationModifier/task_loss [672 - 1706053053.7812595]: tensor(0.1291, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [672 - 1706053053.7817564]: tensor(0.1127, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [672 - 1706053053.781999]: tensor(0.1127, grad_fn=<AddBackward0>)
python ConstantPruningModifier [672 - 1706053053.7822242]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.666666666666666| steps_per_epoch: 63| 
python ConstantPruningModifier [672 - 1706053054.023642]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [672 - 1706053054.0241334]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [672 - 1706053054.024349]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [672 - 1706053054.024519]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [672 - 1706053054.0246966]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [672 - 1706053054.0252717]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [672 - 1706053054.0258758]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [672 - 1706053054.026095]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [672 - 1706053054.0262403]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [672 - 1706053054.0263855]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [672 - 1706053054.0265243]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [672 - 1706053054.0270243]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [672 - 1706053054.0274954]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [672 - 1706053054.027716]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [672 - 1706053054.0278726]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [672 - 1706053054.0280154]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [672 - 1706053054.0281749]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [672 - 1706053054.02865]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [672 - 1706053054.0292146]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [672 - 1706053054.0294154]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [672 - 1706053054.029557]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [672 - 1706053054.0297506]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [672 - 1706053054.029895]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [672 - 1706053054.0302975]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [672 - 1706053054.0307164]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [672 - 1706053054.0309238]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [672 - 1706053054.0310962]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [672 - 1706053054.031267]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [672 - 1706053054.031439]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [672 - 1706053054.031876]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [672 - 1706053054.0324535]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [672 - 1706053054.032699]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [672 - 1706053054.0328877]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [672 - 1706053054.033063]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [672 - 1706053054.033236]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [672 - 1706053054.03373]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [672 - 1706053054.0343618]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [672 - 1706053054.0345972]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [672 - 1706053054.0347817]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [672 - 1706053054.034959]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [672 - 1706053054.03515]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [672 - 1706053054.035698]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [672 - 1706053054.036312]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [672 - 1706053054.0365424]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [672 - 1706053054.0367682]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [672 - 1706053054.036927]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [672 - 1706053054.0370817]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [672 - 1706053054.037619]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [672 - 1706053054.0381007]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [672 - 1706053054.0383067]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [672 - 1706053054.0384562]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [672 - 1706053054.0386286]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [672 - 1706053054.0387814]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [672 - 1706053054.0392044]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [672 - 1706053054.0397675]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [672 - 1706053054.039969]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [672 - 1706053054.0401235]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [672 - 1706053054.0402865]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [672 - 1706053054.0404518]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [672 - 1706053054.0410388]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [672 - 1706053054.0417356]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [672 - 1706053054.0419593]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [672 - 1706053054.0421212]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [672 - 1706053054.042281]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [672 - 1706053054.042437]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [672 - 1706053054.043097]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [672 - 1706053054.0437064]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [672 - 1706053054.0439043]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [672 - 1706053054.044057]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [672 - 1706053054.0442493]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [672 - 1706053054.0444314]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [672 - 1706053054.0449073]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [672 - 1706053054.0454788]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [672 - 1706053054.0457172]: 0.0
python ParamPruning/classifier.weight [672 - 1706053054.0458066]: 0.0
python DistillationModifier [679 - 1706053101.0463474]: Calling loss_update with:
args: 0.04838665574789047| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.777777777777779| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [679 - 1706053102.119759]: 
Returned: 0.0880206972360611| 

python LearningRateFunctionModifier [679 - 1706053104.6642678]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.777777777777779| steps_per_epoch: 63| 
python LearningRateFunctionModifier [679 - 1706053104.6644175]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [679 - 1706053104.6644623]: 2.564102564102562e-05
python LearningRateFunctionModifier/ParamGroup1 [679 - 1706053104.6645036]: 2.564102564102562e-05
python DistillationModifier/task_loss [679 - 1706053104.6645458]: tensor(0.0484, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [679 - 1706053104.665055]: tensor(0.0880, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [679 - 1706053104.6652985]: tensor(0.0880, grad_fn=<AddBackward0>)
python ConstantPruningModifier [679 - 1706053104.6655345]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.777777777777779| steps_per_epoch: 63| 
python ConstantPruningModifier [679 - 1706053104.9411812]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [679 - 1706053104.9417126]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [679 - 1706053104.942035]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [679 - 1706053104.9422357]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [679 - 1706053104.9424105]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [679 - 1706053104.9433212]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [679 - 1706053104.9439695]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [679 - 1706053104.9442093]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [679 - 1706053104.9444647]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [679 - 1706053104.9446914]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [679 - 1706053104.9449167]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [679 - 1706053104.9454782]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [679 - 1706053104.9463015]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [679 - 1706053104.946534]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [679 - 1706053104.9467862]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [679 - 1706053104.9469764]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [679 - 1706053104.9471633]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [679 - 1706053104.9479218]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [679 - 1706053104.9487908]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [679 - 1706053104.949059]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [679 - 1706053104.949274]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [679 - 1706053104.9494588]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [679 - 1706053104.9497137]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [679 - 1706053104.9504933]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [679 - 1706053104.9513333]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [679 - 1706053104.951558]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [679 - 1706053104.951819]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [679 - 1706053104.952058]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [679 - 1706053104.9522736]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [679 - 1706053105.0170522]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [679 - 1706053105.0177014]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [679 - 1706053105.017931]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [679 - 1706053105.018159]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [679 - 1706053105.0184274]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [679 - 1706053105.0186949]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [679 - 1706053105.0192635]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [679 - 1706053105.0200415]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [679 - 1706053105.0202582]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [679 - 1706053105.0204976]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [679 - 1706053105.0207117]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [679 - 1706053105.0209405]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [679 - 1706053105.0215356]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [679 - 1706053105.0222297]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [679 - 1706053105.0224407]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [679 - 1706053105.0227041]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [679 - 1706053105.0228934]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [679 - 1706053105.0230825]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [679 - 1706053105.0237148]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [679 - 1706053105.024516]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [679 - 1706053105.0247571]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [679 - 1706053105.024976]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [679 - 1706053105.0251565]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [679 - 1706053105.0253365]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [679 - 1706053105.0259485]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [679 - 1706053105.0265095]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [679 - 1706053105.0267413]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [679 - 1706053105.0269508]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [679 - 1706053105.0271769]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [679 - 1706053105.0273566]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [679 - 1706053105.027919]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [679 - 1706053105.0286925]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [679 - 1706053105.0289035]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [679 - 1706053105.0291195]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [679 - 1706053105.0293422]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [679 - 1706053105.029527]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [679 - 1706053105.0301793]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [679 - 1706053105.0309713]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [679 - 1706053105.0311906]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [679 - 1706053105.0314088]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [679 - 1706053105.0315962]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [679 - 1706053105.0317972]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [679 - 1706053105.0324004]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [679 - 1706053105.033108]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [679 - 1706053105.033352]: 0.0
python ParamPruning/classifier.weight [679 - 1706053105.033427]: 0.0
python DistillationModifier [686 - 1706053155.3399892]: Calling loss_update with:
args: 0.0507763996720314| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.88888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [686 - 1706053156.7247405]: 
Returned: 0.14697301387786865| 

python LearningRateFunctionModifier [686 - 1706053159.6594613]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.88888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [686 - 1706053159.659645]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [686 - 1706053159.6596963]: 2.435897435897435e-05
python LearningRateFunctionModifier/ParamGroup1 [686 - 1706053159.6597273]: 2.435897435897435e-05
python DistillationModifier/task_loss [686 - 1706053159.6597695]: tensor(0.0508, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [686 - 1706053159.6602545]: tensor(0.1470, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [686 - 1706053159.6604943]: tensor(0.1470, grad_fn=<AddBackward0>)
python ConstantPruningModifier [686 - 1706053159.660764]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.88888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [686 - 1706053159.9400623]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [686 - 1706053159.9406185]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [686 - 1706053159.9409332]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [686 - 1706053159.9411461]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [686 - 1706053159.941341]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [686 - 1706053159.9421268]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [686 - 1706053159.942958]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [686 - 1706053159.943213]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [686 - 1706053159.9434254]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [686 - 1706053159.9436362]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [686 - 1706053159.9438863]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [686 - 1706053159.9445217]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [686 - 1706053159.9452078]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [686 - 1706053159.9454353]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [686 - 1706053159.9456203]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [686 - 1706053159.9457953]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [686 - 1706053159.9459927]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [686 - 1706053159.9465995]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [686 - 1706053159.9471967]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [686 - 1706053159.947401]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [686 - 1706053159.9476547]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [686 - 1706053159.9479084]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [686 - 1706053159.948076]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [686 - 1706053159.9486809]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [686 - 1706053159.9493022]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [686 - 1706053159.9495332]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [686 - 1706053159.9497595]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [686 - 1706053159.9499714]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [686 - 1706053159.9501472]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [686 - 1706053159.9507785]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [686 - 1706053159.951397]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [686 - 1706053159.951616]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [686 - 1706053159.9518504]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [686 - 1706053159.9520855]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [686 - 1706053159.952313]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [686 - 1706053160.016912]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [686 - 1706053160.0176003]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [686 - 1706053160.0178518]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [686 - 1706053160.0180485]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [686 - 1706053160.0182712]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [686 - 1706053160.018446]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [686 - 1706053160.0190656]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [686 - 1706053160.0196836]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [686 - 1706053160.0199203]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [686 - 1706053160.0201144]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [686 - 1706053160.0202813]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [686 - 1706053160.0204659]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [686 - 1706053160.0210924]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [686 - 1706053160.021877]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [686 - 1706053160.0221105]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [686 - 1706053160.0223057]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [686 - 1706053160.0225337]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [686 - 1706053160.0227349]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [686 - 1706053160.0234613]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [686 - 1706053160.0241544]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [686 - 1706053160.024373]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [686 - 1706053160.024588]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [686 - 1706053160.024836]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [686 - 1706053160.025029]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [686 - 1706053160.025601]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [686 - 1706053160.0263057]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [686 - 1706053160.0265439]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [686 - 1706053160.0267706]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [686 - 1706053160.0269787]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [686 - 1706053160.0271835]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [686 - 1706053160.027768]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [686 - 1706053160.0284085]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [686 - 1706053160.0286667]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [686 - 1706053160.0288408]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [686 - 1706053160.0290356]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [686 - 1706053160.029241]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [686 - 1706053160.0298767]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [686 - 1706053160.0304968]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [686 - 1706053160.030751]: 0.0
python ParamPruning/classifier.weight [686 - 1706053160.0308278]: 0.0
python DistillationModifier [693 - 1706053205.6422079]: Calling loss_update with:
args: 0.6608098745346069| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053206.7230477]: 
Returned: 1.611268162727356| 

python DistillationModifier [693 - 1706053208.7509515]: Calling loss_update with:
args: 1.3209508657455444| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053210.020084]: 
Returned: 2.1043426990509033| 

python DistillationModifier [693 - 1706053212.0216846]: Calling loss_update with:
args: 1.385088324546814| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053213.0427532]: 
Returned: 1.5928027629852295| 

python DistillationModifier [693 - 1706053215.1234841]: Calling loss_update with:
args: 0.7952180504798889| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053216.1379805]: 
Returned: 1.0996755361557007| 

python DistillationModifier [693 - 1706053218.3349316]: Calling loss_update with:
args: 0.7367936968803406| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053219.3492794]: 
Returned: 1.2082725763320923| 

python DistillationModifier [693 - 1706053221.4306555]: Calling loss_update with:
args: 0.8446686267852783| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053222.6172242]: 
Returned: 1.6406596899032593| 

python DistillationModifier [693 - 1706053225.4219785]: Calling loss_update with:
args: 0.4559430480003357| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053227.024151]: 
Returned: 1.1009418964385986| 

python DistillationModifier [693 - 1706053230.2537174]: Calling loss_update with:
args: 1.282248616218567| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053231.8332038]: 
Returned: 1.3929930925369263| 

python DistillationModifier [693 - 1706053233.835427]: Calling loss_update with:
args: 0.622778058052063| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053234.8398285]: 
Returned: 0.5136919021606445| 

python DistillationModifier [693 - 1706053236.846375]: Calling loss_update with:
args: 0.9723855257034302| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053238.0298562]: 
Returned: 1.6047418117523193| 

python DistillationModifier [693 - 1706053240.2327678]: Calling loss_update with:
args: 0.8616964817047119| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053241.2406783]: 
Returned: 1.2621338367462158| 

python DistillationModifier [693 - 1706053244.2489057]: Calling loss_update with:
args: 1.2131184339523315| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053245.3429754]: 
Returned: 2.0452961921691895| 

python DistillationModifier [693 - 1706053248.3516424]: Calling loss_update with:
args: 0.8886644244194031| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053249.5196323]: 
Returned: 1.9565171003341675| 

python DistillationModifier [693 - 1706053251.5359585]: Calling loss_update with:
args: 0.7699674367904663| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053252.5419598]: 
Returned: 1.0128653049468994| 

python DistillationModifier [693 - 1706053255.0379825]: Calling loss_update with:
args: 1.5510278940200806| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053256.6241672]: 
Returned: 2.637828826904297| 

python DistillationModifier [693 - 1706053259.825907]: Calling loss_update with:
args: 0.8421777486801147| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053260.8511732]: 
Returned: 1.4489710330963135| 

python DistillationModifier [693 - 1706053263.249293]: Calling loss_update with:
args: 0.6352601647377014| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053264.4467032]: 
Returned: 0.8605642914772034| 

python DistillationModifier [693 - 1706053267.528492]: Calling loss_update with:
args: 1.0897749662399292| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053269.1175017]: 
Returned: 1.9117521047592163| 

python DistillationModifier [693 - 1706053272.347879]: Calling loss_update with:
args: 1.0485211610794067| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053273.9309006]: 
Returned: 1.183076024055481| 

python DistillationModifier [693 - 1706053277.2254953]: Calling loss_update with:
args: 0.865975558757782| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053278.8220956]: 
Returned: 1.3959177732467651| 

python DistillationModifier [693 - 1706053282.051401]: Calling loss_update with:
args: 0.6399677395820618| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053283.635858]: 
Returned: 1.1988691091537476| 

python DistillationModifier [693 - 1706053286.9506137]: Calling loss_update with:
args: 0.5523871183395386| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053288.5474796]: 
Returned: 0.7536847591400146| 

python DistillationModifier [693 - 1706053291.91852]: Calling loss_update with:
args: 0.8592872023582458| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053293.4470804]: 
Returned: 1.03052818775177| 

python DistillationModifier [693 - 1706053295.6496792]: Calling loss_update with:
args: 0.7752143740653992| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053296.7238925]: 
Returned: 1.7054914236068726| 

python DistillationModifier [693 - 1706053299.553063]: Calling loss_update with:
args: 1.2165082693099976| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053301.145887]: 
Returned: 1.4020427465438843| 

python DistillationModifier [693 - 1706053304.4510336]: Calling loss_update with:
args: 0.45103490352630615| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053306.0348353]: 
Returned: 0.9098272919654846| 

python DistillationModifier [693 - 1706053309.3410456]: Calling loss_update with:
args: 0.7785021066665649| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053310.9268978]: 
Returned: 0.9325982928276062| 

python DistillationModifier [693 - 1706053314.1429996]: Calling loss_update with:
args: 0.7597854137420654| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053315.4346619]: 
Returned: 1.0965380668640137| 

python DistillationModifier [693 - 1706053318.7469385]: Calling loss_update with:
args: 0.494490385055542| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053320.333326]: 
Returned: 1.0469614267349243| 

python DistillationModifier [693 - 1706053323.4166121]: Calling loss_update with:
args: 0.8170259594917297| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053324.4363267]: 
Returned: 1.7503803968429565| 

python DistillationModifier [693 - 1706053326.742649]: Calling loss_update with:
args: 1.0726765394210815| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053328.2478666]: 
Returned: 2.1180596351623535| 

python DistillationModifier [693 - 1706053331.4182973]: Calling loss_update with:
args: 0.985480010509491| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053332.8392217]: 
Returned: 1.2044990062713623| 

python DistillationModifier [693 - 1706053334.927852]: Calling loss_update with:
args: 1.0114706754684448| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053336.1407232]: 
Returned: 1.7303348779678345| 

python DistillationModifier [693 - 1706053339.4479249]: Calling loss_update with:
args: 1.49049973487854| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053341.0297554]: 
Returned: 2.248270034790039| 

python DistillationModifier [693 - 1706053343.7529013]: Calling loss_update with:
args: 1.4788577556610107| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053344.9218647]: 
Returned: 1.832902431488037| 

python DistillationModifier [693 - 1706053348.2395568]: Calling loss_update with:
args: 1.2859669923782349| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053349.5518043]: 
Returned: 2.3051364421844482| 

python DistillationModifier [693 - 1706053351.850698]: Calling loss_update with:
args: 1.0610575675964355| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053353.5170672]: 
Returned: 1.312520146369934| 

python DistillationModifier [693 - 1706053356.549701]: Calling loss_update with:
args: 0.7053085565567017| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053357.952074]: 
Returned: 1.1673579216003418| 

python DistillationModifier [693 - 1706053360.9221554]: Calling loss_update with:
args: 0.8754490613937378| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053362.524642]: 
Returned: 1.6372103691101074| 

python DistillationModifier [693 - 1706053365.6189337]: Calling loss_update with:
args: 1.360201358795166| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053366.6339777]: 
Returned: 2.4406399726867676| 

python DistillationModifier [693 - 1706053369.9488027]: Calling loss_update with:
args: 0.6051886677742004| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053371.5394115]: 
Returned: 1.1078013181686401| 

python DistillationModifier [693 - 1706053374.1220043]: Calling loss_update with:
args: 0.7397812008857727| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053375.1414165]: 
Returned: 1.5803089141845703| 

python DistillationModifier [693 - 1706053378.3261576]: Calling loss_update with:
args: 0.9895485043525696| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053379.9231384]: 
Returned: 0.8981782793998718| 

python DistillationModifier [693 - 1706053382.648117]: Calling loss_update with:
args: 0.5744408369064331| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053384.234452]: 
Returned: 1.4209809303283691| 

python DistillationModifier [693 - 1706053387.5422583]: Calling loss_update with:
args: 1.0536044836044312| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053389.0295606]: 
Returned: 2.508443593978882| 

python DistillationModifier [693 - 1706053392.1328912]: Calling loss_update with:
args: 1.3167688846588135| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053393.151101]: 
Returned: 1.6607919931411743| 

python DistillationModifier [693 - 1706053395.3543394]: Calling loss_update with:
args: 0.9406630992889404| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053396.946794]: 
Returned: 1.3817189931869507| 

python DistillationModifier [693 - 1706053399.4168417]: Calling loss_update with:
args: 0.2789280414581299| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053400.427274]: 
Returned: 0.6501445770263672| 

python DistillationModifier [693 - 1706053403.421416]: Calling loss_update with:
args: 1.010951280593872| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053405.021743]: 
Returned: 1.940943717956543| 

python DistillationModifier [693 - 1706053407.6328003]: Calling loss_update with:
args: 0.9672161936759949| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053408.6400094]: 
Returned: 1.683535099029541| 

python DistillationModifier [693 - 1706053410.8243768]: Calling loss_update with:
args: 0.3690072298049927| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053412.4472458]: 
Returned: 0.7668973803520203| 

python DistillationModifier [693 - 1706053415.8340597]: Calling loss_update with:
args: 1.8859325647354126| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053417.436691]: 
Returned: 1.8535099029541016| 

python DistillationModifier [693 - 1706053420.7503707]: Calling loss_update with:
args: 0.8867114782333374| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053422.3369586]: 
Returned: 1.0844138860702515| 

python DistillationModifier [693 - 1706053424.8230193]: Calling loss_update with:
args: 0.8248358368873596| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053425.8288355]: 
Returned: 1.3301197290420532| 

python DistillationModifier [693 - 1706053427.951039]: Calling loss_update with:
args: 0.661145031452179| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053429.0298302]: 
Returned: 1.7589976787567139| 

python DistillationModifier [693 - 1706053431.9436412]: Calling loss_update with:
args: 0.856720507144928| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053432.944018]: 
Returned: 1.2579700946807861| 

python DistillationModifier [693 - 1706053436.3185177]: Calling loss_update with:
args: 1.0494909286499023| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053437.92368]: 
Returned: 0.8709935545921326| 

python DistillationModifier [693 - 1706053441.317709]: Calling loss_update with:
args: 1.0045288801193237| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053442.9214861]: 
Returned: 1.4089813232421875| 

python DistillationModifier [693 - 1706053444.9320166]: Calling loss_update with:
args: 1.0658645629882812| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053445.9402187]: 
Returned: 1.2554619312286377| 

python DistillationModifier [693 - 1706053448.139873]: Calling loss_update with:
args: 0.8879387378692627| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053449.454044]: 
Returned: 1.6702436208724976| 

python DistillationModifier [693 - 1706053452.644761]: Calling loss_update with:
args: 0.9467086791992188| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053453.6497536]: 
Returned: 1.4812028408050537| 

python DistillationModifier [693 - 1706053455.649413]: Calling loss_update with:
args: 1.0469038486480713| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053456.7191434]: 
Returned: 2.100172758102417| 

python DistillationModifier [693 - 1706053457.9518938]: Calling loss_update with:
args: 0.42183101177215576| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053458.5367272]: 
Returned: 0.8822492957115173| 

python DistillationModifier [693 - 1706053461.1183007]: Calling loss_update with:
args: 0.07013256102800369| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1706053462.1245031]: 
Returned: 0.18320831656455994| 

python LearningRateFunctionModifier [693 - 1706053464.7713761]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [693 - 1706053464.7715242]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [693 - 1706053464.7715902]: 2.307692307692308e-05
python LearningRateFunctionModifier/ParamGroup1 [693 - 1706053464.7716427]: 2.307692307692308e-05
python DistillationModifier/task_loss [693 - 1706053464.771687]: tensor(0.0701, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [693 - 1706053464.7721603]: tensor(0.1832, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [693 - 1706053464.772397]: tensor(0.1832, grad_fn=<AddBackward0>)
python ConstantPruningModifier [693 - 1706053464.7726607]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| 
python ConstantPruningModifier [693 - 1706053465.021652]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [693 - 1706053465.022145]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [693 - 1706053465.0223548]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [693 - 1706053465.0225153]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [693 - 1706053465.0227444]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [693 - 1706053465.0233114]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [693 - 1706053465.023854]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [693 - 1706053465.0240722]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [693 - 1706053465.0242815]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [693 - 1706053465.0244448]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [693 - 1706053465.0246863]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [693 - 1706053465.025125]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [693 - 1706053465.0256155]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [693 - 1706053465.0258226]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [693 - 1706053465.0260043]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [693 - 1706053465.026155]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [693 - 1706053465.026336]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [693 - 1706053465.026715]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [693 - 1706053465.02707]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [693 - 1706053465.0272582]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [693 - 1706053465.0274332]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [693 - 1706053465.0276024]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [693 - 1706053465.027788]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [693 - 1706053465.0281744]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [693 - 1706053465.0285265]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [693 - 1706053465.0287516]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [693 - 1706053465.0289211]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [693 - 1706053465.0291038]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [693 - 1706053465.0292523]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [693 - 1706053465.0296388]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [693 - 1706053465.030012]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [693 - 1706053465.030177]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [693 - 1706053465.030352]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [693 - 1706053465.0305085]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [693 - 1706053465.030675]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [693 - 1706053465.0310645]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [693 - 1706053465.0314276]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [693 - 1706053465.031627]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [693 - 1706053465.0317948]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [693 - 1706053465.0319347]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [693 - 1706053465.03208]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [693 - 1706053465.0324428]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [693 - 1706053465.0328786]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [693 - 1706053465.033062]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [693 - 1706053465.0332434]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [693 - 1706053465.0333793]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [693 - 1706053465.033515]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [693 - 1706053465.0339315]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [693 - 1706053465.0343618]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [693 - 1706053465.0345426]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [693 - 1706053465.0347424]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [693 - 1706053465.034885]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [693 - 1706053465.035036]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [693 - 1706053465.0354047]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [693 - 1706053465.0357947]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [693 - 1706053465.0359702]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [693 - 1706053465.0361245]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [693 - 1706053465.0362642]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [693 - 1706053465.0364034]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [693 - 1706053465.0368056]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [693 - 1706053465.0371985]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [693 - 1706053465.0373955]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [693 - 1706053465.0375533]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [693 - 1706053465.037741]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [693 - 1706053465.0378845]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [693 - 1706053465.0382624]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [693 - 1706053465.0387056]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [693 - 1706053465.0389001]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [693 - 1706053465.0390942]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [693 - 1706053465.039249]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [693 - 1706053465.0394096]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [693 - 1706053465.0398843]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [693 - 1706053465.040299]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [693 - 1706053465.0404763]: 0.0
python ParamPruning/classifier.weight [693 - 1706053465.040554]: 0.0
python DistillationModifier [700 - 1706053514.8244507]: Calling loss_update with:
args: 0.08562283217906952| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.11111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [700 - 1706053515.9214313]: 
Returned: 0.11956145614385605| 

python LearningRateFunctionModifier [700 - 1706053518.27539]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.11111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [700 - 1706053518.275533]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [700 - 1706053518.275613]: 2.179487179487181e-05
python LearningRateFunctionModifier/ParamGroup1 [700 - 1706053518.2756462]: 2.179487179487181e-05
python DistillationModifier/task_loss [700 - 1706053518.275689]: tensor(0.0856, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [700 - 1706053518.2761724]: tensor(0.1196, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [700 - 1706053518.276408]: tensor(0.1196, grad_fn=<AddBackward0>)
python ConstantPruningModifier [700 - 1706053518.2766705]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.11111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [700 - 1706053518.5189679]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [700 - 1706053518.5194159]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [700 - 1706053518.5197556]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [700 - 1706053518.5199318]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [700 - 1706053518.5201025]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [700 - 1706053518.5207767]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [700 - 1706053518.52138]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [700 - 1706053518.5216172]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [700 - 1706053518.5218368]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [700 - 1706053518.5219822]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [700 - 1706053518.52212]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [700 - 1706053518.5225534]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [700 - 1706053518.5231197]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [700 - 1706053518.523319]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [700 - 1706053518.523506]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [700 - 1706053518.5236683]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [700 - 1706053518.5238218]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [700 - 1706053518.524219]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [700 - 1706053518.5247202]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [700 - 1706053518.5249133]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [700 - 1706053518.5251007]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [700 - 1706053518.525246]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [700 - 1706053518.5253975]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [700 - 1706053518.5258653]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [700 - 1706053518.526312]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [700 - 1706053518.5265083]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [700 - 1706053518.5266776]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [700 - 1706053518.5268261]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [700 - 1706053518.5269642]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [700 - 1706053518.5273595]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [700 - 1706053518.5278287]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [700 - 1706053518.5280259]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [700 - 1706053518.5282114]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [700 - 1706053518.5283606]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [700 - 1706053518.5285084]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [700 - 1706053518.5289521]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [700 - 1706053518.5294328]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [700 - 1706053518.529643]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [700 - 1706053518.529835]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [700 - 1706053518.5299764]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [700 - 1706053518.530122]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [700 - 1706053518.5305352]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [700 - 1706053518.5310419]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [700 - 1706053518.531229]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [700 - 1706053518.5314097]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [700 - 1706053518.5315478]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [700 - 1706053518.5317252]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [700 - 1706053518.5321472]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [700 - 1706053518.5326009]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [700 - 1706053518.5327978]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [700 - 1706053518.532985]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [700 - 1706053518.5331306]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [700 - 1706053518.5332785]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [700 - 1706053518.5337389]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [700 - 1706053518.5341823]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [700 - 1706053518.534371]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [700 - 1706053518.5345519]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [700 - 1706053518.534728]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [700 - 1706053518.5348766]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [700 - 1706053518.5352652]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [700 - 1706053518.5358121]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [700 - 1706053518.536015]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [700 - 1706053518.5361917]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [700 - 1706053518.5363464]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [700 - 1706053518.5364962]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [700 - 1706053518.5370808]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [700 - 1706053518.537516]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [700 - 1706053518.5377314]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [700 - 1706053518.5379114]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [700 - 1706053518.5380507]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [700 - 1706053518.538195]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [700 - 1706053518.5385828]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [700 - 1706053518.5391304]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [700 - 1706053518.5393302]: 0.0
python ParamPruning/classifier.weight [700 - 1706053518.5394018]: 0.0
python DistillationModifier [707 - 1706053568.3451643]: Calling loss_update with:
args: 0.03673701360821724| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.222222222222221| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [707 - 1706053569.8174834]: 
Returned: 0.09493093192577362| 

python LearningRateFunctionModifier [707 - 1706053572.2749047]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.222222222222221| steps_per_epoch: 63| 
python LearningRateFunctionModifier [707 - 1706053572.2750514]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [707 - 1706053572.275098]: 2.0512820512820512e-05
python LearningRateFunctionModifier/ParamGroup1 [707 - 1706053572.2751293]: 2.0512820512820512e-05
python DistillationModifier/task_loss [707 - 1706053572.2751706]: tensor(0.0367, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [707 - 1706053572.2756836]: tensor(0.0949, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [707 - 1706053572.2759364]: tensor(0.0949, grad_fn=<AddBackward0>)
python ConstantPruningModifier [707 - 1706053572.2761667]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.222222222222221| steps_per_epoch: 63| 
python ConstantPruningModifier [707 - 1706053572.5320814]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [707 - 1706053572.5326424]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [707 - 1706053572.5328884]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [707 - 1706053572.5330677]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [707 - 1706053572.5332365]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [707 - 1706053572.5339944]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [707 - 1706053572.534797]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [707 - 1706053572.535038]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [707 - 1706053572.53527]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [707 - 1706053572.5355074]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [707 - 1706053572.535732]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [707 - 1706053572.536346]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [707 - 1706053572.5370202]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [707 - 1706053572.537248]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [707 - 1706053572.5374665]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [707 - 1706053572.5377018]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [707 - 1706053572.537892]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [707 - 1706053572.5384572]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [707 - 1706053572.5390446]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [707 - 1706053572.5392585]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [707 - 1706053572.5394723]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [707 - 1706053572.5396707]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [707 - 1706053572.5398386]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [707 - 1706053572.5403862]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [707 - 1706053572.5409496]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [707 - 1706053572.54117]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [707 - 1706053572.541333]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [707 - 1706053572.5414944]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [707 - 1706053572.5416727]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [707 - 1706053572.542192]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [707 - 1706053572.542837]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [707 - 1706053572.5430558]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [707 - 1706053572.543257]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [707 - 1706053572.5434809]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [707 - 1706053572.5436854]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [707 - 1706053572.5442278]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [707 - 1706053572.5447962]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [707 - 1706053572.545002]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [707 - 1706053572.5452046]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [707 - 1706053572.5454175]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [707 - 1706053572.5456126]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [707 - 1706053572.5461361]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [707 - 1706053572.5466871]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [707 - 1706053572.546888]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [707 - 1706053572.547055]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [707 - 1706053572.5472162]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [707 - 1706053572.5473752]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [707 - 1706053572.5481017]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [707 - 1706053572.5487595]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [707 - 1706053572.549014]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [707 - 1706053572.5492437]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [707 - 1706053572.5495346]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [707 - 1706053572.5498488]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [707 - 1706053572.550442]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [707 - 1706053572.5510669]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [707 - 1706053572.551337]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [707 - 1706053572.5516505]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [707 - 1706053572.5518868]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [707 - 1706053572.5521324]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [707 - 1706053572.6167011]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [707 - 1706053572.6174424]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [707 - 1706053572.617689]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [707 - 1706053572.61792]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [707 - 1706053572.6181397]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [707 - 1706053572.6183276]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [707 - 1706053572.618938]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [707 - 1706053572.6194708]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [707 - 1706053572.6197076]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [707 - 1706053572.6199012]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [707 - 1706053572.6200657]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [707 - 1706053572.6202247]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [707 - 1706053572.6209247]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [707 - 1706053572.6214757]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [707 - 1706053572.6216998]: 0.0
python ParamPruning/classifier.weight [707 - 1706053572.6217742]: 0.0
python DistillationModifier [714 - 1706053622.031209]: Calling loss_update with:
args: 0.07655008882284164| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.333333333333334| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [714 - 1706053623.1379693]: 
Returned: 0.19994130730628967| 

python LearningRateFunctionModifier [714 - 1706053625.7732463]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.333333333333334| steps_per_epoch: 63| 
python LearningRateFunctionModifier [714 - 1706053625.773413]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [714 - 1706053625.7734594]: 1.9230769230769214e-05
python LearningRateFunctionModifier/ParamGroup1 [714 - 1706053625.7734902]: 1.9230769230769214e-05
python DistillationModifier/task_loss [714 - 1706053625.773551]: tensor(0.0766, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [714 - 1706053625.774071]: tensor(0.1999, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [714 - 1706053625.7743187]: tensor(0.1999, grad_fn=<AddBackward0>)
python ConstantPruningModifier [714 - 1706053625.7745574]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.333333333333334| steps_per_epoch: 63| 
python ConstantPruningModifier [714 - 1706053626.052185]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [714 - 1706053626.116699]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [714 - 1706053626.1170824]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [714 - 1706053626.1173465]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [714 - 1706053626.117657]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [714 - 1706053626.1184955]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [714 - 1706053626.1191707]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [714 - 1706053626.1194184]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [714 - 1706053626.119682]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [714 - 1706053626.1198719]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [714 - 1706053626.1200604]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [714 - 1706053626.1206298]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [714 - 1706053626.1211927]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [714 - 1706053626.1214013]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [714 - 1706053626.121633]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [714 - 1706053626.121809]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [714 - 1706053626.121983]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [714 - 1706053626.1225123]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [714 - 1706053626.1230884]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [714 - 1706053626.1232898]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [714 - 1706053626.1235156]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [714 - 1706053626.1237257]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [714 - 1706053626.1239173]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [714 - 1706053626.1244493]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [714 - 1706053626.1250231]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [714 - 1706053626.1252432]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [714 - 1706053626.1254647]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [714 - 1706053626.1256704]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [714 - 1706053626.1258588]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [714 - 1706053626.1263921]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [714 - 1706053626.126959]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [714 - 1706053626.1271832]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [714 - 1706053626.1273818]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [714 - 1706053626.1275842]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [714 - 1706053626.1278064]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [714 - 1706053626.1283514]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [714 - 1706053626.1289268]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [714 - 1706053626.129133]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [714 - 1706053626.1293411]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [714 - 1706053626.1295083]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [714 - 1706053626.1296937]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [714 - 1706053626.1302216]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [714 - 1706053626.130781]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [714 - 1706053626.1309872]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [714 - 1706053626.1311886]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [714 - 1706053626.1313686]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [714 - 1706053626.1315994]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [714 - 1706053626.1321352]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [714 - 1706053626.1327062]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [714 - 1706053626.1329324]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [714 - 1706053626.1331327]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [714 - 1706053626.1333177]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [714 - 1706053626.1335135]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [714 - 1706053626.134062]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [714 - 1706053626.1346304]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [714 - 1706053626.1348422]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [714 - 1706053626.13504]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [714 - 1706053626.135217]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [714 - 1706053626.1353924]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [714 - 1706053626.1359456]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [714 - 1706053626.1364827]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [714 - 1706053626.1367142]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [714 - 1706053626.1369462]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [714 - 1706053626.1371264]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [714 - 1706053626.1373086]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [714 - 1706053626.1378512]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [714 - 1706053626.1383905]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [714 - 1706053626.1386306]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [714 - 1706053626.1388524]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [714 - 1706053626.1390529]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [714 - 1706053626.1392317]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [714 - 1706053626.1397822]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [714 - 1706053626.1403193]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [714 - 1706053626.1405115]: 0.0
python ParamPruning/classifier.weight [714 - 1706053626.1406116]: 0.0
python DistillationModifier [721 - 1706053673.7534134]: Calling loss_update with:
args: 0.12218125909566879| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.444444444444445| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [721 - 1706053675.3199034]: 
Returned: 0.1111077070236206| 

python LearningRateFunctionModifier [721 - 1706053678.2873323]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.444444444444445| steps_per_epoch: 63| 
python LearningRateFunctionModifier [721 - 1706053678.2874799]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [721 - 1706053678.2875268]: 1.7948717948717944e-05
python LearningRateFunctionModifier/ParamGroup1 [721 - 1706053678.2875578]: 1.7948717948717944e-05
python DistillationModifier/task_loss [721 - 1706053678.287624]: tensor(0.1222, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [721 - 1706053678.2881215]: tensor(0.1111, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [721 - 1706053678.2883723]: tensor(0.1111, grad_fn=<AddBackward0>)
python ConstantPruningModifier [721 - 1706053678.3166063]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.444444444444445| steps_per_epoch: 63| 
python ConstantPruningModifier [721 - 1706053678.5256965]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [721 - 1706053678.5261648]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [721 - 1706053678.526455]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [721 - 1706053678.5266926]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [721 - 1706053678.5268905]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [721 - 1706053678.527529]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [721 - 1706053678.5280051]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [721 - 1706053678.5282369]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [721 - 1706053678.5283918]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [721 - 1706053678.528624]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [721 - 1706053678.5288196]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [721 - 1706053678.5292084]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [721 - 1706053678.529856]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [721 - 1706053678.5300865]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [721 - 1706053678.5303297]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [721 - 1706053678.530554]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [721 - 1706053678.5307925]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [721 - 1706053678.5312712]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [721 - 1706053678.5318358]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [721 - 1706053678.5320635]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [721 - 1706053678.532247]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [721 - 1706053678.5324104]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [721 - 1706053678.532583]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [721 - 1706053678.5330098]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [721 - 1706053678.533475]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [721 - 1706053678.5337229]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [721 - 1706053678.533934]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [721 - 1706053678.534137]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [721 - 1706053678.5343363]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [721 - 1706053678.5347679]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [721 - 1706053678.5351455]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [721 - 1706053678.535349]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [721 - 1706053678.5355237]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [721 - 1706053678.5357168]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [721 - 1706053678.53588]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [721 - 1706053678.5362642]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [721 - 1706053678.5368123]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [721 - 1706053678.5370057]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [721 - 1706053678.5372086]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [721 - 1706053678.5373657]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [721 - 1706053678.537558]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [721 - 1706053678.5380008]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [721 - 1706053678.5385058]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [721 - 1706053678.5387235]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [721 - 1706053678.5389266]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [721 - 1706053678.5390797]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [721 - 1706053678.5392816]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [721 - 1706053678.5397272]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [721 - 1706053678.5401735]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [721 - 1706053678.5403602]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [721 - 1706053678.5405962]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [721 - 1706053678.540801]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [721 - 1706053678.5409656]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [721 - 1706053678.5413866]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [721 - 1706053678.5418274]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [721 - 1706053678.5420213]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [721 - 1706053678.5422025]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [721 - 1706053678.5423472]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [721 - 1706053678.5424907]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [721 - 1706053678.5428913]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [721 - 1706053678.5433745]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [721 - 1706053678.543552]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [721 - 1706053678.543761]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [721 - 1706053678.5439713]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [721 - 1706053678.5441287]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [721 - 1706053678.5445292]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [721 - 1706053678.5450497]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [721 - 1706053678.5452545]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [721 - 1706053678.5454152]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [721 - 1706053678.5456364]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [721 - 1706053678.5457928]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [721 - 1706053678.5461888]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [721 - 1706053678.5466921]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [721 - 1706053678.5468779]: 0.0
python ParamPruning/classifier.weight [721 - 1706053678.546949]: 0.0
python DistillationModifier [728 - 1706053726.0202742]: Calling loss_update with:
args: 0.09375575184822083| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.555555555555555| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [728 - 1706053727.6236916]: 
Returned: 0.1625124216079712| 

python LearningRateFunctionModifier [728 - 1706053730.3573394]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.555555555555555| steps_per_epoch: 63| 
python LearningRateFunctionModifier [728 - 1706053730.3574812]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [728 - 1706053730.3575277]: 1.6666666666666674e-05
python LearningRateFunctionModifier/ParamGroup1 [728 - 1706053730.357559]: 1.6666666666666674e-05
python DistillationModifier/task_loss [728 - 1706053730.3576207]: tensor(0.0938, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [728 - 1706053730.3580983]: tensor(0.1625, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [728 - 1706053730.3583474]: tensor(0.1625, grad_fn=<AddBackward0>)
python ConstantPruningModifier [728 - 1706053730.3586018]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.555555555555555| steps_per_epoch: 63| 
python ConstantPruningModifier [728 - 1706053730.5414422]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [728 - 1706053730.541965]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [728 - 1706053730.54224]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [728 - 1706053730.5424457]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [728 - 1706053730.54269]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [728 - 1706053730.5433156]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [728 - 1706053730.543904]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [728 - 1706053730.5441277]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [728 - 1706053730.544321]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [728 - 1706053730.5444627]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [728 - 1706053730.5446584]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [728 - 1706053730.545213]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [728 - 1706053730.5457058]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [728 - 1706053730.5459092]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [728 - 1706053730.5461032]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [728 - 1706053730.5462663]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [728 - 1706053730.546421]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [728 - 1706053730.5468273]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [728 - 1706053730.54721]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [728 - 1706053730.5474014]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [728 - 1706053730.5475907]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [728 - 1706053730.5477438]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [728 - 1706053730.5478861]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [728 - 1706053730.5482588]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [728 - 1706053730.5486917]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [728 - 1706053730.5488877]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [728 - 1706053730.5490663]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [728 - 1706053730.5492146]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [728 - 1706053730.549368]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [728 - 1706053730.5498953]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [728 - 1706053730.5504875]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [728 - 1706053730.5507047]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [728 - 1706053730.5508904]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [728 - 1706053730.5510657]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [728 - 1706053730.5512202]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [728 - 1706053730.5517387]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [728 - 1706053730.5521188]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [728 - 1706053730.5523093]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [728 - 1706053730.552503]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [728 - 1706053730.6166701]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [728 - 1706053730.61686]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [728 - 1706053730.6172345]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [728 - 1706053730.6176698]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [728 - 1706053730.6178489]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [728 - 1706053730.618023]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [728 - 1706053730.6181772]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [728 - 1706053730.6183262]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [728 - 1706053730.6188583]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [728 - 1706053730.6192756]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [728 - 1706053730.6194682]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [728 - 1706053730.6196647]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [728 - 1706053730.6198184]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [728 - 1706053730.61997]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [728 - 1706053730.6203663]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [728 - 1706053730.6208234]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [728 - 1706053730.621022]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [728 - 1706053730.6211922]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [728 - 1706053730.6213365]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [728 - 1706053730.6214805]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [728 - 1706053730.6218684]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [728 - 1706053730.622223]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [728 - 1706053730.6224048]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [728 - 1706053730.6225927]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [728 - 1706053730.6227508]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [728 - 1706053730.6228924]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [728 - 1706053730.6232488]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [728 - 1706053730.6236525]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [728 - 1706053730.62384]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [728 - 1706053730.6240122]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [728 - 1706053730.6241882]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [728 - 1706053730.6243403]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [728 - 1706053730.6249938]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [728 - 1706053730.62565]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [728 - 1706053730.6258655]: 0.0
python ParamPruning/classifier.weight [728 - 1706053730.6259396]: 0.0
python DistillationModifier [735 - 1706053783.5302124]: Calling loss_update with:
args: 0.09639214724302292| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.666666666666666| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [735 - 1706053784.5422657]: 
Returned: 0.13089673221111298| 

python LearningRateFunctionModifier [735 - 1706053787.7766078]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.666666666666666| steps_per_epoch: 63| 
python LearningRateFunctionModifier [735 - 1706053787.7767525]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [735 - 1706053787.7768004]: 1.5384615384615404e-05
python LearningRateFunctionModifier/ParamGroup1 [735 - 1706053787.7768338]: 1.5384615384615404e-05
python DistillationModifier/task_loss [735 - 1706053787.7768774]: tensor(0.0964, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [735 - 1706053787.7773778]: tensor(0.1309, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [735 - 1706053787.7776487]: tensor(0.1309, grad_fn=<AddBackward0>)
python ConstantPruningModifier [735 - 1706053787.7778926]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.666666666666666| steps_per_epoch: 63| 
python ConstantPruningModifier [735 - 1706053788.1203222]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [735 - 1706053788.1208768]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [735 - 1706053788.1211221]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [735 - 1706053788.1213014]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [735 - 1706053788.1215324]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [735 - 1706053788.1223168]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [735 - 1706053788.1231213]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [735 - 1706053788.1233656]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [735 - 1706053788.123623]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [735 - 1706053788.123822]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [735 - 1706053788.124008]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [735 - 1706053788.124602]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [735 - 1706053788.125394]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [735 - 1706053788.125625]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [735 - 1706053788.1258368]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [735 - 1706053788.1260238]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [735 - 1706053788.126216]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [735 - 1706053788.126919]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [735 - 1706053788.127601]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [735 - 1706053788.1278286]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [735 - 1706053788.1280067]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [735 - 1706053788.1281705]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [735 - 1706053788.1283305]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [735 - 1706053788.1290076]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [735 - 1706053788.1295555]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [735 - 1706053788.1298044]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [735 - 1706053788.1300054]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [735 - 1706053788.130169]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [735 - 1706053788.130385]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [735 - 1706053788.1310856]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [735 - 1706053788.1316602]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [735 - 1706053788.1318746]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [735 - 1706053788.1320376]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [735 - 1706053788.1322536]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [735 - 1706053788.1324294]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [735 - 1706053788.1330652]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [735 - 1706053788.133845]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [735 - 1706053788.1340787]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [735 - 1706053788.1342914]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [735 - 1706053788.1344566]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [735 - 1706053788.1346915]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [735 - 1706053788.1352818]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [735 - 1706053788.135848]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [735 - 1706053788.1360645]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [735 - 1706053788.136278]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [735 - 1706053788.1364427]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [735 - 1706053788.1366212]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [735 - 1706053788.1371598]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [735 - 1706053788.1378963]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [735 - 1706053788.138117]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [735 - 1706053788.1383204]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [735 - 1706053788.1385014]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [735 - 1706053788.1387167]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [735 - 1706053788.1392543]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [735 - 1706053788.1398869]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [735 - 1706053788.1401021]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [735 - 1706053788.140298]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [735 - 1706053788.1404638]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [735 - 1706053788.1407027]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [735 - 1706053788.1412785]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [735 - 1706053788.141895]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [735 - 1706053788.142106]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [735 - 1706053788.1422694]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [735 - 1706053788.1424837]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [735 - 1706053788.1426642]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [735 - 1706053788.1432197]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [735 - 1706053788.1437788]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [735 - 1706053788.1439924]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [735 - 1706053788.1441572]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [735 - 1706053788.1443596]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [735 - 1706053788.1446052]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [735 - 1706053788.145154]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [735 - 1706053788.1458259]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [735 - 1706053788.14604]: 0.0
python ParamPruning/classifier.weight [735 - 1706053788.1461127]: 0.0
python DistillationModifier [742 - 1706053837.8500235]: Calling loss_update with:
args: 0.06193389743566513| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.777777777777779| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [742 - 1706053839.4373765]: 
Returned: 0.09672226756811142| 

python LearningRateFunctionModifier [742 - 1706053842.8768108]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.777777777777779| steps_per_epoch: 63| 
python LearningRateFunctionModifier [742 - 1706053842.876973]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [742 - 1706053842.8776867]: 1.410256410256408e-05
python LearningRateFunctionModifier/ParamGroup1 [742 - 1706053842.877721]: 1.410256410256408e-05
python DistillationModifier/task_loss [742 - 1706053842.8777637]: tensor(0.0619, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [742 - 1706053842.8782349]: tensor(0.0967, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [742 - 1706053842.8784633]: tensor(0.0967, grad_fn=<AddBackward0>)
python ConstantPruningModifier [742 - 1706053842.8787124]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.777777777777779| steps_per_epoch: 63| 
python ConstantPruningModifier [742 - 1706053843.1199138]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [742 - 1706053843.1203938]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [742 - 1706053843.1206307]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [742 - 1706053843.120864]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [742 - 1706053843.1210983]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [742 - 1706053843.121749]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [742 - 1706053843.1223426]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [742 - 1706053843.1225572]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [742 - 1706053843.1227942]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [742 - 1706053843.1229434]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [742 - 1706053843.1230984]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [742 - 1706053843.123557]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [742 - 1706053843.1239972]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [742 - 1706053843.1241808]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [742 - 1706053843.1243246]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [742 - 1706053843.1244643]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [742 - 1706053843.124676]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [742 - 1706053843.1250515]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [742 - 1706053843.1256316]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [742 - 1706053843.1258278]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [742 - 1706053843.1260211]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [742 - 1706053843.126168]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [742 - 1706053843.126325]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [742 - 1706053843.1267333]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [742 - 1706053843.127241]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [742 - 1706053843.1274242]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [742 - 1706053843.1276388]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [742 - 1706053843.1277945]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [742 - 1706053843.1279442]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [742 - 1706053843.1283348]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [742 - 1706053843.128826]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [742 - 1706053843.1290188]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [742 - 1706053843.129159]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [742 - 1706053843.1292937]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [742 - 1706053843.1294363]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [742 - 1706053843.1298945]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [742 - 1706053843.1304176]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [742 - 1706053843.1306236]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [742 - 1706053843.1308296]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [742 - 1706053843.1309786]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [742 - 1706053843.1311696]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [742 - 1706053843.1316776]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [742 - 1706053843.132185]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [742 - 1706053843.1323612]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [742 - 1706053843.132546]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [742 - 1706053843.1327207]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [742 - 1706053843.1328642]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [742 - 1706053843.1333292]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [742 - 1706053843.133759]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [742 - 1706053843.1339562]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [742 - 1706053843.1340978]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [742 - 1706053843.1342373]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [742 - 1706053843.1343727]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [742 - 1706053843.1347945]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [742 - 1706053843.1351702]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [742 - 1706053843.1353586]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [742 - 1706053843.135494]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [742 - 1706053843.135699]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [742 - 1706053843.1358843]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [742 - 1706053843.1363378]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [742 - 1706053843.1367414]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [742 - 1706053843.1369476]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [742 - 1706053843.1371293]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [742 - 1706053843.1372728]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [742 - 1706053843.1374242]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [742 - 1706053843.137883]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [742 - 1706053843.1383069]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [742 - 1706053843.1385014]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [742 - 1706053843.138696]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [742 - 1706053843.1388357]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [742 - 1706053843.1390164]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [742 - 1706053843.139453]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [742 - 1706053843.13987]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [742 - 1706053843.140067]: 0.0
python ParamPruning/classifier.weight [742 - 1706053843.1401389]: 0.0
python DistillationModifier [749 - 1706053894.1296089]: Calling loss_update with:
args: 0.010514057241380215| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.88888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [749 - 1706053895.540065]: 
Returned: 0.05999849736690521| 

python LearningRateFunctionModifier [749 - 1706053897.881619]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.88888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [749 - 1706053897.8817668]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [749 - 1706053897.8818142]: 1.282051282051281e-05
python LearningRateFunctionModifier/ParamGroup1 [749 - 1706053897.8818462]: 1.282051282051281e-05
python DistillationModifier/task_loss [749 - 1706053897.8818896]: tensor(0.0105, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [749 - 1706053897.8823767]: tensor(0.0600, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [749 - 1706053897.8826292]: tensor(0.0600, grad_fn=<AddBackward0>)
python ConstantPruningModifier [749 - 1706053897.8828661]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.88888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [749 - 1706053898.1234512]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [749 - 1706053898.1239545]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [749 - 1706053898.1242297]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [749 - 1706053898.1244187]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [749 - 1706053898.124635]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [749 - 1706053898.1252675]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [749 - 1706053898.1259255]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [749 - 1706053898.1261685]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [749 - 1706053898.1263819]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [749 - 1706053898.1265512]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [749 - 1706053898.1267543]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [749 - 1706053898.1272995]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [749 - 1706053898.1280391]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [749 - 1706053898.1282663]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [749 - 1706053898.1284778]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [749 - 1706053898.1286764]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [749 - 1706053898.1288416]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [749 - 1706053898.1293695]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [749 - 1706053898.1299155]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [749 - 1706053898.1301131]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [749 - 1706053898.1302977]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [749 - 1706053898.1304595]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [749 - 1706053898.1306443]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [749 - 1706053898.131107]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [749 - 1706053898.1317637]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [749 - 1706053898.1319792]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [749 - 1706053898.1321638]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [749 - 1706053898.1323273]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [749 - 1706053898.132505]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [749 - 1706053898.1331835]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [749 - 1706053898.1338708]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [749 - 1706053898.134079]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [749 - 1706053898.1342685]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [749 - 1706053898.134431]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [749 - 1706053898.1346116]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [749 - 1706053898.1352184]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [749 - 1706053898.1358836]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [749 - 1706053898.1361032]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [749 - 1706053898.1362865]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [749 - 1706053898.1364548]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [749 - 1706053898.1366386]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [749 - 1706053898.1371517]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [749 - 1706053898.1378398]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [749 - 1706053898.1380527]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [749 - 1706053898.1382368]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [749 - 1706053898.1383965]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [749 - 1706053898.1385589]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [749 - 1706053898.1390908]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [749 - 1706053898.1397207]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [749 - 1706053898.1399243]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [749 - 1706053898.1401045]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [749 - 1706053898.1402678]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [749 - 1706053898.1404386]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [749 - 1706053898.1409628]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [749 - 1706053898.1415184]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [749 - 1706053898.1417499]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [749 - 1706053898.1419213]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [749 - 1706053898.1420813]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [749 - 1706053898.1422417]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [749 - 1706053898.142706]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [749 - 1706053898.143128]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [749 - 1706053898.143314]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [749 - 1706053898.1434963]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [749 - 1706053898.143676]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [749 - 1706053898.143839]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [749 - 1706053898.144293]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [749 - 1706053898.1448731]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [749 - 1706053898.1450803]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [749 - 1706053898.1452365]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [749 - 1706053898.145413]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [749 - 1706053898.1455934]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [749 - 1706053898.146041]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [749 - 1706053898.1467178]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [749 - 1706053898.1469235]: 0.0
python ParamPruning/classifier.weight [749 - 1706053898.1469958]: 0.0
python DistillationModifier [756 - 1706053939.6430752]: Calling loss_update with:
args: 0.6663291454315186| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706053940.719575]: 
Returned: 1.626334547996521| 

python DistillationModifier [756 - 1706053942.6479344]: Calling loss_update with:
args: 1.4145305156707764| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706053943.729033]: 
Returned: 2.2659027576446533| 

python DistillationModifier [756 - 1706053945.8193798]: Calling loss_update with:
args: 1.504586100578308| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706053946.82864]: 
Returned: 1.7958797216415405| 

python DistillationModifier [756 - 1706053949.1232848]: Calling loss_update with:
args: 0.7181330323219299| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706053950.5282621]: 
Returned: 0.8064423203468323| 

python DistillationModifier [756 - 1706053952.638068]: Calling loss_update with:
args: 0.7347155213356018| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706053953.6417959]: 
Returned: 1.2756794691085815| 

python DistillationModifier [756 - 1706053955.640401]: Calling loss_update with:
args: 0.8906990885734558| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706053957.0549872]: 
Returned: 1.722618579864502| 

python DistillationModifier [756 - 1706053960.442731]: Calling loss_update with:
args: 0.34459567070007324| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706053962.0332193]: 
Returned: 0.8914483785629272| 

python DistillationModifier [756 - 1706053965.3251176]: Calling loss_update with:
args: 1.2500579357147217| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706053966.9238427]: 
Returned: 1.4077068567276| 

python DistillationModifier [756 - 1706053970.241608]: Calling loss_update with:
args: 0.82175612449646| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706053971.6281204]: 
Returned: 0.6702036261558533| 

python DistillationModifier [756 - 1706053974.8494532]: Calling loss_update with:
args: 0.8947932124137878| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706053976.4343307]: 
Returned: 1.4960224628448486| 

python DistillationModifier [756 - 1706053978.7383802]: Calling loss_update with:
args: 0.8995431661605835| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706053979.7433453]: 
Returned: 1.2429485321044922| 

python DistillationModifier [756 - 1706053981.8511238]: Calling loss_update with:
args: 1.2007184028625488| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706053982.9218295]: 
Returned: 2.045870542526245| 

python DistillationModifier [756 - 1706053984.920605]: Calling loss_update with:
args: 0.9810374975204468| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706053985.927094]: 
Returned: 2.1122190952301025| 

python DistillationModifier [756 - 1706053988.021158]: Calling loss_update with:
args: 0.8219946026802063| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706053989.1193924]: 
Returned: 0.9022330641746521| 

python DistillationModifier [756 - 1706053991.5395868]: Calling loss_update with:
args: 1.2860416173934937| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706053992.6271338]: 
Returned: 2.448091983795166| 

python DistillationModifier [756 - 1706053994.621384]: Calling loss_update with:
args: 0.924530029296875| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706053995.6306329]: 
Returned: 1.5957410335540771| 

python DistillationModifier [756 - 1706053998.85195]: Calling loss_update with:
args: 0.6702393889427185| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054000.138625]: 
Returned: 0.9023629426956177| 

python DistillationModifier [756 - 1706054002.247743]: Calling loss_update with:
args: 1.1055998802185059| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054003.8393376]: 
Returned: 1.9457850456237793| 

python DistillationModifier [756 - 1706054007.1286762]: Calling loss_update with:
args: 1.1946134567260742| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054008.7207115]: 
Returned: 1.4667693376541138| 

python DistillationModifier [756 - 1706054011.1305027]: Calling loss_update with:
args: 0.9239794611930847| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054012.5474133]: 
Returned: 1.5791951417922974| 

python DistillationModifier [756 - 1706054015.9273293]: Calling loss_update with:
args: 0.8965843915939331| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054017.4427981]: 
Returned: 1.4380868673324585| 

python DistillationModifier [756 - 1706054019.730825]: Calling loss_update with:
args: 0.6379974484443665| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054020.7357936]: 
Returned: 0.8783585429191589| 

python DistillationModifier [756 - 1706054022.752058]: Calling loss_update with:
args: 0.9598096013069153| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054023.8209264]: 
Returned: 1.1643911600112915| 

python DistillationModifier [756 - 1706054025.821137]: Calling loss_update with:
args: 0.7907708883285522| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054026.8307219]: 
Returned: 1.7275488376617432| 

python DistillationModifier [756 - 1706054029.8529098]: Calling loss_update with:
args: 1.1617560386657715| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054031.4398565]: 
Returned: 1.3348385095596313| 

python DistillationModifier [756 - 1706054034.8182263]: Calling loss_update with:
args: 0.5306617617607117| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054036.424648]: 
Returned: 0.9679425358772278| 

python DistillationModifier [756 - 1706054039.0328653]: Calling loss_update with:
args: 0.9343062043190002| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054040.0451722]: 
Returned: 1.3098052740097046| 

python DistillationModifier [756 - 1706054042.3230817]: Calling loss_update with:
args: 0.881547212600708| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054043.330014]: 
Returned: 1.3907912969589233| 

python DistillationModifier [756 - 1706054045.333605]: Calling loss_update with:
args: 0.5922987461090088| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054046.3464532]: 
Returned: 1.135762095451355| 

python DistillationModifier [756 - 1706054048.9336953]: Calling loss_update with:
args: 0.8375287652015686| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054050.038996]: 
Returned: 1.8301352262496948| 

python DistillationModifier [756 - 1706054052.0378942]: Calling loss_update with:
args: 1.2751622200012207| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054053.122496]: 
Returned: 2.423985481262207| 

python DistillationModifier [756 - 1706054055.1263652]: Calling loss_update with:
args: 0.9961963295936584| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054056.1332605]: 
Returned: 1.2705914974212646| 

python DistillationModifier [756 - 1706054058.3348641]: Calling loss_update with:
args: 1.160041093826294| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054059.3461878]: 
Returned: 1.9887291193008423| 

python DistillationModifier [756 - 1706054062.548084]: Calling loss_update with:
args: 1.302908182144165| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054064.1351686]: 
Returned: 1.992771029472351| 

python DistillationModifier [756 - 1706054067.4199488]: Calling loss_update with:
args: 1.565577745437622| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054068.7207503]: 
Returned: 1.9423611164093018| 

python DistillationModifier [756 - 1706054070.746747]: Calling loss_update with:
args: 1.3570858240127563| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054072.0272274]: 
Returned: 2.4111599922180176| 

python DistillationModifier [756 - 1706054074.0232604]: Calling loss_update with:
args: 0.9812939167022705| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054075.4247572]: 
Returned: 1.123122215270996| 

python DistillationModifier [756 - 1706054078.7344553]: Calling loss_update with:
args: 0.6929542422294617| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054080.3344817]: 
Returned: 1.2147408723831177| 

python DistillationModifier [756 - 1706054083.720057]: Calling loss_update with:
args: 0.945351779460907| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054085.229348]: 
Returned: 1.7157865762710571| 

python DistillationModifier [756 - 1706054087.2266464]: Calling loss_update with:
args: 1.3973537683486938| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054088.3216782]: 
Returned: 2.5346851348876953| 

python DistillationModifier [756 - 1706054090.323864]: Calling loss_update with:
args: 0.752866804599762| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054091.3456545]: 
Returned: 1.2472957372665405| 

python DistillationModifier [756 - 1706054093.3433034]: Calling loss_update with:
args: 0.8310486674308777| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054094.9298806]: 
Returned: 1.6751370429992676| 

python DistillationModifier [756 - 1706054098.3431308]: Calling loss_update with:
args: 0.9652165174484253| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054099.939849]: 
Returned: 0.8009338974952698| 

python DistillationModifier [756 - 1706054103.2509813]: Calling loss_update with:
args: 0.7047452926635742| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054104.8358772]: 
Returned: 1.6907426118850708| 

python DistillationModifier [756 - 1706054108.140933]: Calling loss_update with:
args: 1.001995325088501| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054109.7409024]: 
Returned: 2.3128528594970703| 

python DistillationModifier [756 - 1706054112.8234015]: Calling loss_update with:
args: 1.1827237606048584| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054113.827701]: 
Returned: 1.492457389831543| 

python DistillationModifier [756 - 1706054115.836119]: Calling loss_update with:
args: 1.0563006401062012| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054116.844325]: 
Returned: 1.621739149093628| 

python DistillationModifier [756 - 1706054119.8338537]: Calling loss_update with:
args: 0.31971707940101624| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054120.8447552]: 
Returned: 0.7906967401504517| 

python DistillationModifier [756 - 1706054123.2214751]: Calling loss_update with:
args: 0.9359724521636963| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054124.8223956]: 
Returned: 1.8721750974655151| 

python DistillationModifier [756 - 1706054126.9468734]: Calling loss_update with:
args: 0.7873786687850952| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054128.0248637]: 
Returned: 1.3881291151046753| 

python DistillationModifier [756 - 1706054130.0326521]: Calling loss_update with:
args: 0.37247127294540405| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054131.1182458]: 
Returned: 0.7930756211280823| 

python DistillationModifier [756 - 1706054133.8202648]: Calling loss_update with:
args: 1.970887541770935| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054134.8280811]: 
Returned: 1.9882099628448486| 

python DistillationModifier [756 - 1706054137.526619]: Calling loss_update with:
args: 0.8980724811553955| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054139.054361]: 
Returned: 1.108290672302246| 

python DistillationModifier [756 - 1706054141.240492]: Calling loss_update with:
args: 0.8739073872566223| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054142.2450957]: 
Returned: 1.404853105545044| 

python DistillationModifier [756 - 1706054144.3314388]: Calling loss_update with:
args: 0.6311191916465759| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054145.342092]: 
Returned: 1.4998143911361694| 

python DistillationModifier [756 - 1706054147.430271]: Calling loss_update with:
args: 0.8871399164199829| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054148.5357249]: 
Returned: 1.3043137788772583| 

python DistillationModifier [756 - 1706054151.421105]: Calling loss_update with:
args: 0.8980855345726013| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054152.8171222]: 
Returned: 0.7405247688293457| 

python DistillationModifier [756 - 1706054156.1239235]: Calling loss_update with:
args: 1.1547150611877441| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054157.7252045]: 
Returned: 1.4349274635314941| 

python DistillationModifier [756 - 1706054160.1299396]: Calling loss_update with:
args: 1.2709206342697144| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054161.1441891]: 
Returned: 1.7182260751724243| 

python DistillationModifier [756 - 1706054164.2175908]: Calling loss_update with:
args: 0.909741997718811| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054165.221357]: 
Returned: 1.6972357034683228| 

python DistillationModifier [756 - 1706054167.241535]: Calling loss_update with:
args: 1.0434210300445557| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054168.3272681]: 
Returned: 1.755144476890564| 

python DistillationModifier [756 - 1706054171.0208967]: Calling loss_update with:
args: 1.1664834022521973| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054172.024755]: 
Returned: 2.366483449935913| 

python DistillationModifier [756 - 1706054173.2384236]: Calling loss_update with:
args: 0.4120984971523285| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054173.8189087]: 
Returned: 0.8744981288909912| 

python DistillationModifier [756 - 1706054176.7431114]: Calling loss_update with:
args: 0.015440287999808788| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1706054177.8382902]: 
Returned: 0.029376309365034103| 

python LearningRateFunctionModifier [756 - 1706054180.243714]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [756 - 1706054180.243859]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [756 - 1706054180.2439048]: 1.153846153846154e-05
python LearningRateFunctionModifier/ParamGroup1 [756 - 1706054180.2439358]: 1.153846153846154e-05
python DistillationModifier/task_loss [756 - 1706054180.2439773]: tensor(0.0154, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [756 - 1706054180.2444537]: tensor(0.0294, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [756 - 1706054180.2447362]: tensor(0.0294, grad_fn=<AddBackward0>)
python ConstantPruningModifier [756 - 1706054180.2449803]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| 
python ConstantPruningModifier [756 - 1706054180.4358354]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [756 - 1706054180.4363232]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [756 - 1706054180.4365609]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [756 - 1706054180.4367783]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [756 - 1706054180.4369385]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [756 - 1706054180.4375584]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [756 - 1706054180.438083]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [756 - 1706054180.4382958]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [756 - 1706054180.4384499]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [756 - 1706054180.43863]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [756 - 1706054180.4387915]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [756 - 1706054180.439242]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [756 - 1706054180.4397192]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [756 - 1706054180.439926]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [756 - 1706054180.4400775]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [756 - 1706054180.44023]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [756 - 1706054180.4403799]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [756 - 1706054180.440859]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [756 - 1706054180.4413161]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [756 - 1706054180.4415085]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [756 - 1706054180.4416802]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [756 - 1706054180.4418254]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [756 - 1706054180.4419718]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [756 - 1706054180.4423923]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [756 - 1706054180.4427965]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [756 - 1706054180.4429705]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [756 - 1706054180.4431112]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [756 - 1706054180.4432464]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [756 - 1706054180.4433813]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [756 - 1706054180.4437408]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [756 - 1706054180.4440784]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [756 - 1706054180.4442353]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [756 - 1706054180.4443655]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [756 - 1706054180.4445066]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [756 - 1706054180.4446704]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [756 - 1706054180.4450183]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [756 - 1706054180.4453561]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [756 - 1706054180.4455338]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [756 - 1706054180.4456968]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [756 - 1706054180.4458363]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [756 - 1706054180.4459693]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [756 - 1706054180.4463043]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [756 - 1706054180.4466841]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [756 - 1706054180.4468443]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [756 - 1706054180.4469757]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [756 - 1706054180.447107]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [756 - 1706054180.4472384]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [756 - 1706054180.4476318]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [756 - 1706054180.4479704]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [756 - 1706054180.448135]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [756 - 1706054180.448271]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [756 - 1706054180.4484093]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [756 - 1706054180.4485524]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [756 - 1706054180.4489288]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [756 - 1706054180.4492583]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [756 - 1706054180.4494216]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [756 - 1706054180.4495509]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [756 - 1706054180.4497135]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [756 - 1706054180.4498482]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [756 - 1706054180.4502127]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [756 - 1706054180.4505436]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [756 - 1706054180.450726]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [756 - 1706054180.4508634]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [756 - 1706054180.4509947]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [756 - 1706054180.4511244]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [756 - 1706054180.451457]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [756 - 1706054180.4518285]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [756 - 1706054180.4519954]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [756 - 1706054180.4521232]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [756 - 1706054180.4522548]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [756 - 1706054180.4523842]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [756 - 1706054180.516807]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [756 - 1706054180.5171847]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [756 - 1706054180.5173628]: 0.0
python ParamPruning/classifier.weight [756 - 1706054180.5174334]: 0.0
python DistillationModifier [763 - 1706054226.6394315]: Calling loss_update with:
args: 0.16301272809505463| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.11111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [763 - 1706054227.6400201]: 
Returned: 0.11477537453174591| 

python LearningRateFunctionModifier [763 - 1706054230.680512]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.11111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [763 - 1706054230.680674]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [763 - 1706054230.6807218]: 1.0256410256410242e-05
python LearningRateFunctionModifier/ParamGroup1 [763 - 1706054230.6807532]: 1.0256410256410242e-05
python DistillationModifier/task_loss [763 - 1706054230.6807947]: tensor(0.1630, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [763 - 1706054230.6812809]: tensor(0.1148, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [763 - 1706054230.6815248]: tensor(0.1148, grad_fn=<AddBackward0>)
python ConstantPruningModifier [763 - 1706054230.6817951]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.11111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [763 - 1706054231.0212722]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [763 - 1706054231.021826]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [763 - 1706054231.0221221]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [763 - 1706054231.022335]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [763 - 1706054231.022514]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [763 - 1706054231.0232964]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [763 - 1706054231.0241022]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [763 - 1706054231.0243454]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [763 - 1706054231.0245256]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [763 - 1706054231.024726]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [763 - 1706054231.0249119]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [763 - 1706054231.0255249]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [763 - 1706054231.0263357]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [763 - 1706054231.026601]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [763 - 1706054231.026811]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [763 - 1706054231.0269957]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [763 - 1706054231.027168]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [763 - 1706054231.0278547]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [763 - 1706054231.0284219]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [763 - 1706054231.0286865]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [763 - 1706054231.0288973]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [763 - 1706054231.0291169]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [763 - 1706054231.02931]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [763 - 1706054231.0298615]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [763 - 1706054231.0306065]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [763 - 1706054231.0308347]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [763 - 1706054231.0310001]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [763 - 1706054231.0311623]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [763 - 1706054231.0313487]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [763 - 1706054231.0319655]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [763 - 1706054231.032507]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [763 - 1706054231.0327604]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [763 - 1706054231.0329378]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [763 - 1706054231.0331337]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [763 - 1706054231.033319]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [763 - 1706054231.033871]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [763 - 1706054231.034585]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [763 - 1706054231.0348098]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [763 - 1706054231.034991]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [763 - 1706054231.0351837]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [763 - 1706054231.0353882]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [763 - 1706054231.0359678]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [763 - 1706054231.0366325]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [763 - 1706054231.0368643]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [763 - 1706054231.0370772]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [763 - 1706054231.0372593]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [763 - 1706054231.0374374]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [763 - 1706054231.0380292]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [763 - 1706054231.038652]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [763 - 1706054231.0389001]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [763 - 1706054231.039107]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [763 - 1706054231.0392919]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [763 - 1706054231.039464]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [763 - 1706054231.0400617]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [763 - 1706054231.0407217]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [763 - 1706054231.0409691]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [763 - 1706054231.0412202]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [763 - 1706054231.041393]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [763 - 1706054231.0415585]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [763 - 1706054231.0421472]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [763 - 1706054231.0427504]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [763 - 1706054231.0429697]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [763 - 1706054231.0431356]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [763 - 1706054231.0433154]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [763 - 1706054231.043494]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [763 - 1706054231.0440934]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [763 - 1706054231.0447383]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [763 - 1706054231.0449562]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [763 - 1706054231.0451186]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [763 - 1706054231.0452795]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [763 - 1706054231.0454373]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [763 - 1706054231.0460167]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [763 - 1706054231.0465775]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [763 - 1706054231.0468068]: 0.0
python ParamPruning/classifier.weight [763 - 1706054231.0468767]: 0.0
python DistillationModifier [770 - 1706054276.1267247]: Calling loss_update with:
args: 0.008266851305961609| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.222222222222221| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [770 - 1706054277.2211518]: 
Returned: 0.11177574098110199| 

python LearningRateFunctionModifier [770 - 1706054280.3605936]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.222222222222221| steps_per_epoch: 63| 
python LearningRateFunctionModifier [770 - 1706054280.360765]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [770 - 1706054280.3608105]: 8.974358974358972e-06
python LearningRateFunctionModifier/ParamGroup1 [770 - 1706054280.3608408]: 8.974358974358972e-06
python DistillationModifier/task_loss [770 - 1706054280.3608825]: tensor(0.0083, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [770 - 1706054280.3613503]: tensor(0.1118, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [770 - 1706054280.361606]: tensor(0.1118, grad_fn=<AddBackward0>)
python ConstantPruningModifier [770 - 1706054280.3618424]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.222222222222221| steps_per_epoch: 63| 
python ConstantPruningModifier [770 - 1706054280.6373425]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [770 - 1706054280.637867]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [770 - 1706054280.63818]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [770 - 1706054280.6384218]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [770 - 1706054280.6386862]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [770 - 1706054280.6394348]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [770 - 1706054280.6400347]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [770 - 1706054280.6402624]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [770 - 1706054280.6404712]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [770 - 1706054280.6407318]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [770 - 1706054280.6409235]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [770 - 1706054280.6414585]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [770 - 1706054280.6422353]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [770 - 1706054280.6424582]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [770 - 1706054280.6426802]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [770 - 1706054280.6428666]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [770 - 1706054280.643102]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [770 - 1706054280.6436596]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [770 - 1706054280.6442997]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [770 - 1706054280.64452]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [770 - 1706054280.6447797]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [770 - 1706054280.645019]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [770 - 1706054280.645202]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [770 - 1706054280.6457536]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [770 - 1706054280.6462913]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [770 - 1706054280.6464818]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [770 - 1706054280.6467156]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [770 - 1706054280.6468914]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [770 - 1706054280.6470752]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [770 - 1706054280.6476078]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [770 - 1706054280.6482944]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [770 - 1706054280.6484928]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [770 - 1706054280.6487439]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [770 - 1706054280.6489816]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [770 - 1706054280.649161]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [770 - 1706054280.6497116]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [770 - 1706054280.6503272]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [770 - 1706054280.6505249]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [770 - 1706054280.6507618]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [770 - 1706054280.6509583]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [770 - 1706054280.6511858]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [770 - 1706054280.6517375]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [770 - 1706054280.6523159]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [770 - 1706054280.6525323]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [770 - 1706054280.7167091]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [770 - 1706054280.7169049]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [770 - 1706054280.7170856]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [770 - 1706054280.717692]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [770 - 1706054280.7183719]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [770 - 1706054280.7185867]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [770 - 1706054280.718828]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [770 - 1706054280.719008]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [770 - 1706054280.7192352]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [770 - 1706054280.719785]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [770 - 1706054280.720375]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [770 - 1706054280.7206519]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [770 - 1706054280.7208486]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [770 - 1706054280.7210772]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [770 - 1706054280.72127]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [770 - 1706054280.7218556]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [770 - 1706054280.7224658]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [770 - 1706054280.722688]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [770 - 1706054280.7229161]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [770 - 1706054280.7230847]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [770 - 1706054280.723289]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [770 - 1706054280.723842]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [770 - 1706054280.7243702]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [770 - 1706054280.7245972]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [770 - 1706054280.7248292]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [770 - 1706054280.725009]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [770 - 1706054280.7251885]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [770 - 1706054280.725835]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [770 - 1706054280.726624]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [770 - 1706054280.7268338]: 0.0
python ParamPruning/classifier.weight [770 - 1706054280.726905]: 0.0
python DistillationModifier [777 - 1706054326.9504778]: Calling loss_update with:
args: 0.04451283812522888| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.333333333333334| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [777 - 1706054328.629378]: 
Returned: 0.11297972500324249| 

python LearningRateFunctionModifier [777 - 1706054331.7236814]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.333333333333334| steps_per_epoch: 63| 
python LearningRateFunctionModifier [777 - 1706054331.7238233]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [777 - 1706054331.7238681]: 7.692307692307675e-06
python LearningRateFunctionModifier/ParamGroup1 [777 - 1706054331.7238977]: 7.692307692307675e-06
python DistillationModifier/task_loss [777 - 1706054331.7239385]: tensor(0.0445, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [777 - 1706054331.7244272]: tensor(0.1130, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [777 - 1706054331.7247279]: tensor(0.1130, grad_fn=<AddBackward0>)
python ConstantPruningModifier [777 - 1706054331.7249787]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.333333333333334| steps_per_epoch: 63| 
python ConstantPruningModifier [777 - 1706054332.0346408]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [777 - 1706054332.0351908]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [777 - 1706054332.0357192]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [777 - 1706054332.0359495]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [777 - 1706054332.0361583]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [777 - 1706054332.0369437]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [777 - 1706054332.0375311]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [777 - 1706054332.0377953]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [777 - 1706054332.0380166]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [777 - 1706054332.0381877]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [777 - 1706054332.0383651]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [777 - 1706054332.039144]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [777 - 1706054332.0397272]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [777 - 1706054332.0399406]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [777 - 1706054332.0401466]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [777 - 1706054332.0403154]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [777 - 1706054332.0405993]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [777 - 1706054332.0411494]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [777 - 1706054332.0417256]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [777 - 1706054332.0419438]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [777 - 1706054332.0421624]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [777 - 1706054332.0423303]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [777 - 1706054332.0425894]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [777 - 1706054332.0432951]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [777 - 1706054332.0438704]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [777 - 1706054332.044083]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [777 - 1706054332.0442984]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [777 - 1706054332.0444653]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [777 - 1706054332.0447152]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [777 - 1706054332.0452476]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [777 - 1706054332.045894]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [777 - 1706054332.0461073]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [777 - 1706054332.0463216]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [777 - 1706054332.0464904]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [777 - 1706054332.0466766]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [777 - 1706054332.0472133]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [777 - 1706054332.0477676]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [777 - 1706054332.0479767]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [777 - 1706054332.0481796]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [777 - 1706054332.0483444]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [777 - 1706054332.04853]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [777 - 1706054332.049191]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [777 - 1706054332.0499701]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [777 - 1706054332.050227]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [777 - 1706054332.0504274]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [777 - 1706054332.050634]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [777 - 1706054332.0508153]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [777 - 1706054332.0513988]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [777 - 1706054332.0520704]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [777 - 1706054332.0522795]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [777 - 1706054332.0524445]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [777 - 1706054332.116727]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [777 - 1706054332.1169548]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [777 - 1706054332.1175425]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [777 - 1706054332.1181748]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [777 - 1706054332.1183922]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [777 - 1706054332.1186328]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [777 - 1706054332.1188169]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [777 - 1706054332.1190174]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [777 - 1706054332.1195922]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [777 - 1706054332.1201673]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [777 - 1706054332.1203737]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [777 - 1706054332.1206098]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [777 - 1706054332.120791]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [777 - 1706054332.1210294]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [777 - 1706054332.121803]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [777 - 1706054332.1224008]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [777 - 1706054332.1226332]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [777 - 1706054332.12285]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [777 - 1706054332.1230187]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [777 - 1706054332.123249]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [777 - 1706054332.1238034]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [777 - 1706054332.1243575]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [777 - 1706054332.1245897]: 0.0
python ParamPruning/classifier.weight [777 - 1706054332.1246712]: 0.0
python DistillationModifier [784 - 1706054383.0379105]: Calling loss_update with:
args: 0.1719415783882141| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.444444444444445| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [784 - 1706054384.5235918]: 
Returned: 0.07249458879232407| 

python LearningRateFunctionModifier [784 - 1706054387.967404]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.444444444444445| steps_per_epoch: 63| 
python LearningRateFunctionModifier [784 - 1706054387.9675527]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [784 - 1706054387.9676194]: 6.410256410256405e-06
python LearningRateFunctionModifier/ParamGroup1 [784 - 1706054387.967651]: 6.410256410256405e-06
python DistillationModifier/task_loss [784 - 1706054387.9676943]: tensor(0.1719, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [784 - 1706054387.9681768]: tensor(0.0725, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [784 - 1706054387.9684107]: tensor(0.0725, grad_fn=<AddBackward0>)
python ConstantPruningModifier [784 - 1706054387.9686744]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.444444444444445| steps_per_epoch: 63| 
python ConstantPruningModifier [784 - 1706054388.1438224]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [784 - 1706054388.144331]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [784 - 1706054388.1447875]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [784 - 1706054388.1450274]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [784 - 1706054388.145211]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [784 - 1706054388.1458647]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [784 - 1706054388.1463578]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [784 - 1706054388.1465843]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [784 - 1706054388.1468015]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [784 - 1706054388.1469624]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [784 - 1706054388.1471589]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [784 - 1706054388.147557]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [784 - 1706054388.1479707]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [784 - 1706054388.1481662]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [784 - 1706054388.1483502]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [784 - 1706054388.1485555]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [784 - 1706054388.148738]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [784 - 1706054388.1490984]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [784 - 1706054388.1494727]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [784 - 1706054388.1496763]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [784 - 1706054388.1498475]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [784 - 1706054388.1500313]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [784 - 1706054388.1501796]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [784 - 1706054388.1505837]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [784 - 1706054388.1509488]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [784 - 1706054388.1511254]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [784 - 1706054388.151275]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [784 - 1706054388.1514566]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [784 - 1706054388.1516638]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [784 - 1706054388.152034]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [784 - 1706054388.1523893]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [784 - 1706054388.1525872]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [784 - 1706054388.2167482]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [784 - 1706054388.2169943]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [784 - 1706054388.217173]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [784 - 1706054388.2176833]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [784 - 1706054388.2181516]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [784 - 1706054388.2183468]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [784 - 1706054388.218549]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [784 - 1706054388.2187812]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [784 - 1706054388.218958]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [784 - 1706054388.219402]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [784 - 1706054388.2198884]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [784 - 1706054388.2200975]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [784 - 1706054388.2202935]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [784 - 1706054388.220472]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [784 - 1706054388.2207267]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [784 - 1706054388.22119]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [784 - 1706054388.22165]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [784 - 1706054388.221836]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [784 - 1706054388.2220404]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [784 - 1706054388.2222366]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [784 - 1706054388.2224038]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [784 - 1706054388.2228675]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [784 - 1706054388.223265]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [784 - 1706054388.2234743]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [784 - 1706054388.2236986]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [784 - 1706054388.2238967]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [784 - 1706054388.2240684]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [784 - 1706054388.2245262]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [784 - 1706054388.2249851]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [784 - 1706054388.225174]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [784 - 1706054388.2253425]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [784 - 1706054388.225498]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [784 - 1706054388.2256813]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [784 - 1706054388.2261348]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [784 - 1706054388.2266011]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [784 - 1706054388.2268023]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [784 - 1706054388.227]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [784 - 1706054388.2271564]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [784 - 1706054388.227314]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [784 - 1706054388.227799]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [784 - 1706054388.228267]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [784 - 1706054388.2284553]: 0.0
python ParamPruning/classifier.weight [784 - 1706054388.228541]: 0.0
python DistillationModifier [791 - 1706054432.752329]: Calling loss_update with:
args: 0.013762562535703182| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.555555555555555| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [791 - 1706054434.2212367]: 
Returned: 0.05836816504597664| 

python LearningRateFunctionModifier [791 - 1706054436.8793564]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.555555555555555| steps_per_epoch: 63| 
python LearningRateFunctionModifier [791 - 1706054436.8795269]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [791 - 1706054436.8795965]: 5.128205128205135e-06
python LearningRateFunctionModifier/ParamGroup1 [791 - 1706054436.8796303]: 5.128205128205135e-06
python DistillationModifier/task_loss [791 - 1706054436.8796751]: tensor(0.0138, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [791 - 1706054436.8801587]: tensor(0.0584, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [791 - 1706054436.8804228]: tensor(0.0584, grad_fn=<AddBackward0>)
python ConstantPruningModifier [791 - 1706054436.8807158]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.555555555555555| steps_per_epoch: 63| 
python ConstantPruningModifier [791 - 1706054437.1368322]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [791 - 1706054437.1373458]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [791 - 1706054437.1376967]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [791 - 1706054437.1379297]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [791 - 1706054437.1381788]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [791 - 1706054437.1389496]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [791 - 1706054437.1395426]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [791 - 1706054437.139808]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [791 - 1706054437.140032]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [791 - 1706054437.140217]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [791 - 1706054437.1404066]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [791 - 1706054437.140973]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [791 - 1706054437.1415234]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [791 - 1706054437.1417518]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [791 - 1706054437.1419866]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [791 - 1706054437.142156]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [791 - 1706054437.1423202]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [791 - 1706054437.142856]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [791 - 1706054437.1433933]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [791 - 1706054437.1436057]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [791 - 1706054437.1438262]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [791 - 1706054437.1440036]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [791 - 1706054437.1442115]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [791 - 1706054437.1447575]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [791 - 1706054437.145317]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [791 - 1706054437.1455104]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [791 - 1706054437.1457276]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [791 - 1706054437.1459067]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [791 - 1706054437.1460862]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [791 - 1706054437.1466289]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [791 - 1706054437.147169]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [791 - 1706054437.1473653]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [791 - 1706054437.1476002]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [791 - 1706054437.147789]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [791 - 1706054437.1479678]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [791 - 1706054437.1484897]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [791 - 1706054437.1490686]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [791 - 1706054437.1492648]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [791 - 1706054437.149479]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [791 - 1706054437.1497548]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [791 - 1706054437.1499488]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [791 - 1706054437.1504738]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [791 - 1706054437.1510317]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [791 - 1706054437.1512272]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [791 - 1706054437.1514194]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [791 - 1706054437.151617]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [791 - 1706054437.151818]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [791 - 1706054437.152338]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [791 - 1706054437.216959]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [791 - 1706054437.2171886]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [791 - 1706054437.2173986]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [791 - 1706054437.2176116]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [791 - 1706054437.2178223]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [791 - 1706054437.218361]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [791 - 1706054437.2189248]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [791 - 1706054437.219136]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [791 - 1706054437.219331]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [791 - 1706054437.2195413]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [791 - 1706054437.2197623]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [791 - 1706054437.2202914]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [791 - 1706054437.220873]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [791 - 1706054437.221074]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [791 - 1706054437.221273]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [791 - 1706054437.2214487]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [791 - 1706054437.2216492]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [791 - 1706054437.2221732]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [791 - 1706054437.2227283]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [791 - 1706054437.222921]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [791 - 1706054437.223126]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [791 - 1706054437.2233121]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [791 - 1706054437.2234843]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [791 - 1706054437.2240353]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [791 - 1706054437.2246253]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [791 - 1706054437.224878]: 0.0
python ParamPruning/classifier.weight [791 - 1706054437.2249708]: 0.0
python DistillationModifier [798 - 1706054483.9186683]: Calling loss_update with:
args: 0.02961241640150547| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.666666666666666| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [798 - 1706054484.9416385]: 
Returned: 0.09257496148347855| 

python LearningRateFunctionModifier [798 - 1706054487.276971]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.666666666666666| steps_per_epoch: 63| 
python LearningRateFunctionModifier [798 - 1706054487.2771163]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [798 - 1706054487.2771623]: 3.8461538461538375e-06
python LearningRateFunctionModifier/ParamGroup1 [798 - 1706054487.2771933]: 3.8461538461538375e-06
python DistillationModifier/task_loss [798 - 1706054487.2772348]: tensor(0.0296, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [798 - 1706054487.2777312]: tensor(0.0926, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [798 - 1706054487.2779791]: tensor(0.0926, grad_fn=<AddBackward0>)
python ConstantPruningModifier [798 - 1706054487.278211]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.666666666666666| steps_per_epoch: 63| 
python ConstantPruningModifier [798 - 1706054487.5184927]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [798 - 1706054487.5190325]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [798 - 1706054487.5193312]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [798 - 1706054487.519614]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [798 - 1706054487.5198863]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [798 - 1706054487.520492]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [798 - 1706054487.5209506]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [798 - 1706054487.5211775]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [798 - 1706054487.5213695]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [798 - 1706054487.521527]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [798 - 1706054487.5217605]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [798 - 1706054487.5221398]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [798 - 1706054487.5225043]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [798 - 1706054487.5227304]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [798 - 1706054487.5229297]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [798 - 1706054487.5231156]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [798 - 1706054487.5232778]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [798 - 1706054487.5236824]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [798 - 1706054487.52406]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [798 - 1706054487.524238]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [798 - 1706054487.5243723]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [798 - 1706054487.5245059]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [798 - 1706054487.5247033]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [798 - 1706054487.5250869]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [798 - 1706054487.525441]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [798 - 1706054487.5256534]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [798 - 1706054487.5258598]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [798 - 1706054487.5260332]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [798 - 1706054487.5261922]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [798 - 1706054487.5265565]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [798 - 1706054487.52695]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [798 - 1706054487.5271232]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [798 - 1706054487.527291]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [798 - 1706054487.5274425]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [798 - 1706054487.5276124]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [798 - 1706054487.5280027]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [798 - 1706054487.5283594]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [798 - 1706054487.5285447]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [798 - 1706054487.528725]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [798 - 1706054487.5288727]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [798 - 1706054487.52901]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [798 - 1706054487.529352]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [798 - 1706054487.5297594]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [798 - 1706054487.5299597]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [798 - 1706054487.5301402]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [798 - 1706054487.5302973]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [798 - 1706054487.5304608]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [798 - 1706054487.5308716]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [798 - 1706054487.531235]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [798 - 1706054487.5314178]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [798 - 1706054487.531632]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [798 - 1706054487.5318327]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [798 - 1706054487.5320313]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [798 - 1706054487.5323906]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [798 - 1706054487.5327883]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [798 - 1706054487.5329823]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [798 - 1706054487.5331187]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [798 - 1706054487.5332546]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [798 - 1706054487.533394]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [798 - 1706054487.5337913]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [798 - 1706054487.5342011]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [798 - 1706054487.5343797]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [798 - 1706054487.5345535]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [798 - 1706054487.5347393]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [798 - 1706054487.5349336]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [798 - 1706054487.5352774]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [798 - 1706054487.535667]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [798 - 1706054487.5358498]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [798 - 1706054487.536016]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [798 - 1706054487.5361598]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [798 - 1706054487.536323]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [798 - 1706054487.536708]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [798 - 1706054487.537069]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [798 - 1706054487.5372443]: 0.0
python ParamPruning/classifier.weight [798 - 1706054487.537314]: 0.0
python DistillationModifier [805 - 1706054531.1502068]: Calling loss_update with:
args: 0.15608300268650055| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.777777777777779| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [805 - 1706054532.7475402]: 
Returned: 0.1683983951807022| 

python LearningRateFunctionModifier [805 - 1706054535.4224203]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.777777777777779| steps_per_epoch: 63| 
python LearningRateFunctionModifier [805 - 1706054535.422586]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [805 - 1706054535.4226348]: 2.5641025641025674e-06
python LearningRateFunctionModifier/ParamGroup1 [805 - 1706054535.4226665]: 2.5641025641025674e-06
python DistillationModifier/task_loss [805 - 1706054535.422709]: tensor(0.1561, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [805 - 1706054535.4231806]: tensor(0.1684, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [805 - 1706054535.4234219]: tensor(0.1684, grad_fn=<AddBackward0>)
python ConstantPruningModifier [805 - 1706054535.4236743]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.777777777777779| steps_per_epoch: 63| 
python ConstantPruningModifier [805 - 1706054535.6394734]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [805 - 1706054535.6400352]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [805 - 1706054535.6402614]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [805 - 1706054535.640506]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [805 - 1706054535.6407945]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [805 - 1706054535.6415997]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [805 - 1706054535.642215]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [805 - 1706054535.642455]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [805 - 1706054535.6427026]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [805 - 1706054535.6428938]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [805 - 1706054535.6430771]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [805 - 1706054535.6436265]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [805 - 1706054535.6441689]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [805 - 1706054535.644403]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [805 - 1706054535.6446395]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [805 - 1706054535.6448646]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [805 - 1706054535.6450443]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [805 - 1706054535.6456113]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [805 - 1706054535.6461573]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [805 - 1706054535.6463675]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [805 - 1706054535.6465335]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [805 - 1706054535.6467476]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [805 - 1706054535.6469536]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [805 - 1706054535.6474748]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [805 - 1706054535.6481075]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [805 - 1706054535.6483319]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [805 - 1706054535.6485322]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [805 - 1706054535.6487591]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [805 - 1706054535.6489985]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [805 - 1706054535.6495388]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [805 - 1706054535.650106]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [805 - 1706054535.6502995]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [805 - 1706054535.6505024]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [805 - 1706054535.6507044]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [805 - 1706054535.6508975]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [805 - 1706054535.6514206]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [805 - 1706054535.6519718]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [805 - 1706054535.6521666]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [805 - 1706054535.6523733]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [805 - 1706054535.7165563]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [805 - 1706054535.716936]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [805 - 1706054535.7175357]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [805 - 1706054535.7181082]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [805 - 1706054535.7183204]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [805 - 1706054535.7185214]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [805 - 1706054535.7187307]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [805 - 1706054535.7189136]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [805 - 1706054535.7194538]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [805 - 1706054535.7200146]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [805 - 1706054535.7202144]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [805 - 1706054535.72041]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [805 - 1706054535.7206533]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [805 - 1706054535.7208416]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [805 - 1706054535.7213736]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [805 - 1706054535.721928]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [805 - 1706054535.722132]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [805 - 1706054535.7223225]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [805 - 1706054535.7224996]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [805 - 1706054535.7227285]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [805 - 1706054535.7232742]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [805 - 1706054535.723828]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [805 - 1706054535.7240384]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [805 - 1706054535.724233]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [805 - 1706054535.724436]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [805 - 1706054535.7246518]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [805 - 1706054535.7251925]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [805 - 1706054535.7257378]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [805 - 1706054535.725939]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [805 - 1706054535.7261224]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [805 - 1706054535.7262998]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [805 - 1706054535.7265053]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [805 - 1706054535.7270646]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [805 - 1706054535.7276103]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [805 - 1706054535.7278025]: 0.0
python ParamPruning/classifier.weight [805 - 1706054535.727874]: 0.0
python DistillationModifier [812 - 1706054583.049076]: Calling loss_update with:
args: 0.012450852431356907| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.88888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [812 - 1706054584.1218953]: 
Returned: 0.06921395659446716| 

python LearningRateFunctionModifier [812 - 1706054586.4674985]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.88888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [812 - 1706054586.4676683]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [812 - 1706054586.4677143]: 1.2820512820512701e-06
python LearningRateFunctionModifier/ParamGroup1 [812 - 1706054586.4677463]: 1.2820512820512701e-06
python DistillationModifier/task_loss [812 - 1706054586.4677882]: tensor(0.0125, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [812 - 1706054586.4682717]: tensor(0.0692, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [812 - 1706054586.4685116]: tensor(0.0692, grad_fn=<AddBackward0>)
python ConstantPruningModifier [812 - 1706054586.468784]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.88888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [812 - 1706054586.6435893]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [812 - 1706054586.644136]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [812 - 1706054586.6444197]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [812 - 1706054586.64469]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [812 - 1706054586.6448863]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [812 - 1706054586.645497]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [812 - 1706054586.6461349]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [812 - 1706054586.6463647]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [812 - 1706054586.6465383]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [812 - 1706054586.646724]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [812 - 1706054586.6469269]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [812 - 1706054586.6473968]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [812 - 1706054586.6479495]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [812 - 1706054586.6481576]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [812 - 1706054586.6483538]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [812 - 1706054586.6485555]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [812 - 1706054586.6487584]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [812 - 1706054586.6491864]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [812 - 1706054586.6496434]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [812 - 1706054586.649829]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [812 - 1706054586.6500115]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [812 - 1706054586.6501625]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [812 - 1706054586.6503193]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [812 - 1706054586.6507723]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [812 - 1706054586.6511562]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [812 - 1706054586.6513453]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [812 - 1706054586.651499]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [812 - 1706054586.6516962]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [812 - 1706054586.6518536]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [812 - 1706054586.6522274]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [812 - 1706054586.7167363]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [812 - 1706054586.7169254]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [812 - 1706054586.7171094]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [812 - 1706054586.7172906]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [812 - 1706054586.7174702]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [812 - 1706054586.7179542]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [812 - 1706054586.7184103]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [812 - 1706054586.7186108]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [812 - 1706054586.7188098]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [812 - 1706054586.7189555]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [812 - 1706054586.7190979]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [812 - 1706054586.7195048]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [812 - 1706054586.719949]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [812 - 1706054586.7201326]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [812 - 1706054586.7203329]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [812 - 1706054586.7204869]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [812 - 1706054586.7207193]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [812 - 1706054586.7211092]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [812 - 1706054586.721628]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [812 - 1706054586.7218087]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [812 - 1706054586.721997]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [812 - 1706054586.7221482]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [812 - 1706054586.7222998]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [812 - 1706054586.722715]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [812 - 1706054586.723165]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [812 - 1706054586.7233458]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [812 - 1706054586.723486]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [812 - 1706054586.723694]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [812 - 1706054586.7238467]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [812 - 1706054586.7242496]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [812 - 1706054586.724725]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [812 - 1706054586.7249053]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [812 - 1706054586.7250962]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [812 - 1706054586.7252934]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [812 - 1706054586.7254426]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [812 - 1706054586.7258828]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [812 - 1706054586.7263167]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [812 - 1706054586.7264943]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [812 - 1706054586.7266974]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [812 - 1706054586.726889]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [812 - 1706054586.7270362]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [812 - 1706054586.7274482]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [812 - 1706054586.7278743]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [812 - 1706054586.72806]: 0.0
python ParamPruning/classifier.weight [812 - 1706054586.728131]: 0.0
python DistillationModifier [819 - 1706054626.4397442]: Calling loss_update with:
args: 0.6597836017608643| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054627.5404243]: 
Returned: 1.5583689212799072| 

python DistillationModifier [819 - 1706054629.4187388]: Calling loss_update with:
args: 1.3428419828414917| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054630.4398618]: 
Returned: 2.169893980026245| 

python DistillationModifier [819 - 1706054632.3198743]: Calling loss_update with:
args: 1.3316599130630493| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054633.4522362]: 
Returned: 1.5631011724472046| 

python DistillationModifier [819 - 1706054635.9482327]: Calling loss_update with:
args: 0.8906235098838806| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054637.5474515]: 
Returned: 1.092529535293579| 

python DistillationModifier [819 - 1706054639.5505004]: Calling loss_update with:
args: 0.8189568519592285| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054641.1221387]: 
Returned: 1.473924994468689| 

python DistillationModifier [819 - 1706054642.9479024]: Calling loss_update with:
args: 0.8609796166419983| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054643.951006]: 
Returned: 1.6051405668258667| 

python DistillationModifier [819 - 1706054645.7508008]: Calling loss_update with:
args: 0.3019631803035736| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054646.82436]: 
Returned: 0.8299151062965393| 

python DistillationModifier [819 - 1706054649.0363855]: Calling loss_update with:
args: 1.1826072931289673| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054650.1463537]: 
Returned: 1.149714708328247| 

python DistillationModifier [819 - 1706054651.9415512]: Calling loss_update with:
args: 0.9409550428390503| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054653.0183616]: 
Returned: 0.823859453201294| 

python DistillationModifier [819 - 1706054654.7500527]: Calling loss_update with:
args: 0.9562034606933594| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054655.9294543]: 
Returned: 1.5784542560577393| 

python DistillationModifier [819 - 1706054659.019309]: Calling loss_update with:
args: 0.8673487901687622| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054660.6172566]: 
Returned: 1.3679141998291016| 

python DistillationModifier [819 - 1706054662.6186]: Calling loss_update with:
args: 1.1410592794418335| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054663.6370754]: 
Returned: 1.9067329168319702| 

python DistillationModifier [819 - 1706054666.7443354]: Calling loss_update with:
args: 1.0542898178100586| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054668.3359842]: 
Returned: 2.0591518878936768| 

python DistillationModifier [819 - 1706054671.1215112]: Calling loss_update with:
args: 0.6845129132270813| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054672.5411952]: 
Returned: 0.8795817494392395| 

python DistillationModifier [819 - 1706054675.6229327]: Calling loss_update with:
args: 1.3401603698730469| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054676.8306165]: 
Returned: 2.3203227519989014| 

python DistillationModifier [819 - 1706054678.9389932]: Calling loss_update with:
args: 0.788311779499054| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054680.5345595]: 
Returned: 1.3761396408081055| 

python DistillationModifier [819 - 1706054682.6414185]: Calling loss_update with:
args: 0.5663915872573853| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054684.1248474]: 
Returned: 0.7234324812889099| 

python DistillationModifier [819 - 1706054686.3422043]: Calling loss_update with:
args: 1.1410642862319946| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054687.4365103]: 
Returned: 1.963386058807373| 

python DistillationModifier [819 - 1706054689.3297148]: Calling loss_update with:
args: 0.9597361087799072| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054690.426419]: 
Returned: 1.202218770980835| 

python DistillationModifier [819 - 1706054693.5254505]: Calling loss_update with:
args: 0.7640023827552795| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054695.1375048]: 
Returned: 1.2371602058410645| 

python DistillationModifier [819 - 1706054698.3398516]: Calling loss_update with:
args: 0.6089814901351929| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054699.939398]: 
Returned: 0.9954318404197693| 

python DistillationModifier [819 - 1706054702.8226552]: Calling loss_update with:
args: 0.7230445742607117| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054703.8274143]: 
Returned: 0.9487250447273254| 

python DistillationModifier [819 - 1706054706.5505214]: Calling loss_update with:
args: 0.7884038090705872| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054707.6236882]: 
Returned: 1.2056355476379395| 

python DistillationModifier [819 - 1706054710.5191207]: Calling loss_update with:
args: 0.5618526935577393| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054711.8434155]: 
Returned: 1.232413411140442| 

python DistillationModifier [819 - 1706054713.7232513]: Calling loss_update with:
args: 1.1745532751083374| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054714.734433]: 
Returned: 1.2652322053909302| 

python DistillationModifier [819 - 1706054717.1492789]: Calling loss_update with:
args: 0.3169337809085846| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054718.2501907]: 
Returned: 0.6194257736206055| 

python DistillationModifier [819 - 1706054720.6414146]: Calling loss_update with:
args: 0.7472167611122131| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054722.22389]: 
Returned: 0.9431769847869873| 

python DistillationModifier [819 - 1706054725.3195844]: Calling loss_update with:
args: 0.7758584022521973| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054726.8257968]: 
Returned: 1.1456491947174072| 

python DistillationModifier [819 - 1706054728.6281447]: Calling loss_update with:
args: 0.42278555035591125| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054729.6412015]: 
Returned: 1.0194365978240967| 

python DistillationModifier [819 - 1706054731.6331968]: Calling loss_update with:
args: 0.7812870740890503| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054732.8360531]: 
Returned: 1.5121922492980957| 

python DistillationModifier [819 - 1706054734.6198833]: Calling loss_update with:
args: 1.121656894683838| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054735.6315324]: 
Returned: 2.087355613708496| 

python DistillationModifier [819 - 1706054737.4392464]: Calling loss_update with:
args: 0.9310457706451416| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054738.5250337]: 
Returned: 1.1110657453536987| 

python DistillationModifier [819 - 1706054741.4196784]: Calling loss_update with:
args: 1.0341421365737915| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054743.0241745]: 
Returned: 1.7401598691940308| 

python DistillationModifier [819 - 1706054745.3435152]: Calling loss_update with:
args: 1.324658989906311| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054746.4186163]: 
Returned: 2.055736780166626| 

python DistillationModifier [819 - 1706054748.5528734]: Calling loss_update with:
args: 1.477964162826538| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054749.628975]: 
Returned: 1.887083888053894| 

python DistillationModifier [819 - 1706054751.4321375]: Calling loss_update with:
args: 1.2395260334014893| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054752.443627]: 
Returned: 2.240867853164673| 

python DistillationModifier [819 - 1706054754.733854]: Calling loss_update with:
args: 0.770123302936554| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054755.830736]: 
Returned: 0.9193485975265503| 

python DistillationModifier [819 - 1706054758.119713]: Calling loss_update with:
args: 0.7786812782287598| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054759.7230496]: 
Returned: 1.155955195426941| 

python DistillationModifier [819 - 1706054762.5265377]: Calling loss_update with:
args: 0.9265763163566589| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054763.6303508]: 
Returned: 1.7986379861831665| 

python DistillationModifier [819 - 1706054765.546491]: Calling loss_update with:
args: 1.3315924406051636| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054766.6202285]: 
Returned: 2.3347692489624023| 

python DistillationModifier [819 - 1706054768.4504993]: Calling loss_update with:
args: 0.5212090015411377| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054770.017924]: 
Returned: 1.0611003637313843| 

python DistillationModifier [819 - 1706054772.7466063]: Calling loss_update with:
args: 0.7489845156669617| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054774.0367787]: 
Returned: 1.5752745866775513| 

python DistillationModifier [819 - 1706054776.139623]: Calling loss_update with:
args: 1.1030867099761963| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054777.74048]: 
Returned: 1.020797610282898| 

python DistillationModifier [819 - 1706054780.8424163]: Calling loss_update with:
args: 0.6151940822601318| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054782.4288278]: 
Returned: 1.4125123023986816| 

python DistillationModifier [819 - 1706054785.5233948]: Calling loss_update with:
args: 0.932843804359436| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054787.1238768]: 
Returned: 2.108952522277832| 

python DistillationModifier [819 - 1706054790.145927]: Calling loss_update with:
args: 1.2087996006011963| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054791.7408102]: 
Returned: 1.5131609439849854| 

python DistillationModifier [819 - 1706054794.8383698]: Calling loss_update with:
args: 0.8191874623298645| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054796.4243543]: 
Returned: 1.3583059310913086| 

python DistillationModifier [819 - 1706054799.518709]: Calling loss_update with:
args: 0.20337824523448944| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054801.0477862]: 
Returned: 0.5425291061401367| 

python DistillationModifier [819 - 1706054804.0340228]: Calling loss_update with:
args: 0.9686496257781982| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054805.0404556]: 
Returned: 1.8929463624954224| 

python DistillationModifier [819 - 1706054806.8287013]: Calling loss_update with:
args: 0.668471097946167| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054807.9278467]: 
Returned: 1.244883418083191| 

python DistillationModifier [819 - 1706054809.7372787]: Calling loss_update with:
args: 0.30781179666519165| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054810.7495968]: 
Returned: 0.6868748664855957| 

python DistillationModifier [819 - 1706054812.6383777]: Calling loss_update with:
args: 1.8771470785140991| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054813.9485493]: 
Returned: 1.8455551862716675| 

python DistillationModifier [819 - 1706054817.121466]: Calling loss_update with:
args: 0.8077177405357361| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054818.2211466]: 
Returned: 1.0564212799072266| 

python DistillationModifier [819 - 1706054820.8533971]: Calling loss_update with:
args: 0.7164350748062134| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054822.0239668]: 
Returned: 1.1251585483551025| 

python DistillationModifier [819 - 1706054824.9463513]: Calling loss_update with:
args: 0.6312749981880188| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054826.0247283]: 
Returned: 1.355560541152954| 

python DistillationModifier [819 - 1706054827.8212724]: Calling loss_update with:
args: 0.95915287733078| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054829.2396739]: 
Returned: 1.3958849906921387| 

python DistillationModifier [819 - 1706054832.3415313]: Calling loss_update with:
args: 0.9971365332603455| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054833.9245503]: 
Returned: 0.9303310513496399| 

python DistillationModifier [819 - 1706054837.0179563]: Calling loss_update with:
args: 1.0695505142211914| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054838.6221502]: 
Returned: 1.1495227813720703| 

python DistillationModifier [819 - 1706054841.7198093]: Calling loss_update with:
args: 1.073197841644287| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054843.3298812]: 
Returned: 1.3663960695266724| 

python DistillationModifier [819 - 1706054846.438479]: Calling loss_update with:
args: 0.6775028109550476| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054848.035376]: 
Returned: 1.3181732892990112| 

python DistillationModifier [819 - 1706054851.118046]: Calling loss_update with:
args: 0.919075608253479| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054852.7223294]: 
Returned: 1.5219330787658691| 

python DistillationModifier [819 - 1706054855.7472005]: Calling loss_update with:
args: 0.9070623517036438| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054857.323086]: 
Returned: 1.8956727981567383| 

python DistillationModifier [819 - 1706054859.1201618]: Calling loss_update with:
args: 0.514081597328186| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1706054859.951318]: 
Returned: 1.0585514307022095| 

