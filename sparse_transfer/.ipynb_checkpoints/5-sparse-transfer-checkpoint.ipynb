{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2308c9f9-6120-4892-a3d7-fa6b9b6af3a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Multi-Sequence Text Classification: Sparse Transfer Learning with the Python API**\n",
    "\n",
    "In this example, you will fine-tune a 90% pruned BERT model onto the QQP dataset (a multi-sequence binary classification problem) using SparseML's Hugging Face Integration.\n",
    "\n",
    "### **Sparse Transfer Learning Overview**\n",
    "\n",
    "Sparse Transfer Learning is very similiar to typical fine-tuning you are used to when training models. However, with Sparse Transfer Learning, we start the training process from a pre-sparsified checkpoint and maintain the sparsity structure while the fine tuning occurs.\n",
    "\n",
    "At the end, you will have a sparse model trained on your dataset, ready to be deployed with DeepSparse for GPU-class performance on CPUs!\n",
    "\n",
    "### **Pre-Sparsified BERT**\n",
    "SparseZoo, Neural Magic's open source repository of pre-sparsified models, contains a 90% pruned version of BERT, which has been sparsified on the upstream Wikipedia and BookCorpus datasets with the\n",
    "masked language modeling objective. [Check out the model card](https://sparsezoo.neuralmagic.com/models/nlp%2Fmasked_language_modeling%2Fobert-base%2Fpytorch%2Fhuggingface%2Fwikipedia_bookcorpus%2Fpruned90-none). We will use this model as the starting point for the transfer learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844fc29-3c56-4c3f-a069-5b91b1f7912e",
   "metadata": {},
   "source": [
    "## **Installation**\n",
    "\n",
    "Install SparseML via `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92fa8638-2730-4eab-982c-d7d0b5300c92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install sparseml[transformers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcfa6f1f-a1cc-4fa1-9c53-a65301119a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sparseml\n",
    "from sparsezoo import Model\n",
    "from sparseml.transformers.utils import SparseAutoModel\n",
    "from sparseml.transformers.sparsification import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig, \n",
    "    AutoTokenizer, \n",
    "    EvalPrediction, \n",
    "    default_data_collator\n",
    ")\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1875892-4f06-4a93-a052-46bca7294930",
   "metadata": {},
   "source": [
    "## **Step 1: Load a Dataset**\n",
    "\n",
    "SparseML is integrated with Hugging Face, so we can use the `datasets` class to load datasets from the Hugging Face hub or from local files. \n",
    "\n",
    "[QQP Dataset Card](https://huggingface.co/datasets/glue/viewer/qqp/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e649c2a-0544-4084-96cc-fabaf8460e98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5428b6019b4a27826268d09b88cd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/31.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4ae84af9d742179e1042acd97a43cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cdd972a89c45d2bc299f02215656c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/33.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33b252bd45e4995a1e977093f1d2f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6156af34e78d4d03ba4d904067f20c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/36.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7630c8699a341508262308e7c4a84cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dac83694afd40089464e76f166132fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/363846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7fbab14d4f4ed4a8720e09a0bc3cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/40430 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c41f31c3c244daa563b9d65f22959e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/390965 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0728bd78a76c433d86fcdb91c9be75fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/364 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6d13fa47264ba680eee511f4e7114c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/41 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e338209483b48628d49372bad2d79fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc243a7199034d4299e3e590aea595b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f960929adeb49cbba70441aa95ae8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87042d07b49440ebb074bd9a4ceb16e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset from HF Hub\n",
    "dataset = load_dataset(\"glue\", \"qqp\")\n",
    "\n",
    "# alternatively, load from local CSV files\n",
    "dataset[\"train\"].to_csv(\"qqp-train.csv\")\n",
    "dataset[\"validation\"].to_csv(\"qqp-validation.csv\")\n",
    "data_files = {\n",
    "  \"train\": \"qqp-train.csv\",\n",
    "  \"validation\": \"qqp-validation.csv\"\n",
    "}\n",
    "dataset_from_json = load_dataset(\"csv\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89fe7a1e-7d36-4d4d-bf92-9fe3ef92533a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question1', 'question2', 'label', 'idx'],\n",
      "    num_rows: 363846\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_from_json[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd268201-a5fd-4fb3-9946-70a225b5178a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question1,question2,label,idx\n",
      "How is the life of a math student? Could you describe your own experiences?,Which level of prepration is enough for the exam jlpt5?,0,0\n",
      "How do I control my horny emotions?,How do you control your horniness?,1,1\n",
      "What causes stool color to change to yellow?,What can cause stool to come out as little balls?,0,2\n",
      "What can one do after MBBS?,What do i do after my MBBS ?,1,3\n"
     ]
    }
   ],
   "source": [
    "!head qqp-train.csv --lines=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7acd569-11bf-45e7-994f-1ba7ac924cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# configs for below\n",
    "INPUT_COL_1 = \"question1\"\n",
    "INPUT_COL_2 = \"question2\"\n",
    "LABEL_COL = \"label\"\n",
    "NUM_LABELS = len(dataset_from_json[\"train\"].unique(LABEL_COL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b703b3-6654-4007-9e21-c09d5a23972a",
   "metadata": {},
   "source": [
    "## **Step 2: Setup Evaluation Metric**\n",
    "\n",
    "QQP is a multi-input binary classification problem where we predict one of two class labels (duplicat, not duplicate) for each input pair. We will use the `accuracy` metric as the evaluation metric. \n",
    "\n",
    "Since SparseML is integrated with Hugging Face, we can pass `compute_metrics` function for evaluation (which will be passed to the `Trainer` class below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c42e20c0-0658-4c50-bad3-6410959a0a75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_3348/824195765.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", \"qqp\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52b2d9daf1448338f46245b177e2841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setup dataset and tokenize\n",
    "metric = load_metric(\"glue\", \"qqp\")\n",
    "\n",
    "# setup metrics function\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "  preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "  preds = np.argmax(preds, axis=1)\n",
    "  result = metric.compute(predictions=preds, references=p.label_ids)\n",
    "  if len(result) > 1:\n",
    "      result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d62395-e4df-4a5c-9075-1a3edb672e21",
   "metadata": {},
   "source": [
    "## **Step 3: Download Files for Sparse Transfer Learning**\n",
    "\n",
    "First, we need to select a sparse checkpoint to begin the training process. In this case, we will fine-tune a 90% pruned version of BERT onto the QQP dataset. This model is available in SparseZoo, identified by the following stub:\n",
    "```\n",
    "zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none\n",
    "```\n",
    "\n",
    "Next, we need to create a sparsification recipe for usage in the training process. Recipes are YAML files that encode the sparsity related algorithms and parameters to be applied by SparseML. For Sparse Transfer Learning, we need to use a recipe that instructs SparseML to maintain sparsity during the training process and to apply quantization over the final few epochs. \n",
    "\n",
    "In the case of QQP, there is a transfer learning recipe available in the SparseZoo, identified by the following stub:\n",
    "\n",
    "```\n",
    "zoo:nlp/text_classification/obert-base/pytorch/huggingface/qqp/pruned90_quant-none\n",
    "```\n",
    "\n",
    "Finally, SparseML has the optional ability to apply model distillation from a teacher model during the transfer learning process to boost accuracy. In this case, we will use a dense version of BERT trained on the QQP dataset which is hosted in SparseZoo. This model is identified by the following stub:\n",
    "\n",
    "```\n",
    "zoo:nlp/text_classification/obert-base/pytorch/huggingface/qqp/base-none\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadca82c-787a-4672-a6ae-449227e676aa",
   "metadata": {},
   "source": [
    "Use the `sparsezoo` python client to download the models and recipe using their SparseZoo stubs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e361eb79-f244-4935-9ecb-6d0b8f3d286c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b718041775fa4b6f8e7c8c6b24325b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c542ecaeb4ed46eb9beb9380deeed0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l/training/vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1caa5777082f4561a8bbe7ac0a09fd0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ng/pytorch_model.bin:   0%|          | 0.00/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1e3eeea8b349e8b7d1f1378957d341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ng/eval_results.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27eb803d618a4f318911cd8eb3f44b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)g/train_results.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41be61131d024372a2553240fb1b72d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d305d4656a479b87c504353dc3d416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)training/config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a054c2b5db2c449d83024d84574da3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ining/tokenizer.json:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f0eee0fd5c4b1cbffc3114ac38f8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ng/training_args.bin:   0%|          | 0.00/2.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbd543b4fa24cb2bf8f75ca13c9ade1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)g/trainer_state.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c135e33ceb8940cc8da5e95564d153af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ing/all_results.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45de11b8459a47469e61858a9602674f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)training/config.json:   0%|          | 0.00/701 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2140014e5e4fe4b9e71020672caa16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lidation-metric.yaml:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c368ffe706a4f8b9680d9e59300a8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42915f91dc04c949aad0f063555114a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ining/tokenizer.json:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05d010d21b9491db8652a34e8c1efbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ing/eval_results.txt:   0%|          | 0.00/27.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f43bf4552e41f596b6e2ff2c7c0549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)r/training/vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58a52aa91874a95af4fdfd88da17486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68fa988de4714eeebcf74dcc3c44f1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ng/pytorch_model.bin:   0%|          | 0.00/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1816a019ba4e4d0980ab6a25301dd115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fer_recipe/recipe.md:   0%|          | 0.00/3.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# download 90% pruned upstream BERT trained on MLM objective\n",
    "model_stub = \"zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none\"\n",
    "model_path = Model(model_stub, download_path=\"./model\").training.path \n",
    "\n",
    "# download dense BERT trained on QQP dataset\n",
    "teacher_stub = \"zoo:nlp/text_classification/obert-base/pytorch/huggingface/qqp/base-none\"\n",
    "teacher_path = Model(teacher_stub, download_path=\"./teacher\").training.path \n",
    "\n",
    "# download transfer recipe for QQP\n",
    "transfer_stub = \"zoo:nlp/text_classification/obert-base/pytorch/huggingface/qqp/pruned90_quant-none\"\n",
    "recipe_path = Model(transfer_stub, download_path=\"./transfer_recipe\").recipes.default.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05679eaf-d86c-45fd-abf8-614b1a60769f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_results.json   special_tokens_map.json  trainer_state.json\n",
      "config.json        tokenizer.json           training_args.bin\n",
      "eval_results.json  tokenizer_config.json    vocab.txt\n",
      "pytorch_model.bin  train_results.json\n"
     ]
    }
   ],
   "source": [
    "%ls ./model/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d74aecb0-ffc0-405c-b01d-3bb4cf5c4b88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!--\n",
      "Copyright (c) 2021 - present / Neuralmagic, Inc. All Rights Reserved.\n",
      "\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "   http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing,\n",
      "software distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "-->\n",
      "\n",
      "---\n",
      "\n",
      "version: 1.1.0\n",
      "\n",
      "# General Variables\n",
      "num_epochs: &num_epochs 13\n",
      "init_lr: 1.5e-4 \n",
      "final_lr: 0\n",
      "\n",
      "qat_start_epoch: &qat_start_epoch 8.0\n",
      "observer_epoch: &observer_epoch 12.0\n",
      "quantize_embeddings: &quantize_embeddings 1\n",
      "\n",
      "distill_hardness: &distill_hardness 1.0\n",
      "distill_temperature: &distill_temperature 2.0\n",
      "\n",
      "# Modifiers:\n",
      "\n",
      "training_modifiers:\n",
      "  - !EpochRangeModifier\n",
      "      end_epoch: eval(num_epochs)\n",
      "      start_epoch: 0.0\n",
      "\n",
      "  - !LearningRateFunctionModifier\n",
      "      start_epoch: 0\n",
      "      end_epoch: eval(num_epochs)\n",
      "      lr_func: linear\n",
      "      init_lr: eval(init_lr)\n",
      "      final_lr: eval(final_lr)\n",
      "    \n",
      "quantization_modifiers:\n",
      "\n",
      "  - !QuantizationModifier\n",
      "      start_epoch: eval(qat_start_epoch)\n",
      "      disable_quantization_observer_epoch: eval(observer_epoch)\n",
      "      freeze_bn_stats_epoch: eval(observer_epoch)\n",
      "      quantize_embeddings: eval(quantize_embeddings)\n",
      "      quantize_linear_activations: 0\n",
      "      exclude_module_types: ['LayerNorm', 'Tanh']\n",
      "      submodules:\n",
      "        - bert.embeddings\n",
      "        - bert.encoder\n",
      "        - bert.pooler\n",
      "        - classifier\n",
      "\n",
      "\n",
      "distillation_modifiers:\n",
      "  - !DistillationModifier\n",
      "     hardness: eval(distill_hardness)\n",
      "     temperature: eval(distill_temperature)\n",
      "     distill_output_keys: [logits]\n",
      "\n",
      "constant_modifiers:\n",
      "\n",
      "  - !ConstantPruningModifier\n",
      "      start_epoch: 0.0\n",
      "      params: __ALL_PRUNABLE__\n",
      "\n",
      "---\n",
      "\n",
      "# 90% Pruned Quantized oBERT base uncased\n",
      "\n",
      "This model is the result of transferring and quantizing a pruned 90% oBERT base uncased model for text classification on QQP.\n",
      "\n",
      "This model is from [The Optimal BERT Surgeon](https://arxiv.org/abs/2203.07259) paper.\n",
      "\n",
      "# Training command\n",
      "\n",
      "```bash\n",
      "sparseml.transformers.train.text_classification \\\n",
      "  --model_name_or_path zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none \\\n",
      "  --distill_teacher zoo:nlp/text_classification/obert-base/pytorch/huggingface/qqp/base-none \\\n",
      "  --task_name qqp \\\n",
      "  --do_train \\\n",
      "  --do_eval \\\n",
      "  --evaluation_strategy epoch \\\n",
      "  --logging_steps 1000 \\\n",
      "  --save_steps 1000 \\\n",
      "  --per_device_train_batch_size 32 \\\n",
      "  --per_device_eval_batch_size 32 \\\n",
      "  --max_seq_length 128 \\\n",
      "  --recipe zoo:nlp/text_classification/obert-base/pytorch/huggingface/qqp/pruned90_quant-none \\\n",
      "  --output_dir obert_base_pruned90_quant_qqp \\\n",
      "  --preprocessing_num_workers 32 \\\n",
      "  --seed 10194\n",
      "```\n",
      "\n",
      "## Evaluation\n",
      "\n",
      "The model could be evaluated with the following command:\n",
      "\n",
      "```bash\n",
      "sparseml.transformers.train.text_classification \\\n",
      "  --output_dir dense_bert-text_classification_qqp_eval \\\n",
      "  --model_name_or_path zoo:nlp/text_classification/obert-base/pytorch/huggingface/qqp/pruned90_quant-none \\\n",
      "  --task_name qqp --max_seq_length 128 --per_device_eval_batch_size 32 --preprocessing_num_workers 6 \\\n",
      "  --do_eval \n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cat ./transfer_recipe/recipe.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4510bfa4-f6d6-4d8f-a1f3-90a8c775d5df",
   "metadata": {},
   "source": [
    "## **Step 4: Setup Hugging Face Model Objects**\n",
    "\n",
    "Next, we will set up the Hugging Face `tokenizer`, `config`, and `model`. \n",
    "\n",
    "These are all native Hugging Face objects, so check out the Hugging Face docs for more details on `AutoModel`, `AutoConfig`, and `AutoTokenizer` as needed. \n",
    "\n",
    "We instantiate these classes by passing the local path to the directory containing the `pytorch_model.bin`, `tokenizer.json`, and `config.json` files from the SparseZoo download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74e35e74-2d8d-4151-9bba-a918eb611c42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./model/training and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-01-21 11:43:04 sparseml.transformers.utils.model INFO     Loaded student from ./model/training with 109483778 total params. Of those there are 85526016 prunable params which have 89.3777046740959 avg sparsity.\n",
      "2024-01-21 11:43:04 sparseml.transformers.utils.model INFO     sparse model detected, all sparsification info: {\"params_summary\": {\"total\": 109483778, \"sparse\": 76441960, \"sparsity_percent\": 69.82035274668728, \"prunable\": 85526016, \"prunable_sparse\": 76441190, \"prunable_sparsity_percent\": 89.3777046740959, \"quantizable\": 85609730, \"quantized\": 0, \"quantized_percent\": 0.0}, \"params_info\": {\"bert.encoder.layer.0.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.8644917607307434, \"quantized\": false}, \"bert.encoder.layer.0.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.8680216670036316, \"quantized\": false}, \"bert.encoder.layer.0.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.9312150478363037, \"quantized\": false}, \"bert.encoder.layer.0.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.9232262372970581, \"quantized\": false}, \"bert.encoder.layer.0.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.9153103232383728, \"quantized\": false}, \"bert.encoder.layer.0.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.9356380105018616, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.8620554804801941, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.8625064492225647, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.9279242753982544, \"quantized\": false}, \"bert.encoder.layer.1.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.9245097041130066, \"quantized\": false}, \"bert.encoder.layer.1.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.8945091962814331, \"quantized\": false}, \"bert.encoder.layer.1.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.9265751242637634, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.8451063632965088, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.8532799482345581, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.9295670986175537, \"quantized\": false}, \"bert.encoder.layer.2.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.9288228154182434, \"quantized\": false}, \"bert.encoder.layer.2.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.8895581364631653, \"quantized\": false}, \"bert.encoder.layer.2.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.9237624406814575, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.8711106777191162, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.8704121708869934, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.9085676670074463, \"quantized\": false}, \"bert.encoder.layer.3.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.9130028486251831, \"quantized\": false}, \"bert.encoder.layer.3.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.8868213295936584, \"quantized\": false}, \"bert.encoder.layer.3.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.9209082126617432, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.8635711669921875, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.8665059208869934, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.8824039101600647, \"quantized\": false}, \"bert.encoder.layer.4.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.8957400918006897, \"quantized\": false}, \"bert.encoder.layer.4.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.8822059631347656, \"quantized\": false}, \"bert.encoder.layer.4.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.9172935485839844, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.8685166835784912, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.8675944209098816, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.8843333125114441, \"quantized\": false}, \"bert.encoder.layer.5.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.8958756923675537, \"quantized\": false}, \"bert.encoder.layer.5.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.8838331699371338, \"quantized\": false}, \"bert.encoder.layer.5.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.91917884349823, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.8699256181716919, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.8717482089996338, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.8924729824066162, \"quantized\": false}, \"bert.encoder.layer.6.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.9078572392463684, \"quantized\": false}, \"bert.encoder.layer.6.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.8827946782112122, \"quantized\": false}, \"bert.encoder.layer.6.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.9222526550292969, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.8792538046836853, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.8780839443206787, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.8773871660232544, \"quantized\": false}, \"bert.encoder.layer.7.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.8886498212814331, \"quantized\": false}, \"bert.encoder.layer.7.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.8991622924804688, \"quantized\": false}, \"bert.encoder.layer.7.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.9271066188812256, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.8583950400352478, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.856842041015625, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.8692152500152588, \"quantized\": false}, \"bert.encoder.layer.8.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.8843451738357544, \"quantized\": false}, \"bert.encoder.layer.8.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.9019758701324463, \"quantized\": false}, \"bert.encoder.layer.8.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.9253442287445068, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.8518574833869934, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.8523983359336853, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.8783705234527588, \"quantized\": false}, \"bert.encoder.layer.9.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.8867306113243103, \"quantized\": false}, \"bert.encoder.layer.9.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.9011484980583191, \"quantized\": false}, \"bert.encoder.layer.9.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.91860032081604, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.8570064902305603, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.8588087558746338, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.8733994960784912, \"quantized\": false}, \"bert.encoder.layer.10.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.8768836259841919, \"quantized\": false}, \"bert.encoder.layer.10.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.9072990417480469, \"quantized\": false}, \"bert.encoder.layer.10.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.9249801635742188, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.8541886806488037, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.8596123456954956, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.8792198896408081, \"quantized\": false}, \"bert.encoder.layer.11.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.8855014443397522, \"quantized\": false}, \"bert.encoder.layer.11.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.8986706137657166, \"quantized\": false}, \"bert.encoder.layer.11.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.9309417009353638, \"quantized\": false}, \"bert.pooler.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"classifier.weight\": {\"numel\": 1536, \"sparsity\": 0.0, \"quantized\": false}}}\n",
      "2024-01-21 11:43:05 sparseml.transformers.utils.model INFO     Loaded teacher from ./teacher/training with 109483778 total params. Of those there are 85526016 prunable params which have 0.0 avg sparsity.\n",
      "2024-01-21 11:43:06 sparseml.transformers.utils.model INFO     dense model detected, all sparsification info: {\"params_summary\": {\"total\": 109483778, \"sparse\": 0, \"sparsity_percent\": 0.0, \"prunable\": 85526016, \"prunable_sparse\": 0, \"prunable_sparsity_percent\": 0.0, \"quantizable\": 85609730, \"quantized\": 0, \"quantized_percent\": 0.0}, \"params_info\": {\"bert.encoder.layer.0.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.pooler.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"classifier.weight\": {\"numel\": 1536, \"sparsity\": 0.0, \"quantized\": false}}}\n"
     ]
    }
   ],
   "source": [
    "# shared tokenizer between teacher and student\n",
    "# see examples for how to use models with different tokenizers\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# setup configs\n",
    "model_config = AutoConfig.from_pretrained(model_path, num_labels=NUM_LABELS)\n",
    "teacher_config = AutoConfig.from_pretrained(teacher_path, num_labels=NUM_LABELS)\n",
    "\n",
    "# initialize model using familiar HF AutoModel\n",
    "model_kwargs = {\"config\": model_config}\n",
    "model_kwargs[\"state_dict\"], s_delayed = SparseAutoModel._loadable_state_dict(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, **model_kwargs,)\n",
    "SparseAutoModel.log_model_load(model, model_path, \"student\", s_delayed) # prints metrics on sparsity profile\n",
    "\n",
    "# initialize teacher using familiar HF AutoModel\n",
    "teacher_kwargs = {\"config\": teacher_config}\n",
    "teacher_kwargs[\"state_dict\"], t_delayed = SparseAutoModel._loadable_state_dict(teacher_path)\n",
    "teacher = AutoModelForSequenceClassification.from_pretrained(teacher_path, **teacher_kwargs,)\n",
    "SparseAutoModel.log_model_load(teacher, teacher_path, \"teacher\", t_delayed) # prints metrics on sparsity profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defdced4-5e01-4b25-9c2c-86b5f2980c76",
   "metadata": {},
   "source": [
    "## **Step 5: Tokenize Dataset**\n",
    "\n",
    "Run the tokenizer on the dataset. This is standard Hugging Face functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4d684b2-c7af-4b93-b79a-cd3c053e5b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8c1ae32c5b42e1bf7140da4fa121de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/363846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270cee376a554581b47468d72f54a4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/40430 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "def preprocess_fn(examples):\n",
    "  args = None\n",
    "  if INPUT_COL_2 is None:\n",
    "    args = (examples[INPUT_COL_1], )\n",
    "  else:\n",
    "    args = (examples[INPUT_COL_1], examples[INPUT_COL_2])\n",
    "  result = tokenizer(*args, \n",
    "                   padding=\"max_length\", \n",
    "                   max_length=min(tokenizer.model_max_length, 128), \n",
    "                   truncation=True)\n",
    "  return result\n",
    "\n",
    "# tokenize the dataset\n",
    "tokenized_dataset = dataset_from_json.map(\n",
    "    preprocess_fn,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7948f845-b948-4dbf-85fc-58e3fba5ba23",
   "metadata": {},
   "source": [
    "## **Step 6: Run Training**\n",
    "\n",
    "SparseML has a custom `Trainer` class that inherits from the [Hugging Face `Trainer` Class](https://huggingface.co/docs/transformers/main_classes/trainer). As such, the SparseML `Trainer` has all of the existing functionality of the HF trainer. However, in addition, we can supply a `recipe` and (optionally) a `teacher`. \n",
    "\n",
    "\n",
    "As we saw above, the `recipe` encodes the sparsity related algorithms and hyperparameters of the training process in a YAML file. The SparseML `Trainer` parses the `recipe` and adjusts the training workflow to apply the algorithms in the recipe.\n",
    "\n",
    "The `teacher` is an optional argument that instructs SparseML to apply model distillation to support the training process.\n",
    "\n",
    "***We run with only a subset of training samples for the purposes of a quick demo. Set `MAX_SAMPLES=None` to train on the entire dataset.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d49510a4-d573-423c-a34d-fdb6bec127a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-21 11:44:02 sparseml.core.logger INFO     Logging all SparseML modifier-level logs to sparse_logs/21-01-2024_11.44.02.log\n",
      "2024-01-21 11:44:02 sparseml.transformers.sparsification.trainer INFO     Loaded SparseML recipe variable into manager for recipe: ./transfer_recipe/recipe.md, recipe_variables: None and metadata {'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'fp16': False}\n",
      "2024-01-21 11:44:02 sparseml.transformers.sparsification.trainer WARNING  Overriding num_train_epochs from Recipe to 13\n"
     ]
    }
   ],
   "source": [
    "# optionally run with subset (for timing)\n",
    "MAX_SAMPLES = 2000\n",
    "if MAX_SAMPLES is not None:\n",
    "  train_dataset = tokenized_dataset[\"train\"].select(range(MAX_SAMPLES))\n",
    "  eval_dataset = tokenized_dataset[\"validation\"].select(range(MAX_SAMPLES))\n",
    "else:\n",
    "  train_dataset = tokenized_dataset[\"train\"]\n",
    "  eval_dataset = tokenized_dataset[\"validation\"]\n",
    "\n",
    "# setup trainer arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./training_output\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    resume_from_checkpoint=False,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    fp16=False)\n",
    "\n",
    "# initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    model_state_path=model_path,\n",
    "    recipe=recipe_path,\n",
    "    teacher=teacher,\n",
    "    metadata_args=[\"per_device_train_batch_size\",\"per_device_eval_batch_size\",\"fp16\"],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "163e74d7-e2c5-47eb-8e69-dfcc64461208",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-01-21 11:44:27 sparseml.transformers.sparsification.trainer INFO     Applied structure from SparseML recipe argument to model at epoch 0.0\n",
      "2024-01-21 11:44:28 sparseml.pytorch.sparsification.distillation.modifier_distillation_base INFO     distillation modifier using distillation_teacher object\n",
      "2024-01-21 11:44:28 sparseml.transformers.sparsification.trainer INFO     Modified the optimizer from the recipe for training with total_batch_size: 32 and steps_per_epoch: 63\n",
      "2024-01-21 11:44:28 sparseml.transformers.sparsification.trainer WARNING  Overrode the lr_scheduler from SparseML recipe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='819' max='819' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [819/819 2:25:26, Epoch 13/13]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Combined Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.637900</td>\n",
       "      <td>1.615663</td>\n",
       "      <td>0.591000</td>\n",
       "      <td>0.609360</td>\n",
       "      <td>0.600180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.146200</td>\n",
       "      <td>1.044757</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.670498</td>\n",
       "      <td>0.706249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.833200</td>\n",
       "      <td>1.116542</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.682088</td>\n",
       "      <td>0.707044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.543600</td>\n",
       "      <td>1.090774</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.692557</td>\n",
       "      <td>0.727528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.424500</td>\n",
       "      <td>1.290807</td>\n",
       "      <td>0.728500</td>\n",
       "      <td>0.682642</td>\n",
       "      <td>0.705571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.263400</td>\n",
       "      <td>1.508554</td>\n",
       "      <td>0.729500</td>\n",
       "      <td>0.686377</td>\n",
       "      <td>0.707938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.243100</td>\n",
       "      <td>1.371804</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.693520</td>\n",
       "      <td>0.715510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>1.223852</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.685639</td>\n",
       "      <td>0.724069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.225700</td>\n",
       "      <td>1.612654</td>\n",
       "      <td>0.721000</td>\n",
       "      <td>0.681507</td>\n",
       "      <td>0.701253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.164500</td>\n",
       "      <td>1.589019</td>\n",
       "      <td>0.723000</td>\n",
       "      <td>0.681243</td>\n",
       "      <td>0.702121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.167100</td>\n",
       "      <td>1.404719</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.684685</td>\n",
       "      <td>0.711092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.138100</td>\n",
       "      <td>1.537841</td>\n",
       "      <td>0.724500</td>\n",
       "      <td>0.683515</td>\n",
       "      <td>0.704008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.123800</td>\n",
       "      <td>1.374394</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.718444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-21 11:54:27 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to ./training_output/checkpoint-63/recipe.yaml\n",
      "2024-01-21 12:04:30 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to ./training_output/checkpoint-126/recipe.yaml\n",
      "2024-01-21 12:14:32 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to ./training_output/checkpoint-189/recipe.yaml\n",
      "2024-01-21 12:24:26 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to ./training_output/checkpoint-252/recipe.yaml\n",
      "2024-01-21 12:34:19 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to ./training_output/checkpoint-315/recipe.yaml\n",
      "2024-01-21 12:44:06 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to ./training_output/checkpoint-378/recipe.yaml\n",
      "2024-01-21 12:53:58 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to ./training_output/checkpoint-441/recipe.yaml\n",
      "2024-01-21 13:03:59 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to ./training_output/checkpoint-504/recipe.yaml\n",
      "2024-01-21 13:17:06 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to ./training_output/checkpoint-567/recipe.yaml\n",
      "2024-01-21 13:30:19 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to ./training_output/checkpoint-630/recipe.yaml\n",
      "2024-01-21 13:43:39 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to ./training_output/checkpoint-693/recipe.yaml\n",
      "2024-01-21 13:57:06 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to ./training_output/checkpoint-756/recipe.yaml\n",
      "2024-01-21 14:10:01 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to ./training_output/checkpoint-819/recipe.yaml\n",
      "2024-01-21 14:10:01 sparseml.transformers.sparsification.trainer INFO     Finalized SparseML recipe argument applied to the model\n",
      "2024-01-21 14:10:01 sparseml.transformers.sparsification.trainer INFO     Sparsification info for ./model/training: 109483778 total params. Of those there are 85526016 prunable params which have 89.3777046740959 avg sparsity.\n",
      "2024-01-21 14:10:02 sparseml.transformers.sparsification.trainer INFO     sparse model detected, all sparsification info: {\"params_summary\": {\"total\": 109483778, \"sparse\": 76441190, \"sparsity_percent\": 69.819649446149, \"prunable\": 85526016, \"prunable_sparse\": 76441190, \"prunable_sparsity_percent\": 89.3777046740959, \"quantizable\": 85609730, \"quantized\": 85609730, \"quantized_percent\": 100.0}, \"params_info\": {\"bert.encoder.layer.0.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8644917607307434, \"quantized\": true}, \"bert.encoder.layer.0.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8680216670036316, \"quantized\": true}, \"bert.encoder.layer.0.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9312150478363037, \"quantized\": true}, \"bert.encoder.layer.0.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9232262372970581, \"quantized\": true}, \"bert.encoder.layer.0.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9153103232383728, \"quantized\": true}, \"bert.encoder.layer.0.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9356380105018616, \"quantized\": true}, \"bert.encoder.layer.1.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8620554804801941, \"quantized\": true}, \"bert.encoder.layer.1.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8625064492225647, \"quantized\": true}, \"bert.encoder.layer.1.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9279242753982544, \"quantized\": true}, \"bert.encoder.layer.1.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9245097041130066, \"quantized\": true}, \"bert.encoder.layer.1.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8945091962814331, \"quantized\": true}, \"bert.encoder.layer.1.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9265751242637634, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8451063632965088, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8532799482345581, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9295670986175537, \"quantized\": true}, \"bert.encoder.layer.2.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9288228154182434, \"quantized\": true}, \"bert.encoder.layer.2.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8895581364631653, \"quantized\": true}, \"bert.encoder.layer.2.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9237624406814575, \"quantized\": true}, \"bert.encoder.layer.3.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8711106777191162, \"quantized\": true}, \"bert.encoder.layer.3.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8704121708869934, \"quantized\": true}, \"bert.encoder.layer.3.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9085676670074463, \"quantized\": true}, \"bert.encoder.layer.3.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9130028486251831, \"quantized\": true}, \"bert.encoder.layer.3.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8868213295936584, \"quantized\": true}, \"bert.encoder.layer.3.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9209082126617432, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8635711669921875, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8665059208869934, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8824039101600647, \"quantized\": true}, \"bert.encoder.layer.4.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8957400918006897, \"quantized\": true}, \"bert.encoder.layer.4.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8822059631347656, \"quantized\": true}, \"bert.encoder.layer.4.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9172935485839844, \"quantized\": true}, \"bert.encoder.layer.5.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8685166835784912, \"quantized\": true}, \"bert.encoder.layer.5.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8675944209098816, \"quantized\": true}, \"bert.encoder.layer.5.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8843333125114441, \"quantized\": true}, \"bert.encoder.layer.5.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8958756923675537, \"quantized\": true}, \"bert.encoder.layer.5.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8838331699371338, \"quantized\": true}, \"bert.encoder.layer.5.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.91917884349823, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8699256181716919, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8717482089996338, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8924729824066162, \"quantized\": true}, \"bert.encoder.layer.6.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9078572392463684, \"quantized\": true}, \"bert.encoder.layer.6.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8827946782112122, \"quantized\": true}, \"bert.encoder.layer.6.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9222526550292969, \"quantized\": true}, \"bert.encoder.layer.7.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8792538046836853, \"quantized\": true}, \"bert.encoder.layer.7.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8780839443206787, \"quantized\": true}, \"bert.encoder.layer.7.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8773871660232544, \"quantized\": true}, \"bert.encoder.layer.7.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8886498212814331, \"quantized\": true}, \"bert.encoder.layer.7.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8991622924804688, \"quantized\": true}, \"bert.encoder.layer.7.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9271066188812256, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8583950400352478, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.856842041015625, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8692152500152588, \"quantized\": true}, \"bert.encoder.layer.8.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8843451738357544, \"quantized\": true}, \"bert.encoder.layer.8.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9019758701324463, \"quantized\": true}, \"bert.encoder.layer.8.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9253442287445068, \"quantized\": true}, \"bert.encoder.layer.9.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8518574833869934, \"quantized\": true}, \"bert.encoder.layer.9.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8523983359336853, \"quantized\": true}, \"bert.encoder.layer.9.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8783705234527588, \"quantized\": true}, \"bert.encoder.layer.9.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8867306113243103, \"quantized\": true}, \"bert.encoder.layer.9.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9011484980583191, \"quantized\": true}, \"bert.encoder.layer.9.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.91860032081604, \"quantized\": true}, \"bert.encoder.layer.10.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8570064902305603, \"quantized\": true}, \"bert.encoder.layer.10.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8588087558746338, \"quantized\": true}, \"bert.encoder.layer.10.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8733994960784912, \"quantized\": true}, \"bert.encoder.layer.10.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8768836259841919, \"quantized\": true}, \"bert.encoder.layer.10.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9072990417480469, \"quantized\": true}, \"bert.encoder.layer.10.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9249801635742188, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8541886806488037, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8596123456954956, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8792198896408081, \"quantized\": true}, \"bert.encoder.layer.11.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8855014443397522, \"quantized\": true}, \"bert.encoder.layer.11.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8986706137657166, \"quantized\": true}, \"bert.encoder.layer.11.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9309417009353638, \"quantized\": true}, \"bert.pooler.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": true}, \"classifier.module.weight\": {\"numel\": 1536, \"sparsity\": 0.0, \"quantized\": true}}}\n",
      "2024-01-21 14:10:02 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to ./training_output/recipe.yaml\n"
     ]
    }
   ],
   "source": [
    "# step 5: run training\n",
    "%rm -rf training_output\n",
    "train_result = trainer.train(resume_from_checkpoint=False)\n",
    "trainer.save_model()\n",
    "trainer.save_state()\n",
    "trainer.save_optimizer_and_scheduler(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef413c64-0ecf-4338-8d42-65e64c9045ad",
   "metadata": {},
   "source": [
    "## Step 7: Export To ONNX\n",
    "\n",
    "Run the following to export the model to ONNX. The script creates a `deployment` folder containing ONNX file and the necessary configuration files (e.g. `tokenizer.json`) for deployment with DeepSparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca233126-99ea-438f-9d3a-a13a72fe28d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-21 14:11:50 sparseml.transformers.export INFO     Attempting onnx export for model at /home/jupyter/training_output for task text-classification\n",
      "2024-01-21 14:11:50 sparseml.transformers.export INFO     Using default sequence length of 512 (inferred from HF transformers config) \n",
      "2024-01-21 14:11:50 sparseml.transformers.utils.model WARNING  QAT state detected, ignore any loading errors, weights will reload after SparseML recipes have been applied /home/jupyter/training_output\n",
      "Some weights of the model checkpoint at /home/jupyter/training_output were not used when initializing BertForSequenceClassification: ['bert.embeddings.word_embeddings.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.5.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.7.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.11.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.10.attention.self.value.module.weight', 'bert.encoder.layer.7.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.embeddings.token_type_embeddings.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.11.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.value.module.weight', 'bert.encoder.layer.9.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.4.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.11.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.4.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.1.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.4.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.6.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.7.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.1.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.7.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.self.query.module.weight', 'bert.encoder.layer.4.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.0.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.6.intermediate.dense.module.bias', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.3.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.9.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.0.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.9.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.0.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.11.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.query.module.bias', 'bert.encoder.layer.1.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.1.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.11.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.7.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.5.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.embeddings.token_type_embeddings.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.attention.self.query.module.weight', 'bert.encoder.layer.1.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.10.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.2.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.6.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.6.attention.self.value.module.bias', 'bert.encoder.layer.11.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.1.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.2.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.6.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.6.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.key.module.bias', 'bert.encoder.layer.11.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.5.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.key.module.weight_fake_quant.zero_point', 'bert.embeddings.position_embeddings.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.self.key.module.weight', 'bert.encoder.layer.6.attention.self.key.module.weight_fake_quant.zero_point', 'classifier.quant.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.10.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.5.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.8.attention.self.query.module.bias', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.0.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.2.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.pooler.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.1.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.1.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.embeddings.word_embeddings.activation_post_process.scale', 'bert.encoder.layer.6.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.embeddings.token_type_embeddings.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'classifier.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.2.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.10.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.8.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.11.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.10.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.0.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.7.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.4.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.2.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.8.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.11.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.3.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.5.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.output.dense.module.weight', 'bert.encoder.layer.9.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.0.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.4.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.10.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.2.intermediate.dense.module.weight', 'bert.encoder.layer.4.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.embeddings.token_type_embeddings.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.9.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.output.dense.module.bias', 'bert.encoder.layer.1.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.1.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.key.module.bias', 'bert.encoder.layer.9.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.3.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.output.dense.module.weight', 'bert.encoder.layer.5.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.7.intermediate.dense.module.bias', 'bert.encoder.layer.8.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.2.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.3.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.pooler.dense.module.weight', 'bert.encoder.layer.10.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.8.output.dense.module.bias', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.9.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.value.module.bias', 'bert.encoder.layer.7.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.4.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.8.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.query.module.bias', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.4.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.output.dense.module.bias', 'bert.encoder.layer.3.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.4.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.11.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.1.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.11.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.4.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.pooler.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.0.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.2.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.1.intermediate.dense.module.weight', 'bert.encoder.layer.10.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.3.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.5.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.key.module.bias', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.3.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.intermediate.dense.module.bias', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.5.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.5.attention.self.key.module.weight', 'bert.encoder.layer.7.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.self.value.module.bias', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.1.attention.self.key.module.weight', 'bert.encoder.layer.2.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.8.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.query.module.weight', 'bert.encoder.layer.4.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.2.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.5.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.query.module.weight', 'bert.encoder.layer.1.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.10.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.11.output.dense.module.bias', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.7.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.2.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.0.intermediate.dense.module.weight', 'bert.encoder.layer.1.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.6.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.value.module.bias', 'bert.embeddings.word_embeddings.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.attention.output.dense.module.bias', 'bert.encoder.layer.10.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.value.module.bias', 'bert.encoder.layer.11.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.5.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.embeddings.token_type_embeddings.activation_post_process.zero_point', 'bert.encoder.layer.3.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.1.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.10.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.5.intermediate.dense.quant.activation_post_process.zero_point', 'classifier.quant.activation_post_process.scale', 'bert.encoder.layer.6.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.2.output.dense.quant.activation_post_process.scale', 'bert.embeddings.position_embeddings.weight_fake_quant.observer_enabled', 'bert.encoder.layer.4.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.3.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.7.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.9.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.3.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.5.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.7.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.3.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.4.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.11.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.embeddings.token_type_embeddings.weight_fake_quant.zero_point', 'bert.encoder.layer.5.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.8.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.0.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.9.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.2.attention.self.query.module.weight_fake_quant.zero_point', 'bert.pooler.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.0.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.embeddings.token_type_embeddings.weight_fake_quant.activation_post_process.eps', 'classifier.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.8.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.3.intermediate.dense.module.weight', 'bert.encoder.layer.6.attention.self.query.quant.activation_post_process.scale', 'bert.embeddings.position_embeddings.activation_post_process.scale', 'bert.encoder.layer.6.attention.output.dense.module.bias', 'bert.encoder.layer.11.output.dense.module.weight', 'bert.encoder.layer.8.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.output.dense.module.weight', 'bert.encoder.layer.8.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.3.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.0.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.8.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.pooler.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.3.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.pooler.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.0.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.10.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.5.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.key.module.bias', 'bert.encoder.layer.5.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.4.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.9.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.1.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.0.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.embeddings.position_embeddings.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.2.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.2.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.4.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.intermediate.dense.module.weight', 'bert.encoder.layer.9.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.2.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.9.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.query.module.bias', 'bert.encoder.layer.6.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.1.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.9.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.10.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.11.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.10.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.1.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.9.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.2.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.5.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.0.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.2.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.2.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.7.attention.self.value.module.weight', 'bert.encoder.layer.7.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.0.attention.self.value.quant.activation_post_process.scale', 'bert.embeddings.token_type_embeddings.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.1.attention.output.dense.module.bias', 'bert.encoder.layer.1.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.2.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.6.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.4.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.3.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.4.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.7.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.10.attention.self.query.module.bias', 'bert.encoder.layer.10.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.pooler.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.9.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.11.intermediate.dense.module.bias', 'bert.encoder.layer.10.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.11.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.5.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.5.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.query.module.bias', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.key.module.weight', 'bert.encoder.layer.9.attention.self.value.quant.activation_post_process.zero_point', 'bert.pooler.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.6.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.7.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.8.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.key.module.bias', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.embeddings.position_embeddings.activation_post_process.zero_point', 'bert.encoder.layer.6.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.9.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.1.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.7.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.attention.output.dense.module.weight', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.5.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.10.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.output.dense.module.bias', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.attention.output.dense.quant.activation_post_process.zero_point', 'classifier.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.0.attention.self.query.module.weight', 'bert.encoder.layer.2.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.2.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.value.module.weight', 'bert.encoder.layer.1.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.8.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.10.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.2.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.8.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.8.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.6.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.11.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.embeddings.token_type_embeddings.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.9.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.7.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.pooler.dense.quant.activation_post_process.scale', 'bert.encoder.layer.5.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.embeddings.token_type_embeddings.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.1.output.dense.module.bias', 'bert.encoder.layer.5.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.0.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.11.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.10.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.7.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.4.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.key.module.bias', 'bert.encoder.layer.2.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.1.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.5.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.0.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.11.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.query.module.bias', 'classifier.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.4.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.0.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.5.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.4.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.5.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.embeddings.position_embeddings.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.4.attention.self.value.module.weight', 'bert.encoder.layer.2.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.5.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.2.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.value.module.bias', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.11.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.9.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.pooler.dense.module.bias', 'bert.encoder.layer.10.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.5.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.10.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.4.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.4.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.6.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.4.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.1.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.3.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.8.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.11.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.5.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.output.dense.module.bias', 'bert.encoder.layer.11.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.11.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.8.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.2.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.query.module.weight', 'bert.encoder.layer.0.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.11.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.6.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.11.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.7.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.7.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.intermediate.dense.module.bias', 'bert.encoder.layer.4.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.intermediate.dense.module.weight', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.self.query.module.weight', 'bert.encoder.layer.4.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.query.module.bias', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.4.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.1.intermediate.dense.module.bias', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.11.intermediate.dense.module.weight', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.11.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.0.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.3.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.1.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.1.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.intermediate.dense.module.bias', 'bert.embeddings.word_embeddings.weight_fake_quant.zero_point', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.output.dense.module.bias', 'bert.encoder.layer.3.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.10.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.5.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.2.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.0.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.4.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.11.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.11.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.output.dense.module.weight', 'bert.encoder.layer.4.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.1.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.7.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.embeddings.token_type_embeddings.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.self.query.module.bias', 'bert.encoder.layer.2.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.11.attention.self.key.module.weight', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.7.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.4.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.8.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.6.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.4.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.2.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.7.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.11.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.8.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.7.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.6.output.dense.module.weight', 'bert.encoder.layer.6.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.8.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.8.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.7.intermediate.dense.module.weight', 'bert.encoder.layer.6.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.4.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.4.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.2.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.7.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.9.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.2.attention.self.key.module.weight', 'bert.encoder.layer.9.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.5.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.5.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.key.module.bias', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.3.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.6.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.intermediate.dense.module.bias', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.9.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.embeddings.word_embeddings.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.6.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.key.module.bias', 'bert.encoder.layer.10.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.4.intermediate.dense.module.bias', 'bert.encoder.layer.10.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.1.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.2.output.dense.module.weight', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.0.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.5.attention.output.dense.module.weight', 'bert.encoder.layer.0.attention.self.value.module.weight', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.value.module.weight', 'bert.encoder.layer.4.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.11.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.8.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.1.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.8.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.5.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.7.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.9.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.9.intermediate.dense.module.weight', 'bert.encoder.layer.7.attention.output.dense.module.weight', 'bert.encoder.layer.0.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.attention.output.dense.module.weight', 'bert.encoder.layer.1.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.value.module.bias', 'bert.encoder.layer.8.attention.self.key.module.weight', 'bert.encoder.layer.2.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.output.dense.module.weight', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.5.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.0.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'classifier.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.7.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'classifier.quant.activation_post_process.fake_quant_enabled', 'bert.pooler.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.6.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.6.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.4.attention.output.dense.module.bias', 'bert.encoder.layer.0.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.10.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.9.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.self.key.module.weight', 'bert.encoder.layer.3.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.6.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.2.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.3.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.embeddings.token_type_embeddings.activation_post_process.scale', 'bert.encoder.layer.7.attention.self.key.module.weight', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.value.module.bias', 'bert.encoder.layer.8.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.2.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.1.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.0.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.output.dense.module.weight', 'bert.encoder.layer.7.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.9.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.9.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.3.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.10.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.6.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.10.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.3.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.9.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.3.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.2.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.7.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.2.attention.output.dense.module.bias', 'bert.encoder.layer.11.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.0.output.dense.module.bias', 'bert.encoder.layer.9.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.8.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.pooler.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.2.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'classifier.module.weight', 'bert.encoder.layer.10.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.1.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.embeddings.position_embeddings.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.output.dense.module.bias', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.output.dense.module.weight', 'bert.encoder.layer.0.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.1.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.11.attention.self.query.quant.activation_post_process.scale', 'bert.embeddings.position_embeddings.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.7.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.4.attention.self.query.module.bias', 'bert.encoder.layer.11.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.0.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.10.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.9.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.11.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.0.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.11.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.9.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.pooler.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.2.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.output.dense.module.weight_fake_quant.zero_point', 'bert.embeddings.position_embeddings.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.key.module.bias', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.8.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.4.intermediate.dense.module.weight', 'bert.encoder.layer.9.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.4.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.3.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.4.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.6.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.6.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.8.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.0.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.1.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.11.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.3.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.5.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.8.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.10.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.7.output.dense.module.bias', 'bert.encoder.layer.9.attention.self.key.module.weight', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.3.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.attention.output.dense.module.bias', 'bert.encoder.layer.4.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.10.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'classifier.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.key.module.bias', 'bert.encoder.layer.6.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.output.dense.module.bias', 'bert.encoder.layer.4.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.8.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.5.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.value.module.bias', 'bert.encoder.layer.3.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.8.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.10.attention.self.query.module.weight', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'classifier.module.weight_fake_quant.scale', 'bert.encoder.layer.7.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'classifier.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.10.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.key.module.weight', 'bert.encoder.layer.8.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.4.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.0.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.4.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.2.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.9.attention.self.key.quant.activation_post_process.zero_point', 'classifier.module.weight_fake_quant.zero_point', 'bert.encoder.layer.9.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.embeddings.word_embeddings.activation_post_process.zero_point', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.1.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.2.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.self.query.module.weight', 'bert.encoder.layer.10.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.1.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.5.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.0.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.7.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.query.module.bias', 'bert.encoder.layer.11.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.10.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.11.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.value.module.bias', 'bert.encoder.layer.7.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.6.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.value.module.weight_fake_quant.scale', 'bert.embeddings.position_embeddings.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.4.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.8.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.7.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.4.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.intermediate.dense.module.bias', 'bert.encoder.layer.0.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.6.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.1.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.10.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.query.module.weight', 'bert.encoder.layer.3.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.11.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.7.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.4.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.9.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.embeddings.word_embeddings.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.6.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.10.intermediate.dense.module.weight', 'bert.encoder.layer.11.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.8.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.2.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.5.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.4.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.2.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.11.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.6.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.2.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.1.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.8.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.8.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.4.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.5.output.dense.module.weight', 'bert.encoder.layer.0.attention.output.dense.module.weight', 'bert.encoder.layer.8.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.10.intermediate.dense.module.bias', 'bert.encoder.layer.10.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.attention.self.key.module.weight', 'bert.encoder.layer.11.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.value.module.weight', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.5.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.output.dense.module.bias', 'bert.encoder.layer.5.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.3.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.6.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.4.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.4.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.value.module.weight', 'bert.encoder.layer.2.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.intermediate.dense.module.bias', 'bert.encoder.layer.4.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.6.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.4.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.0.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.1.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.2.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.6.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.6.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.0.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.5.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.2.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.embeddings.position_embeddings.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.output.dense.module.weight', 'bert.embeddings.position_embeddings.weight_fake_quant.scale', 'bert.encoder.layer.6.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.11.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.7.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.0.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.8.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.0.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.3.output.dense.module.weight', 'bert.encoder.layer.6.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.5.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.10.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.5.attention.self.query.module.bias', 'bert.encoder.layer.2.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.8.output.dense.module.weight', 'bert.encoder.layer.9.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.9.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.output.dense.module.weight', 'bert.encoder.layer.5.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.attention.output.dense.quant.activation_post_process.zero_point', 'bert.embeddings.word_embeddings.activation_post_process.observer_enabled', 'bert.encoder.layer.9.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.2.attention.self.value.module.weight', 'bert.encoder.layer.2.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.6.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.0.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.2.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.9.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.9.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.8.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.5.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.embeddings.word_embeddings.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.2.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.6.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.11.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.0.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.10.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.1.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.intermediate.dense.module.weight', 'bert.encoder.layer.2.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.0.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.6.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.1.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.3.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.7.intermediate.dense.quant.activation_post_process.zero_point', 'bert.embeddings.word_embeddings.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.9.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.5.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.10.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.10.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.8.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.9.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.2.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.9.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.8.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.1.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.5.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'classifier.module.bias', 'bert.encoder.layer.0.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.2.attention.output.dense.module.weight', 'bert.pooler.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.7.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.7.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.10.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.0.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.7.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.11.output.dense.module.weight_fake_quant.observer_enabled', 'bert.embeddings.position_embeddings.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.0.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'classifier.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.8.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.11.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.1.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.output.dense.module.bias', 'bert.encoder.layer.1.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.1.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.4.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.9.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.9.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.11.attention.output.dense.module.weight', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.6.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.5.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.output.dense.module.bias', 'bert.encoder.layer.6.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.query.module.weight', 'bert.encoder.layer.4.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.query.module.weight', 'bert.encoder.layer.3.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.10.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.5.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.0.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.2.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.8.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.self.key.module.bias', 'bert.encoder.layer.10.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.7.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.pooler.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.4.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.5.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.embeddings.word_embeddings.weight_fake_quant.scale', 'bert.encoder.layer.10.attention.self.value.module.bias', 'bert.encoder.layer.7.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.5.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.0.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.0.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.11.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.value.quant.activation_post_process.scale', 'classifier.quant.activation_post_process.observer_enabled', 'bert.embeddings.token_type_embeddings.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.query.quant.activation_post_process.zero_point', 'bert.embeddings.word_embeddings.weight_fake_quant.observer_enabled', 'bert.embeddings.word_embeddings.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.6.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.0.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.1.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.0.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.embeddings.token_type_embeddings.weight_fake_quant.scale', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.pooler.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.0.output.dense.module.weight', 'bert.encoder.layer.2.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.9.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.9.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.9.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.5.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.3.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.8.attention.output.dense.module.bias', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.output.dense.module.bias', 'bert.encoder.layer.5.attention.self.key.module.bias', 'bert.encoder.layer.1.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.10.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.0.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.5.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.3.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.embeddings.position_embeddings.weight_fake_quant.zero_point', 'bert.encoder.layer.10.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.0.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.5.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.9.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.3.output.dense.module.bias', 'bert.encoder.layer.4.output.dense.module.weight', 'bert.encoder.layer.6.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.11.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.1.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.2.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.3.attention.self.value.module.bias', 'bert.encoder.layer.5.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.11.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.0.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.2.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.1.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.0.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.11.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.embeddings.word_embeddings.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.6.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.9.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.7.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.5.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.self.value.module.weight', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.10.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.6.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.7.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.4.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.3.attention.self.value.module.weight', 'bert.encoder.layer.7.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.11.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.value.quant.activation_post_process.observer_enabled']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/jupyter/training_output and are newly initialized: ['bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'classifier.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'classifier.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-01-21 14:11:52 sparseml.transformers.utils.model INFO     Delayed load of model /home/jupyter/training_output detected. Will print out model information once SparseML recipes have loaded\n",
      "2024-01-21 14:11:52 sparseml.transformers.export INFO     loaded model, config, and tokenizer from /home/jupyter/training_output\n",
      "2024-01-21 14:11:52 sparseml.core.logger INFO     Logging all SparseML modifier-level logs to sparse_logs/21-01-2024_14.11.52.log\n",
      "2024-01-21 14:11:52 sparseml.transformers.sparsification.trainer INFO     Loaded 1 SparseML checkpoint recipe stage(s) from /home/jupyter/training_output/recipe.yaml to replicate model sparse state\n",
      "2024-01-21 14:11:53 sparseml.transformers.sparsification.trainer INFO     Applied structure from 1 previous recipe stage(s) to model and finalized (recipes saved with model_path)\n",
      "2024-01-21 14:11:53 sparseml.transformers.sparsification.trainer INFO     Reloaded 1783 model params for SparseML Recipe from /home/jupyter/training_output\n",
      "2024-01-21 14:11:53 sparseml.transformers.utils.model INFO     Loaded model from /home/jupyter/training_output with 109483778 total params. Of those there are 85526016 prunable params which have 89.3777046740959 avg sparsity.\n",
      "2024-01-21 14:11:54 sparseml.transformers.utils.model INFO     sparse model detected, all sparsification info: {\"params_summary\": {\"total\": 109483778, \"sparse\": 76441190, \"sparsity_percent\": 69.819649446149, \"prunable\": 85526016, \"prunable_sparse\": 76441190, \"prunable_sparsity_percent\": 89.3777046740959, \"quantizable\": 85609730, \"quantized\": 85609730, \"quantized_percent\": 100.0}, \"params_info\": {\"bert.encoder.layer.0.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8644917607307434, \"quantized\": true}, \"bert.encoder.layer.0.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8680216670036316, \"quantized\": true}, \"bert.encoder.layer.0.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9312150478363037, \"quantized\": true}, \"bert.encoder.layer.0.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9232262372970581, \"quantized\": true}, \"bert.encoder.layer.0.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9153103232383728, \"quantized\": true}, \"bert.encoder.layer.0.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9356380105018616, \"quantized\": true}, \"bert.encoder.layer.1.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8620554804801941, \"quantized\": true}, \"bert.encoder.layer.1.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8625064492225647, \"quantized\": true}, \"bert.encoder.layer.1.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9279242753982544, \"quantized\": true}, \"bert.encoder.layer.1.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9245097041130066, \"quantized\": true}, \"bert.encoder.layer.1.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8945091962814331, \"quantized\": true}, \"bert.encoder.layer.1.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9265751242637634, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8451063632965088, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8532799482345581, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9295670986175537, \"quantized\": true}, \"bert.encoder.layer.2.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9288228154182434, \"quantized\": true}, \"bert.encoder.layer.2.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8895581364631653, \"quantized\": true}, \"bert.encoder.layer.2.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9237624406814575, \"quantized\": true}, \"bert.encoder.layer.3.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8711106777191162, \"quantized\": true}, \"bert.encoder.layer.3.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8704121708869934, \"quantized\": true}, \"bert.encoder.layer.3.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9085676670074463, \"quantized\": true}, \"bert.encoder.layer.3.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9130028486251831, \"quantized\": true}, \"bert.encoder.layer.3.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8868213295936584, \"quantized\": true}, \"bert.encoder.layer.3.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9209082126617432, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8635711669921875, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8665059208869934, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8824039101600647, \"quantized\": true}, \"bert.encoder.layer.4.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8957400918006897, \"quantized\": true}, \"bert.encoder.layer.4.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8822059631347656, \"quantized\": true}, \"bert.encoder.layer.4.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9172935485839844, \"quantized\": true}, \"bert.encoder.layer.5.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8685166835784912, \"quantized\": true}, \"bert.encoder.layer.5.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8675944209098816, \"quantized\": true}, \"bert.encoder.layer.5.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8843333125114441, \"quantized\": true}, \"bert.encoder.layer.5.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8958756923675537, \"quantized\": true}, \"bert.encoder.layer.5.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8838331699371338, \"quantized\": true}, \"bert.encoder.layer.5.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.91917884349823, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8699256181716919, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8717482089996338, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8924729824066162, \"quantized\": true}, \"bert.encoder.layer.6.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9078572392463684, \"quantized\": true}, \"bert.encoder.layer.6.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8827946782112122, \"quantized\": true}, \"bert.encoder.layer.6.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9222526550292969, \"quantized\": true}, \"bert.encoder.layer.7.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8792538046836853, \"quantized\": true}, \"bert.encoder.layer.7.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8780839443206787, \"quantized\": true}, \"bert.encoder.layer.7.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8773871660232544, \"quantized\": true}, \"bert.encoder.layer.7.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8886498212814331, \"quantized\": true}, \"bert.encoder.layer.7.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8991622924804688, \"quantized\": true}, \"bert.encoder.layer.7.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9271066188812256, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8583950400352478, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.856842041015625, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8692152500152588, \"quantized\": true}, \"bert.encoder.layer.8.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8843451738357544, \"quantized\": true}, \"bert.encoder.layer.8.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9019758701324463, \"quantized\": true}, \"bert.encoder.layer.8.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9253442287445068, \"quantized\": true}, \"bert.encoder.layer.9.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8518574833869934, \"quantized\": true}, \"bert.encoder.layer.9.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8523983359336853, \"quantized\": true}, \"bert.encoder.layer.9.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8783705234527588, \"quantized\": true}, \"bert.encoder.layer.9.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8867306113243103, \"quantized\": true}, \"bert.encoder.layer.9.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9011484980583191, \"quantized\": true}, \"bert.encoder.layer.9.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.91860032081604, \"quantized\": true}, \"bert.encoder.layer.10.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8570064902305603, \"quantized\": true}, \"bert.encoder.layer.10.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8588087558746338, \"quantized\": true}, \"bert.encoder.layer.10.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8733994960784912, \"quantized\": true}, \"bert.encoder.layer.10.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8768836259841919, \"quantized\": true}, \"bert.encoder.layer.10.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9072990417480469, \"quantized\": true}, \"bert.encoder.layer.10.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9249801635742188, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8541886806488037, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8596123456954956, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8792198896408081, \"quantized\": true}, \"bert.encoder.layer.11.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8855014443397522, \"quantized\": true}, \"bert.encoder.layer.11.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8986706137657166, \"quantized\": true}, \"bert.encoder.layer.11.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9309417009353638, \"quantized\": true}, \"bert.pooler.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": true}, \"classifier.module.weight\": {\"numel\": 1536, \"sparsity\": 0.0, \"quantized\": true}}}\n",
      "2024-01-21 14:11:54 sparseml.transformers.sparsification.trainer INFO     Reloaded model state after SparseML recipe structure modifications from /home/jupyter/training_output\n",
      "2024-01-21 14:11:54 sparseml.transformers.export INFO     Applied an unstaged recipe to the model at /home/jupyter/training_output\n",
      "2024-01-21 14:11:54 sparseml.transformers.export INFO     Created sample inputs for the ONNX export process: {'input_ids': 'torch.int64: [1, 512]', 'attention_mask': 'torch.int64: [1, 512]', 'token_type_ids': 'torch.int64: [1, 512]'}\n",
      "/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:823: UserWarning: It is recommended that constant folding be turned off ('do_constant_folding=False') when exporting the model in training-amenable mode, i.e. with 'training=TrainingMode.TRAIN' or 'training=TrainingMode.PRESERVE' (when model is in training mode). Otherwise, some learnable model parameters may not translate correctly in the exported ONNX model because constant folding mutates model parameters. Please consider turning off constant folding or setting the training=TrainingMode.EVAL.\n",
      "  warnings.warn(\n",
      "2024-01-21 14:12:12 sparseml.exporters.transforms.onnx_transform INFO     [ConstantsToInitializers] Transformed 385 matches\n",
      "2024-01-21 14:12:12 sparseml.exporters.transforms.onnx_transform INFO     [FoldIdentityInitializers] Transformed 0 matches\n",
      "2024-01-21 14:12:13 sparseml.exporters.transforms.onnx_transform INFO     [InitializersToUint8] Transformed 1 matches\n",
      "2024-01-21 14:12:14 sparseml.exporters.transforms.onnx_transform INFO     [FlattenQParams] Transformed 0 matches\n",
      "2024-01-21 14:12:15 sparseml.exporters.transforms.onnx_transform INFO     [FoldConvDivBn] Transformed 0 matches\n",
      "2024-01-21 14:12:16 sparseml.exporters.transforms.onnx_transform INFO     [DeleteRepeatedQdq] Transformed 0 matches\n",
      "2024-01-21 14:12:17 sparseml.exporters.transforms.onnx_transform INFO     [QuantizeQATEmbedding] Transformed 3 matches\n",
      "2024-01-21 14:12:17 sparseml.exporters.transforms.onnx_transform INFO     [PropagateEmbeddingQuantization] Transformed 0 matches\n",
      "2024-01-21 14:12:18 sparseml.exporters.transforms.onnx_transform INFO     [PropagateDequantThroughSplit] Transformed 0 matches\n",
      "2024-01-21 14:12:20 sparseml.exporters.transforms.onnx_transform INFO     [MatMulAddToMatMulIntegerAddCastMul] Transformed 72 matches\n",
      "2024-01-21 14:12:21 sparseml.exporters.transforms.onnx_transform INFO     [MatMulToMatMulIntegerCastMul] Transformed 24 matches\n",
      "2024-01-21 14:12:22 sparseml.exporters.transforms.onnx_transform INFO     [FoldReLUQuants] Transformed 0 matches\n",
      "2024-01-21 14:12:22 sparseml.exporters.transforms.onnx_transform INFO     [ConvToQLinearConv] Transformed 0 matches\n",
      "2024-01-21 14:12:22 sparseml.exporters.transforms.onnx_transform INFO     [GemmToQLinearMatMul] Transformed 0 matches\n",
      "2024-01-21 14:12:23 sparseml.exporters.transforms.onnx_transform INFO     [GemmToMatMulIntegerAddCastMul] Transformed 2 matches\n",
      "2024-01-21 14:12:24 sparseml.exporters.transforms.onnx_transform INFO     [QuantizeResiduals] Transformed 0 matches\n",
      "2024-01-21 14:12:25 sparseml.exporters.transforms.onnx_transform INFO     [RemoveDuplicateQConvWeights] Transformed 0 matches\n",
      "2024-01-21 14:12:26 sparseml.exporters.transforms.onnx_transform INFO     [RemoveDuplicateQuantizeOps] Transformed 0 matches\n",
      "2024-01-21 14:12:27 sparseml.transformers.export INFO     ONNX exported to /home/jupyter/training_output/model.onnx\n",
      "2024-01-21 14:12:27 sparseml.transformers.export INFO     0 sample inputs/outputs exported\n",
      "2024-01-21 14:12:27 sparseml.transformers.export INFO     Saved model.onnx in the deployment folder at /home/jupyter/deployment/model.onnx\n",
      "2024-01-21 14:12:27 sparseml.transformers.export INFO     Saved tokenizer_config.json in the deployment folder at /home/jupyter/deployment/tokenizer_config.json\n",
      "2024-01-21 14:12:27 sparseml.transformers.export INFO     Saved config.json in the deployment folder at /home/jupyter/deployment/config.json\n",
      "2024-01-21 14:12:27 sparseml.transformers.export INFO     Saved tokenizer.json in the deployment folder at /home/jupyter/deployment/tokenizer.json\n",
      "2024-01-21 14:12:27 sparseml.transformers.export WARNING  Optional file model.data not found in /home/jupyter/training_output. Skipping copying to deployment folder.\n",
      "2024-01-21 14:12:27 sparseml.transformers.export INFO     Saved special_tokens_map.json in the deployment folder at /home/jupyter/deployment/special_tokens_map.json\n",
      "2024-01-21 14:12:27 sparseml.transformers.export WARNING  Optional file vocab.json not found in /home/jupyter/training_output. Skipping copying to deployment folder.\n",
      "2024-01-21 14:12:27 sparseml.transformers.export WARNING  Optional file merges.txt not found in /home/jupyter/training_output. Skipping copying to deployment folder.\n",
      "2024-01-21 14:12:27 sparseml.transformers.export INFO     Created deployment folder at /home/jupyter/deployment with files: ['tokenizer_config.json', 'tokenizer.json', 'special_tokens_map.json', 'config.json', 'model.onnx']\n"
     ]
    }
   ],
   "source": [
    "!sparseml.transformers.export_onnx \\\n",
    "  --model_path training_output \\\n",
    "  --task text_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f61c537-1022-4940-8e7b-9912912e8bea",
   "metadata": {},
   "source": [
    "## Deploy with DeepSparse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f22b49c-73d4-4c2e-9ec7-4d934a7e19d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepsparse\n",
      "  Downloading deepsparse-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
      "Requirement already satisfied: sparsezoo~=1.6.0 in /opt/conda/lib/python3.10/site-packages (from deepsparse) (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.16.3 in /opt/conda/lib/python3.10/site-packages (from deepsparse) (1.25.2)\n",
      "Requirement already satisfied: onnx<1.15.0,>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from deepsparse) (1.14.1)\n",
      "Requirement already satisfied: pydantic<2.0.0,>=1.8.2 in /opt/conda/lib/python3.10/site-packages (from deepsparse) (1.10.14)\n",
      "Requirement already satisfied: requests>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepsparse) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from deepsparse) (4.66.1)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /opt/conda/lib/python3.10/site-packages (from deepsparse) (3.20.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from deepsparse) (8.1.7)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /opt/conda/lib/python3.10/site-packages (from onnx<1.15.0,>=1.5.0->deepsparse) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.0.0->deepsparse) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.0.0->deepsparse) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.0.0->deepsparse) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.0.0->deepsparse) (2023.11.17)\n",
      "Requirement already satisfied: pyyaml>=5.1.0 in /opt/conda/lib/python3.10/site-packages (from sparsezoo~=1.6.0->deepsparse) (6.0.1)\n",
      "Requirement already satisfied: pandas>1.3 in /opt/conda/lib/python3.10/site-packages (from sparsezoo~=1.6.0->deepsparse) (2.1.4)\n",
      "Requirement already satisfied: py-machineid>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from sparsezoo~=1.6.0->deepsparse) (0.5.1)\n",
      "Requirement already satisfied: geocoder>=1.38.0 in /opt/conda/lib/python3.10/site-packages (from sparsezoo~=1.6.0->deepsparse) (1.38.1)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.10/site-packages (from geocoder>=1.38.0->sparsezoo~=1.6.0->deepsparse) (0.18.3)\n",
      "Requirement already satisfied: ratelim in /opt/conda/lib/python3.10/site-packages (from geocoder>=1.38.0->sparsezoo~=1.6.0->deepsparse) (0.1.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from geocoder>=1.38.0->sparsezoo~=1.6.0->deepsparse) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>1.3->sparsezoo~=1.6.0->deepsparse) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>1.3->sparsezoo~=1.6.0->deepsparse) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>1.3->sparsezoo~=1.6.0->deepsparse) (2023.4)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ratelim->geocoder>=1.38.0->sparsezoo~=1.6.0->deepsparse) (5.1.1)\n",
      "Downloading deepsparse-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: deepsparse\n",
      "Successfully installed deepsparse-1.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install deepsparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d68cea0-d07e-4b12-910d-d91d60cfaa5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeepSparse, Copyright 2021-present / Neuralmagic, Inc. version: 1.6.1 COMMUNITY | (eff4f95d) (release) (optimized) (system=avx512_vnni, binary=avx512)\n"
     ]
    }
   ],
   "source": [
    "from deepsparse import Pipeline\n",
    "\n",
    "pipeline = Pipeline.create(\"text_classification\", model_path=\"./deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe57d55-c9c4-4904-829d-02eeede6e1b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels=['LABEL_0'] scores=[0.9995847940444946]\n"
     ]
    }
   ],
   "source": [
    "prediction = pipeline(\n",
    "    sequences=[\n",
    "        [\n",
    "            \"What is the plural of hypothesis?\",\n",
    "            \"What is the plural of thesis?\"\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "print(prediction) # label 0 is not a duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6406c5dd-6dd8-4281-9ada-5bf95c701814",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels=['LABEL_1'] scores=[0.9903117418289185]\n"
     ]
    }
   ],
   "source": [
    "prediction = pipeline(\n",
    "    sequences=[\n",
    "        [\n",
    "          \"Why don't people simply 'Google' instead of asking questions on Quora?\",\n",
    "          \"Why do people ask Quora questions instead of just searching google?\"\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "print(prediction) # label 1 is a duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03162f6-31d3-4943-8de4-6e3a25e3fa6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu113.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu113:m115"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
