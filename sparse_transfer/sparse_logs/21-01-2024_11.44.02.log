Logging all SparseML modifier-level logs to sparse_logs/21-01-2024_11.44.02.log
python DistillationModifier [0 - 1705837470.0848691]: Calling loss_update with:
args: 0.6816268563270569| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [0 - 1705837471.5974798]: 
Returned: 1.5523860454559326| 

python LearningRateFunctionModifier [0 - 1705837474.4647756]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [0 - 1705837474.464965]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [0 - 1705837474.4650407]: 0.00015
python LearningRateFunctionModifier/ParamGroup1 [0 - 1705837474.465335]: 0.00015
python DistillationModifier/task_loss [0 - 1705837474.4656498]: tensor(0.6816, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [0 - 1705837474.4671311]: tensor(1.5524, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [0 - 1705837474.4678137]: tensor(1.5524, grad_fn=<AddBackward0>)
python ConstantPruningModifier [0 - 1705837474.46848]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.0| steps_per_epoch: 63| 
python ConstantPruningModifier [0 - 1705837474.6994421]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [0 - 1705837474.7001941]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [0 - 1705837474.7011101]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [0 - 1705837474.701822]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [0 - 1705837474.7024136]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [0 - 1705837474.7041173]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [0 - 1705837474.7058828]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [0 - 1705837474.7066402]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [0 - 1705837474.7072747]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [0 - 1705837474.7078433]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [0 - 1705837474.7083833]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [0 - 1705837474.710082]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [0 - 1705837474.7118213]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [0 - 1705837474.7125604]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [0 - 1705837474.7132034]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [0 - 1705837474.713775]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [0 - 1705837474.714325]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [0 - 1705837474.715984]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [0 - 1705837474.7177246]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [0 - 1705837474.71847]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [0 - 1705837474.719088]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [0 - 1705837474.719663]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [0 - 1705837474.7202272]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [0 - 1705837474.7219195]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [0 - 1705837474.7236145]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [0 - 1705837474.7243333]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [0 - 1705837474.7249548]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [0 - 1705837474.7255168]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [0 - 1705837474.726062]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [0 - 1705837474.727714]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [0 - 1705837474.7294688]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [0 - 1705837474.7302244]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [0 - 1705837474.7308285]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [0 - 1705837474.731359]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [0 - 1705837474.731934]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [0 - 1705837474.7335916]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [0 - 1705837474.7353122]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [0 - 1705837474.7360175]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [0 - 1705837474.7366188]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [0 - 1705837474.7372017]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [0 - 1705837474.7377317]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [0 - 1705837474.739316]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [0 - 1705837474.7410867]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [0 - 1705837474.7418756]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [0 - 1705837474.7424889]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [0 - 1705837474.7430518]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [0 - 1705837474.7435946]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [0 - 1705837474.7452226]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [0 - 1705837474.7469876]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [0 - 1705837474.7477438]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [0 - 1705837474.7483597]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [0 - 1705837474.7489493]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [0 - 1705837474.7495148]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [0 - 1705837474.7511635]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [0 - 1705837474.752925]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [0 - 1705837474.7536573]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [0 - 1705837474.7542627]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [0 - 1705837474.7548344]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [0 - 1705837474.7553728]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [0 - 1705837474.757072]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [0 - 1705837474.7588072]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [0 - 1705837474.7595048]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [0 - 1705837474.7601085]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [0 - 1705837474.760704]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [0 - 1705837474.76127]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [0 - 1705837474.7628748]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [0 - 1705837474.7643976]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [0 - 1705837474.7652607]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [0 - 1705837474.765928]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [0 - 1705837474.766507]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [0 - 1705837474.7670453]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [0 - 1705837474.7686412]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [0 - 1705837474.7701962]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [0 - 1705837474.770965]: 0.0
python ParamPruning/classifier.weight [0 - 1705837474.7714186]: 0.0
python DistillationModifier [7 - 1705837516.6460633]: Calling loss_update with:
args: 0.7800438404083252| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 0.1111111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [7 - 1705837518.2029982]: 
Returned: 1.5538161993026733| 

python LearningRateFunctionModifier [7 - 1705837521.4824922]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.1111111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [7 - 1705837521.4826584]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [7 - 1705837521.4827225]: 0.00014871794871794872
python LearningRateFunctionModifier/ParamGroup1 [7 - 1705837521.4829607]: 0.00014871794871794872
python DistillationModifier/task_loss [7 - 1705837521.4832613]: tensor(0.7800, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [7 - 1705837521.4843712]: tensor(1.5538, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [7 - 1705837521.4850194]: tensor(1.5538, grad_fn=<AddBackward0>)
python ConstantPruningModifier [7 - 1705837521.485632]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.1111111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [7 - 1705837521.826501]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [7 - 1705837521.827313]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [7 - 1705837521.8292072]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [7 - 1705837521.829988]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [7 - 1705837521.8307528]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [7 - 1705837521.832505]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [7 - 1705837521.834354]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [7 - 1705837521.8351457]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [7 - 1705837521.8358529]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [7 - 1705837521.8365376]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [7 - 1705837521.8372462]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [7 - 1705837521.8389332]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [7 - 1705837521.8406932]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [7 - 1705837521.8414278]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [7 - 1705837521.8421004]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [7 - 1705837521.8427198]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [7 - 1705837521.843303]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [7 - 1705837521.8450475]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [7 - 1705837521.8468268]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [7 - 1705837521.8475335]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [7 - 1705837521.8482134]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [7 - 1705837521.8488445]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [7 - 1705837521.849428]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [7 - 1705837521.8511105]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [7 - 1705837521.8529377]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [7 - 1705837521.853701]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [7 - 1705837521.8543496]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [7 - 1705837521.8550103]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [7 - 1705837521.8556073]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [7 - 1705837521.8573039]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [7 - 1705837521.8591077]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [7 - 1705837521.8598244]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [7 - 1705837521.8604813]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [7 - 1705837521.8611128]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [7 - 1705837521.861756]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [7 - 1705837521.863441]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [7 - 1705837521.86495]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [7 - 1705837521.865687]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [7 - 1705837521.8663826]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [7 - 1705837521.867045]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [7 - 1705837521.8676565]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [7 - 1705837521.8693314]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [7 - 1705837521.8711262]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [7 - 1705837521.8719065]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [7 - 1705837521.872594]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [7 - 1705837521.8732555]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [7 - 1705837521.8738925]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [7 - 1705837521.875571]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [7 - 1705837521.8773632]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [7 - 1705837521.878081]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [7 - 1705837521.878692]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [7 - 1705837521.8793285]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [7 - 1705837521.8799882]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [7 - 1705837521.8817015]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [7 - 1705837521.8834581]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [7 - 1705837521.8841667]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [7 - 1705837521.8848686]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [7 - 1705837521.8854716]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [7 - 1705837521.8860393]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [7 - 1705837521.8877363]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [7 - 1705837521.8895283]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [7 - 1705837521.8902707]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [7 - 1705837521.890951]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [7 - 1705837521.8915973]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [7 - 1705837521.8922167]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [7 - 1705837521.8939085]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [7 - 1705837521.8956573]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [7 - 1705837521.896316]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [7 - 1705837521.8970156]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [7 - 1705837521.8977044]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [7 - 1705837521.8983212]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [7 - 1705837521.899995]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [7 - 1705837521.9017801]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [7 - 1705837521.9025147]: 0.0
python ParamPruning/classifier.weight [7 - 1705837521.902972]: 0.0
python DistillationModifier [14 - 1705837562.429974]: Calling loss_update with:
args: 0.9270107746124268| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 0.2222222222222222| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [14 - 1705837564.460858]: 
Returned: 1.8521265983581543| 

python LearningRateFunctionModifier [14 - 1705837567.7283099]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.2222222222222222| steps_per_epoch: 63| 
python LearningRateFunctionModifier [14 - 1705837567.7285078]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [14 - 1705837567.728578]: 0.00014743589743589742
python LearningRateFunctionModifier/ParamGroup1 [14 - 1705837567.728821]: 0.00014743589743589742
python DistillationModifier/task_loss [14 - 1705837567.7291253]: tensor(0.9270, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [14 - 1705837567.7301245]: tensor(1.8521, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [14 - 1705837567.7306664]: tensor(1.8521, grad_fn=<AddBackward0>)
python ConstantPruningModifier [14 - 1705837567.7312]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.2222222222222222| steps_per_epoch: 63| 
python ConstantPruningModifier [14 - 1705837567.9512663]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [14 - 1705837567.952073]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [14 - 1705837567.9530947]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [14 - 1705837567.9538841]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [14 - 1705837567.9545326]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [14 - 1705837567.9562416]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [14 - 1705837567.9580607]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [14 - 1705837567.9588404]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [14 - 1705837567.9595418]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [14 - 1705837567.9602132]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [14 - 1705837567.9608898]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [14 - 1705837567.9626558]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [14 - 1705837567.9642751]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [14 - 1705837567.965133]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [14 - 1705837567.965913]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [14 - 1705837567.9666243]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [14 - 1705837567.967285]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [14 - 1705837567.9690192]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [14 - 1705837567.970827]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [14 - 1705837567.971606]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [14 - 1705837567.972367]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [14 - 1705837567.973078]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [14 - 1705837567.9737453]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [14 - 1705837567.9755044]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [14 - 1705837567.97736]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [14 - 1705837567.9781668]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [14 - 1705837567.978884]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [14 - 1705837567.9796238]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [14 - 1705837567.980302]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [14 - 1705837567.9820936]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [14 - 1705837567.9839325]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [14 - 1705837567.9847682]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [14 - 1705837567.9855354]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [14 - 1705837567.9862638]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [14 - 1705837567.9869335]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [14 - 1705837567.9887195]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [14 - 1705837567.9904432]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [14 - 1705837567.9913325]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [14 - 1705837567.9921808]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [14 - 1705837567.9929173]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [14 - 1705837567.9936213]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [14 - 1705837567.9953623]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [14 - 1705837567.9971106]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [14 - 1705837567.9980729]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [14 - 1705837567.9988983]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [14 - 1705837567.9996686]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [14 - 1705837568.000385]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [14 - 1705837568.0021849]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [14 - 1705837568.0040708]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [14 - 1705837568.0049562]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [14 - 1705837568.005733]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [14 - 1705837568.0064561]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [14 - 1705837568.0071633]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [14 - 1705837568.0089834]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [14 - 1705837568.0108628]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [14 - 1705837568.0116878]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [14 - 1705837568.0124528]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [14 - 1705837568.013193]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [14 - 1705837568.0138657]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [14 - 1705837568.0156465]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [14 - 1705837568.0174556]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [14 - 1705837568.0182161]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [14 - 1705837568.018916]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [14 - 1705837568.0196195]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [14 - 1705837568.0202107]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [14 - 1705837568.021959]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [14 - 1705837568.0238235]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [14 - 1705837568.0246897]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [14 - 1705837568.0254364]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [14 - 1705837568.0261796]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [14 - 1705837568.026828]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [14 - 1705837568.0286393]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [14 - 1705837568.0305767]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [14 - 1705837568.031465]: 0.0
python ParamPruning/classifier.weight [14 - 1705837568.0320263]: 0.0
python DistillationModifier [21 - 1705837608.258421]: Calling loss_update with:
args: 0.7148825526237488| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 0.3333333333333333| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [21 - 1705837609.8190346]: 
Returned: 1.7543766498565674| 

python LearningRateFunctionModifier [21 - 1705837613.3209844]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.3333333333333333| steps_per_epoch: 63| 
python LearningRateFunctionModifier [21 - 1705837613.3211815]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [21 - 1705837613.3281915]: 0.00014615384615384615
python LearningRateFunctionModifier/ParamGroup1 [21 - 1705837613.3284433]: 0.00014615384615384615
python DistillationModifier/task_loss [21 - 1705837613.3287566]: tensor(0.7149, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [21 - 1705837613.3297532]: tensor(1.7544, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [21 - 1705837613.3302774]: tensor(1.7544, grad_fn=<AddBackward0>)
python ConstantPruningModifier [21 - 1705837613.3308442]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.3333333333333333| steps_per_epoch: 63| 
python ConstantPruningModifier [21 - 1705837613.557553]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [21 - 1705837613.558348]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [21 - 1705837613.559243]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [21 - 1705837613.5600135]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [21 - 1705837613.5608158]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [21 - 1705837613.5626202]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [21 - 1705837613.564564]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [21 - 1705837613.565504]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [21 - 1705837613.56629]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [21 - 1705837613.5670097]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [21 - 1705837613.5677233]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [21 - 1705837613.5695639]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [21 - 1705837613.5714486]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [21 - 1705837613.5723984]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [21 - 1705837613.5732098]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [21 - 1705837613.573942]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [21 - 1705837613.574604]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [21 - 1705837613.5764093]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [21 - 1705837613.5783358]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [21 - 1705837613.5792396]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [21 - 1705837613.5800102]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [21 - 1705837613.580713]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [21 - 1705837613.5814087]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [21 - 1705837613.583237]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [21 - 1705837613.5851564]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [21 - 1705837613.5860496]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [21 - 1705837613.5868113]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [21 - 1705837613.587528]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [21 - 1705837613.588236]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [21 - 1705837613.590043]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [21 - 1705837613.5920093]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [21 - 1705837613.5929604]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [21 - 1705837613.593745]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [21 - 1705837613.5944605]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [21 - 1705837613.5951324]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [21 - 1705837613.5969589]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [21 - 1705837613.5988557]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [21 - 1705837613.5997536]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [21 - 1705837613.600507]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [21 - 1705837613.601214]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [21 - 1705837613.6019006]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [21 - 1705837613.603746]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [21 - 1705837613.605697]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [21 - 1705837613.6066084]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [21 - 1705837613.607368]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [21 - 1705837613.6080496]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [21 - 1705837613.6087728]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [21 - 1705837613.6105523]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [21 - 1705837613.61253]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [21 - 1705837613.613472]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [21 - 1705837613.61416]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [21 - 1705837613.6148357]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [21 - 1705837613.6154711]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [21 - 1705837613.617262]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [21 - 1705837613.6192036]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [21 - 1705837613.620088]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [21 - 1705837613.6208777]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [21 - 1705837613.6215568]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [21 - 1705837613.6222024]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [21 - 1705837613.6239827]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [21 - 1705837613.625903]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [21 - 1705837613.626807]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [21 - 1705837613.6275706]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [21 - 1705837613.6282518]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [21 - 1705837613.6289332]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [21 - 1705837613.6307347]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [21 - 1705837613.6325023]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [21 - 1705837613.6334155]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [21 - 1705837613.6341913]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [21 - 1705837613.634857]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [21 - 1705837613.6354935]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [21 - 1705837613.6372974]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [21 - 1705837613.6392088]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [21 - 1705837613.6401308]: 0.0
python ParamPruning/classifier.weight [21 - 1705837613.6406174]: 0.0
python DistillationModifier [28 - 1705837655.2811904]: Calling loss_update with:
args: 0.7517673373222351| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 0.4444444444444444| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [28 - 1705837656.7673738]: 
Returned: 1.7954267263412476| 

python LearningRateFunctionModifier [28 - 1705837659.6958554]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.4444444444444444| steps_per_epoch: 63| 
python LearningRateFunctionModifier [28 - 1705837659.696025]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [28 - 1705837659.6960924]: 0.00014487179487179485
python LearningRateFunctionModifier/ParamGroup1 [28 - 1705837659.69634]: 0.00014487179487179485
python DistillationModifier/task_loss [28 - 1705837659.6966393]: tensor(0.7518, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [28 - 1705837659.697743]: tensor(1.7954, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [28 - 1705837659.6983023]: tensor(1.7954, grad_fn=<AddBackward0>)
python ConstantPruningModifier [28 - 1705837659.6988544]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.4444444444444444| steps_per_epoch: 63| 
python ConstantPruningModifier [28 - 1705837659.9235222]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [28 - 1705837659.9243135]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [28 - 1705837659.9253566]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [28 - 1705837659.9261465]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [28 - 1705837659.926802]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [28 - 1705837659.928591]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [28 - 1705837659.930381]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [28 - 1705837659.9313154]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [28 - 1705837659.9321501]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [28 - 1705837659.9328196]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [28 - 1705837659.933481]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [28 - 1705837659.9352553]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [28 - 1705837659.9372005]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [28 - 1705837659.9381282]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [28 - 1705837659.9388764]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [28 - 1705837659.939533]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [28 - 1705837659.940107]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [28 - 1705837659.9418888]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [28 - 1705837659.943663]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [28 - 1705837659.944591]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [28 - 1705837659.9453428]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [28 - 1705837659.9460094]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [28 - 1705837659.9466074]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [28 - 1705837659.948363]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [28 - 1705837659.9502652]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [28 - 1705837659.9511857]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [28 - 1705837659.9519813]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [28 - 1705837659.9526386]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [28 - 1705837659.9532368]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [28 - 1705837659.9549809]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [28 - 1705837659.9567626]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [28 - 1705837659.9576817]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [28 - 1705837659.958394]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [28 - 1705837659.9590497]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [28 - 1705837659.959648]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [28 - 1705837659.9613779]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [28 - 1705837659.9633064]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [28 - 1705837659.9642224]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [28 - 1705837659.9650156]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [28 - 1705837659.9657416]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [28 - 1705837659.966373]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [28 - 1705837659.968126]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [28 - 1705837659.9700062]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [28 - 1705837659.9709046]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [28 - 1705837659.9715998]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [28 - 1705837659.9723284]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [28 - 1705837659.9729571]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [28 - 1705837659.9747474]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [28 - 1705837659.97666]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [28 - 1705837659.9775903]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [28 - 1705837659.9783776]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [28 - 1705837659.9790337]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [28 - 1705837659.9797475]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [28 - 1705837659.9815376]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [28 - 1705837659.9832923]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [28 - 1705837659.984243]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [28 - 1705837659.9850852]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [28 - 1705837659.9858522]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [28 - 1705837659.9865267]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [28 - 1705837659.9883208]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [28 - 1705837659.9902353]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [28 - 1705837659.991145]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [28 - 1705837659.9918787]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [28 - 1705837659.9925063]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [28 - 1705837659.9931643]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [28 - 1705837659.994941]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [28 - 1705837659.996885]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [28 - 1705837659.9978259]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [28 - 1705837659.9986038]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [28 - 1705837659.9992566]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [28 - 1705837659.9999006]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [28 - 1705837660.0017]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [28 - 1705837660.0034926]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [28 - 1705837660.004436]: 0.0
python ParamPruning/classifier.weight [28 - 1705837660.0049953]: 0.0
python DistillationModifier [35 - 1705837701.4791765]: Calling loss_update with:
args: 0.811194658279419| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 0.5555555555555556| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [35 - 1705837702.9675944]: 
Returned: 1.7437303066253662| 

python LearningRateFunctionModifier [35 - 1705837705.859599]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.5555555555555556| steps_per_epoch: 63| 
python LearningRateFunctionModifier [35 - 1705837705.8597689]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [35 - 1705837705.859827]: 0.00014358974358974358
python LearningRateFunctionModifier/ParamGroup1 [35 - 1705837705.8600621]: 0.00014358974358974358
python DistillationModifier/task_loss [35 - 1705837705.8603656]: tensor(0.8112, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [35 - 1705837705.861399]: tensor(1.7437, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [35 - 1705837705.8619714]: tensor(1.7437, grad_fn=<AddBackward0>)
python ConstantPruningModifier [35 - 1705837705.8625028]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.5555555555555556| steps_per_epoch: 63| 
python ConstantPruningModifier [35 - 1705837706.0827465]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [35 - 1705837706.0835214]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [35 - 1705837706.0844984]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [35 - 1705837706.0852532]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [35 - 1705837706.0858612]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [35 - 1705837706.0875845]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [35 - 1705837706.0894585]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [35 - 1705837706.090312]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [35 - 1705837706.0909495]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [35 - 1705837706.0915947]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [35 - 1705837706.0921679]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [35 - 1705837706.0939384]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [35 - 1705837706.0957491]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [35 - 1705837706.0965583]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [35 - 1705837706.0973182]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [35 - 1705837706.0979006]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [35 - 1705837706.0985098]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [35 - 1705837706.1002166]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [35 - 1705837706.1018772]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [35 - 1705837706.1027296]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [35 - 1705837706.103463]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [35 - 1705837706.1041574]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [35 - 1705837706.1047769]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [35 - 1705837706.1064754]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [35 - 1705837706.1081495]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [35 - 1705837706.1090271]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [35 - 1705837706.109728]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [35 - 1705837706.1103652]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [35 - 1705837706.1109047]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [35 - 1705837706.112645]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [35 - 1705837706.114423]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [35 - 1705837706.1153345]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [35 - 1705837706.1161263]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [35 - 1705837706.1168435]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [35 - 1705837706.117433]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [35 - 1705837706.1191893]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [35 - 1705837706.1209283]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [35 - 1705837706.121811]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [35 - 1705837706.1225665]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [35 - 1705837706.123237]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [35 - 1705837706.1238756]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [35 - 1705837706.12563]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [35 - 1705837706.1275103]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [35 - 1705837706.1284108]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [35 - 1705837706.129192]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [35 - 1705837706.1298652]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [35 - 1705837706.1304657]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [35 - 1705837706.1322396]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [35 - 1705837706.1339285]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [35 - 1705837706.1348011]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [35 - 1705837706.135558]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [35 - 1705837706.136207]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [35 - 1705837706.1367655]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [35 - 1705837706.1384518]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [35 - 1705837706.1409836]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [35 - 1705837706.1418977]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [35 - 1705837706.1426704]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [35 - 1705837706.14331]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [35 - 1705837706.1439488]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [35 - 1705837706.1456888]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [35 - 1705837706.1472957]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [35 - 1705837706.1481345]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [35 - 1705837706.1488848]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [35 - 1705837706.1495485]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [35 - 1705837706.1501157]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [35 - 1705837706.151833]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [35 - 1705837706.1536663]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [35 - 1705837706.1544948]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [35 - 1705837706.1552]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [35 - 1705837706.1558294]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [35 - 1705837706.1563728]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [35 - 1705837706.1580825]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [35 - 1705837706.1597176]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [35 - 1705837706.1605556]: 0.0
python ParamPruning/classifier.weight [35 - 1705837706.1610718]: 0.0
python DistillationModifier [42 - 1705837747.888271]: Calling loss_update with:
args: 0.5868166089057922| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 0.6666666666666666| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [42 - 1705837749.3128676]: 
Returned: 1.442744255065918| 

python LearningRateFunctionModifier [42 - 1705837752.1929555]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.6666666666666666| steps_per_epoch: 63| 
python LearningRateFunctionModifier [42 - 1705837752.193121]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [42 - 1705837752.1931796]: 0.00014230769230769228
python LearningRateFunctionModifier/ParamGroup1 [42 - 1705837752.1934083]: 0.00014230769230769228
python DistillationModifier/task_loss [42 - 1705837752.1937063]: tensor(0.5868, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [42 - 1705837752.1946733]: tensor(1.4427, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [42 - 1705837752.1952024]: tensor(1.4427, grad_fn=<AddBackward0>)
python ConstantPruningModifier [42 - 1705837752.1957355]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.6666666666666666| steps_per_epoch: 63| 
python ConstantPruningModifier [42 - 1705837752.4172404]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [42 - 1705837752.4180262]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [42 - 1705837752.4188826]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [42 - 1705837752.419566]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [42 - 1705837752.420261]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [42 - 1705837752.421993]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [42 - 1705837752.423616]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [42 - 1705837752.424441]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [42 - 1705837752.4251907]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [42 - 1705837752.4258478]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [42 - 1705837752.4264553]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [42 - 1705837752.4281454]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [42 - 1705837752.4299853]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [42 - 1705837752.4307702]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [42 - 1705837752.4314518]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [42 - 1705837752.432155]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [42 - 1705837752.4327896]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [42 - 1705837752.434478]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [42 - 1705837752.4362519]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [42 - 1705837752.436993]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [42 - 1705837752.437611]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [42 - 1705837752.4381394]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [42 - 1705837752.4387558]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [42 - 1705837752.4404418]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [42 - 1705837752.4422195]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [42 - 1705837752.4429722]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [42 - 1705837752.4436655]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [42 - 1705837752.444304]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [42 - 1705837752.4449298]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [42 - 1705837752.4466147]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [42 - 1705837752.448432]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [42 - 1705837752.4492037]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [42 - 1705837752.4498453]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [42 - 1705837752.4503746]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [42 - 1705837752.4509683]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [42 - 1705837752.452704]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [42 - 1705837752.4545012]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [42 - 1705837752.4552488]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [42 - 1705837752.455848]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [42 - 1705837752.4564528]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [42 - 1705837752.4570882]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [42 - 1705837752.4588096]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [42 - 1705837752.4606552]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [42 - 1705837752.4614625]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [42 - 1705837752.4620903]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [42 - 1705837752.4627013]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [42 - 1705837752.4633498]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [42 - 1705837752.4650524]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [42 - 1705837752.4668577]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [42 - 1705837752.467779]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [42 - 1705837752.4684818]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [42 - 1705837752.4691007]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [42 - 1705837752.469741]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [42 - 1705837752.4714825]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [42 - 1705837752.4731696]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [42 - 1705837752.4740493]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [42 - 1705837752.4748068]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [42 - 1705837752.4755383]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [42 - 1705837752.476217]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [42 - 1705837752.4780078]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [42 - 1705837752.4799242]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [42 - 1705837752.4808064]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [42 - 1705837752.4814978]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [42 - 1705837752.4821842]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [42 - 1705837752.4827936]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [42 - 1705837752.4845247]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [42 - 1705837752.4864213]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [42 - 1705837752.4872177]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [42 - 1705837752.487922]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [42 - 1705837752.4886627]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [42 - 1705837752.4892967]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [42 - 1705837752.4910388]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [42 - 1705837752.4928272]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [42 - 1705837752.4936683]: 0.0
python ParamPruning/classifier.weight [42 - 1705837752.4941518]: 0.0
python DistillationModifier [49 - 1705837792.4251573]: Calling loss_update with:
args: 0.5385098457336426| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 0.7777777777777778| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [49 - 1705837793.98768]: 
Returned: 1.3549563884735107| 

python LearningRateFunctionModifier [49 - 1705837796.86036]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.7777777777777778| steps_per_epoch: 63| 
python LearningRateFunctionModifier [49 - 1705837796.8605292]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [49 - 1705837796.8605921]: 0.00014102564102564101
python LearningRateFunctionModifier/ParamGroup1 [49 - 1705837796.8608418]: 0.00014102564102564101
python DistillationModifier/task_loss [49 - 1705837796.861159]: tensor(0.5385, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [49 - 1705837796.8621364]: tensor(1.3550, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [49 - 1705837796.8627489]: tensor(1.3550, grad_fn=<AddBackward0>)
python ConstantPruningModifier [49 - 1705837796.8633492]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.7777777777777778| steps_per_epoch: 63| 
python ConstantPruningModifier [49 - 1705837797.0823066]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [49 - 1705837797.0831282]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [49 - 1705837797.0841951]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [49 - 1705837797.0850377]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [49 - 1705837797.0858083]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [49 - 1705837797.0876167]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [49 - 1705837797.0894208]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [49 - 1705837797.0903685]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [49 - 1705837797.0912042]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [49 - 1705837797.0919507]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [49 - 1705837797.0926187]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [49 - 1705837797.0944476]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [49 - 1705837797.096254]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [49 - 1705837797.0972044]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [49 - 1705837797.0980356]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [49 - 1705837797.0988076]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [49 - 1705837797.0995102]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [49 - 1705837797.101393]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [49 - 1705837797.1031861]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [49 - 1705837797.1040936]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [49 - 1705837797.104926]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [49 - 1705837797.105695]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [49 - 1705837797.1063974]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [49 - 1705837797.1081877]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [49 - 1705837797.1099129]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [49 - 1705837797.110758]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [49 - 1705837797.1114976]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [49 - 1705837797.112259]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [49 - 1705837797.112909]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [49 - 1705837797.1146417]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [49 - 1705837797.116326]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [49 - 1705837797.117189]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [49 - 1705837797.1179283]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [49 - 1705837797.1185858]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [49 - 1705837797.1192129]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [49 - 1705837797.1209612]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [49 - 1705837797.1226156]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [49 - 1705837797.1234663]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [49 - 1705837797.1242359]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [49 - 1705837797.1249]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [49 - 1705837797.1255577]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [49 - 1705837797.1272972]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [49 - 1705837797.129]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [49 - 1705837797.1298459]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [49 - 1705837797.1306248]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [49 - 1705837797.1312797]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [49 - 1705837797.1319664]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [49 - 1705837797.1337051]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [49 - 1705837797.135328]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [49 - 1705837797.13616]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [49 - 1705837797.1369457]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [49 - 1705837797.137603]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [49 - 1705837797.1383185]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [49 - 1705837797.140046]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [49 - 1705837797.141748]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [49 - 1705837797.1425917]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [49 - 1705837797.1433456]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [49 - 1705837797.143995]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [49 - 1705837797.144698]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [49 - 1705837797.1464553]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [49 - 1705837797.1481438]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [49 - 1705837797.1489913]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [49 - 1705837797.1497002]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [49 - 1705837797.1504083]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [49 - 1705837797.1509938]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [49 - 1705837797.152748]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [49 - 1705837797.15439]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [49 - 1705837797.1552222]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [49 - 1705837797.1559799]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [49 - 1705837797.1566525]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [49 - 1705837797.15735]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [49 - 1705837797.1590834]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [49 - 1705837797.1607823]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [49 - 1705837797.1616395]: 0.0
python ParamPruning/classifier.weight [49 - 1705837797.1621225]: 0.0
python DistillationModifier [56 - 1705837838.7981799]: Calling loss_update with:
args: 0.8539763689041138| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 0.8888888888888888| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [56 - 1705837840.3260682]: 
Returned: 1.5167315006256104| 

python LearningRateFunctionModifier [56 - 1705837843.2006166]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.8888888888888888| steps_per_epoch: 63| 
python LearningRateFunctionModifier [56 - 1705837843.2008007]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [56 - 1705837843.2008712]: 0.00013974358974358974
python LearningRateFunctionModifier/ParamGroup1 [56 - 1705837843.2010858]: 0.00013974358974358974
python DistillationModifier/task_loss [56 - 1705837843.2013907]: tensor(0.8540, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [56 - 1705837843.202361]: tensor(1.5167, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [56 - 1705837843.202892]: tensor(1.5167, grad_fn=<AddBackward0>)
python ConstantPruningModifier [56 - 1705837843.203419]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 0.8888888888888888| steps_per_epoch: 63| 
python ConstantPruningModifier [56 - 1705837843.421504]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [56 - 1705837843.4223142]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [56 - 1705837843.4231668]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [56 - 1705837843.4237702]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [56 - 1705837843.4243]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [56 - 1705837843.4260292]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [56 - 1705837843.4276435]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [56 - 1705837843.428419]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [56 - 1705837843.4290884]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [56 - 1705837843.42965]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [56 - 1705837843.4301524]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [56 - 1705837843.431795]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [56 - 1705837843.4334214]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [56 - 1705837843.4341934]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [56 - 1705837843.4348352]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [56 - 1705837843.4353797]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [56 - 1705837843.4359117]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [56 - 1705837843.4375155]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [56 - 1705837843.4390545]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [56 - 1705837843.4398034]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [56 - 1705837843.4404209]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [56 - 1705837843.4409902]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [56 - 1705837843.441503]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [56 - 1705837843.4430838]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [56 - 1705837843.444892]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [56 - 1705837843.4456415]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [56 - 1705837843.4462302]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [56 - 1705837843.446757]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [56 - 1705837843.4472556]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [56 - 1705837843.4489343]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [56 - 1705837843.4504917]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [56 - 1705837843.4512339]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [56 - 1705837843.451851]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [56 - 1705837843.4523826]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [56 - 1705837843.4529123]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [56 - 1705837843.454515]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [56 - 1705837843.4562771]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [56 - 1705837843.4570148]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [56 - 1705837843.457591]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [56 - 1705837843.458118]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [56 - 1705837843.458667]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [56 - 1705837843.4602938]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [56 - 1705837843.4620872]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [56 - 1705837843.4628277]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [56 - 1705837843.4634335]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [56 - 1705837843.4639595]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [56 - 1705837843.464469]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [56 - 1705837843.466235]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [56 - 1705837843.467902]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [56 - 1705837843.4687314]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [56 - 1705837843.4693856]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [56 - 1705837843.469934]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [56 - 1705837843.4704452]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [56 - 1705837843.4720638]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [56 - 1705837843.4736297]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [56 - 1705837843.474396]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [56 - 1705837843.4750164]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [56 - 1705837843.4755554]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [56 - 1705837843.476063]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [56 - 1705837843.4776936]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [56 - 1705837843.479232]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [56 - 1705837843.4799783]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [56 - 1705837843.480612]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [56 - 1705837843.481172]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [56 - 1705837843.481683]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [56 - 1705837843.4832494]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [56 - 1705837843.4850214]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [56 - 1705837843.485764]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [56 - 1705837843.4863615]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [56 - 1705837843.4869096]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [56 - 1705837843.4874194]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [56 - 1705837843.4891415]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [56 - 1705837843.4908996]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [56 - 1705837843.4916537]: 0.0
python ParamPruning/classifier.weight [56 - 1705837843.4920893]: 0.0
python DistillationModifier [63 - 1705837881.1399744]: Calling loss_update with:
args: 0.9464573860168457| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837882.5652575]: 
Returned: 1.9469448328018188| 

python DistillationModifier [63 - 1705837884.0194654]: Calling loss_update with:
args: 0.7768958806991577| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837885.7762964]: 
Returned: 1.7692899703979492| 

python DistillationModifier [63 - 1705837887.4445415]: Calling loss_update with:
args: 0.8820074200630188| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837888.8763573]: 
Returned: 1.877394437789917| 

python DistillationModifier [63 - 1705837890.3277872]: Calling loss_update with:
args: 0.7534249424934387| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837891.7506146]: 
Returned: 1.5300596952438354| 

python DistillationModifier [63 - 1705837893.2020307]: Calling loss_update with:
args: 0.7368133664131165| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837894.6277711]: 
Returned: 1.3880407810211182| 

python DistillationModifier [63 - 1705837896.0836277]: Calling loss_update with:
args: 0.5637067556381226| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837897.5070677]: 
Returned: 1.2879104614257812| 

python DistillationModifier [63 - 1705837898.9619234]: Calling loss_update with:
args: 0.6456726789474487| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837900.378944]: 
Returned: 1.7209992408752441| 

python DistillationModifier [63 - 1705837901.8440027]: Calling loss_update with:
args: 0.7949608564376831| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837903.2809238]: 
Returned: 1.7986756563186646| 

python DistillationModifier [63 - 1705837904.7465367]: Calling loss_update with:
args: 0.5581240057945251| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837906.18545]: 
Returned: 1.2270407676696777| 

python DistillationModifier [63 - 1705837907.6540565]: Calling loss_update with:
args: 0.5782851576805115| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837909.0927362]: 
Returned: 1.5661373138427734| 

python DistillationModifier [63 - 1705837910.5689685]: Calling loss_update with:
args: 0.7404752969741821| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837912.0184178]: 
Returned: 1.5368040800094604| 

python DistillationModifier [63 - 1705837913.4886515]: Calling loss_update with:
args: 0.6970163583755493| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837914.9225056]: 
Returned: 1.3753061294555664| 

python DistillationModifier [63 - 1705837916.430902]: Calling loss_update with:
args: 0.7988996505737305| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837917.8680007]: 
Returned: 1.993536114692688| 

python DistillationModifier [63 - 1705837919.346058]: Calling loss_update with:
args: 0.7532771229743958| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837920.7959268]: 
Returned: 1.274122714996338| 

python DistillationModifier [63 - 1705837922.50697]: Calling loss_update with:
args: 0.8314822912216187| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837924.5355532]: 
Returned: 1.675390362739563| 

python DistillationModifier [63 - 1705837926.3376744]: Calling loss_update with:
args: 0.8046398162841797| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837927.7837517]: 
Returned: 1.9470443725585938| 

python DistillationModifier [63 - 1705837929.2567933]: Calling loss_update with:
args: 0.6087195873260498| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837930.7031047]: 
Returned: 1.3933284282684326| 

python DistillationModifier [63 - 1705837932.1839898]: Calling loss_update with:
args: 0.6940927505493164| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837933.6345146]: 
Returned: 1.736053705215454| 

python DistillationModifier [63 - 1705837935.115082]: Calling loss_update with:
args: 0.9239374399185181| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837936.5577602]: 
Returned: 1.734434723854065| 

python DistillationModifier [63 - 1705837938.0521216]: Calling loss_update with:
args: 0.7760050892829895| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837939.5003817]: 
Returned: 1.5749731063842773| 

python DistillationModifier [63 - 1705837940.973376]: Calling loss_update with:
args: 0.5871966481208801| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837942.4229937]: 
Returned: 1.232383370399475| 

python DistillationModifier [63 - 1705837943.8967297]: Calling loss_update with:
args: 0.6712377667427063| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837945.3454096]: 
Returned: 1.5183446407318115| 

python DistillationModifier [63 - 1705837946.8636544]: Calling loss_update with:
args: 0.6848045587539673| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837948.3148222]: 
Returned: 1.588671326637268| 

python DistillationModifier [63 - 1705837949.7897103]: Calling loss_update with:
args: 0.5328174233436584| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837951.2320187]: 
Returned: 1.450924038887024| 

python DistillationModifier [63 - 1705837952.7096663]: Calling loss_update with:
args: 0.9464438557624817| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837954.1572456]: 
Returned: 1.8834035396575928| 

python DistillationModifier [63 - 1705837955.6314445]: Calling loss_update with:
args: 0.6426951885223389| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837957.0750372]: 
Returned: 1.4651561975479126| 

python DistillationModifier [63 - 1705837958.545135]: Calling loss_update with:
args: 0.6394565105438232| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837959.984526]: 
Returned: 1.4239661693572998| 

python DistillationModifier [63 - 1705837961.4598036]: Calling loss_update with:
args: 0.663133442401886| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837962.901431]: 
Returned: 1.5437039136886597| 

python DistillationModifier [63 - 1705837964.36831]: Calling loss_update with:
args: 0.8113073706626892| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837965.807839]: 
Returned: 1.8344064950942993| 

python DistillationModifier [63 - 1705837967.2696555]: Calling loss_update with:
args: 0.5431889891624451| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837968.7167015]: 
Returned: 1.3691329956054688| 

python DistillationModifier [63 - 1705837970.1983206]: Calling loss_update with:
args: 0.7903780937194824| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837971.6415908]: 
Returned: 1.8264895677566528| 

python DistillationModifier [63 - 1705837973.114701]: Calling loss_update with:
args: 0.6749346852302551| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837974.5496097]: 
Returned: 1.6480188369750977| 

python DistillationModifier [63 - 1705837976.1250975]: Calling loss_update with:
args: 0.872033953666687| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837978.060072]: 
Returned: 1.9783968925476074| 

python DistillationModifier [63 - 1705837979.536418]: Calling loss_update with:
args: 0.8309798240661621| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837980.9728684]: 
Returned: 1.6548494100570679| 

python DistillationModifier [63 - 1705837982.7551827]: Calling loss_update with:
args: 0.7713735699653625| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837984.783424]: 
Returned: 1.7980592250823975| 

python DistillationModifier [63 - 1705837986.5104125]: Calling loss_update with:
args: 0.963102400302887| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837987.9475918]: 
Returned: 2.091510534286499| 

python DistillationModifier [63 - 1705837989.4238732]: Calling loss_update with:
args: 0.8461157083511353| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837990.860003]: 
Returned: 1.659471869468689| 

python DistillationModifier [63 - 1705837992.3360987]: Calling loss_update with:
args: 0.5898138880729675| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837993.7791665]: 
Returned: 1.5031214952468872| 

python DistillationModifier [63 - 1705837995.2498624]: Calling loss_update with:
args: 0.831753134727478| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837996.6863346]: 
Returned: 1.8051602840423584| 

python DistillationModifier [63 - 1705837998.1605601]: Calling loss_update with:
args: 0.8100122213363647| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705837999.6125433]: 
Returned: 1.9258819818496704| 

python DistillationModifier [63 - 1705838001.0888135]: Calling loss_update with:
args: 0.6535601615905762| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838002.5303836]: 
Returned: 1.6515414714813232| 

python DistillationModifier [63 - 1705838004.0047932]: Calling loss_update with:
args: 0.5612171292304993| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838005.4454508]: 
Returned: 1.53281569480896| 

python DistillationModifier [63 - 1705838006.957108]: Calling loss_update with:
args: 0.6610413193702698| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838008.3972986]: 
Returned: 1.4598194360733032| 

python DistillationModifier [63 - 1705838009.8761477]: Calling loss_update with:
args: 0.5888146162033081| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838011.3175583]: 
Returned: 1.5238947868347168| 

python DistillationModifier [63 - 1705838012.7859282]: Calling loss_update with:
args: 0.8098070621490479| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838014.2238865]: 
Returned: 1.9769952297210693| 

python DistillationModifier [63 - 1705838015.6945248]: Calling loss_update with:
args: 0.7839549779891968| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838017.1383603]: 
Returned: 1.7101950645446777| 

python DistillationModifier [63 - 1705838018.606182]: Calling loss_update with:
args: 0.7319726347923279| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838020.0501087]: 
Returned: 1.7592757940292358| 

python DistillationModifier [63 - 1705838021.518314]: Calling loss_update with:
args: 0.6629967093467712| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838022.9556086]: 
Returned: 1.8077901601791382| 

python DistillationModifier [63 - 1705838024.4276326]: Calling loss_update with:
args: 0.7559803128242493| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838025.8689265]: 
Returned: 1.6283231973648071| 

python DistillationModifier [63 - 1705838027.3451817]: Calling loss_update with:
args: 0.6137759685516357| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838028.790032]: 
Returned: 1.514427661895752| 

python DistillationModifier [63 - 1705838030.2712386]: Calling loss_update with:
args: 0.5902864933013916| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838031.715002]: 
Returned: 1.3087661266326904| 

python DistillationModifier [63 - 1705838033.1908643]: Calling loss_update with:
args: 0.8605732917785645| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838034.6350033]: 
Returned: 1.3255263566970825| 

python DistillationModifier [63 - 1705838036.1098082]: Calling loss_update with:
args: 0.69405597448349| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838037.6027768]: 
Returned: 1.574514389038086| 

python DistillationModifier [63 - 1705838039.0921452]: Calling loss_update with:
args: 0.7528899908065796| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838040.5322535]: 
Returned: 1.4954583644866943| 

python DistillationModifier [63 - 1705838042.1421494]: Calling loss_update with:
args: 0.5753974914550781| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838044.2040448]: 
Returned: 1.464097023010254| 

python DistillationModifier [63 - 1705838046.098915]: Calling loss_update with:
args: 0.6990266442298889| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838047.5370684]: 
Returned: 1.5330325365066528| 

python DistillationModifier [63 - 1705838049.0072925]: Calling loss_update with:
args: 0.7279353141784668| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838050.4560037]: 
Returned: 1.2595540285110474| 

python DistillationModifier [63 - 1705838051.9283247]: Calling loss_update with:
args: 0.7841033339500427| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838053.3763633]: 
Returned: 1.3119536638259888| 

python DistillationModifier [63 - 1705838054.8481848]: Calling loss_update with:
args: 0.945914089679718| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838056.2874405]: 
Returned: 1.9069652557373047| 

python DistillationModifier [63 - 1705838057.7798827]: Calling loss_update with:
args: 0.6584880352020264| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838059.2397704]: 
Returned: 1.6440415382385254| 

python DistillationModifier [63 - 1705838060.7110283]: Calling loss_update with:
args: 0.7447539567947388| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838062.1494339]: 
Returned: 1.6587320566177368| 

python DistillationModifier [63 - 1705838063.6190717]: Calling loss_update with:
args: 0.8176577091217041| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838065.0646484]: 
Returned: 1.7845337390899658| 

python DistillationModifier [63 - 1705838065.835613]: Calling loss_update with:
args: 0.4871220886707306| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838066.582341]: 
Returned: 1.1842557191848755| 

python DistillationModifier [63 - 1705838068.6675248]: Calling loss_update with:
args: 0.4800061285495758| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [63 - 1705838070.1253657]: 
Returned: 1.189700722694397| 

python LearningRateFunctionModifier [63 - 1705838073.048012]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [63 - 1705838073.0481865]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [63 - 1705838073.0482554]: 0.00013846153846153845
python LearningRateFunctionModifier/ParamGroup1 [63 - 1705838073.0485094]: 0.00013846153846153845
python DistillationModifier/task_loss [63 - 1705838073.0487974]: tensor(0.4800, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [63 - 1705838073.0498722]: tensor(1.1897, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [63 - 1705838073.0504894]: tensor(1.1897, grad_fn=<AddBackward0>)
python ConstantPruningModifier [63 - 1705838073.0511038]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.0| steps_per_epoch: 63| 
python ConstantPruningModifier [63 - 1705838073.2755268]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [63 - 1705838073.2762845]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [63 - 1705838073.2771623]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [63 - 1705838073.2779636]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [63 - 1705838073.278678]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [63 - 1705838073.2804856]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [63 - 1705838073.282343]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [63 - 1705838073.2833495]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [63 - 1705838073.2842286]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [63 - 1705838073.2850292]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [63 - 1705838073.2857852]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [63 - 1705838073.2875943]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [63 - 1705838073.2894046]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [63 - 1705838073.29041]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [63 - 1705838073.2912946]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [63 - 1705838073.2921147]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [63 - 1705838073.2928927]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [63 - 1705838073.2947276]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [63 - 1705838073.2967358]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [63 - 1705838073.2977433]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [63 - 1705838073.2985935]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [63 - 1705838073.2993803]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [63 - 1705838073.3001158]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [63 - 1705838073.3019595]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [63 - 1705838073.303933]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [63 - 1705838073.3049204]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [63 - 1705838073.3057706]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [63 - 1705838073.3065703]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [63 - 1705838073.3073158]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [63 - 1705838073.3091636]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [63 - 1705838073.3110192]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [63 - 1705838073.3120313]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [63 - 1705838073.3129375]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [63 - 1705838073.3137228]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [63 - 1705838073.3144734]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [63 - 1705838073.316311]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [63 - 1705838073.3181834]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [63 - 1705838073.3191848]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [63 - 1705838073.3200412]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [63 - 1705838073.3208654]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [63 - 1705838073.321558]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [63 - 1705838073.3233497]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [63 - 1705838073.3254776]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [63 - 1705838073.3265865]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [63 - 1705838073.3273594]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [63 - 1705838073.3281248]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [63 - 1705838073.3288743]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [63 - 1705838073.3306947]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [63 - 1705838073.332733]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [63 - 1705838073.3337293]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [63 - 1705838073.334544]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [63 - 1705838073.3352797]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [63 - 1705838073.335988]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [63 - 1705838073.3378334]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [63 - 1705838073.3396754]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [63 - 1705838073.3407185]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [63 - 1705838073.3415976]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [63 - 1705838073.3423898]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [63 - 1705838073.3431294]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [63 - 1705838073.3449743]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [63 - 1705838073.3468335]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [63 - 1705838073.347864]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [63 - 1705838073.3486624]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [63 - 1705838073.3494325]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [63 - 1705838073.3501658]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [63 - 1705838073.3519642]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [63 - 1705838073.353962]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [63 - 1705838073.3549576]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [63 - 1705838073.3558073]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [63 - 1705838073.356615]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [63 - 1705838073.357376]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [63 - 1705838073.3592155]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [63 - 1705838073.3611093]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [63 - 1705838073.3621275]: 0.0
python ParamPruning/classifier.weight [63 - 1705838073.3627076]: 0.0
python DistillationModifier [70 - 1705838115.2506883]: Calling loss_update with:
args: 0.4992658197879791| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.1111111111111112| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [70 - 1705838116.723061]: 
Returned: 1.2083607912063599| 

python LearningRateFunctionModifier [70 - 1705838119.626936]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.1111111111111112| steps_per_epoch: 63| 
python LearningRateFunctionModifier [70 - 1705838119.6271095]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [70 - 1705838119.627182]: 0.00013717948717948718
python LearningRateFunctionModifier/ParamGroup1 [70 - 1705838119.6274023]: 0.00013717948717948718
python DistillationModifier/task_loss [70 - 1705838119.6277068]: tensor(0.4993, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [70 - 1705838119.6286917]: tensor(1.2084, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [70 - 1705838119.629242]: tensor(1.2084, grad_fn=<AddBackward0>)
python ConstantPruningModifier [70 - 1705838119.629784]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.1111111111111112| steps_per_epoch: 63| 
python ConstantPruningModifier [70 - 1705838119.8519847]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [70 - 1705838119.8527966]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [70 - 1705838119.8537195]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [70 - 1705838119.8543448]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [70 - 1705838119.854884]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [70 - 1705838119.8566644]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [70 - 1705838119.8585827]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [70 - 1705838119.85944]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [70 - 1705838119.8600986]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [70 - 1705838119.8606625]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [70 - 1705838119.861201]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [70 - 1705838119.8629327]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [70 - 1705838119.864837]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [70 - 1705838119.8657014]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [70 - 1705838119.8663838]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [70 - 1705838119.8669567]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [70 - 1705838119.8674731]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [70 - 1705838119.8692753]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [70 - 1705838119.871185]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [70 - 1705838119.8720622]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [70 - 1705838119.872772]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [70 - 1705838119.873343]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [70 - 1705838119.8738642]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [70 - 1705838119.8755863]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [70 - 1705838119.8775153]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [70 - 1705838119.8783567]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [70 - 1705838119.8789806]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [70 - 1705838119.8795252]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [70 - 1705838119.8800306]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [70 - 1705838119.88177]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [70 - 1705838119.8836656]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [70 - 1705838119.8845003]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [70 - 1705838119.8851848]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [70 - 1705838119.8857458]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [70 - 1705838119.8862839]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [70 - 1705838119.8880577]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [70 - 1705838119.8899255]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [70 - 1705838119.8907874]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [70 - 1705838119.891433]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [70 - 1705838119.8920348]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [70 - 1705838119.8925526]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [70 - 1705838119.8943129]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [70 - 1705838119.896152]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [70 - 1705838119.8969936]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [70 - 1705838119.8976326]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [70 - 1705838119.8982031]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [70 - 1705838119.898728]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [70 - 1705838119.9004903]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [70 - 1705838119.9024074]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [70 - 1705838119.9032576]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [70 - 1705838119.9039268]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [70 - 1705838119.904473]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [70 - 1705838119.90501]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [70 - 1705838119.9067304]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [70 - 1705838119.9086013]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [70 - 1705838119.9094625]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [70 - 1705838119.9101133]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [70 - 1705838119.9106584]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [70 - 1705838119.9111593]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [70 - 1705838119.912955]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [70 - 1705838119.9149222]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [70 - 1705838119.915809]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [70 - 1705838119.9164944]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [70 - 1705838119.9170802]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [70 - 1705838119.9176164]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [70 - 1705838119.9193792]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [70 - 1705838119.9213126]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [70 - 1705838119.9221542]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [70 - 1705838119.922803]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [70 - 1705838119.9233618]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [70 - 1705838119.9238775]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [70 - 1705838119.9256563]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [70 - 1705838119.927547]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [70 - 1705838119.928413]: 0.0
python ParamPruning/classifier.weight [70 - 1705838119.9289382]: 0.0
python DistillationModifier [77 - 1705838160.3935285]: Calling loss_update with:
args: 0.35448917746543884| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.2222222222222223| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [77 - 1705838162.123782]: 
Returned: 0.9948418140411377| 

python LearningRateFunctionModifier [77 - 1705838166.000859]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.2222222222222223| steps_per_epoch: 63| 
python LearningRateFunctionModifier [77 - 1705838166.001038]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [77 - 1705838166.0011046]: 0.00013589743589743588
python LearningRateFunctionModifier/ParamGroup1 [77 - 1705838166.0013247]: 0.00013589743589743588
python DistillationModifier/task_loss [77 - 1705838166.0016255]: tensor(0.3545, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [77 - 1705838166.0026515]: tensor(0.9948, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [77 - 1705838166.003235]: tensor(0.9948, grad_fn=<AddBackward0>)
python ConstantPruningModifier [77 - 1705838166.0037992]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.2222222222222223| steps_per_epoch: 63| 
python ConstantPruningModifier [77 - 1705838166.228039]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [77 - 1705838166.2288704]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [77 - 1705838166.229776]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [77 - 1705838166.2305958]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [77 - 1705838166.2313128]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [77 - 1705838166.2333386]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [77 - 1705838166.2351186]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [77 - 1705838166.2360117]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [77 - 1705838166.2368195]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [77 - 1705838166.2375152]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [77 - 1705838166.2381322]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [77 - 1705838166.2399316]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [77 - 1705838166.2418647]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [77 - 1705838166.2427602]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [77 - 1705838166.2435281]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [77 - 1705838166.244217]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [77 - 1705838166.2448525]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [77 - 1705838166.2466333]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [77 - 1705838166.2485259]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [77 - 1705838166.2494268]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [77 - 1705838166.2501173]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [77 - 1705838166.2507868]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [77 - 1705838166.2513957]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [77 - 1705838166.2532208]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [77 - 1705838166.2551906]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [77 - 1705838166.2561183]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [77 - 1705838166.256844]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [77 - 1705838166.2575433]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [77 - 1705838166.2581506]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [77 - 1705838166.2599134]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [77 - 1705838166.261892]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [77 - 1705838166.2628047]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [77 - 1705838166.2635512]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [77 - 1705838166.2642417]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [77 - 1705838166.2648954]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [77 - 1705838166.2666523]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [77 - 1705838166.2685614]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [77 - 1705838166.269489]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [77 - 1705838166.2703]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [77 - 1705838166.2710075]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [77 - 1705838166.271599]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [77 - 1705838166.2733815]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [77 - 1705838166.275351]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [77 - 1705838166.2762716]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [77 - 1705838166.277098]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [77 - 1705838166.2778149]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [77 - 1705838166.2784452]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [77 - 1705838166.2802572]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [77 - 1705838166.2822466]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [77 - 1705838166.283184]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [77 - 1705838166.2839437]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [77 - 1705838166.2846217]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [77 - 1705838166.2852898]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [77 - 1705838166.2870686]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [77 - 1705838166.2890084]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [77 - 1705838166.2898765]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [77 - 1705838166.2906184]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [77 - 1705838166.291259]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [77 - 1705838166.291923]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [77 - 1705838166.293716]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [77 - 1705838166.2955885]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [77 - 1705838166.2964332]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [77 - 1705838166.2971454]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [77 - 1705838166.2978232]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [77 - 1705838166.2984188]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [77 - 1705838166.3001797]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [77 - 1705838166.3021574]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [77 - 1705838166.3031058]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [77 - 1705838166.3038907]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [77 - 1705838166.3045757]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [77 - 1705838166.3052137]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [77 - 1705838166.3070223]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [77 - 1705838166.3088248]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [77 - 1705838166.309705]: 0.0
python ParamPruning/classifier.weight [77 - 1705838166.3102102]: 0.0
python DistillationModifier [84 - 1705838207.3359234]: Calling loss_update with:
args: 0.4405909776687622| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.3333333333333333| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [84 - 1705838208.8955681]: 
Returned: 1.1227377653121948| 

python LearningRateFunctionModifier [84 - 1705838211.8327687]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.3333333333333333| steps_per_epoch: 63| 
python LearningRateFunctionModifier [84 - 1705838211.8329372]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [84 - 1705838211.833003]: 0.0001346153846153846
python LearningRateFunctionModifier/ParamGroup1 [84 - 1705838211.8332334]: 0.0001346153846153846
python DistillationModifier/task_loss [84 - 1705838211.8335292]: tensor(0.4406, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [84 - 1705838211.8345792]: tensor(1.1227, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [84 - 1705838211.835184]: tensor(1.1227, grad_fn=<AddBackward0>)
python ConstantPruningModifier [84 - 1705838211.835763]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.3333333333333333| steps_per_epoch: 63| 
python ConstantPruningModifier [84 - 1705838212.062355]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [84 - 1705838212.0631766]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [84 - 1705838212.0640643]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [84 - 1705838212.0649168]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [84 - 1705838212.0656247]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [84 - 1705838212.0674865]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [84 - 1705838212.0695734]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [84 - 1705838212.0705998]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [84 - 1705838212.0715156]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [84 - 1705838212.0723338]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [84 - 1705838212.0731525]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [84 - 1705838212.075041]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [84 - 1705838212.0771058]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [84 - 1705838212.078106]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [84 - 1705838212.0789566]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [84 - 1705838212.079809]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [84 - 1705838212.0806177]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [84 - 1705838212.0825248]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [84 - 1705838212.0845578]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [84 - 1705838212.0856137]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [84 - 1705838212.0864317]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [84 - 1705838212.0871394]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [84 - 1705838212.0878663]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [84 - 1705838212.0897386]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [84 - 1705838212.0918136]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [84 - 1705838212.0928817]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [84 - 1705838212.0937872]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [84 - 1705838212.094627]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [84 - 1705838212.0954442]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [84 - 1705838212.0973675]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [84 - 1705838212.0994198]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [84 - 1705838212.100457]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [84 - 1705838212.1013443]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [84 - 1705838212.1021054]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [84 - 1705838212.1028683]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [84 - 1705838212.1047688]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [84 - 1705838212.1066997]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [84 - 1705838212.1077344]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [84 - 1705838212.1086125]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [84 - 1705838212.1093707]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [84 - 1705838212.1101248]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [84 - 1705838212.1120126]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [84 - 1705838212.1140993]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [84 - 1705838212.1151123]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [84 - 1705838212.1160002]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [84 - 1705838212.1168473]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [84 - 1705838212.1176133]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [84 - 1705838212.1194987]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [84 - 1705838212.1215966]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [84 - 1705838212.1226494]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [84 - 1705838212.1235106]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [84 - 1705838212.1243622]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [84 - 1705838212.1251788]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [84 - 1705838212.1270676]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [84 - 1705838212.1291723]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [84 - 1705838212.1302156]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [84 - 1705838212.1310914]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [84 - 1705838212.1318676]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [84 - 1705838212.1326866]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [84 - 1705838212.1346405]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [84 - 1705838212.1367605]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [84 - 1705838212.1377656]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [84 - 1705838212.1387186]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [84 - 1705838212.1396303]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [84 - 1705838212.140521]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [84 - 1705838212.1425407]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [84 - 1705838212.1445582]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [84 - 1705838212.1456485]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [84 - 1705838212.1465654]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [84 - 1705838212.1474402]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [84 - 1705838212.1482613]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [84 - 1705838212.1501975]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [84 - 1705838212.1522665]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [84 - 1705838212.1532896]: 0.0
python ParamPruning/classifier.weight [84 - 1705838212.1538467]: 0.0
python DistillationModifier [91 - 1705838254.4821377]: Calling loss_update with:
args: 0.4801308214664459| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.4444444444444444| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [91 - 1705838256.043053]: 
Returned: 0.8934695720672607| 

python LearningRateFunctionModifier [91 - 1705838258.9790246]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.4444444444444444| steps_per_epoch: 63| 
python LearningRateFunctionModifier [91 - 1705838258.9792123]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [91 - 1705838258.9792962]: 0.0001333333333333333
python LearningRateFunctionModifier/ParamGroup1 [91 - 1705838258.9795208]: 0.0001333333333333333
python DistillationModifier/task_loss [91 - 1705838258.9798477]: tensor(0.4801, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [91 - 1705838258.9809768]: tensor(0.8935, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [91 - 1705838258.9817045]: tensor(0.8935, grad_fn=<AddBackward0>)
python ConstantPruningModifier [91 - 1705838258.982415]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.4444444444444444| steps_per_epoch: 63| 
python ConstantPruningModifier [91 - 1705838259.206993]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [91 - 1705838259.2077575]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [91 - 1705838259.2086139]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [91 - 1705838259.209365]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [91 - 1705838259.21009]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [91 - 1705838259.21195]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [91 - 1705838259.2139044]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [91 - 1705838259.2149246]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [91 - 1705838259.2157726]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [91 - 1705838259.2165911]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [91 - 1705838259.2173982]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [91 - 1705838259.219332]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [91 - 1705838259.221413]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [91 - 1705838259.2224534]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [91 - 1705838259.223281]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [91 - 1705838259.2240572]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [91 - 1705838259.224822]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [91 - 1705838259.2266712]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [91 - 1705838259.2286952]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [91 - 1705838259.229662]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [91 - 1705838259.2304006]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [91 - 1705838259.2311397]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [91 - 1705838259.231821]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [91 - 1705838259.2336483]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [91 - 1705838259.235643]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [91 - 1705838259.236598]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [91 - 1705838259.2374277]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [91 - 1705838259.2382057]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [91 - 1705838259.2389157]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [91 - 1705838259.2407897]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [91 - 1705838259.24278]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [91 - 1705838259.243719]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [91 - 1705838259.2447145]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [91 - 1705838259.2455335]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [91 - 1705838259.2462778]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [91 - 1705838259.248125]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [91 - 1705838259.2501342]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [91 - 1705838259.2510655]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [91 - 1705838259.2519157]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [91 - 1705838259.2527542]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [91 - 1705838259.2534988]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [91 - 1705838259.2553303]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [91 - 1705838259.257299]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [91 - 1705838259.2582352]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [91 - 1705838259.259074]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [91 - 1705838259.2598372]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [91 - 1705838259.2605622]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [91 - 1705838259.262382]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [91 - 1705838259.264358]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [91 - 1705838259.2653081]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [91 - 1705838259.2660632]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [91 - 1705838259.2667117]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [91 - 1705838259.2674055]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [91 - 1705838259.2692902]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [91 - 1705838259.2712717]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [91 - 1705838259.2722402]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [91 - 1705838259.2730403]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [91 - 1705838259.273807]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [91 - 1705838259.274522]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [91 - 1705838259.2763274]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [91 - 1705838259.27831]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [91 - 1705838259.2792416]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [91 - 1705838259.2799706]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [91 - 1705838259.2807207]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [91 - 1705838259.2813904]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [91 - 1705838259.283201]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [91 - 1705838259.2851906]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [91 - 1705838259.2861416]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [91 - 1705838259.2869089]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [91 - 1705838259.2876499]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [91 - 1705838259.2883067]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [91 - 1705838259.2901468]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [91 - 1705838259.2922513]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [91 - 1705838259.2932148]: 0.0
python ParamPruning/classifier.weight [91 - 1705838259.2937958]: 0.0
python DistillationModifier [98 - 1705838301.0632064]: Calling loss_update with:
args: 0.5395245552062988| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.5555555555555556| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [98 - 1705838302.5051975]: 
Returned: 1.3905298709869385| 

python LearningRateFunctionModifier [98 - 1705838305.4225636]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.5555555555555556| steps_per_epoch: 63| 
python LearningRateFunctionModifier [98 - 1705838305.4227448]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [98 - 1705838305.4228153]: 0.00013205128205128204
python LearningRateFunctionModifier/ParamGroup1 [98 - 1705838305.4230278]: 0.00013205128205128204
python DistillationModifier/task_loss [98 - 1705838305.4233289]: tensor(0.5395, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [98 - 1705838305.4243352]: tensor(1.3905, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [98 - 1705838305.4249969]: tensor(1.3905, grad_fn=<AddBackward0>)
python ConstantPruningModifier [98 - 1705838305.4256375]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.5555555555555556| steps_per_epoch: 63| 
python ConstantPruningModifier [98 - 1705838305.6496894]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [98 - 1705838305.6504617]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [98 - 1705838305.651332]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [98 - 1705838305.6521358]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [98 - 1705838305.6528738]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [98 - 1705838305.6546903]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [98 - 1705838305.656613]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [98 - 1705838305.6575413]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [98 - 1705838305.6583257]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [98 - 1705838305.659012]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [98 - 1705838305.6596797]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [98 - 1705838305.6615474]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [98 - 1705838305.6634936]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [98 - 1705838305.6644242]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [98 - 1705838305.665268]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [98 - 1705838305.665984]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [98 - 1705838305.6666586]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [98 - 1705838305.6684816]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [98 - 1705838305.6704032]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [98 - 1705838305.6713448]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [98 - 1705838305.672179]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [98 - 1705838305.672883]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [98 - 1705838305.6736073]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [98 - 1705838305.675466]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [98 - 1705838305.6774573]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [98 - 1705838305.6784172]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [98 - 1705838305.6792133]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [98 - 1705838305.6798868]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [98 - 1705838305.6806097]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [98 - 1705838305.6824634]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [98 - 1705838305.6844282]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [98 - 1705838305.6853716]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [98 - 1705838305.6861312]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [98 - 1705838305.6867876]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [98 - 1705838305.6874032]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [98 - 1705838305.6891801]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [98 - 1705838305.6911001]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [98 - 1705838305.6920657]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [98 - 1705838305.6928804]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [98 - 1705838305.6935687]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [98 - 1705838305.694334]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [98 - 1705838305.6961424]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [98 - 1705838305.697929]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [98 - 1705838305.698821]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [98 - 1705838305.6996307]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [98 - 1705838305.7003257]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [98 - 1705838305.7010396]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [98 - 1705838305.7028286]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [98 - 1705838305.7047555]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [98 - 1705838305.7056966]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [98 - 1705838305.7064693]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [98 - 1705838305.7071311]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [98 - 1705838305.7077408]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [98 - 1705838305.7095785]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [98 - 1705838305.7115264]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [98 - 1705838305.7125058]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [98 - 1705838305.713331]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [98 - 1705838305.714015]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [98 - 1705838305.7147186]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [98 - 1705838305.716516]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [98 - 1705838305.7184567]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [98 - 1705838305.719384]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [98 - 1705838305.720117]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [98 - 1705838305.7208574]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [98 - 1705838305.7215726]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [98 - 1705838305.7233913]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [98 - 1705838305.7252514]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [98 - 1705838305.7261775]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [98 - 1705838305.7269945]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [98 - 1705838305.7277033]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [98 - 1705838305.728322]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [98 - 1705838305.7301292]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [98 - 1705838305.7321372]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [98 - 1705838305.7331777]: 0.0
python ParamPruning/classifier.weight [98 - 1705838305.73378]: 0.0
python DistillationModifier [105 - 1705838347.7394407]: Calling loss_update with:
args: 0.47055163979530334| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.6666666666666665| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [105 - 1705838349.265566]: 
Returned: 1.0651699304580688| 

python LearningRateFunctionModifier [105 - 1705838352.1946852]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.6666666666666665| steps_per_epoch: 63| 
python LearningRateFunctionModifier [105 - 1705838352.19485]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [105 - 1705838352.1949086]: 0.00013076923076923075
python LearningRateFunctionModifier/ParamGroup1 [105 - 1705838352.1951478]: 0.00013076923076923075
python DistillationModifier/task_loss [105 - 1705838352.1954098]: tensor(0.4706, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [105 - 1705838352.196421]: tensor(1.0652, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [105 - 1705838352.197043]: tensor(1.0652, grad_fn=<AddBackward0>)
python ConstantPruningModifier [105 - 1705838352.197633]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.6666666666666665| steps_per_epoch: 63| 
python ConstantPruningModifier [105 - 1705838352.4217227]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [105 - 1705838352.4225261]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [105 - 1705838352.4235613]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [105 - 1705838352.4243586]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [105 - 1705838352.4250011]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [105 - 1705838352.4268913]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [105 - 1705838352.4289618]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [105 - 1705838352.4299185]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [105 - 1705838352.4307506]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [105 - 1705838352.4314978]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [105 - 1705838352.432244]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [105 - 1705838352.434074]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [105 - 1705838352.4359856]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [105 - 1705838352.4369297]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [105 - 1705838352.4377468]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [105 - 1705838352.438503]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [105 - 1705838352.439215]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [105 - 1705838352.4411206]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [105 - 1705838352.4431357]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [105 - 1705838352.4440699]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [105 - 1705838352.4448855]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [105 - 1705838352.4456017]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [105 - 1705838352.4462605]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [105 - 1705838352.448043]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [105 - 1705838352.4499774]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [105 - 1705838352.4508736]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [105 - 1705838352.4516573]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [105 - 1705838352.452423]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [105 - 1705838352.453156]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [105 - 1705838352.4549768]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [105 - 1705838352.4569018]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [105 - 1705838352.4578433]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [105 - 1705838352.4586506]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [105 - 1705838352.4593756]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [105 - 1705838352.4600945]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [105 - 1705838352.4619112]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [105 - 1705838352.4637053]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [105 - 1705838352.4646318]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [105 - 1705838352.4654799]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [105 - 1705838352.4662375]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [105 - 1705838352.4669564]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [105 - 1705838352.468813]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [105 - 1705838352.4707077]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [105 - 1705838352.4716258]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [105 - 1705838352.4724352]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [105 - 1705838352.4732113]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [105 - 1705838352.4738421]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [105 - 1705838352.4756339]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [105 - 1705838352.47765]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [105 - 1705838352.4785905]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [105 - 1705838352.4794207]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [105 - 1705838352.4801729]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [105 - 1705838352.4809058]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [105 - 1705838352.482797]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [105 - 1705838352.4847229]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [105 - 1705838352.4856994]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [105 - 1705838352.4865665]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [105 - 1705838352.4873228]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [105 - 1705838352.4881248]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [105 - 1705838352.4900148]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [105 - 1705838352.4920075]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [105 - 1705838352.4929445]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [105 - 1705838352.4937437]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [105 - 1705838352.494414]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [105 - 1705838352.4950993]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [105 - 1705838352.4969242]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [105 - 1705838352.4988623]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [105 - 1705838352.499797]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [105 - 1705838352.5006049]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [105 - 1705838352.5013256]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [105 - 1705838352.5020058]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [105 - 1705838352.503798]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [105 - 1705838352.5057273]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [105 - 1705838352.5066557]: 0.0
python ParamPruning/classifier.weight [105 - 1705838352.5072117]: 0.0
python DistillationModifier [112 - 1705838393.2474868]: Calling loss_update with:
args: 0.3821524679660797| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.7777777777777777| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [112 - 1705838394.6941347]: 
Returned: 0.9253702759742737| 

python LearningRateFunctionModifier [112 - 1705838397.62113]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.7777777777777777| steps_per_epoch: 63| 
python LearningRateFunctionModifier [112 - 1705838397.6212983]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [112 - 1705838397.6213675]: 0.00012948717948717948
python LearningRateFunctionModifier/ParamGroup1 [112 - 1705838397.6215916]: 0.00012948717948717948
python DistillationModifier/task_loss [112 - 1705838397.621881]: tensor(0.3822, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [112 - 1705838397.6230054]: tensor(0.9254, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [112 - 1705838397.6237485]: tensor(0.9254, grad_fn=<AddBackward0>)
python ConstantPruningModifier [112 - 1705838397.6244667]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.7777777777777777| steps_per_epoch: 63| 
python ConstantPruningModifier [112 - 1705838397.85074]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [112 - 1705838397.8515105]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [112 - 1705838397.8523939]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [112 - 1705838397.8532279]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [112 - 1705838397.8539653]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [112 - 1705838397.8558748]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [112 - 1705838397.857965]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [112 - 1705838397.858979]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [112 - 1705838397.8599088]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [112 - 1705838397.8608308]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [112 - 1705838397.8616614]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [112 - 1705838397.8636672]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [112 - 1705838397.8657176]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [112 - 1705838397.866762]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [112 - 1705838397.867688]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [112 - 1705838397.868541]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [112 - 1705838397.869408]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [112 - 1705838397.8714244]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [112 - 1705838397.873661]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [112 - 1705838397.8747296]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [112 - 1705838397.875727]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [112 - 1705838397.8767126]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [112 - 1705838397.8776417]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [112 - 1705838397.8796375]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [112 - 1705838397.8816688]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [112 - 1705838397.8826585]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [112 - 1705838397.8834758]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [112 - 1705838397.8842404]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [112 - 1705838397.8849947]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [112 - 1705838397.8868222]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [112 - 1705838397.8888574]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [112 - 1705838397.8898852]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [112 - 1705838397.8907235]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [112 - 1705838397.8915071]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [112 - 1705838397.8922677]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [112 - 1705838397.8942108]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [112 - 1705838397.89621]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [112 - 1705838397.8971968]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [112 - 1705838397.8980274]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [112 - 1705838397.8987787]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [112 - 1705838397.8994672]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [112 - 1705838397.9012895]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [112 - 1705838397.9032948]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [112 - 1705838397.9042554]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [112 - 1705838397.9050808]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [112 - 1705838397.9058895]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [112 - 1705838397.9066799]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [112 - 1705838397.908532]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [112 - 1705838397.9105465]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [112 - 1705838397.911497]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [112 - 1705838397.9123414]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [112 - 1705838397.9131145]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [112 - 1705838397.9138088]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [112 - 1705838397.9156926]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [112 - 1705838397.9176836]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [112 - 1705838397.9186695]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [112 - 1705838397.919491]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [112 - 1705838397.920201]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [112 - 1705838397.920924]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [112 - 1705838397.9227903]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [112 - 1705838397.9248552]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [112 - 1705838397.9258718]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [112 - 1705838397.926754]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [112 - 1705838397.927598]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [112 - 1705838397.9283714]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [112 - 1705838397.9303157]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [112 - 1705838397.9323404]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [112 - 1705838397.933393]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [112 - 1705838397.934231]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [112 - 1705838397.9349937]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [112 - 1705838397.9357255]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [112 - 1705838397.9376223]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [112 - 1705838397.9396656]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [112 - 1705838397.940661]: 0.0
python ParamPruning/classifier.weight [112 - 1705838397.9412704]: 0.0
python DistillationModifier [119 - 1705838440.248804]: Calling loss_update with:
args: 0.42232224345207214| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 1.8888888888888888| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [119 - 1705838441.8053412]: 
Returned: 1.2425904273986816| 

python LearningRateFunctionModifier [119 - 1705838444.7462447]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.8888888888888888| steps_per_epoch: 63| 
python LearningRateFunctionModifier [119 - 1705838444.7464194]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [119 - 1705838444.7464879]: 0.0001282051282051282
python LearningRateFunctionModifier/ParamGroup1 [119 - 1705838444.7467191]: 0.0001282051282051282
python DistillationModifier/task_loss [119 - 1705838444.7470028]: tensor(0.4223, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [119 - 1705838444.7481654]: tensor(1.2426, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [119 - 1705838444.7489135]: tensor(1.2426, grad_fn=<AddBackward0>)
python ConstantPruningModifier [119 - 1705838444.7496583]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 1.8888888888888888| steps_per_epoch: 63| 
python ConstantPruningModifier [119 - 1705838444.9771159]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [119 - 1705838444.9779475]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [119 - 1705838444.9788892]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [119 - 1705838444.9798322]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [119 - 1705838444.9807854]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [119 - 1705838444.9828355]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [119 - 1705838444.9848466]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [119 - 1705838444.9858801]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [119 - 1705838444.9867513]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [119 - 1705838444.9874632]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [119 - 1705838444.9881608]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [119 - 1705838444.9900298]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [119 - 1705838444.992067]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [119 - 1705838444.99302]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [119 - 1705838444.993887]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [119 - 1705838444.9946408]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [119 - 1705838444.9953578]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [119 - 1705838444.9972699]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [119 - 1705838444.999251]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [119 - 1705838445.0002408]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [119 - 1705838445.001088]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [119 - 1705838445.0018926]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [119 - 1705838445.0026498]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [119 - 1705838445.0046256]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [119 - 1705838445.0066059]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [119 - 1705838445.0076215]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [119 - 1705838445.0085058]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [119 - 1705838445.0092993]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [119 - 1705838445.0100193]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [119 - 1705838445.0119934]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [119 - 1705838445.014067]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [119 - 1705838445.0150812]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [119 - 1705838445.015948]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [119 - 1705838445.016808]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [119 - 1705838445.0176241]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [119 - 1705838445.019632]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [119 - 1705838445.0217185]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [119 - 1705838445.0227213]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [119 - 1705838445.0235803]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [119 - 1705838445.024301]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [119 - 1705838445.0249662]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [119 - 1705838445.026791]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [119 - 1705838445.0287797]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [119 - 1705838445.0297456]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [119 - 1705838445.0305035]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [119 - 1705838445.031286]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [119 - 1705838445.032026]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [119 - 1705838445.0339077]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [119 - 1705838445.0359848]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [119 - 1705838445.0370069]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [119 - 1705838445.037872]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [119 - 1705838445.0385938]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [119 - 1705838445.0393746]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [119 - 1705838445.0413287]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [119 - 1705838445.0433536]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [119 - 1705838445.0444329]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [119 - 1705838445.0454109]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [119 - 1705838445.046279]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [119 - 1705838445.0470877]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [119 - 1705838445.0490289]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [119 - 1705838445.05106]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [119 - 1705838445.0520778]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [119 - 1705838445.0529656]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [119 - 1705838445.0537384]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [119 - 1705838445.054477]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [119 - 1705838445.0564249]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [119 - 1705838445.0584717]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [119 - 1705838445.0594633]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [119 - 1705838445.060247]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [119 - 1705838445.0610356]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [119 - 1705838445.0617642]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [119 - 1705838445.063625]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [119 - 1705838445.0656476]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [119 - 1705838445.0666084]: 0.0
python ParamPruning/classifier.weight [119 - 1705838445.0671382]: 0.0
python DistillationModifier [126 - 1705838484.5975938]: Calling loss_update with:
args: 0.5466172695159912| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838486.0496519]: 
Returned: 1.0787845849990845| 

python DistillationModifier [126 - 1705838487.534482]: Calling loss_update with:
args: 0.6395303010940552| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838488.984273]: 
Returned: 1.2790186405181885| 

python DistillationModifier [126 - 1705838490.466402]: Calling loss_update with:
args: 0.7139224410057068| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838491.916072]: 
Returned: 0.9799759984016418| 

python DistillationModifier [126 - 1705838493.4409099]: Calling loss_update with:
args: 0.6306372880935669| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838494.8960369]: 
Returned: 0.9824753403663635| 

python DistillationModifier [126 - 1705838496.376835]: Calling loss_update with:
args: 0.7362776398658752| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838497.8344567]: 
Returned: 1.1893644332885742| 

python DistillationModifier [126 - 1705838499.328553]: Calling loss_update with:
args: 0.45579957962036133| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838500.7872727]: 
Returned: 0.9386194944381714| 

python DistillationModifier [126 - 1705838502.266155]: Calling loss_update with:
args: 0.3894226551055908| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838503.7166317]: 
Returned: 1.102658748626709| 

python DistillationModifier [126 - 1705838505.2135336]: Calling loss_update with:
args: 0.7224973440170288| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838506.666351]: 
Returned: 1.3797149658203125| 

python DistillationModifier [126 - 1705838508.1522217]: Calling loss_update with:
args: 0.34179121255874634| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838509.6120865]: 
Returned: 0.4894951581954956| 

python DistillationModifier [126 - 1705838511.0947185]: Calling loss_update with:
args: 0.5389745235443115| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838512.5465724]: 
Returned: 1.0667874813079834| 

python DistillationModifier [126 - 1705838514.0283656]: Calling loss_update with:
args: 0.3677428364753723| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838515.4858704]: 
Returned: 0.889188289642334| 

python DistillationModifier [126 - 1705838516.96458]: Calling loss_update with:
args: 0.8946077823638916| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838518.4109585]: 
Returned: 1.10228431224823| 

python DistillationModifier [126 - 1705838519.888801]: Calling loss_update with:
args: 0.7001847624778748| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838521.3650985]: 
Returned: 1.3468763828277588| 

python DistillationModifier [126 - 1705838523.4565284]: Calling loss_update with:
args: 0.47202903032302856| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838525.4215357]: 
Returned: 0.5997760891914368| 

python DistillationModifier [126 - 1705838526.901128]: Calling loss_update with:
args: 0.8417736291885376| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838528.3467746]: 
Returned: 1.4686076641082764| 

python DistillationModifier [126 - 1705838529.8233886]: Calling loss_update with:
args: 0.5075706243515015| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838531.2684526]: 
Returned: 1.0976487398147583| 

python DistillationModifier [126 - 1705838532.7466788]: Calling loss_update with:
args: 0.4937880039215088| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838534.1905093]: 
Returned: 0.9267753958702087| 

python DistillationModifier [126 - 1705838535.6778235]: Calling loss_update with:
args: 0.5989521741867065| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838537.1285427]: 
Returned: 1.2548834085464478| 

python DistillationModifier [126 - 1705838538.6226885]: Calling loss_update with:
args: 0.7199815511703491| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838540.0931575]: 
Returned: 0.9570384621620178| 

python DistillationModifier [126 - 1705838541.5703793]: Calling loss_update with:
args: 0.41769102215766907| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838543.0134976]: 
Returned: 0.8054982423782349| 

python DistillationModifier [126 - 1705838544.4896524]: Calling loss_update with:
args: 0.3977183997631073| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838545.9332592]: 
Returned: 0.884162187576294| 

python DistillationModifier [126 - 1705838547.4071043]: Calling loss_update with:
args: 0.5270156860351562| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838548.852433]: 
Returned: 0.9577725529670715| 

python DistillationModifier [126 - 1705838550.3329813]: Calling loss_update with:
args: 0.5018374919891357| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838551.7770417]: 
Returned: 0.9757834672927856| 

python DistillationModifier [126 - 1705838553.3189752]: Calling loss_update with:
args: 0.326180636882782| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838555.296011]: 
Returned: 0.7388009428977966| 

python DistillationModifier [126 - 1705838556.7653208]: Calling loss_update with:
args: 0.6740723848342896| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838558.20789]: 
Returned: 0.8878971934318542| 

python DistillationModifier [126 - 1705838559.6921606]: Calling loss_update with:
args: 0.30143922567367554| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838561.1418297]: 
Returned: 0.6915045976638794| 

python DistillationModifier [126 - 1705838562.6210167]: Calling loss_update with:
args: 0.33010390400886536| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838564.0681052]: 
Returned: 0.5866525769233704| 

python DistillationModifier [126 - 1705838565.5490296]: Calling loss_update with:
args: 0.3665311336517334| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838566.986771]: 
Returned: 0.656245231628418| 

python DistillationModifier [126 - 1705838568.463492]: Calling loss_update with:
args: 0.6368603110313416| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838569.9163322]: 
Returned: 1.057140827178955| 

python DistillationModifier [126 - 1705838571.3940225]: Calling loss_update with:
args: 0.36996570229530334| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838572.8374527]: 
Returned: 0.7086999416351318| 

python DistillationModifier [126 - 1705838574.3088589]: Calling loss_update with:
args: 0.4664924740791321| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838575.7520232]: 
Returned: 1.0712153911590576| 

python DistillationModifier [126 - 1705838577.224616]: Calling loss_update with:
args: 0.711158037185669| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838578.6673265]: 
Returned: 1.1484644412994385| 

python DistillationModifier [126 - 1705838580.1576517]: Calling loss_update with:
args: 0.6680580973625183| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838581.7076726]: 
Returned: 1.32203209400177| 

python DistillationModifier [126 - 1705838583.7662566]: Calling loss_update with:
args: 0.9581220746040344| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838585.6802096]: 
Returned: 1.5682032108306885| 

python DistillationModifier [126 - 1705838587.1525788]: Calling loss_update with:
args: 0.8029231429100037| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838588.6003249]: 
Returned: 1.1284185647964478| 

python DistillationModifier [126 - 1705838590.0845609]: Calling loss_update with:
args: 0.8937208652496338| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838591.5313194]: 
Returned: 1.8042991161346436| 

python DistillationModifier [126 - 1705838593.0045578]: Calling loss_update with:
args: 0.6365577578544617| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838594.4488993]: 
Returned: 0.8585389256477356| 

python DistillationModifier [126 - 1705838595.920448]: Calling loss_update with:
args: 0.5324663519859314| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838597.3622994]: 
Returned: 0.897718071937561| 

python DistillationModifier [126 - 1705838598.8356738]: Calling loss_update with:
args: 0.6879661679267883| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838600.267893]: 
Returned: 1.4744635820388794| 

python DistillationModifier [126 - 1705838601.7389853]: Calling loss_update with:
args: 0.5918176174163818| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838603.1809194]: 
Returned: 1.329955816268921| 

python DistillationModifier [126 - 1705838604.651322]: Calling loss_update with:
args: 0.4671269655227661| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838606.0841975]: 
Returned: 1.081169843673706| 

python DistillationModifier [126 - 1705838607.552156]: Calling loss_update with:
args: 0.5607471466064453| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838608.9798734]: 
Returned: 1.0563981533050537| 

python DistillationModifier [126 - 1705838610.4443264]: Calling loss_update with:
args: 0.4775923788547516| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838611.8857682]: 
Returned: 0.7460659742355347| 

python DistillationModifier [126 - 1705838613.351424]: Calling loss_update with:
args: 0.4580996334552765| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838614.831337]: 
Returned: 1.1047866344451904| 

python DistillationModifier [126 - 1705838616.2984247]: Calling loss_update with:
args: 0.5763999223709106| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838617.7387936]: 
Returned: 1.2354507446289062| 

python DistillationModifier [126 - 1705838619.2109144]: Calling loss_update with:
args: 0.6361438632011414| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838620.6459076]: 
Returned: 1.2358964681625366| 

python DistillationModifier [126 - 1705838622.1154344]: Calling loss_update with:
args: 0.3272836208343506| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838623.5545]: 
Returned: 0.7441319227218628| 

python DistillationModifier [126 - 1705838625.0227966]: Calling loss_update with:
args: 0.26389434933662415| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838626.4602568]: 
Returned: 0.7718751430511475| 

python DistillationModifier [126 - 1705838627.9347148]: Calling loss_update with:
args: 0.8992594480514526| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838629.372747]: 
Returned: 1.7520344257354736| 

python DistillationModifier [126 - 1705838630.8403568]: Calling loss_update with:
args: 0.8545457720756531| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838632.276716]: 
Returned: 1.5035514831542969| 

python DistillationModifier [126 - 1705838633.7392821]: Calling loss_update with:
args: 0.3585938513278961| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838635.1681466]: 
Returned: 0.7198801040649414| 

python DistillationModifier [126 - 1705838636.6357543]: Calling loss_update with:
args: 1.074310064315796| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838638.074017]: 
Returned: 1.167994737625122| 

python DistillationModifier [126 - 1705838639.5408206]: Calling loss_update with:
args: 0.4745733141899109| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838640.973126]: 
Returned: 0.9181690812110901| 

python DistillationModifier [126 - 1705838642.8676913]: Calling loss_update with:
args: 0.5402798056602478| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838644.9086084]: 
Returned: 0.9410883784294128| 

python DistillationModifier [126 - 1705838646.7682252]: Calling loss_update with:
args: 0.44207724928855896| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838648.2037747]: 
Returned: 1.1088119745254517| 

python DistillationModifier [126 - 1705838649.6767247]: Calling loss_update with:
args: 0.5599597692489624| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838651.1098242]: 
Returned: 1.0079509019851685| 

python DistillationModifier [126 - 1705838652.5822563]: Calling loss_update with:
args: 0.6514394283294678| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838654.0193293]: 
Returned: 0.6617998480796814| 

python DistillationModifier [126 - 1705838655.4841807]: Calling loss_update with:
args: 0.6502121686935425| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838656.9242463]: 
Returned: 1.0437871217727661| 

python DistillationModifier [126 - 1705838658.391719]: Calling loss_update with:
args: 0.6083177924156189| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838659.8348482]: 
Returned: 1.181051254272461| 

python DistillationModifier [126 - 1705838661.322584]: Calling loss_update with:
args: 0.4441060721874237| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838662.7638316]: 
Returned: 0.9143087267875671| 

python DistillationModifier [126 - 1705838664.2373104]: Calling loss_update with:
args: 0.4622723460197449| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838665.6707616]: 
Returned: 1.1353415250778198| 

python DistillationModifier [126 - 1705838667.132923]: Calling loss_update with:
args: 0.45158642530441284| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838668.5646467]: 
Returned: 1.0391331911087036| 

python DistillationModifier [126 - 1705838669.3332715]: Calling loss_update with:
args: 0.5311652421951294| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838670.0754879]: 
Returned: 1.0864301919937134| 

python DistillationModifier [126 - 1705838672.1445735]: Calling loss_update with:
args: 0.5067121386528015| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [126 - 1705838673.5907605]: 
Returned: 0.9108954668045044| 

python LearningRateFunctionModifier [126 - 1705838676.5356822]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [126 - 1705838676.5358531]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [126 - 1705838676.5359106]: 0.0001269230769230769
python LearningRateFunctionModifier/ParamGroup1 [126 - 1705838676.5361395]: 0.0001269230769230769
python DistillationModifier/task_loss [126 - 1705838676.536442]: tensor(0.5067, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [126 - 1705838676.5374365]: tensor(0.9109, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [126 - 1705838676.538039]: tensor(0.9109, grad_fn=<AddBackward0>)
python ConstantPruningModifier [126 - 1705838676.5386417]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.0| steps_per_epoch: 63| 
python ConstantPruningModifier [126 - 1705838676.759022]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [126 - 1705838676.759789]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [126 - 1705838676.760663]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [126 - 1705838676.7614012]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [126 - 1705838676.7620711]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [126 - 1705838676.7638736]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [126 - 1705838676.7658024]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [126 - 1705838676.766713]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [126 - 1705838676.767481]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [126 - 1705838676.7681453]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [126 - 1705838676.7688131]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [126 - 1705838676.770639]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [126 - 1705838676.7724]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [126 - 1705838676.7733014]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [126 - 1705838676.774088]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [126 - 1705838676.774764]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [126 - 1705838676.7754078]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [126 - 1705838676.7771688]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [126 - 1705838676.7790816]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [126 - 1705838676.78]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [126 - 1705838676.7808287]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [126 - 1705838676.781594]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [126 - 1705838676.782281]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [126 - 1705838676.7841003]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [126 - 1705838676.7860045]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [126 - 1705838676.786893]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [126 - 1705838676.7876546]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [126 - 1705838676.7883232]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [126 - 1705838676.788935]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [126 - 1705838676.7907073]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [126 - 1705838676.7926524]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [126 - 1705838676.7935395]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [126 - 1705838676.7943017]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [126 - 1705838676.7950106]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [126 - 1705838676.795651]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [126 - 1705838676.7974203]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [126 - 1705838676.799334]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [126 - 1705838676.8001928]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [126 - 1705838676.8009505]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [126 - 1705838676.8016164]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [126 - 1705838676.8022454]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [126 - 1705838676.8040342]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [126 - 1705838676.8059175]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [126 - 1705838676.806817]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [126 - 1705838676.8075924]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [126 - 1705838676.8082283]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [126 - 1705838676.8089197]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [126 - 1705838676.810699]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [126 - 1705838676.812576]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [126 - 1705838676.8134732]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [126 - 1705838676.8142474]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [126 - 1705838676.8149116]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [126 - 1705838676.815622]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [126 - 1705838676.8174088]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [126 - 1705838676.8193057]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [126 - 1705838676.820206]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [126 - 1705838676.820994]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [126 - 1705838676.8216858]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [126 - 1705838676.8223407]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [126 - 1705838676.8241296]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [126 - 1705838676.826136]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [126 - 1705838676.8270698]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [126 - 1705838676.8278728]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [126 - 1705838676.8286395]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [126 - 1705838676.8293633]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [126 - 1705838676.8312037]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [126 - 1705838676.8332317]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [126 - 1705838676.8342226]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [126 - 1705838676.835065]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [126 - 1705838676.8358283]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [126 - 1705838676.8365302]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [126 - 1705838676.8383603]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [126 - 1705838676.8401473]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [126 - 1705838676.8411062]: 0.0
python ParamPruning/classifier.weight [126 - 1705838676.8416674]: 0.0
python DistillationModifier [133 - 1705838718.5680006]: Calling loss_update with:
args: 0.3007188141345978| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.111111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [133 - 1705838720.1252558]: 
Returned: 0.7370888590812683| 

python LearningRateFunctionModifier [133 - 1705838722.99629]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.111111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [133 - 1705838722.9964545]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [133 - 1705838722.9965143]: 0.00012564102564102564
python LearningRateFunctionModifier/ParamGroup1 [133 - 1705838722.996748]: 0.00012564102564102564
python DistillationModifier/task_loss [133 - 1705838722.9970574]: tensor(0.3007, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [133 - 1705838722.9980114]: tensor(0.7371, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [133 - 1705838722.9986398]: tensor(0.7371, grad_fn=<AddBackward0>)
python ConstantPruningModifier [133 - 1705838722.9992383]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.111111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [133 - 1705838723.218118]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [133 - 1705838723.2189023]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [133 - 1705838723.2197375]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [133 - 1705838723.2204607]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [133 - 1705838723.2211056]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [133 - 1705838723.2228105]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [133 - 1705838723.2246187]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [133 - 1705838723.2254157]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [133 - 1705838723.2260265]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [133 - 1705838723.2265956]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [133 - 1705838723.2271287]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [133 - 1705838723.2287872]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [133 - 1705838723.230426]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [133 - 1705838723.2312446]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [133 - 1705838723.2319245]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [133 - 1705838723.2324946]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [133 - 1705838723.2330415]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [133 - 1705838723.2347372]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [133 - 1705838723.236457]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [133 - 1705838723.2372236]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [133 - 1705838723.2378404]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [133 - 1705838723.2383811]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [133 - 1705838723.2389095]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [133 - 1705838723.240482]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [133 - 1705838723.2422898]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [133 - 1705838723.2430537]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [133 - 1705838723.243702]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [133 - 1705838723.2443352]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [133 - 1705838723.2449205]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [133 - 1705838723.2466168]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [133 - 1705838723.248207]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [133 - 1705838723.2490127]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [133 - 1705838723.249697]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [133 - 1705838723.2502756]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [133 - 1705838723.2509391]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [133 - 1705838723.252629]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [133 - 1705838723.2544785]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [133 - 1705838723.255284]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [133 - 1705838723.2559369]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [133 - 1705838723.2565293]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [133 - 1705838723.2570956]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [133 - 1705838723.2587829]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [133 - 1705838723.260352]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [133 - 1705838723.261159]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [133 - 1705838723.2618148]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [133 - 1705838723.262381]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [133 - 1705838723.2629058]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [133 - 1705838723.2645032]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [133 - 1705838723.266094]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [133 - 1705838723.2668793]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [133 - 1705838723.267553]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [133 - 1705838723.2681372]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [133 - 1705838723.2686853]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [133 - 1705838723.270284]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [133 - 1705838723.2718365]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [133 - 1705838723.2726204]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [133 - 1705838723.2733054]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [133 - 1705838723.2738848]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [133 - 1705838723.2744162]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [133 - 1705838723.2760084]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [133 - 1705838723.2775588]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [133 - 1705838723.2783332]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [133 - 1705838723.2789798]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [133 - 1705838723.2795508]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [133 - 1705838723.2800725]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [133 - 1705838723.2816749]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [133 - 1705838723.2834444]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [133 - 1705838723.2842314]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [133 - 1705838723.2848742]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [133 - 1705838723.2854197]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [133 - 1705838723.285943]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [133 - 1705838723.2876163]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [133 - 1705838723.28942]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [133 - 1705838723.290184]: 0.0
python ParamPruning/classifier.weight [133 - 1705838723.2906125]: 0.0
python DistillationModifier [140 - 1705838764.4698596]: Calling loss_update with:
args: 0.39548051357269287| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.2222222222222223| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [140 - 1705838766.0496197]: 
Returned: 0.7691570520401001| 

python LearningRateFunctionModifier [140 - 1705838768.9608564]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.2222222222222223| steps_per_epoch: 63| 
python LearningRateFunctionModifier [140 - 1705838768.9610314]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [140 - 1705838768.961102]: 0.00012435897435897434
python LearningRateFunctionModifier/ParamGroup1 [140 - 1705838768.9613214]: 0.00012435897435897434
python DistillationModifier/task_loss [140 - 1705838768.9616008]: tensor(0.3955, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [140 - 1705838768.9627035]: tensor(0.7692, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [140 - 1705838768.9633236]: tensor(0.7692, grad_fn=<AddBackward0>)
python ConstantPruningModifier [140 - 1705838768.9639297]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.2222222222222223| steps_per_epoch: 63| 
python ConstantPruningModifier [140 - 1705838769.186584]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [140 - 1705838769.1873796]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [140 - 1705838769.188262]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [140 - 1705838769.189014]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [140 - 1705838769.1897492]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [140 - 1705838769.191561]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [140 - 1705838769.1935472]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [140 - 1705838769.194486]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [140 - 1705838769.1952763]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [140 - 1705838769.195945]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [140 - 1705838769.1965847]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [140 - 1705838769.1984239]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [140 - 1705838769.200328]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [140 - 1705838769.2012563]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [140 - 1705838769.2020202]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [140 - 1705838769.202743]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [140 - 1705838769.20339]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [140 - 1705838769.2051895]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [140 - 1705838769.207041]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [140 - 1705838769.207925]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [140 - 1705838769.2087119]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [140 - 1705838769.209417]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [140 - 1705838769.2100503]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [140 - 1705838769.2118483]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [140 - 1705838769.2137792]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [140 - 1705838769.2147157]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [140 - 1705838769.2154875]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [140 - 1705838769.216235]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [140 - 1705838769.2169096]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [140 - 1705838769.2186673]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [140 - 1705838769.2205474]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [140 - 1705838769.2214239]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [140 - 1705838769.2221265]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [140 - 1705838769.2228205]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [140 - 1705838769.2233915]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [140 - 1705838769.22518]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [140 - 1705838769.2270367]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [140 - 1705838769.2279117]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [140 - 1705838769.2285771]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [140 - 1705838769.2292256]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [140 - 1705838769.2298732]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [140 - 1705838769.231612]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [140 - 1705838769.2335792]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [140 - 1705838769.2344785]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [140 - 1705838769.2352467]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [140 - 1705838769.2358966]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [140 - 1705838769.2365193]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [140 - 1705838769.2383673]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [140 - 1705838769.2402837]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [140 - 1705838769.2411785]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [140 - 1705838769.2419553]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [140 - 1705838769.2426057]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [140 - 1705838769.2432482]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [140 - 1705838769.2450204]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [140 - 1705838769.2469218]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [140 - 1705838769.2477963]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [140 - 1705838769.2485654]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [140 - 1705838769.249241]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [140 - 1705838769.24992]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [140 - 1705838769.25171]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [140 - 1705838769.2536373]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [140 - 1705838769.2545476]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [140 - 1705838769.2553174]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [140 - 1705838769.255982]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [140 - 1705838769.2567055]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [140 - 1705838769.2584898]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [140 - 1705838769.2604303]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [140 - 1705838769.2614045]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [140 - 1705838769.2622092]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [140 - 1705838769.2629716]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [140 - 1705838769.2636507]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [140 - 1705838769.2654428]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [140 - 1705838769.2672167]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [140 - 1705838769.2681284]: 0.0
python ParamPruning/classifier.weight [140 - 1705838769.268678]: 0.0
python DistillationModifier [147 - 1705838810.2164972]: Calling loss_update with:
args: 0.40536120533943176| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.3333333333333335| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [147 - 1705838811.7213542]: 
Returned: 0.8640605211257935| 

python LearningRateFunctionModifier [147 - 1705838814.6221159]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.3333333333333335| steps_per_epoch: 63| 
python LearningRateFunctionModifier [147 - 1705838814.622283]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [147 - 1705838814.6223412]: 0.00012307692307692307
python LearningRateFunctionModifier/ParamGroup1 [147 - 1705838814.6225696]: 0.00012307692307692307
python DistillationModifier/task_loss [147 - 1705838814.6228395]: tensor(0.4054, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [147 - 1705838814.6239073]: tensor(0.8641, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [147 - 1705838814.6245072]: tensor(0.8641, grad_fn=<AddBackward0>)
python ConstantPruningModifier [147 - 1705838814.6251614]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.3333333333333335| steps_per_epoch: 63| 
python ConstantPruningModifier [147 - 1705838814.8511608]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [147 - 1705838814.8519802]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [147 - 1705838814.8529108]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [147 - 1705838814.8537164]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [147 - 1705838814.8544366]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [147 - 1705838814.8562965]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [147 - 1705838814.858303]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [147 - 1705838814.85928]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [147 - 1705838814.860131]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [147 - 1705838814.8609197]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [147 - 1705838814.8616395]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [147 - 1705838814.8634312]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [147 - 1705838814.8654382]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [147 - 1705838814.866412]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [147 - 1705838814.867268]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [147 - 1705838814.867975]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [147 - 1705838814.868711]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [147 - 1705838814.8705204]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [147 - 1705838814.8725383]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [147 - 1705838814.8735242]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [147 - 1705838814.8743768]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [147 - 1705838814.8751574]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [147 - 1705838814.8758788]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [147 - 1705838814.877706]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [147 - 1705838814.879657]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [147 - 1705838814.8806286]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [147 - 1705838814.8814821]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [147 - 1705838814.8822615]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [147 - 1705838814.882959]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [147 - 1705838814.8847737]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [147 - 1705838814.886722]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [147 - 1705838814.8876922]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [147 - 1705838814.8885322]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [147 - 1705838814.8893514]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [147 - 1705838814.8900936]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [147 - 1705838814.8919096]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [147 - 1705838814.893892]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [147 - 1705838814.8948495]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [147 - 1705838814.8956919]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [147 - 1705838814.8964696]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [147 - 1705838814.8971846]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [147 - 1705838814.898965]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [147 - 1705838814.900958]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [147 - 1705838814.9019153]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [147 - 1705838814.90268]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [147 - 1705838814.9033701]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [147 - 1705838814.9039817]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [147 - 1705838814.9057775]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [147 - 1705838814.9077237]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [147 - 1705838814.9087303]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [147 - 1705838814.909596]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [147 - 1705838814.9103699]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [147 - 1705838814.9110863]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [147 - 1705838814.9129457]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [147 - 1705838814.9148583]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [147 - 1705838814.9157999]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [147 - 1705838814.916523]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [147 - 1705838814.9172556]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [147 - 1705838814.9178524]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [147 - 1705838814.9196095]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [147 - 1705838814.9215057]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [147 - 1705838814.9223847]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [147 - 1705838814.9231634]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [147 - 1705838814.9238148]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [147 - 1705838814.9244957]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [147 - 1705838814.9263294]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [147 - 1705838814.928301]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [147 - 1705838814.929282]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [147 - 1705838814.9300976]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [147 - 1705838814.9308836]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [147 - 1705838814.931604]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [147 - 1705838814.9334931]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [147 - 1705838814.9354668]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [147 - 1705838814.936433]: 0.0
python ParamPruning/classifier.weight [147 - 1705838814.9370286]: 0.0
python DistillationModifier [154 - 1705838856.7582028]: Calling loss_update with:
args: 0.24583806097507477| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.4444444444444446| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [154 - 1705838858.8625674]: 
Returned: 0.7252160906791687| 

python LearningRateFunctionModifier [154 - 1705838861.8038652]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.4444444444444446| steps_per_epoch: 63| 
python LearningRateFunctionModifier [154 - 1705838861.8040526]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [154 - 1705838861.8188903]: 0.00012179487179487177
python LearningRateFunctionModifier/ParamGroup1 [154 - 1705838861.8191402]: 0.00012179487179487177
python DistillationModifier/task_loss [154 - 1705838861.8194354]: tensor(0.2458, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [154 - 1705838861.8205383]: tensor(0.7252, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [154 - 1705838861.8211362]: tensor(0.7252, grad_fn=<AddBackward0>)
python ConstantPruningModifier [154 - 1705838861.8216689]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.4444444444444446| steps_per_epoch: 63| 
python ConstantPruningModifier [154 - 1705838862.043197]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [154 - 1705838862.0439801]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [154 - 1705838862.044894]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [154 - 1705838862.0455127]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [154 - 1705838862.046042]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [154 - 1705838862.0477285]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [154 - 1705838862.0494666]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [154 - 1705838862.050309]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [154 - 1705838862.0509543]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [154 - 1705838862.0514915]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [154 - 1705838862.0520647]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [154 - 1705838862.0537598]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [154 - 1705838862.0556352]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [154 - 1705838862.0564969]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [154 - 1705838862.0571768]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [154 - 1705838862.057716]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [154 - 1705838862.0582194]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [154 - 1705838862.0599215]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [154 - 1705838862.061807]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [154 - 1705838862.062644]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [154 - 1705838862.0632896]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [154 - 1705838862.063827]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [154 - 1705838862.064357]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [154 - 1705838862.0660682]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [154 - 1705838862.067816]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [154 - 1705838862.0687034]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [154 - 1705838862.0694146]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [154 - 1705838862.0700047]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [154 - 1705838862.070511]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [154 - 1705838862.072212]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [154 - 1705838862.0739615]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [154 - 1705838862.0748265]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [154 - 1705838862.0755224]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [154 - 1705838862.0760741]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [154 - 1705838862.076577]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [154 - 1705838862.0782552]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [154 - 1705838862.0800962]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [154 - 1705838862.080947]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [154 - 1705838862.081635]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [154 - 1705838862.0821884]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [154 - 1705838862.082713]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [154 - 1705838862.0844283]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [154 - 1705838862.0862038]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [154 - 1705838862.0870807]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [154 - 1705838862.0877748]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [154 - 1705838862.0883808]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [154 - 1705838862.088948]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [154 - 1705838862.0906646]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [154 - 1705838862.0924006]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [154 - 1705838862.0932434]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [154 - 1705838862.0939467]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [154 - 1705838862.094509]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [154 - 1705838862.0950284]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [154 - 1705838862.0967412]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [154 - 1705838862.098655]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [154 - 1705838862.0995293]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [154 - 1705838862.1002223]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [154 - 1705838862.100833]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [154 - 1705838862.1013863]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [154 - 1705838862.1031098]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [154 - 1705838862.1050274]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [154 - 1705838862.1058924]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [154 - 1705838862.1065779]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [154 - 1705838862.1071432]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [154 - 1705838862.107668]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [154 - 1705838862.1093836]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [154 - 1705838862.111227]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [154 - 1705838862.112106]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [154 - 1705838862.1127899]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [154 - 1705838862.1133792]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [154 - 1705838862.1138947]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [154 - 1705838862.1155782]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [154 - 1705838862.1173353]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [154 - 1705838862.1182077]: 0.0
python ParamPruning/classifier.weight [154 - 1705838862.1187046]: 0.0
python DistillationModifier [161 - 1705838903.5958009]: Calling loss_update with:
args: 0.39981675148010254| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.5555555555555554| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [161 - 1705838905.0354648]: 
Returned: 0.7337738275527954| 

python LearningRateFunctionModifier [161 - 1705838907.9497654]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.5555555555555554| steps_per_epoch: 63| 
python LearningRateFunctionModifier [161 - 1705838907.9499307]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [161 - 1705838907.9499998]: 0.0001205128205128205
python LearningRateFunctionModifier/ParamGroup1 [161 - 1705838907.9502242]: 0.0001205128205128205
python DistillationModifier/task_loss [161 - 1705838907.9505262]: tensor(0.3998, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [161 - 1705838907.9515002]: tensor(0.7338, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [161 - 1705838907.9521406]: tensor(0.7338, grad_fn=<AddBackward0>)
python ConstantPruningModifier [161 - 1705838907.9527795]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.5555555555555554| steps_per_epoch: 63| 
python ConstantPruningModifier [161 - 1705838908.174923]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [161 - 1705838908.1757154]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [161 - 1705838908.1765964]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [161 - 1705838908.1773586]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [161 - 1705838908.178078]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [161 - 1705838908.1798909]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [161 - 1705838908.1816697]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [161 - 1705838908.1826084]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [161 - 1705838908.1833758]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [161 - 1705838908.1841302]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [161 - 1705838908.1848538]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [161 - 1705838908.1865964]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [161 - 1705838908.1884515]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [161 - 1705838908.1893213]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [161 - 1705838908.1900995]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [161 - 1705838908.1907394]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [161 - 1705838908.191307]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [161 - 1705838908.1930723]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [161 - 1705838908.1949775]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [161 - 1705838908.1958559]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [161 - 1705838908.1966252]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [161 - 1705838908.1973631]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [161 - 1705838908.1980627]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [161 - 1705838908.1998098]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [161 - 1705838908.20172]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [161 - 1705838908.20261]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [161 - 1705838908.2033813]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [161 - 1705838908.2041063]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [161 - 1705838908.204775]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [161 - 1705838908.2065232]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [161 - 1705838908.2084298]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [161 - 1705838908.2093248]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [161 - 1705838908.2100956]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [161 - 1705838908.210776]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [161 - 1705838908.2114117]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [161 - 1705838908.2132237]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [161 - 1705838908.2151253]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [161 - 1705838908.2159777]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [161 - 1705838908.2167344]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [161 - 1705838908.217462]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [161 - 1705838908.2181077]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [161 - 1705838908.2198226]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [161 - 1705838908.2217515]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [161 - 1705838908.2226207]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [161 - 1705838908.2233748]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [161 - 1705838908.2240925]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [161 - 1705838908.2247505]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [161 - 1705838908.226503]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [161 - 1705838908.228384]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [161 - 1705838908.2292733]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [161 - 1705838908.2300358]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [161 - 1705838908.2307627]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [161 - 1705838908.2314198]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [161 - 1705838908.2332146]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [161 - 1705838908.2351117]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [161 - 1705838908.2360108]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [161 - 1705838908.236825]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [161 - 1705838908.2375617]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [161 - 1705838908.2382107]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [161 - 1705838908.2399726]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [161 - 1705838908.241935]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [161 - 1705838908.242867]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [161 - 1705838908.2436626]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [161 - 1705838908.2443554]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [161 - 1705838908.245041]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [161 - 1705838908.2467816]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [161 - 1705838908.2486863]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [161 - 1705838908.2495737]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [161 - 1705838908.2503428]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [161 - 1705838908.2509956]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [161 - 1705838908.2516432]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [161 - 1705838908.2534192]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [161 - 1705838908.255312]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [161 - 1705838908.256205]: 0.0
python ParamPruning/classifier.weight [161 - 1705838908.2567306]: 0.0
python DistillationModifier [168 - 1705838949.9829898]: Calling loss_update with:
args: 0.29836466908454895| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.6666666666666665| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [168 - 1705838951.4302943]: 
Returned: 0.7507467865943909| 

python LearningRateFunctionModifier [168 - 1705838954.3469033]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.6666666666666665| steps_per_epoch: 63| 
python LearningRateFunctionModifier [168 - 1705838954.3470771]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [168 - 1705838954.3471384]: 0.00011923076923076922
python LearningRateFunctionModifier/ParamGroup1 [168 - 1705838954.3473635]: 0.00011923076923076922
python DistillationModifier/task_loss [168 - 1705838954.3476684]: tensor(0.2984, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [168 - 1705838954.348815]: tensor(0.7507, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [168 - 1705838954.3494012]: tensor(0.7507, grad_fn=<AddBackward0>)
python ConstantPruningModifier [168 - 1705838954.3499696]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.6666666666666665| steps_per_epoch: 63| 
python ConstantPruningModifier [168 - 1705838954.5754435]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [168 - 1705838954.5762465]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [168 - 1705838954.577058]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [168 - 1705838954.5777445]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [168 - 1705838954.578364]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [168 - 1705838954.5801563]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [168 - 1705838954.5821588]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [168 - 1705838954.5830734]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [168 - 1705838954.5838313]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [168 - 1705838954.5845191]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [168 - 1705838954.5851738]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [168 - 1705838954.5869412]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [168 - 1705838954.588854]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [168 - 1705838954.5897932]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [168 - 1705838954.5905354]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [168 - 1705838954.59117]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [168 - 1705838954.5918238]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [168 - 1705838954.5936282]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [168 - 1705838954.5955465]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [168 - 1705838954.596458]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [168 - 1705838954.5972466]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [168 - 1705838954.5978744]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [168 - 1705838954.5984726]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [168 - 1705838954.600245]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [168 - 1705838954.6022308]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [168 - 1705838954.6031463]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [168 - 1705838954.603939]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [168 - 1705838954.6046367]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [168 - 1705838954.6053088]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [168 - 1705838954.6071181]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [168 - 1705838954.609067]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [168 - 1705838954.6099854]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [168 - 1705838954.6106827]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [168 - 1705838954.611349]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [168 - 1705838954.6119616]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [168 - 1705838954.6137621]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [168 - 1705838954.6157036]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [168 - 1705838954.6165864]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [168 - 1705838954.6173222]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [168 - 1705838954.617961]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [168 - 1705838954.6185799]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [168 - 1705838954.6203847]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [168 - 1705838954.622276]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [168 - 1705838954.623193]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [168 - 1705838954.6239305]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [168 - 1705838954.6246545]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [168 - 1705838954.6253157]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [168 - 1705838954.627101]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [168 - 1705838954.6290581]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [168 - 1705838954.6299696]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [168 - 1705838954.630746]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [168 - 1705838954.6313734]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [168 - 1705838954.6320078]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [168 - 1705838954.6338115]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [168 - 1705838954.635779]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [168 - 1705838954.636731]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [168 - 1705838954.637501]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [168 - 1705838954.638209]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [168 - 1705838954.6388464]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [168 - 1705838954.640646]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [168 - 1705838954.6425848]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [168 - 1705838954.6434858]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [168 - 1705838954.6441941]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [168 - 1705838954.6448827]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [168 - 1705838954.6455364]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [168 - 1705838954.647319]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [168 - 1705838954.6492233]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [168 - 1705838954.6501312]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [168 - 1705838954.6508849]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [168 - 1705838954.6515489]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [168 - 1705838954.6522448]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [168 - 1705838954.654065]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [168 - 1705838954.655964]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [168 - 1705838954.6568778]: 0.0
python ParamPruning/classifier.weight [168 - 1705838954.6573849]: 0.0
python DistillationModifier [175 - 1705838995.0947828]: Calling loss_update with:
args: 0.25869476795196533| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.7777777777777777| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [175 - 1705838996.622109]: 
Returned: 0.6500350832939148| 

python LearningRateFunctionModifier [175 - 1705838999.547048]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.7777777777777777| steps_per_epoch: 63| 
python LearningRateFunctionModifier [175 - 1705838999.5472155]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [175 - 1705838999.5472758]: 0.00011794871794871794
python LearningRateFunctionModifier/ParamGroup1 [175 - 1705838999.5475037]: 0.00011794871794871794
python DistillationModifier/task_loss [175 - 1705838999.5478141]: tensor(0.2587, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [175 - 1705838999.548818]: tensor(0.6500, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [175 - 1705838999.5494642]: tensor(0.6500, grad_fn=<AddBackward0>)
python ConstantPruningModifier [175 - 1705838999.5500917]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.7777777777777777| steps_per_epoch: 63| 
python ConstantPruningModifier [175 - 1705838999.772315]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [175 - 1705838999.7731044]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [175 - 1705838999.7739892]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [175 - 1705838999.774773]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [175 - 1705838999.7754338]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [175 - 1705838999.7772567]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [175 - 1705838999.7792385]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [175 - 1705838999.7801685]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [175 - 1705838999.7808824]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [175 - 1705838999.7816021]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [175 - 1705838999.7822745]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [175 - 1705838999.7840905]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [175 - 1705838999.7860503]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [175 - 1705838999.786981]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [175 - 1705838999.7877817]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [175 - 1705838999.7885058]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [175 - 1705838999.7891622]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [175 - 1705838999.7909484]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [175 - 1705838999.7929657]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [175 - 1705838999.7939565]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [175 - 1705838999.794767]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [175 - 1705838999.7954903]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [175 - 1705838999.7961323]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [175 - 1705838999.7979262]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [175 - 1705838999.7996953]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [175 - 1705838999.8005953]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [175 - 1705838999.8014224]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [175 - 1705838999.8021216]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [175 - 1705838999.8027904]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [175 - 1705838999.804533]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [175 - 1705838999.8064604]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [175 - 1705838999.807365]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [175 - 1705838999.8081472]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [175 - 1705838999.8088682]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [175 - 1705838999.8095412]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [175 - 1705838999.8113618]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [175 - 1705838999.813365]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [175 - 1705838999.814321]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [175 - 1705838999.815061]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [175 - 1705838999.8157697]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [175 - 1705838999.8164198]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [175 - 1705838999.8182416]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [175 - 1705838999.8200898]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [175 - 1705838999.8210413]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [175 - 1705838999.821786]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [175 - 1705838999.8224926]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [175 - 1705838999.823149]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [175 - 1705838999.824955]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [175 - 1705838999.8268664]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [175 - 1705838999.8277347]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [175 - 1705838999.828446]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [175 - 1705838999.829086]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [175 - 1705838999.829648]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [175 - 1705838999.8313892]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [175 - 1705838999.8332705]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [175 - 1705838999.8342483]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [175 - 1705838999.835043]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [175 - 1705838999.8357303]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [175 - 1705838999.8363438]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [175 - 1705838999.8381014]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [175 - 1705838999.8398736]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [175 - 1705838999.8407938]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [175 - 1705838999.8415422]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [175 - 1705838999.8422687]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [175 - 1705838999.8429344]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [175 - 1705838999.844768]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [175 - 1705838999.8467212]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [175 - 1705838999.8476481]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [175 - 1705838999.8483732]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [175 - 1705838999.8491194]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [175 - 1705838999.849797]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [175 - 1705838999.8516223]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [175 - 1705838999.8535516]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [175 - 1705838999.854552]: 0.0
python ParamPruning/classifier.weight [175 - 1705838999.8551357]: 0.0
python DistillationModifier [182 - 1705839042.15065]: Calling loss_update with:
args: 0.33383575081825256| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 2.888888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [182 - 1705839043.6697986]: 
Returned: 0.5744494199752808| 

python LearningRateFunctionModifier [182 - 1705839046.575086]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.888888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [182 - 1705839046.5752554]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [182 - 1705839046.5753133]: 0.00011666666666666667
python LearningRateFunctionModifier/ParamGroup1 [182 - 1705839046.5755415]: 0.00011666666666666667
python DistillationModifier/task_loss [182 - 1705839046.5758305]: tensor(0.3338, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [182 - 1705839046.5768533]: tensor(0.5744, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [182 - 1705839046.5774198]: tensor(0.5744, grad_fn=<AddBackward0>)
python ConstantPruningModifier [182 - 1705839046.5779922]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 2.888888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [182 - 1705839046.8025575]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [182 - 1705839046.8033452]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [182 - 1705839046.8043742]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [182 - 1705839046.80513]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [182 - 1705839046.8058279]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [182 - 1705839046.807658]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [182 - 1705839046.8096075]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [182 - 1705839046.81051]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [182 - 1705839046.8112519]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [182 - 1705839046.8119655]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [182 - 1705839046.8126934]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [182 - 1705839046.814503]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [182 - 1705839046.81639]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [182 - 1705839046.817297]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [182 - 1705839046.8180506]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [182 - 1705839046.8187609]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [182 - 1705839046.8194294]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [182 - 1705839046.821227]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [182 - 1705839046.8231168]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [182 - 1705839046.8240414]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [182 - 1705839046.8248296]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [182 - 1705839046.8255546]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [182 - 1705839046.8262317]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [182 - 1705839046.828038]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [182 - 1705839046.83]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [182 - 1705839046.8309247]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [182 - 1705839046.8316991]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [182 - 1705839046.8323677]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [182 - 1705839046.8330562]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [182 - 1705839046.834837]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [182 - 1705839046.8367429]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [182 - 1705839046.8376515]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [182 - 1705839046.8383718]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [182 - 1705839046.8390484]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [182 - 1705839046.839657]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [182 - 1705839046.8414454]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [182 - 1705839046.843356]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [182 - 1705839046.8442543]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [182 - 1705839046.845059]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [182 - 1705839046.8457747]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [182 - 1705839046.846515]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [182 - 1705839046.8483205]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [182 - 1705839046.85025]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [182 - 1705839046.8511655]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [182 - 1705839046.8519554]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [182 - 1705839046.8527129]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [182 - 1705839046.8534017]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [182 - 1705839046.8551726]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [182 - 1705839046.857095]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [182 - 1705839046.8580081]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [182 - 1705839046.8587563]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [182 - 1705839046.8594558]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [182 - 1705839046.8600981]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [182 - 1705839046.8619103]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [182 - 1705839046.8637094]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [182 - 1705839046.8646095]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [182 - 1705839046.8653905]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [182 - 1705839046.8661332]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [182 - 1705839046.8668158]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [182 - 1705839046.8685932]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [182 - 1705839046.8705418]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [182 - 1705839046.8714707]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [182 - 1705839046.8722405]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [182 - 1705839046.8729377]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [182 - 1705839046.8735797]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [182 - 1705839046.8753524]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [182 - 1705839046.8772793]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [182 - 1705839046.8781955]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [182 - 1705839046.8789527]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [182 - 1705839046.8796544]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [182 - 1705839046.8802826]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [182 - 1705839046.8820822]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [182 - 1705839046.884007]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [182 - 1705839046.8849287]: 0.0
python ParamPruning/classifier.weight [182 - 1705839046.8854406]: 0.0
python DistillationModifier [189 - 1705839086.1813033]: Calling loss_update with:
args: 0.6339170336723328| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839087.6308424]: 
Returned: 1.2170350551605225| 

python DistillationModifier [189 - 1705839089.0936506]: Calling loss_update with:
args: 0.8208523392677307| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839090.537963]: 
Returned: 1.4489085674285889| 

python DistillationModifier [189 - 1705839092.0061316]: Calling loss_update with:
args: 0.9259036779403687| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839093.4491708]: 
Returned: 1.11698579788208| 

python DistillationModifier [189 - 1705839094.9149787]: Calling loss_update with:
args: 0.5721019506454468| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839096.345273]: 
Returned: 0.8379762172698975| 

python DistillationModifier [189 - 1705839097.8131576]: Calling loss_update with:
args: 0.7728482484817505| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839099.2453697]: 
Returned: 1.0515965223312378| 

python DistillationModifier [189 - 1705839100.767491]: Calling loss_update with:
args: 0.5092231631278992| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839102.214178]: 
Returned: 0.9947985410690308| 

python DistillationModifier [189 - 1705839103.6952903]: Calling loss_update with:
args: 0.37128105759620667| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839105.1352441]: 
Returned: 1.0348753929138184| 

python DistillationModifier [189 - 1705839106.6033854]: Calling loss_update with:
args: 0.7833105325698853| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839108.0406332]: 
Returned: 1.2244669198989868| 

python DistillationModifier [189 - 1705839109.5132205]: Calling loss_update with:
args: 0.5261713266372681| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839110.965717]: 
Returned: 0.4932205080986023| 

python DistillationModifier [189 - 1705839112.445435]: Calling loss_update with:
args: 0.7408016324043274| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839113.8968446]: 
Returned: 1.3595600128173828| 

python DistillationModifier [189 - 1705839115.3691578]: Calling loss_update with:
args: 0.4646852910518646| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839116.809779]: 
Returned: 0.9579785466194153| 

python DistillationModifier [189 - 1705839118.286095]: Calling loss_update with:
args: 0.8580746054649353| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839119.735339]: 
Returned: 1.1518079042434692| 

python DistillationModifier [189 - 1705839121.2046518]: Calling loss_update with:
args: 0.796981692314148| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839123.105361]: 
Returned: 1.5362277030944824| 

python DistillationModifier [189 - 1705839125.16849]: Calling loss_update with:
args: 0.5625991821289062| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839126.723758]: 
Returned: 0.5199911594390869| 

python DistillationModifier [189 - 1705839128.2028322]: Calling loss_update with:
args: 0.7244879007339478| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839129.646982]: 
Returned: 1.1672953367233276| 

python DistillationModifier [189 - 1705839131.1634622]: Calling loss_update with:
args: 0.627577006816864| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839132.60064]: 
Returned: 1.3008015155792236| 

python DistillationModifier [189 - 1705839134.3974972]: Calling loss_update with:
args: 0.5749202370643616| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839136.080265]: 
Returned: 1.0158556699752808| 

python DistillationModifier [189 - 1705839137.5430558]: Calling loss_update with:
args: 0.7663494944572449| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839138.971045]: 
Returned: 1.4718338251113892| 

python DistillationModifier [189 - 1705839140.437998]: Calling loss_update with:
args: 0.8731207847595215| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839141.8820524]: 
Returned: 1.0204944610595703| 

python DistillationModifier [189 - 1705839143.3730264]: Calling loss_update with:
args: 0.5051551461219788| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839144.8091316]: 
Returned: 0.9261648058891296| 

python DistillationModifier [189 - 1705839146.2749798]: Calling loss_update with:
args: 0.48786187171936035| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839147.7166133]: 
Returned: 1.006476640701294| 

python DistillationModifier [189 - 1705839149.182697]: Calling loss_update with:
args: 0.4717554748058319| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839150.6193194]: 
Returned: 0.8535670638084412| 

python DistillationModifier [189 - 1705839152.0862975]: Calling loss_update with:
args: 0.5234454274177551| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839153.5322194]: 
Returned: 0.9218211770057678| 

python DistillationModifier [189 - 1705839155.0062969]: Calling loss_update with:
args: 0.45356717705726624| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839156.442241]: 
Returned: 0.8322566151618958| 

python DistillationModifier [189 - 1705839157.9060607]: Calling loss_update with:
args: 0.7919467687606812| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839159.3406909]: 
Returned: 1.0005183219909668| 

python DistillationModifier [189 - 1705839160.8177955]: Calling loss_update with:
args: 0.4370875954627991| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839162.2982104]: 
Returned: 0.8659093379974365| 

python DistillationModifier [189 - 1705839163.793416]: Calling loss_update with:
args: 0.5542236566543579| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839165.232459]: 
Returned: 0.8324588537216187| 

python DistillationModifier [189 - 1705839166.7045105]: Calling loss_update with:
args: 0.6068570613861084| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839168.147888]: 
Returned: 0.9852404594421387| 

python DistillationModifier [189 - 1705839169.6229863]: Calling loss_update with:
args: 0.5887510180473328| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839171.0585043]: 
Returned: 1.0360254049301147| 

python DistillationModifier [189 - 1705839172.5314977]: Calling loss_update with:
args: 0.472237765789032| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839173.9762278]: 
Returned: 0.9408555030822754| 

python DistillationModifier [189 - 1705839175.4459462]: Calling loss_update with:
args: 0.6132938861846924| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839176.8795836]: 
Returned: 1.3225746154785156| 

python DistillationModifier [189 - 1705839178.3498366]: Calling loss_update with:
args: 0.8479135632514954| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839179.7901165]: 
Returned: 1.2149392366409302| 

python DistillationModifier [189 - 1705839181.260906]: Calling loss_update with:
args: 0.7005326151847839| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839183.185848]: 
Returned: 1.2896071672439575| 

python DistillationModifier [189 - 1705839185.2432055]: Calling loss_update with:
args: 0.9950538277626038| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839186.759143]: 
Returned: 1.5851647853851318| 

python DistillationModifier [189 - 1705839188.2256474]: Calling loss_update with:
args: 0.8470516800880432| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839189.6643271]: 
Returned: 1.3017020225524902| 

python DistillationModifier [189 - 1705839191.1349792]: Calling loss_update with:
args: 0.9689611792564392| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839192.6225338]: 
Returned: 1.872003197669983| 

python DistillationModifier [189 - 1705839194.1074207]: Calling loss_update with:
args: 0.6614537835121155| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839195.5424476]: 
Returned: 0.6788743734359741| 

python DistillationModifier [189 - 1705839197.0146964]: Calling loss_update with:
args: 0.3880200982093811| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839198.4515738]: 
Returned: 0.7592846751213074| 

python DistillationModifier [189 - 1705839199.9242923]: Calling loss_update with:
args: 0.7228257656097412| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839201.3614073]: 
Returned: 1.414358377456665| 

python DistillationModifier [189 - 1705839202.8331342]: Calling loss_update with:
args: 0.7811934947967529| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839204.28273]: 
Returned: 1.5611844062805176| 

python DistillationModifier [189 - 1705839205.7524712]: Calling loss_update with:
args: 0.46682611107826233| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839207.1874554]: 
Returned: 1.179353952407837| 

python DistillationModifier [189 - 1705839208.657713]: Calling loss_update with:
args: 0.6189621686935425| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839210.0933883]: 
Returned: 1.3237943649291992| 

python DistillationModifier [189 - 1705839211.5682724]: Calling loss_update with:
args: 0.5536924600601196| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839213.0114527]: 
Returned: 0.7422130703926086| 

python DistillationModifier [189 - 1705839214.4916189]: Calling loss_update with:
args: 0.4872707724571228| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839215.9336085]: 
Returned: 1.1209834814071655| 

python DistillationModifier [189 - 1705839217.402691]: Calling loss_update with:
args: 0.7301363945007324| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839218.839415]: 
Returned: 1.6854016780853271| 

python DistillationModifier [189 - 1705839220.3120785]: Calling loss_update with:
args: 0.8619447350502014| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839221.822068]: 
Returned: 1.4610185623168945| 

python DistillationModifier [189 - 1705839223.806931]: Calling loss_update with:
args: 0.40818628668785095| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839225.2563553]: 
Returned: 0.866140604019165| 

python DistillationModifier [189 - 1705839226.7227833]: Calling loss_update with:
args: 0.31893110275268555| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839228.1657574]: 
Returned: 0.79115229845047| 

python DistillationModifier [189 - 1705839229.6391332]: Calling loss_update with:
args: 1.163383960723877| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839231.0757291]: 
Returned: 2.158130168914795| 

python DistillationModifier [189 - 1705839232.5513499]: Calling loss_update with:
args: 0.7299587726593018| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839233.98928]: 
Returned: 1.3961797952651978| 

python DistillationModifier [189 - 1705839235.4673686]: Calling loss_update with:
args: 0.3396533727645874| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839236.9094434]: 
Returned: 0.7251044511795044| 

python DistillationModifier [189 - 1705839238.379925]: Calling loss_update with:
args: 1.1674540042877197| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839239.8179429]: 
Returned: 1.132473349571228| 

python DistillationModifier [189 - 1705839241.2829854]: Calling loss_update with:
args: 0.4788539707660675| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839243.225872]: 
Returned: 0.7445869445800781| 

python DistillationModifier [189 - 1705839245.2866402]: Calling loss_update with:
args: 0.5683577656745911| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839246.7979484]: 
Returned: 0.9382137656211853| 

python DistillationModifier [189 - 1705839248.2625282]: Calling loss_update with:
args: 0.486566424369812| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839249.7024562]: 
Returned: 1.2159852981567383| 

python DistillationModifier [189 - 1705839251.1702757]: Calling loss_update with:
args: 0.48916372656822205| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839252.6587322]: 
Returned: 0.830221951007843| 

python DistillationModifier [189 - 1705839254.1292748]: Calling loss_update with:
args: 0.820497989654541| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839255.5739868]: 
Returned: 0.7715974450111389| 

python DistillationModifier [189 - 1705839257.042713]: Calling loss_update with:
args: 0.7697902917861938| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839258.4853983]: 
Returned: 1.0885878801345825| 

python DistillationModifier [189 - 1705839259.9591646]: Calling loss_update with:
args: 0.7133195400238037| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839261.3942046]: 
Returned: 1.2824814319610596| 

python DistillationModifier [189 - 1705839262.8619788]: Calling loss_update with:
args: 0.7589071989059448| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839264.3203537]: 
Returned: 1.386419653892517| 

python DistillationModifier [189 - 1705839265.8003538]: Calling loss_update with:
args: 0.5122426748275757| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839267.2440553]: 
Returned: 1.0855005979537964| 

python DistillationModifier [189 - 1705839268.7154734]: Calling loss_update with:
args: 0.6382718086242676| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839270.1578662]: 
Returned: 1.2331702709197998| 

python DistillationModifier [189 - 1705839270.9279659]: Calling loss_update with:
args: 0.4862169027328491| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839271.6788685]: 
Returned: 1.0049974918365479| 

python DistillationModifier [189 - 1705839273.7607865]: Calling loss_update with:
args: 0.2585879862308502| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [189 - 1705839275.2152371]: 
Returned: 0.6263667345046997| 

python LearningRateFunctionModifier [189 - 1705839278.1360028]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [189 - 1705839278.1361842]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [189 - 1705839278.136252]: 0.00011538461538461537
python LearningRateFunctionModifier/ParamGroup1 [189 - 1705839278.1364815]: 0.00011538461538461537
python DistillationModifier/task_loss [189 - 1705839278.1368139]: tensor(0.2586, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [189 - 1705839278.1378758]: tensor(0.6264, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [189 - 1705839278.1386065]: tensor(0.6264, grad_fn=<AddBackward0>)
python ConstantPruningModifier [189 - 1705839278.1392686]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.0| steps_per_epoch: 63| 
python ConstantPruningModifier [189 - 1705839278.3619356]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [189 - 1705839278.3626974]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [189 - 1705839278.3635783]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [189 - 1705839278.3643775]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [189 - 1705839278.365172]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [189 - 1705839278.3670366]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [189 - 1705839278.3689532]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [189 - 1705839278.3699331]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [189 - 1705839278.3707316]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [189 - 1705839278.3714187]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [189 - 1705839278.3720784]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [189 - 1705839278.3739257]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [189 - 1705839278.375921]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [189 - 1705839278.3769355]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [189 - 1705839278.3777647]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [189 - 1705839278.3785293]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [189 - 1705839278.3792458]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [189 - 1705839278.3811615]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [189 - 1705839278.3831975]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [189 - 1705839278.3841023]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [189 - 1705839278.3849382]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [189 - 1705839278.385709]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [189 - 1705839278.3863778]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [189 - 1705839278.3882267]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [189 - 1705839278.3901312]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [189 - 1705839278.3911066]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [189 - 1705839278.3919759]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [189 - 1705839278.3927803]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [189 - 1705839278.393521]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [189 - 1705839278.3953505]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [189 - 1705839278.3973124]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [189 - 1705839278.3983352]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [189 - 1705839278.3991332]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [189 - 1705839278.3998065]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [189 - 1705839278.4004328]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [189 - 1705839278.402271]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [189 - 1705839278.4041445]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [189 - 1705839278.4051592]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [189 - 1705839278.40602]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [189 - 1705839278.406769]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [189 - 1705839278.407493]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [189 - 1705839278.4093537]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [189 - 1705839278.4113889]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [189 - 1705839278.4124472]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [189 - 1705839278.4132986]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [189 - 1705839278.4140792]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [189 - 1705839278.4148114]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [189 - 1705839278.4167032]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [189 - 1705839278.4187064]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [189 - 1705839278.4195921]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [189 - 1705839278.4203472]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [189 - 1705839278.4210956]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [189 - 1705839278.4218035]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [189 - 1705839278.4236543]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [189 - 1705839278.4256177]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [189 - 1705839278.426584]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [189 - 1705839278.4273536]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [189 - 1705839278.4281054]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [189 - 1705839278.428818]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [189 - 1705839278.43067]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [189 - 1705839278.432595]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [189 - 1705839278.4336104]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [189 - 1705839278.434398]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [189 - 1705839278.4351547]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [189 - 1705839278.4358497]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [189 - 1705839278.437696]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [189 - 1705839278.4396677]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [189 - 1705839278.4406316]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [189 - 1705839278.4414856]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [189 - 1705839278.4422276]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [189 - 1705839278.4429548]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [189 - 1705839278.4448078]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [189 - 1705839278.4468024]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [189 - 1705839278.447807]: 0.0
python ParamPruning/classifier.weight [189 - 1705839278.4483693]: 0.0
python DistillationModifier [196 - 1705839320.487628]: Calling loss_update with:
args: 0.22485828399658203| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.111111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [196 - 1705839321.9489937]: 
Returned: 0.4003044366836548| 

python LearningRateFunctionModifier [196 - 1705839324.8828418]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.111111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [196 - 1705839324.8830314]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [196 - 1705839324.8831024]: 0.0001141025641025641
python LearningRateFunctionModifier/ParamGroup1 [196 - 1705839324.883314]: 0.0001141025641025641
python DistillationModifier/task_loss [196 - 1705839324.8836203]: tensor(0.2249, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [196 - 1705839324.884686]: tensor(0.4003, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [196 - 1705839324.8853514]: tensor(0.4003, grad_fn=<AddBackward0>)
python ConstantPruningModifier [196 - 1705839324.8859932]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.111111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [196 - 1705839325.1122224]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [196 - 1705839325.1130261]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [196 - 1705839325.113865]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [196 - 1705839325.1146376]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [196 - 1705839325.1153908]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [196 - 1705839325.1173415]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [196 - 1705839325.119287]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [196 - 1705839325.1202428]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [196 - 1705839325.121089]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [196 - 1705839325.1218548]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [196 - 1705839325.1225877]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [196 - 1705839325.12451]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [196 - 1705839325.126442]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [196 - 1705839325.1273644]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [196 - 1705839325.1281183]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [196 - 1705839325.1287863]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [196 - 1705839325.1294692]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [196 - 1705839325.1313791]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [196 - 1705839325.1333404]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [196 - 1705839325.1342762]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [196 - 1705839325.1350436]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [196 - 1705839325.1357684]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [196 - 1705839325.1364322]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [196 - 1705839325.1383812]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [196 - 1705839325.1403143]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [196 - 1705839325.1412716]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [196 - 1705839325.1421008]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [196 - 1705839325.1428561]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [196 - 1705839325.1436043]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [196 - 1705839325.145503]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [196 - 1705839325.1473796]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [196 - 1705839325.1483016]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [196 - 1705839325.1490781]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [196 - 1705839325.149718]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [196 - 1705839325.1503174]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [196 - 1705839325.1521943]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [196 - 1705839325.1541739]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [196 - 1705839325.155115]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [196 - 1705839325.1559417]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [196 - 1705839325.156729]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [196 - 1705839325.1573982]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [196 - 1705839325.1593463]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [196 - 1705839325.1613317]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [196 - 1705839325.1623013]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [196 - 1705839325.16306]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [196 - 1705839325.1637957]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [196 - 1705839325.164524]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [196 - 1705839325.1664329]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [196 - 1705839325.168358]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [196 - 1705839325.169348]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [196 - 1705839325.170197]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [196 - 1705839325.1709595]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [196 - 1705839325.1716673]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [196 - 1705839325.1736102]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [196 - 1705839325.1755412]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [196 - 1705839325.1764834]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [196 - 1705839325.177334]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [196 - 1705839325.1781144]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [196 - 1705839325.1788354]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [196 - 1705839325.1807585]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [196 - 1705839325.182665]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [196 - 1705839325.183602]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [196 - 1705839325.1843991]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [196 - 1705839325.1851084]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [196 - 1705839325.1857698]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [196 - 1705839325.187627]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [196 - 1705839325.1895626]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [196 - 1705839325.1905148]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [196 - 1705839325.191309]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [196 - 1705839325.1920137]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [196 - 1705839325.1926432]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [196 - 1705839325.1945393]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [196 - 1705839325.1964626]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [196 - 1705839325.1974304]: 0.0
python ParamPruning/classifier.weight [196 - 1705839325.1979873]: 0.0
python DistillationModifier [203 - 1705839367.3560154]: Calling loss_update with:
args: 0.292875200510025| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.2222222222222223| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [203 - 1705839368.815639]: 
Returned: 0.6010494828224182| 

python LearningRateFunctionModifier [203 - 1705839371.7354412]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.2222222222222223| steps_per_epoch: 63| 
python LearningRateFunctionModifier [203 - 1705839371.735614]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [203 - 1705839371.735693]: 0.0001128205128205128
python LearningRateFunctionModifier/ParamGroup1 [203 - 1705839371.7359061]: 0.0001128205128205128
python DistillationModifier/task_loss [203 - 1705839371.736215]: tensor(0.2929, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [203 - 1705839371.7372317]: tensor(0.6010, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [203 - 1705839371.7378523]: tensor(0.6010, grad_fn=<AddBackward0>)
python ConstantPruningModifier [203 - 1705839371.7384539]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.2222222222222223| steps_per_epoch: 63| 
python ConstantPruningModifier [203 - 1705839371.9601552]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [203 - 1705839371.9609559]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [203 - 1705839371.961787]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [203 - 1705839371.9625175]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [203 - 1705839371.9632423]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [203 - 1705839371.9650786]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [203 - 1705839371.9670231]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [203 - 1705839371.9679704]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [203 - 1705839371.968752]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [203 - 1705839371.9693863]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [203 - 1705839371.970028]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [203 - 1705839371.9718559]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [203 - 1705839371.9738312]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [203 - 1705839371.9748118]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [203 - 1705839371.9755573]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [203 - 1705839371.9762058]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [203 - 1705839371.9768124]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [203 - 1705839371.9786162]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [203 - 1705839371.9805384]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [203 - 1705839371.981568]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [203 - 1705839371.9824302]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [203 - 1705839371.9832025]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [203 - 1705839371.9838848]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [203 - 1705839371.985771]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [203 - 1705839371.9877012]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [203 - 1705839371.9887323]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [203 - 1705839371.989609]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [203 - 1705839371.9903548]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [203 - 1705839371.9910333]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [203 - 1705839371.9928806]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [203 - 1705839371.9948735]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [203 - 1705839371.9958327]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [203 - 1705839371.9966493]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [203 - 1705839371.9974415]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [203 - 1705839371.9981601]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [203 - 1705839372.0000005]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [203 - 1705839372.0020177]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [203 - 1705839372.0030293]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [203 - 1705839372.003826]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [203 - 1705839372.0045092]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [203 - 1705839372.0051968]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [203 - 1705839372.0070336]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [203 - 1705839372.0089107]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [203 - 1705839372.009901]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [203 - 1705839372.010771]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [203 - 1705839372.011582]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [203 - 1705839372.012331]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [203 - 1705839372.0141854]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [203 - 1705839372.016107]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [203 - 1705839372.0170603]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [203 - 1705839372.0178628]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [203 - 1705839372.018561]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [203 - 1705839372.019198]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [203 - 1705839372.0210154]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [203 - 1705839372.0228045]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [203 - 1705839372.0237737]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [203 - 1705839372.0246196]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [203 - 1705839372.025398]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [203 - 1705839372.0260887]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [203 - 1705839372.0279052]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [203 - 1705839372.0297678]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [203 - 1705839372.0307488]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [203 - 1705839372.0315912]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [203 - 1705839372.0323362]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [203 - 1705839372.0330136]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [203 - 1705839372.0348175]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [203 - 1705839372.0366113]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [203 - 1705839372.0375934]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [203 - 1705839372.0384276]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [203 - 1705839372.0391552]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [203 - 1705839372.039818]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [203 - 1705839372.0416448]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [203 - 1705839372.0434766]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [203 - 1705839372.0444887]: 0.0
python ParamPruning/classifier.weight [203 - 1705839372.0450811]: 0.0
python DistillationModifier [210 - 1705839412.44947]: Calling loss_update with:
args: 0.2517012655735016| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.3333333333333335| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [210 - 1705839414.000914]: 
Returned: 0.6741973161697388| 

python LearningRateFunctionModifier [210 - 1705839416.9042828]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.3333333333333335| steps_per_epoch: 63| 
python LearningRateFunctionModifier [210 - 1705839416.9044516]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [210 - 1705839416.9045131]: 0.00011153846153846153
python LearningRateFunctionModifier/ParamGroup1 [210 - 1705839416.904774]: 0.00011153846153846153
python DistillationModifier/task_loss [210 - 1705839416.9050589]: tensor(0.2517, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [210 - 1705839416.9061255]: tensor(0.6742, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [210 - 1705839416.9067225]: tensor(0.6742, grad_fn=<AddBackward0>)
python ConstantPruningModifier [210 - 1705839416.9073105]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.3333333333333335| steps_per_epoch: 63| 
python ConstantPruningModifier [210 - 1705839417.1307433]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [210 - 1705839417.1315475]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [210 - 1705839417.1324244]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [210 - 1705839417.1331618]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [210 - 1705839417.1338198]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [210 - 1705839417.1356406]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [210 - 1705839417.1376266]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [210 - 1705839417.1385987]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [210 - 1705839417.1393266]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [210 - 1705839417.1399236]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [210 - 1705839417.1404817]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [210 - 1705839417.1422548]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [210 - 1705839417.1442134]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [210 - 1705839417.1451776]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [210 - 1705839417.145888]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [210 - 1705839417.1465077]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [210 - 1705839417.1470685]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [210 - 1705839417.1488774]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [210 - 1705839417.1506937]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [210 - 1705839417.1516285]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [210 - 1705839417.152438]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [210 - 1705839417.1531348]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [210 - 1705839417.153733]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [210 - 1705839417.155521]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [210 - 1705839417.1574502]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [210 - 1705839417.158396]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [210 - 1705839417.1590955]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [210 - 1705839417.159696]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [210 - 1705839417.1602712]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [210 - 1705839417.1620624]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [210 - 1705839417.163956]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [210 - 1705839417.164927]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [210 - 1705839417.1656857]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [210 - 1705839417.1663082]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [210 - 1705839417.1668584]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [210 - 1705839417.1686199]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [210 - 1705839417.1705887]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [210 - 1705839417.1715338]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [210 - 1705839417.1722937]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [210 - 1705839417.172918]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [210 - 1705839417.1734793]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [210 - 1705839417.1752603]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [210 - 1705839417.1771913]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [210 - 1705839417.1781325]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [210 - 1705839417.1787937]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [210 - 1705839417.1793935]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [210 - 1705839417.1799395]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [210 - 1705839417.1817198]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [210 - 1705839417.1835265]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [210 - 1705839417.1844413]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [210 - 1705839417.185208]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [210 - 1705839417.1858232]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [210 - 1705839417.1863778]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [210 - 1705839417.188121]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [210 - 1705839417.1900277]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [210 - 1705839417.1909342]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [210 - 1705839417.191641]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [210 - 1705839417.1922808]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [210 - 1705839417.192862]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [210 - 1705839417.194651]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [210 - 1705839417.196574]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [210 - 1705839417.1974998]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [210 - 1705839417.1982272]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [210 - 1705839417.198835]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [210 - 1705839417.1993792]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [210 - 1705839417.201179]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [210 - 1705839417.2031298]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [210 - 1705839417.2040694]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [210 - 1705839417.2048292]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [210 - 1705839417.2054582]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [210 - 1705839417.2060103]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [210 - 1705839417.2077749]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [210 - 1705839417.2096114]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [210 - 1705839417.210546]: 0.0
python ParamPruning/classifier.weight [210 - 1705839417.211093]: 0.0
python DistillationModifier [217 - 1705839459.116721]: Calling loss_update with:
args: 0.2665320038795471| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.4444444444444446| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [217 - 1705839460.665728]: 
Returned: 0.5622284412384033| 

python LearningRateFunctionModifier [217 - 1705839463.5392077]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.4444444444444446| steps_per_epoch: 63| 
python LearningRateFunctionModifier [217 - 1705839463.5393698]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [217 - 1705839463.5394292]: 0.00011025641025641025
python LearningRateFunctionModifier/ParamGroup1 [217 - 1705839463.5396705]: 0.00011025641025641025
python DistillationModifier/task_loss [217 - 1705839463.5399532]: tensor(0.2665, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [217 - 1705839463.541018]: tensor(0.5622, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [217 - 1705839463.5416093]: tensor(0.5622, grad_fn=<AddBackward0>)
python ConstantPruningModifier [217 - 1705839463.5421855]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.4444444444444446| steps_per_epoch: 63| 
python ConstantPruningModifier [217 - 1705839463.7606115]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [217 - 1705839463.7614195]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [217 - 1705839463.7622821]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [217 - 1705839463.763017]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [217 - 1705839463.7636776]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [217 - 1705839463.765401]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [217 - 1705839463.767214]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [217 - 1705839463.768023]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [217 - 1705839463.7687273]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [217 - 1705839463.7693682]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [217 - 1705839463.7699432]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [217 - 1705839463.7716649]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [217 - 1705839463.7735157]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [217 - 1705839463.7743354]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [217 - 1705839463.7750494]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [217 - 1705839463.7756855]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [217 - 1705839463.776302]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [217 - 1705839463.7780476]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [217 - 1705839463.7798772]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [217 - 1705839463.78069]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [217 - 1705839463.7814112]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [217 - 1705839463.7820988]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [217 - 1705839463.7827203]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [217 - 1705839463.7844045]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [217 - 1705839463.7861984]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [217 - 1705839463.7869544]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [217 - 1705839463.78767]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [217 - 1705839463.788337]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [217 - 1705839463.789021]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [217 - 1705839463.7907498]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [217 - 1705839463.7925553]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [217 - 1705839463.7933507]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [217 - 1705839463.7940338]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [217 - 1705839463.7946959]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [217 - 1705839463.795326]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [217 - 1705839463.7970438]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [217 - 1705839463.7988472]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [217 - 1705839463.799612]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [217 - 1705839463.8003054]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [217 - 1705839463.801001]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [217 - 1705839463.801585]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [217 - 1705839463.8032827]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [217 - 1705839463.8049161]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [217 - 1705839463.8058085]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [217 - 1705839463.8065622]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [217 - 1705839463.8072104]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [217 - 1705839463.8077571]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [217 - 1705839463.8094313]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [217 - 1705839463.8111925]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [217 - 1705839463.811933]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [217 - 1705839463.812641]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [217 - 1705839463.8133383]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [217 - 1705839463.8139343]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [217 - 1705839463.8156528]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [217 - 1705839463.8174767]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [217 - 1705839463.8182707]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [217 - 1705839463.8189585]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [217 - 1705839463.8196251]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [217 - 1705839463.8202345]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [217 - 1705839463.8219926]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [217 - 1705839463.8237975]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [217 - 1705839463.8245947]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [217 - 1705839463.8252523]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [217 - 1705839463.8258917]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [217 - 1705839463.8265038]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [217 - 1705839463.8281891]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [217 - 1705839463.829779]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [217 - 1705839463.8305693]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [217 - 1705839463.8312783]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [217 - 1705839463.831949]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [217 - 1705839463.8325548]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [217 - 1705839463.8342402]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [217 - 1705839463.835871]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [217 - 1705839463.8367176]: 0.0
python ParamPruning/classifier.weight [217 - 1705839463.837224]: 0.0
python DistillationModifier [224 - 1705839505.2456267]: Calling loss_update with:
args: 0.5369744300842285| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.5555555555555554| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [224 - 1705839506.7730236]: 
Returned: 0.561142086982727| 

python LearningRateFunctionModifier [224 - 1705839509.6676548]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.5555555555555554| steps_per_epoch: 63| 
python LearningRateFunctionModifier [224 - 1705839509.6678305]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [224 - 1705839509.6678884]: 0.00010897435897435896
python LearningRateFunctionModifier/ParamGroup1 [224 - 1705839509.6681163]: 0.00010897435897435896
python DistillationModifier/task_loss [224 - 1705839509.6684072]: tensor(0.5370, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [224 - 1705839509.6694489]: tensor(0.5611, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [224 - 1705839509.6699793]: tensor(0.5611, grad_fn=<AddBackward0>)
python ConstantPruningModifier [224 - 1705839509.6705096]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.5555555555555554| steps_per_epoch: 63| 
python ConstantPruningModifier [224 - 1705839509.892407]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [224 - 1705839509.8932376]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [224 - 1705839509.8941936]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [224 - 1705839509.8949192]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [224 - 1705839509.8955843]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [224 - 1705839509.89736]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [224 - 1705839509.8991868]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [224 - 1705839509.900104]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [224 - 1705839509.9008834]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [224 - 1705839509.9015832]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [224 - 1705839509.9022079]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [224 - 1705839509.9039648]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [224 - 1705839509.9058485]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [224 - 1705839509.9067264]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [224 - 1705839509.9074476]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [224 - 1705839509.9081101]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [224 - 1705839509.9087255]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [224 - 1705839509.9104648]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [224 - 1705839509.9123347]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [224 - 1705839509.913256]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [224 - 1705839509.9139283]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [224 - 1705839509.9145615]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [224 - 1705839509.915178]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [224 - 1705839509.9169307]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [224 - 1705839509.9186604]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [224 - 1705839509.919573]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [224 - 1705839509.9203305]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [224 - 1705839509.9210262]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [224 - 1705839509.921689]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [224 - 1705839509.9234264]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [224 - 1705839509.925175]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [224 - 1705839509.9260588]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [224 - 1705839509.9268477]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [224 - 1705839509.9274604]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [224 - 1705839509.928082]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [224 - 1705839509.9298406]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [224 - 1705839509.9316087]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [224 - 1705839509.9325814]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [224 - 1705839509.9334016]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [224 - 1705839509.9341123]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [224 - 1705839509.934719]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [224 - 1705839509.9364626]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [224 - 1705839509.9383368]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [224 - 1705839509.939222]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [224 - 1705839509.9398804]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [224 - 1705839509.9405131]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [224 - 1705839509.9411008]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [224 - 1705839509.9428275]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [224 - 1705839509.944714]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [224 - 1705839509.9456532]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [224 - 1705839509.9463842]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [224 - 1705839509.9469967]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [224 - 1705839509.9475608]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [224 - 1705839509.9493043]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [224 - 1705839509.9511578]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [224 - 1705839509.9520476]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [224 - 1705839509.9527457]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [224 - 1705839509.9534032]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [224 - 1705839509.954021]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [224 - 1705839509.955796]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [224 - 1705839509.9576464]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [224 - 1705839509.9585083]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [224 - 1705839509.9592602]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [224 - 1705839509.9599254]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [224 - 1705839509.9605067]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [224 - 1705839509.9622607]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [224 - 1705839509.9640007]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [224 - 1705839509.9649165]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [224 - 1705839509.9656954]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [224 - 1705839509.9663367]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [224 - 1705839509.966955]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [224 - 1705839509.9687011]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [224 - 1705839509.9705462]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [224 - 1705839509.9714222]: 0.0
python ParamPruning/classifier.weight [224 - 1705839509.971938]: 0.0
python DistillationModifier [231 - 1705839551.2124598]: Calling loss_update with:
args: 0.2949705123901367| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.6666666666666665| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [231 - 1705839552.7267756]: 
Returned: 0.5592197775840759| 

python LearningRateFunctionModifier [231 - 1705839555.586336]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.6666666666666665| steps_per_epoch: 63| 
python LearningRateFunctionModifier [231 - 1705839555.5865037]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [231 - 1705839555.586566]: 0.0001076923076923077
python LearningRateFunctionModifier/ParamGroup1 [231 - 1705839555.5867908]: 0.0001076923076923077
python DistillationModifier/task_loss [231 - 1705839555.5870886]: tensor(0.2950, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [231 - 1705839555.588056]: tensor(0.5592, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [231 - 1705839555.5885863]: tensor(0.5592, grad_fn=<AddBackward0>)
python ConstantPruningModifier [231 - 1705839555.5891333]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.6666666666666665| steps_per_epoch: 63| 
python ConstantPruningModifier [231 - 1705839555.8092287]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [231 - 1705839555.810117]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [231 - 1705839555.8112414]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [231 - 1705839555.8121996]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [231 - 1705839555.8129652]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [231 - 1705839555.815063]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [231 - 1705839555.8171184]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [231 - 1705839555.8181221]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [231 - 1705839555.818981]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [231 - 1705839555.819774]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [231 - 1705839555.8204162]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [231 - 1705839555.8225415]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [231 - 1705839555.8248096]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [231 - 1705839555.825753]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [231 - 1705839555.8265653]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [231 - 1705839555.8273194]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [231 - 1705839555.8280158]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [231 - 1705839555.8300798]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [231 - 1705839555.8322616]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [231 - 1705839555.8333085]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [231 - 1705839555.8340602]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [231 - 1705839555.8347292]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [231 - 1705839555.8353393]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [231 - 1705839555.8373601]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [231 - 1705839555.839294]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [231 - 1705839555.8404436]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [231 - 1705839555.8415132]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [231 - 1705839555.8424766]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [231 - 1705839555.8432634]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [231 - 1705839555.8459048]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [231 - 1705839555.848429]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [231 - 1705839555.8494864]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [231 - 1705839555.8504753]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [231 - 1705839555.8514142]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [231 - 1705839555.8522255]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [231 - 1705839555.8549232]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [231 - 1705839555.8576994]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [231 - 1705839555.8587618]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [231 - 1705839555.8597116]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [231 - 1705839555.8606281]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [231 - 1705839555.8615491]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [231 - 1705839555.864183]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [231 - 1705839555.866957]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [231 - 1705839555.8680227]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [231 - 1705839555.8689601]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [231 - 1705839555.8698444]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [231 - 1705839555.8706152]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [231 - 1705839555.8732722]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [231 - 1705839555.8758707]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [231 - 1705839555.876943]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [231 - 1705839555.877886]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [231 - 1705839555.878657]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [231 - 1705839555.8793833]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [231 - 1705839555.882004]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [231 - 1705839555.8845325]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [231 - 1705839555.88563]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [231 - 1705839555.8865907]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [231 - 1705839555.8873672]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [231 - 1705839555.8881333]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [231 - 1705839555.8907866]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [231 - 1705839555.8935385]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [231 - 1705839555.8945982]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [231 - 1705839555.8955014]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [231 - 1705839555.8962724]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [231 - 1705839555.897151]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [231 - 1705839555.8997571]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [231 - 1705839555.9024851]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [231 - 1705839555.9035432]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [231 - 1705839555.9044573]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [231 - 1705839555.9053593]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [231 - 1705839555.906257]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [231 - 1705839555.9089327]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [231 - 1705839555.9116352]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [231 - 1705839555.9126623]: 0.0
python ParamPruning/classifier.weight [231 - 1705839555.9131699]: 0.0
python DistillationModifier [238 - 1705839596.3520205]: Calling loss_update with:
args: 0.0698639377951622| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.7777777777777777| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [238 - 1705839597.779006]: 
Returned: 0.22206759452819824| 

python LearningRateFunctionModifier [238 - 1705839600.6287215]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.7777777777777777| steps_per_epoch: 63| 
python LearningRateFunctionModifier [238 - 1705839600.6288815]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [238 - 1705839600.6289399]: 0.0001064102564102564
python LearningRateFunctionModifier/ParamGroup1 [238 - 1705839600.6291714]: 0.0001064102564102564
python DistillationModifier/task_loss [238 - 1705839600.6294627]: tensor(0.0699, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [238 - 1705839600.6304297]: tensor(0.2221, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [238 - 1705839600.6309621]: tensor(0.2221, grad_fn=<AddBackward0>)
python ConstantPruningModifier [238 - 1705839600.63151]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.7777777777777777| steps_per_epoch: 63| 
python ConstantPruningModifier [238 - 1705839600.8474944]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [238 - 1705839600.8482447]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [238 - 1705839600.8490534]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [238 - 1705839600.8497229]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [238 - 1705839600.8502817]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [238 - 1705839600.8519852]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [238 - 1705839600.8536146]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [238 - 1705839600.8543425]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [238 - 1705839600.8550344]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [238 - 1705839600.8556485]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [238 - 1705839600.856204]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [238 - 1705839600.857875]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [238 - 1705839600.859482]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [238 - 1705839600.8602257]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [238 - 1705839600.8609087]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [238 - 1705839600.8614783]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [238 - 1705839600.8620324]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [238 - 1705839600.8636277]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [238 - 1705839600.8653944]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [238 - 1705839600.866135]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [238 - 1705839600.8667834]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [238 - 1705839600.8673985]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [238 - 1705839600.8679988]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [238 - 1705839600.8697574]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [238 - 1705839600.8715932]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [238 - 1705839600.8724442]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [238 - 1705839600.8731904]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [238 - 1705839600.8737586]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [238 - 1705839600.8743553]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [238 - 1705839600.8760607]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [238 - 1705839600.8778598]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [238 - 1705839600.878574]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [238 - 1705839600.8791313]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [238 - 1705839600.8797383]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [238 - 1705839600.8803499]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [238 - 1705839600.8821561]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [238 - 1705839600.8838172]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [238 - 1705839600.884547]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [238 - 1705839600.8852487]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [238 - 1705839600.8858309]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [238 - 1705839600.8864243]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [238 - 1705839600.8881218]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [238 - 1705839600.8899894]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [238 - 1705839600.890731]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [238 - 1705839600.8914433]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [238 - 1705839600.8921537]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [238 - 1705839600.8927953]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [238 - 1705839600.8946202]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [238 - 1705839600.8963764]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [238 - 1705839600.8971274]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [238 - 1705839600.8977716]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [238 - 1705839600.8984127]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [238 - 1705839600.8989782]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [238 - 1705839600.900686]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [238 - 1705839600.9024239]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [238 - 1705839600.9031308]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [238 - 1705839600.903762]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [238 - 1705839600.904359]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [238 - 1705839600.9049687]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [238 - 1705839600.9066715]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [238 - 1705839600.908404]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [238 - 1705839600.9091341]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [238 - 1705839600.9097629]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [238 - 1705839600.9102874]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [238 - 1705839600.9107873]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [238 - 1705839600.9124565]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [238 - 1705839600.914173]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [238 - 1705839600.9148672]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [238 - 1705839600.9154956]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [238 - 1705839600.9160492]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [238 - 1705839600.9165378]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [238 - 1705839600.9182084]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [238 - 1705839600.919877]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [238 - 1705839600.920531]: 0.0
python ParamPruning/classifier.weight [238 - 1705839600.9209137]: 0.0
python DistillationModifier [245 - 1705839641.64429]: Calling loss_update with:
args: 0.159160315990448| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 3.888888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [245 - 1705839643.1912398]: 
Returned: 0.4476458430290222| 

python LearningRateFunctionModifier [245 - 1705839646.0685072]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.888888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [245 - 1705839646.0687141]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [245 - 1705839646.068793]: 0.00010512820512820513
python LearningRateFunctionModifier/ParamGroup1 [245 - 1705839646.0690184]: 0.00010512820512820513
python DistillationModifier/task_loss [245 - 1705839646.069317]: tensor(0.1592, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [245 - 1705839646.070414]: tensor(0.4476, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [245 - 1705839646.0710342]: tensor(0.4476, grad_fn=<AddBackward0>)
python ConstantPruningModifier [245 - 1705839646.07165]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 3.888888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [245 - 1705839646.2947595]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [245 - 1705839646.2955317]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [245 - 1705839646.296527]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [245 - 1705839646.2972574]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [245 - 1705839646.2978873]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [245 - 1705839646.29962]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [245 - 1705839646.3014944]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [245 - 1705839646.3022792]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [245 - 1705839646.3029735]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [245 - 1705839646.3036401]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [245 - 1705839646.3042657]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [245 - 1705839646.3059773]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [245 - 1705839646.30775]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [245 - 1705839646.3085148]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [245 - 1705839646.3092272]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [245 - 1705839646.3098912]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [245 - 1705839646.3104773]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [245 - 1705839646.3122327]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [245 - 1705839646.3140607]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [245 - 1705839646.3148603]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [245 - 1705839646.315576]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [245 - 1705839646.316249]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [245 - 1705839646.316871]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [245 - 1705839646.3186042]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [245 - 1705839646.3204114]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [245 - 1705839646.3212585]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [245 - 1705839646.3219752]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [245 - 1705839646.3226538]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [245 - 1705839646.3232915]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [245 - 1705839646.325001]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [245 - 1705839646.3268256]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [245 - 1705839646.327629]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [245 - 1705839646.3283143]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [245 - 1705839646.329003]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [245 - 1705839646.3296685]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [245 - 1705839646.3313649]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [245 - 1705839646.3330374]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [245 - 1705839646.3338645]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [245 - 1705839646.3346226]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [245 - 1705839646.3354106]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [245 - 1705839646.336095]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [245 - 1705839646.337833]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [245 - 1705839646.3396294]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [245 - 1705839646.340428]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [245 - 1705839646.3411539]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [245 - 1705839646.3417726]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [245 - 1705839646.342348]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [245 - 1705839646.3440251]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [245 - 1705839646.3458426]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [245 - 1705839646.3466601]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [245 - 1705839646.3473663]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [245 - 1705839646.3480356]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [245 - 1705839646.348646]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [245 - 1705839646.3503964]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [245 - 1705839646.3521924]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [245 - 1705839646.3530138]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [245 - 1705839646.3537338]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [245 - 1705839646.3544042]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [245 - 1705839646.3550303]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [245 - 1705839646.3567536]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [245 - 1705839646.3585777]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [245 - 1705839646.3593807]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [245 - 1705839646.3600867]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [245 - 1705839646.360791]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [245 - 1705839646.3614004]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [245 - 1705839646.3630993]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [245 - 1705839646.3649204]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [245 - 1705839646.3657188]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [245 - 1705839646.366411]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [245 - 1705839646.3670902]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [245 - 1705839646.3677201]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [245 - 1705839646.3694289]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [245 - 1705839646.371229]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [245 - 1705839646.372024]: 0.0
python ParamPruning/classifier.weight [245 - 1705839646.3724766]: 0.0
python DistillationModifier [252 - 1705839684.2094448]: Calling loss_update with:
args: 0.6875935196876526| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839685.6276107]: 
Returned: 1.0761457681655884| 

python DistillationModifier [252 - 1705839687.062387]: Calling loss_update with:
args: 0.9506950974464417| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839688.4711936]: 
Returned: 1.6075539588928223| 

python DistillationModifier [252 - 1705839689.9066541]: Calling loss_update with:
args: 1.0288406610488892| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839691.309119]: 
Returned: 1.111806869506836| 

python DistillationModifier [252 - 1705839692.7531683]: Calling loss_update with:
args: 0.5852225422859192| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839694.1585493]: 
Returned: 0.7782372832298279| 

python DistillationModifier [252 - 1705839695.5906355]: Calling loss_update with:
args: 1.1560629606246948| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839697.0055246]: 
Returned: 1.6004046201705933| 

python DistillationModifier [252 - 1705839698.4434247]: Calling loss_update with:
args: 0.5192009210586548| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839699.8554695]: 
Returned: 0.9138059616088867| 

python DistillationModifier [252 - 1705839701.2845964]: Calling loss_update with:
args: 0.27412286400794983| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839702.6917658]: 
Returned: 0.824338972568512| 

python DistillationModifier [252 - 1705839704.1307902]: Calling loss_update with:
args: 1.0251470804214478| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839705.5335736]: 
Returned: 1.5037450790405273| 

python DistillationModifier [252 - 1705839706.9759352]: Calling loss_update with:
args: 0.5008563995361328| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839708.427789]: 
Returned: 0.40816235542297363| 

python DistillationModifier [252 - 1705839709.8675323]: Calling loss_update with:
args: 0.61093670129776| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839711.2680516]: 
Returned: 0.979061484336853| 

python DistillationModifier [252 - 1705839712.710323]: Calling loss_update with:
args: 0.44748154282569885| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839714.121279]: 
Returned: 0.9120132327079773| 

python DistillationModifier [252 - 1705839715.5618706]: Calling loss_update with:
args: 0.9918005466461182| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839716.9744167]: 
Returned: 1.1284209489822388| 

python DistillationModifier [252 - 1705839718.4136438]: Calling loss_update with:
args: 0.9528883099555969| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839719.8253598]: 
Returned: 1.5440832376480103| 

python DistillationModifier [252 - 1705839721.2661493]: Calling loss_update with:
args: 0.5170694589614868| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839723.2167156]: 
Returned: 0.44334572553634644| 

python DistillationModifier [252 - 1705839725.2514899]: Calling loss_update with:
args: 0.8872323632240295| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839727.178645]: 
Returned: 1.2928104400634766| 

python DistillationModifier [252 - 1705839728.6154537]: Calling loss_update with:
args: 0.6023285388946533| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839730.034929]: 
Returned: 1.0901076793670654| 

python DistillationModifier [252 - 1705839731.4806366]: Calling loss_update with:
args: 0.46653592586517334| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839732.8989437]: 
Returned: 0.6748635768890381| 

python DistillationModifier [252 - 1705839734.3427653]: Calling loss_update with:
args: 0.8413054943084717| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839735.7524428]: 
Returned: 1.510483980178833| 

python DistillationModifier [252 - 1705839737.2072523]: Calling loss_update with:
args: 0.699347198009491| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839738.6586263]: 
Returned: 0.7727896571159363| 

python DistillationModifier [252 - 1705839740.1116133]: Calling loss_update with:
args: 0.4109066128730774| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839741.516713]: 
Returned: 0.9429422616958618| 

python DistillationModifier [252 - 1705839742.9765706]: Calling loss_update with:
args: 0.35923418402671814| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839744.3886344]: 
Returned: 0.9244604110717773| 

python DistillationModifier [252 - 1705839745.8471868]: Calling loss_update with:
args: 0.6916579008102417| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839747.2599092]: 
Returned: 1.1151646375656128| 

python DistillationModifier [252 - 1705839748.6953838]: Calling loss_update with:
args: 0.5329089164733887| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839750.1067626]: 
Returned: 1.0079041719436646| 

python DistillationModifier [252 - 1705839751.5429444]: Calling loss_update with:
args: 0.5373638868331909| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839752.951386]: 
Returned: 0.7595047950744629| 

python DistillationModifier [252 - 1705839754.3862653]: Calling loss_update with:
args: 0.7926835417747498| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839755.7970994]: 
Returned: 0.7584961652755737| 

python DistillationModifier [252 - 1705839757.229544]: Calling loss_update with:
args: 0.30849355459213257| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839758.6317952]: 
Returned: 0.5576761960983276| 

python DistillationModifier [252 - 1705839760.075273]: Calling loss_update with:
args: 0.3943497836589813| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839761.4790003]: 
Returned: 0.5810875296592712| 

python DistillationModifier [252 - 1705839762.9478002]: Calling loss_update with:
args: 0.6161611676216125| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839764.3568444]: 
Returned: 0.9916907548904419| 

python DistillationModifier [252 - 1705839765.8024082]: Calling loss_update with:
args: 0.5490069389343262| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839767.210477]: 
Returned: 0.7515007257461548| 

python DistillationModifier [252 - 1705839768.6916492]: Calling loss_update with:
args: 0.506806492805481| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839770.111238]: 
Returned: 0.6715704202651978| 

python DistillationModifier [252 - 1705839771.5493906]: Calling loss_update with:
args: 0.6094971299171448| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839772.962729]: 
Returned: 1.1506818532943726| 

python DistillationModifier [252 - 1705839774.3958266]: Calling loss_update with:
args: 0.8800908923149109| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839775.8037324]: 
Returned: 1.071736216545105| 

python DistillationModifier [252 - 1705839777.2379718]: Calling loss_update with:
args: 0.7426497936248779| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839778.645217]: 
Returned: 1.2413082122802734| 

python DistillationModifier [252 - 1705839780.0825634]: Calling loss_update with:
args: 1.2404223680496216| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839781.6697292]: 
Returned: 1.9332692623138428| 

python DistillationModifier [252 - 1705839783.725981]: Calling loss_update with:
args: 1.3031517267227173| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839785.4390907]: 
Returned: 1.655360460281372| 

python DistillationModifier [252 - 1705839786.8882217]: Calling loss_update with:
args: 0.9765022993087769| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839788.2976122]: 
Returned: 1.7941222190856934| 

python DistillationModifier [252 - 1705839789.7389557]: Calling loss_update with:
args: 0.8010287284851074| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839791.1457295]: 
Returned: 0.8667659759521484| 

python DistillationModifier [252 - 1705839792.5777922]: Calling loss_update with:
args: 0.44947224855422974| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839794.3044195]: 
Returned: 0.7580659985542297| 

python DistillationModifier [252 - 1705839795.980102]: Calling loss_update with:
args: 0.7157489657402039| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839797.3874745]: 
Returned: 1.4025975465774536| 

python DistillationModifier [252 - 1705839798.8326087]: Calling loss_update with:
args: 0.8143357038497925| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839800.2662911]: 
Returned: 1.5474631786346436| 

python DistillationModifier [252 - 1705839801.7037513]: Calling loss_update with:
args: 0.4512408971786499| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839803.10948]: 
Returned: 0.9651938080787659| 

python DistillationModifier [252 - 1705839804.5449696]: Calling loss_update with:
args: 0.5466935634613037| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839805.9521348]: 
Returned: 1.0040873289108276| 

python DistillationModifier [252 - 1705839807.3932204]: Calling loss_update with:
args: 0.530311107635498| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839808.7959783]: 
Returned: 0.5775565505027771| 

python DistillationModifier [252 - 1705839810.2349417]: Calling loss_update with:
args: 0.5727517604827881| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839811.646261]: 
Returned: 1.2798562049865723| 

python DistillationModifier [252 - 1705839813.088075]: Calling loss_update with:
args: 0.5822941660881042| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839814.499077]: 
Returned: 1.2500476837158203| 

python DistillationModifier [252 - 1705839815.940135]: Calling loss_update with:
args: 0.8925883173942566| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839817.3438892]: 
Returned: 1.6341356039047241| 

python DistillationModifier [252 - 1705839818.8045545]: Calling loss_update with:
args: 0.4586077034473419| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839820.2134175]: 
Returned: 0.8735837936401367| 

python DistillationModifier [252 - 1705839821.6497416]: Calling loss_update with:
args: 0.3632226288318634| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839823.0569808]: 
Returned: 0.7700752019882202| 

python DistillationModifier [252 - 1705839824.5008605]: Calling loss_update with:
args: 1.1870920658111572| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839825.9122894]: 
Returned: 2.162252426147461| 

python DistillationModifier [252 - 1705839827.3548625]: Calling loss_update with:
args: 0.8767205476760864| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839828.7787118]: 
Returned: 1.5935726165771484| 

python DistillationModifier [252 - 1705839830.2640135]: Calling loss_update with:
args: 0.25104162096977234| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839831.6781838]: 
Returned: 0.47557154297828674| 

python DistillationModifier [252 - 1705839833.1180499]: Calling loss_update with:
args: 1.725542664527893| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839834.5268424]: 
Returned: 1.6279652118682861| 

python DistillationModifier [252 - 1705839835.9667008]: Calling loss_update with:
args: 0.5668554902076721| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839837.379972]: 
Returned: 0.9155439138412476| 

python DistillationModifier [252 - 1705839838.8250933]: Calling loss_update with:
args: 0.4893714189529419| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839840.2359061]: 
Returned: 0.7561672925949097| 

python DistillationModifier [252 - 1705839841.6763642]: Calling loss_update with:
args: 0.6978479027748108| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839843.6822152]: 
Returned: 1.444532036781311| 

python DistillationModifier [252 - 1705839845.5982559]: Calling loss_update with:
args: 0.6071599721908569| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839847.0019352]: 
Returned: 0.9815061688423157| 

python DistillationModifier [252 - 1705839848.4459403]: Calling loss_update with:
args: 0.813666582107544| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839849.8480184]: 
Returned: 0.7030590772628784| 

python DistillationModifier [252 - 1705839851.2804816]: Calling loss_update with:
args: 0.9874716997146606| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839852.6865082]: 
Returned: 1.3777966499328613| 

python DistillationModifier [252 - 1705839854.127992]: Calling loss_update with:
args: 0.7105275988578796| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839855.5320065]: 
Returned: 1.3608108758926392| 

python DistillationModifier [252 - 1705839856.9718115]: Calling loss_update with:
args: 0.5734001398086548| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839858.3807168]: 
Returned: 1.0323576927185059| 

python DistillationModifier [252 - 1705839859.970078]: Calling loss_update with:
args: 0.6554957628250122| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839861.8130548]: 
Returned: 1.2712403535842896| 

python DistillationModifier [252 - 1705839863.247967]: Calling loss_update with:
args: 0.4712322950363159| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839864.655173]: 
Returned: 0.855279803276062| 

python DistillationModifier [252 - 1705839865.4060364]: Calling loss_update with:
args: 0.6424177289009094| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839866.1468313]: 
Returned: 1.2112770080566406| 

python DistillationModifier [252 - 1705839868.1761973]: Calling loss_update with:
args: 0.12674789130687714| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [252 - 1705839869.5877233]: 
Returned: 0.17855115234851837| 

python LearningRateFunctionModifier [252 - 1705839872.431995]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [252 - 1705839872.432164]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [252 - 1705839872.4322224]: 0.00010384615384615383
python LearningRateFunctionModifier/ParamGroup1 [252 - 1705839872.432463]: 0.00010384615384615383
python DistillationModifier/task_loss [252 - 1705839872.4327734]: tensor(0.1267, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [252 - 1705839872.433856]: tensor(0.1786, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [252 - 1705839872.4344532]: tensor(0.1786, grad_fn=<AddBackward0>)
python ConstantPruningModifier [252 - 1705839872.4350603]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.0| steps_per_epoch: 63| 
python ConstantPruningModifier [252 - 1705839872.6544833]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [252 - 1705839872.655221]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [252 - 1705839872.6560552]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [252 - 1705839872.6567166]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [252 - 1705839872.6572804]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [252 - 1705839872.6589952]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [252 - 1705839872.6607413]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [252 - 1705839872.6614363]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [252 - 1705839872.662034]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [252 - 1705839872.6625671]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [252 - 1705839872.663086]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [252 - 1705839872.6647968]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [252 - 1705839872.6663182]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [252 - 1705839872.6670318]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [252 - 1705839872.6676521]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [252 - 1705839872.6682851]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [252 - 1705839872.6688504]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [252 - 1705839872.670433]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [252 - 1705839872.6722045]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [252 - 1705839872.6729293]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [252 - 1705839872.6735375]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [252 - 1705839872.674067]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [252 - 1705839872.6745892]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [252 - 1705839872.676249]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [252 - 1705839872.677842]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [252 - 1705839872.678606]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [252 - 1705839872.6792233]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [252 - 1705839872.6798635]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [252 - 1705839872.6804152]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [252 - 1705839872.6822271]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [252 - 1705839872.6840265]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [252 - 1705839872.6848383]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [252 - 1705839872.6854393]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [252 - 1705839872.6859825]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [252 - 1705839872.6866148]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [252 - 1705839872.6883063]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [252 - 1705839872.6900196]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [252 - 1705839872.6907573]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [252 - 1705839872.691358]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [252 - 1705839872.691943]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [252 - 1705839872.6924703]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [252 - 1705839872.6941473]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [252 - 1705839872.6956851]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [252 - 1705839872.6963904]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [252 - 1705839872.6970396]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [252 - 1705839872.697589]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [252 - 1705839872.6981018]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [252 - 1705839872.6997395]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [252 - 1705839872.7014978]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [252 - 1705839872.7021704]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [252 - 1705839872.7027516]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [252 - 1705839872.7033777]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [252 - 1705839872.7039418]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [252 - 1705839872.7056816]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [252 - 1705839872.7074435]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [252 - 1705839872.7082243]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [252 - 1705839872.7088912]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [252 - 1705839872.7094898]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [252 - 1705839872.7100255]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [252 - 1705839872.7117138]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [252 - 1705839872.7135162]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [252 - 1705839872.7142675]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [252 - 1705839872.7148907]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [252 - 1705839872.715571]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [252 - 1705839872.7162466]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [252 - 1705839872.7179847]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [252 - 1705839872.7197149]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [252 - 1705839872.7204905]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [252 - 1705839872.7211106]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [252 - 1705839872.7216587]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [252 - 1705839872.7222762]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [252 - 1705839872.7239413]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [252 - 1705839872.7256591]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [252 - 1705839872.7263868]: 0.0
python ParamPruning/classifier.weight [252 - 1705839872.726792]: 0.0
python DistillationModifier [259 - 1705839913.3209696]: Calling loss_update with:
args: 0.439909964799881| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.111111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [259 - 1705839914.8334508]: 
Returned: 0.6553349494934082| 

python LearningRateFunctionModifier [259 - 1705839917.6791034]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.111111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [259 - 1705839917.679275]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [259 - 1705839917.6793356]: 0.00010256410256410256
python LearningRateFunctionModifier/ParamGroup1 [259 - 1705839917.6795838]: 0.00010256410256410256
python DistillationModifier/task_loss [259 - 1705839917.6798825]: tensor(0.4399, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [259 - 1705839917.6809707]: tensor(0.6553, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [259 - 1705839917.68159]: tensor(0.6553, grad_fn=<AddBackward0>)
python ConstantPruningModifier [259 - 1705839917.6821783]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.111111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [259 - 1705839917.903924]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [259 - 1705839917.9047093]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [259 - 1705839917.9055452]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [259 - 1705839917.9061701]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [259 - 1705839917.9067764]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [259 - 1705839917.9085066]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [259 - 1705839917.910224]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [259 - 1705839917.9109237]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [259 - 1705839917.9115825]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [259 - 1705839917.9121618]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [259 - 1705839917.9127293]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [259 - 1705839917.9144363]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [259 - 1705839917.9161499]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [259 - 1705839917.9168553]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [259 - 1705839917.9174793]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [259 - 1705839917.9180915]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [259 - 1705839917.9186473]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [259 - 1705839917.9202619]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [259 - 1705839917.9220746]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [259 - 1705839917.9228177]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [259 - 1705839917.9234648]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [259 - 1705839917.9240248]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [259 - 1705839917.9245732]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [259 - 1705839917.926236]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [259 - 1705839917.9279847]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [259 - 1705839917.9287288]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [259 - 1705839917.9293652]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [259 - 1705839917.9299853]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [259 - 1705839917.930603]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [259 - 1705839917.932329]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [259 - 1705839917.934056]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [259 - 1705839917.9347239]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [259 - 1705839917.935339]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [259 - 1705839917.9358757]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [259 - 1705839917.9364092]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [259 - 1705839917.938123]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [259 - 1705839917.9398375]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [259 - 1705839917.9405324]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [259 - 1705839917.94118]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [259 - 1705839917.9417915]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [259 - 1705839917.9424386]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [259 - 1705839917.9441555]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [259 - 1705839917.9458685]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [259 - 1705839917.946551]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [259 - 1705839917.9471622]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [259 - 1705839917.9477768]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [259 - 1705839917.9483225]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [259 - 1705839917.9500577]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [259 - 1705839917.9517844]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [259 - 1705839917.9524722]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [259 - 1705839917.9531288]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [259 - 1705839917.9537024]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [259 - 1705839917.9542065]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [259 - 1705839917.9559052]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [259 - 1705839917.957658]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [259 - 1705839917.9583979]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [259 - 1705839917.9589562]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [259 - 1705839917.9595652]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [259 - 1705839917.9600675]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [259 - 1705839917.9618232]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [259 - 1705839917.9635687]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [259 - 1705839917.964231]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [259 - 1705839917.9648557]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [259 - 1705839917.9654047]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [259 - 1705839917.965947]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [259 - 1705839917.967617]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [259 - 1705839917.969295]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [259 - 1705839917.9699504]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [259 - 1705839917.9705741]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [259 - 1705839917.9711185]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [259 - 1705839917.9716525]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [259 - 1705839917.9733977]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [259 - 1705839917.9751325]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [259 - 1705839917.9758158]: 0.0
python ParamPruning/classifier.weight [259 - 1705839917.9761992]: 0.0
python DistillationModifier [266 - 1705839957.9375763]: Calling loss_update with:
args: 0.2931838631629944| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.222222222222222| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [266 - 1705839959.3473928]: 
Returned: 0.5070616602897644| 

python LearningRateFunctionModifier [266 - 1705839962.2557414]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.222222222222222| steps_per_epoch: 63| 
python LearningRateFunctionModifier [266 - 1705839962.2558966]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [266 - 1705839962.2559552]: 0.00010128205128205126
python LearningRateFunctionModifier/ParamGroup1 [266 - 1705839962.2561831]: 0.00010128205128205126
python DistillationModifier/task_loss [266 - 1705839962.2564723]: tensor(0.2932, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [266 - 1705839962.2574532]: tensor(0.5071, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [266 - 1705839962.2580109]: tensor(0.5071, grad_fn=<AddBackward0>)
python ConstantPruningModifier [266 - 1705839962.258558]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.222222222222222| steps_per_epoch: 63| 
python ConstantPruningModifier [266 - 1705839962.5510674]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [266 - 1705839962.5519092]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [266 - 1705839962.5527618]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [266 - 1705839962.5535142]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [266 - 1705839962.5541093]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [266 - 1705839962.5559707]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [266 - 1705839962.5578907]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [266 - 1705839962.5585926]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [266 - 1705839962.5591564]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [266 - 1705839962.5597398]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [266 - 1705839962.560249]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [266 - 1705839962.5621183]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [266 - 1705839962.56403]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [266 - 1705839962.5647357]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [266 - 1705839962.5653012]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [266 - 1705839962.5658076]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [266 - 1705839962.5663264]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [266 - 1705839962.568091]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [266 - 1705839962.5697334]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [266 - 1705839962.5704772]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [266 - 1705839962.5712023]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [266 - 1705839962.571936]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [266 - 1705839962.5726292]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [266 - 1705839962.5745883]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [266 - 1705839962.576365]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [266 - 1705839962.577172]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [266 - 1705839962.5778317]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [266 - 1705839962.5785053]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [266 - 1705839962.5790765]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [266 - 1705839962.5811157]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [266 - 1705839962.5830894]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [266 - 1705839962.5837996]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [266 - 1705839962.584387]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [266 - 1705839962.5849903]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [266 - 1705839962.585546]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [266 - 1705839962.5874758]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [266 - 1705839962.5895233]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [266 - 1705839962.5902755]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [266 - 1705839962.5908873]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [266 - 1705839962.5915577]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [266 - 1705839962.5922313]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [266 - 1705839962.594187]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [266 - 1705839962.5961738]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [266 - 1705839962.5969422]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [266 - 1705839962.5976245]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [266 - 1705839962.59822]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [266 - 1705839962.598826]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [266 - 1705839962.6008105]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [266 - 1705839962.6028166]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [266 - 1705839962.6035712]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [266 - 1705839962.6042676]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [266 - 1705839962.6049552]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [266 - 1705839962.6056008]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [266 - 1705839962.6075416]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [266 - 1705839962.6093462]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [266 - 1705839962.6101286]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [266 - 1705839962.6108418]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [266 - 1705839962.6115165]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [266 - 1705839962.6121645]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [266 - 1705839962.6140742]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [266 - 1705839962.6160803]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [266 - 1705839962.61688]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [266 - 1705839962.6175437]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [266 - 1705839962.6182117]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [266 - 1705839962.6188307]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [266 - 1705839962.620791]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [266 - 1705839962.622788]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [266 - 1705839962.623539]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [266 - 1705839962.6241353]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [266 - 1705839962.6247044]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [266 - 1705839962.6253726]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [266 - 1705839962.6272414]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [266 - 1705839962.6292746]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [266 - 1705839962.6300516]: 0.0
python ParamPruning/classifier.weight [266 - 1705839962.6304867]: 0.0
python DistillationModifier [273 - 1705840003.490452]: Calling loss_update with:
args: 0.23838025331497192| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.333333333333333| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [273 - 1705840004.9929845]: 
Returned: 0.24740555882453918| 

python LearningRateFunctionModifier [273 - 1705840007.8843281]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.333333333333333| steps_per_epoch: 63| 
python LearningRateFunctionModifier [273 - 1705840007.8844917]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [273 - 1705840007.8845506]: 9.999999999999999e-05
python LearningRateFunctionModifier/ParamGroup1 [273 - 1705840007.884805]: 9.999999999999999e-05
python DistillationModifier/task_loss [273 - 1705840007.885089]: tensor(0.2384, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [273 - 1705840007.8860583]: tensor(0.2474, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [273 - 1705840007.8865747]: tensor(0.2474, grad_fn=<AddBackward0>)
python ConstantPruningModifier [273 - 1705840007.8871083]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.333333333333333| steps_per_epoch: 63| 
python ConstantPruningModifier [273 - 1705840008.108781]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [273 - 1705840008.1095476]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [273 - 1705840008.1103566]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [273 - 1705840008.1109464]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [273 - 1705840008.1114688]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [273 - 1705840008.113242]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [273 - 1705840008.11499]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [273 - 1705840008.1158144]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [273 - 1705840008.11647]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [273 - 1705840008.1170657]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [273 - 1705840008.11759]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [273 - 1705840008.1192994]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [273 - 1705840008.1210845]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [273 - 1705840008.121943]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [273 - 1705840008.122606]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [273 - 1705840008.123164]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [273 - 1705840008.1236925]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [273 - 1705840008.1254456]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [273 - 1705840008.1272318]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [273 - 1705840008.12807]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [273 - 1705840008.128767]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [273 - 1705840008.129361]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [273 - 1705840008.1298923]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [273 - 1705840008.1316652]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [273 - 1705840008.1335018]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [273 - 1705840008.134364]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [273 - 1705840008.1350412]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [273 - 1705840008.1355884]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [273 - 1705840008.1361065]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [273 - 1705840008.1378388]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [273 - 1705840008.139635]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [273 - 1705840008.1404877]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [273 - 1705840008.1411934]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [273 - 1705840008.1417627]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [273 - 1705840008.1422741]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [273 - 1705840008.143996]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [273 - 1705840008.145822]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [273 - 1705840008.1466644]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [273 - 1705840008.1473126]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [273 - 1705840008.147899]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [273 - 1705840008.148419]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [273 - 1705840008.1501586]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [273 - 1705840008.1519306]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [273 - 1705840008.1527996]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [273 - 1705840008.1534512]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [273 - 1705840008.1539922]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [273 - 1705840008.1544993]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [273 - 1705840008.1562188]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [273 - 1705840008.1579635]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [273 - 1705840008.158806]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [273 - 1705840008.1594608]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [273 - 1705840008.1600232]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [273 - 1705840008.1605313]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [273 - 1705840008.1622655]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [273 - 1705840008.1639948]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [273 - 1705840008.1648474]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [273 - 1705840008.1655161]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [273 - 1705840008.1660862]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [273 - 1705840008.1666026]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [273 - 1705840008.168325]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [273 - 1705840008.1701124]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [273 - 1705840008.170983]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [273 - 1705840008.1716576]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [273 - 1705840008.172259]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [273 - 1705840008.1728086]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [273 - 1705840008.174521]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [273 - 1705840008.1762972]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [273 - 1705840008.177176]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [273 - 1705840008.1778505]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [273 - 1705840008.1784012]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [273 - 1705840008.1789203]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [273 - 1705840008.1806526]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [273 - 1705840008.182416]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [273 - 1705840008.1832385]: 0.0
python ParamPruning/classifier.weight [273 - 1705840008.183709]: 0.0
python DistillationModifier [280 - 1705840049.3545513]: Calling loss_update with:
args: 0.2467721551656723| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.444444444444445| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [280 - 1705840050.9219062]: 
Returned: 0.36554571986198425| 

python LearningRateFunctionModifier [280 - 1705840053.8264136]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.444444444444445| steps_per_epoch: 63| 
python LearningRateFunctionModifier [280 - 1705840053.8265924]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [280 - 1705840053.8266635]: 9.871794871794871e-05
python LearningRateFunctionModifier/ParamGroup1 [280 - 1705840053.8268847]: 9.871794871794871e-05
python DistillationModifier/task_loss [280 - 1705840053.8271909]: tensor(0.2468, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [280 - 1705840053.8281682]: tensor(0.3655, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [280 - 1705840053.8288317]: tensor(0.3655, grad_fn=<AddBackward0>)
python ConstantPruningModifier [280 - 1705840053.8294675]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.444444444444445| steps_per_epoch: 63| 
python ConstantPruningModifier [280 - 1705840054.0536816]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [280 - 1705840054.054459]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [280 - 1705840054.0553277]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [280 - 1705840054.0559413]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [280 - 1705840054.0565531]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [280 - 1705840054.058335]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [280 - 1705840054.0602763]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [280 - 1705840054.0612192]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [280 - 1705840054.0619507]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [280 - 1705840054.0625796]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [280 - 1705840054.0632584]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [280 - 1705840054.0650682]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [280 - 1705840054.067029]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [280 - 1705840054.0679526]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [280 - 1705840054.0687795]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [280 - 1705840054.0695276]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [280 - 1705840054.0702395]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [280 - 1705840054.072057]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [280 - 1705840054.0740485]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [280 - 1705840054.0749924]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [280 - 1705840054.0757277]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [280 - 1705840054.0764356]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [280 - 1705840054.0771708]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [280 - 1705840054.0789788]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [280 - 1705840054.0809748]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [280 - 1705840054.0819192]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [280 - 1705840054.082732]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [280 - 1705840054.0834887]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [280 - 1705840054.084194]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [280 - 1705840054.0860014]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [280 - 1705840054.08795]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [280 - 1705840054.0888944]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [280 - 1705840054.0896227]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [280 - 1705840054.0903323]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [280 - 1705840054.0909922]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [280 - 1705840054.0928352]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [280 - 1705840054.0947888]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [280 - 1705840054.0957363]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [280 - 1705840054.0964546]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [280 - 1705840054.0972252]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [280 - 1705840054.0979395]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [280 - 1705840054.0997555]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [280 - 1705840054.101731]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [280 - 1705840054.1026723]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [280 - 1705840054.103388]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [280 - 1705840054.1041145]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [280 - 1705840054.1048462]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [280 - 1705840054.1066673]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [280 - 1705840054.1085815]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [280 - 1705840054.1095102]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [280 - 1705840054.110299]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [280 - 1705840054.1110463]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [280 - 1705840054.1117935]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [280 - 1705840054.1136112]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [280 - 1705840054.1155539]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [280 - 1705840054.1164935]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [280 - 1705840054.1173432]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [280 - 1705840054.1181505]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [280 - 1705840054.1188312]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [280 - 1705840054.120687]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [280 - 1705840054.1226218]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [280 - 1705840054.1235418]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [280 - 1705840054.1243591]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [280 - 1705840054.1251943]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [280 - 1705840054.1259513]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [280 - 1705840054.1280024]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [280 - 1705840054.1299477]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [280 - 1705840054.130897]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [280 - 1705840054.1317227]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [280 - 1705840054.1324697]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [280 - 1705840054.1331992]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [280 - 1705840054.134989]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [280 - 1705840054.1369174]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [280 - 1705840054.1378534]: 0.0
python ParamPruning/classifier.weight [280 - 1705840054.1383533]: 0.0
python DistillationModifier [287 - 1705840095.2711298]: Calling loss_update with:
args: 0.1320725530385971| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.555555555555555| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [287 - 1705840096.7067673]: 
Returned: 0.23421214520931244| 

python LearningRateFunctionModifier [287 - 1705840099.6041389]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.555555555555555| steps_per_epoch: 63| 
python LearningRateFunctionModifier [287 - 1705840099.6043062]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [287 - 1705840099.6043653]: 9.743589743589742e-05
python LearningRateFunctionModifier/ParamGroup1 [287 - 1705840099.6045978]: 9.743589743589742e-05
python DistillationModifier/task_loss [287 - 1705840099.604915]: tensor(0.1321, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [287 - 1705840099.6058767]: tensor(0.2342, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [287 - 1705840099.606392]: tensor(0.2342, grad_fn=<AddBackward0>)
python ConstantPruningModifier [287 - 1705840099.606922]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.555555555555555| steps_per_epoch: 63| 
python ConstantPruningModifier [287 - 1705840099.8243032]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [287 - 1705840099.8250775]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [287 - 1705840099.8260438]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [287 - 1705840099.8267076]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [287 - 1705840099.8273325]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [287 - 1705840099.8290517]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [287 - 1705840099.8308756]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [287 - 1705840099.8317244]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [287 - 1705840099.8324225]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [287 - 1705840099.833097]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [287 - 1705840099.8336883]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [287 - 1705840099.835381]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [287 - 1705840099.8372042]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [287 - 1705840099.8380208]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [287 - 1705840099.8387349]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [287 - 1705840099.8393734]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [287 - 1705840099.8399582]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [287 - 1705840099.8416777]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [287 - 1705840099.8434856]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [287 - 1705840099.8442976]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [287 - 1705840099.8450305]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [287 - 1705840099.8457005]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [287 - 1705840099.8463254]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [287 - 1705840099.848007]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [287 - 1705840099.849678]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [287 - 1705840099.850482]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [287 - 1705840099.8511207]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [287 - 1705840099.8517704]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [287 - 1705840099.8523803]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [287 - 1705840099.8540566]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [287 - 1705840099.855856]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [287 - 1705840099.8566406]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [287 - 1705840099.8573434]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [287 - 1705840099.8579984]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [287 - 1705840099.8586094]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [287 - 1705840099.8603117]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [287 - 1705840099.862124]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [287 - 1705840099.8629117]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [287 - 1705840099.86352]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [287 - 1705840099.8641593]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [287 - 1705840099.8647199]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [287 - 1705840099.8663929]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [287 - 1705840099.8681982]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [287 - 1705840099.8690085]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [287 - 1705840099.8697054]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [287 - 1705840099.8702948]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [287 - 1705840099.8708076]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [287 - 1705840099.8724895]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [287 - 1705840099.8742907]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [287 - 1705840099.8750656]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [287 - 1705840099.8757443]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [287 - 1705840099.8763742]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [287 - 1705840099.8769503]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [287 - 1705840099.8786337]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [287 - 1705840099.880443]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [287 - 1705840099.8812509]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [287 - 1705840099.8819242]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [287 - 1705840099.8825045]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [287 - 1705840099.883049]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [287 - 1705840099.884765]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [287 - 1705840099.8865824]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [287 - 1705840099.8873868]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [287 - 1705840099.8880641]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [287 - 1705840099.888651]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [287 - 1705840099.8892272]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [287 - 1705840099.8908932]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [287 - 1705840099.8927422]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [287 - 1705840099.893547]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [287 - 1705840099.8942213]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [287 - 1705840099.8948073]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [287 - 1705840099.8953476]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [287 - 1705840099.8970575]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [287 - 1705840099.898869]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [287 - 1705840099.8996613]: 0.0
python ParamPruning/classifier.weight [287 - 1705840099.9000905]: 0.0
python DistillationModifier [294 - 1705840139.892222]: Calling loss_update with:
args: 0.055277686566114426| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.666666666666667| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [294 - 1705840141.34798]: 
Returned: 0.29299673438072205| 

python LearningRateFunctionModifier [294 - 1705840145.3192592]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.666666666666667| steps_per_epoch: 63| 
python LearningRateFunctionModifier [294 - 1705840145.3452587]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [294 - 1705840145.345395]: 9.615384615384615e-05
python LearningRateFunctionModifier/ParamGroup1 [294 - 1705840145.3456204]: 9.615384615384615e-05
python DistillationModifier/task_loss [294 - 1705840145.3458915]: tensor(0.0553, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [294 - 1705840145.3468826]: tensor(0.2930, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [294 - 1705840145.3474834]: tensor(0.2930, grad_fn=<AddBackward0>)
python ConstantPruningModifier [294 - 1705840145.3480785]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.666666666666667| steps_per_epoch: 63| 
python ConstantPruningModifier [294 - 1705840145.7098298]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [294 - 1705840145.710826]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [294 - 1705840145.7118835]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [294 - 1705840145.712622]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [294 - 1705840145.7133033]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [294 - 1705840145.7158623]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [294 - 1705840145.7182806]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [294 - 1705840145.719289]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [294 - 1705840145.7200725]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [294 - 1705840145.7207577]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [294 - 1705840145.721398]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [294 - 1705840145.7238142]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [294 - 1705840145.726174]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [294 - 1705840145.7271664]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [294 - 1705840145.7279403]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [294 - 1705840145.7286124]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [294 - 1705840145.729272]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [294 - 1705840145.7316358]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [294 - 1705840145.7339897]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [294 - 1705840145.734973]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [294 - 1705840145.7357533]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [294 - 1705840145.7364347]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [294 - 1705840145.7371159]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [294 - 1705840145.7396135]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [294 - 1705840145.7421281]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [294 - 1705840145.7431479]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [294 - 1705840145.743917]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [294 - 1705840145.7446027]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [294 - 1705840145.745271]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [294 - 1705840145.7477248]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [294 - 1705840145.7500784]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [294 - 1705840145.7510512]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [294 - 1705840145.7518482]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [294 - 1705840145.7525332]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [294 - 1705840145.7532036]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [294 - 1705840145.7556462]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [294 - 1705840145.758021]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [294 - 1705840145.7590346]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [294 - 1705840145.7598498]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [294 - 1705840145.7605393]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [294 - 1705840145.7612114]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [294 - 1705840145.7636998]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [294 - 1705840145.7661932]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [294 - 1705840145.7672436]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [294 - 1705840145.7680104]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [294 - 1705840145.7687092]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [294 - 1705840145.7693648]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [294 - 1705840145.7718973]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [294 - 1705840145.774358]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [294 - 1705840145.775376]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [294 - 1705840145.7761788]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [294 - 1705840145.776868]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [294 - 1705840145.7775152]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [294 - 1705840145.7800062]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [294 - 1705840145.7823868]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [294 - 1705840145.7833993]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [294 - 1705840145.784213]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [294 - 1705840145.784927]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [294 - 1705840145.7856083]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [294 - 1705840145.788061]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [294 - 1705840145.790511]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [294 - 1705840145.7915633]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [294 - 1705840145.7924159]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [294 - 1705840145.7931497]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [294 - 1705840145.79384]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [294 - 1705840145.7962978]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [294 - 1705840145.7986698]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [294 - 1705840145.7996628]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [294 - 1705840145.8004792]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [294 - 1705840145.8012555]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [294 - 1705840145.8019547]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [294 - 1705840145.8044803]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [294 - 1705840145.806855]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [294 - 1705840145.8078706]: 0.0
python ParamPruning/classifier.weight [294 - 1705840145.808375]: 0.0
python DistillationModifier [301 - 1705840185.5707543]: Calling loss_update with:
args: 0.025279967114329338| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.777777777777778| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [301 - 1705840187.022547]: 
Returned: 0.10745909810066223| 

python LearningRateFunctionModifier [301 - 1705840189.8841043]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.777777777777778| steps_per_epoch: 63| 
python LearningRateFunctionModifier [301 - 1705840189.8842692]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [301 - 1705840189.8843296]: 9.487179487179487e-05
python LearningRateFunctionModifier/ParamGroup1 [301 - 1705840189.8845723]: 9.487179487179487e-05
python DistillationModifier/task_loss [301 - 1705840189.8848786]: tensor(0.0253, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [301 - 1705840189.8859298]: tensor(0.1075, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [301 - 1705840189.8865113]: tensor(0.1075, grad_fn=<AddBackward0>)
python ConstantPruningModifier [301 - 1705840189.887093]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.777777777777778| steps_per_epoch: 63| 
python ConstantPruningModifier [301 - 1705840190.1049178]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [301 - 1705840190.1056921]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [301 - 1705840190.1064937]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [301 - 1705840190.1072338]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [301 - 1705840190.1078975]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [301 - 1705840190.109615]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [301 - 1705840190.1114132]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [301 - 1705840190.1121771]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [301 - 1705840190.1129255]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [301 - 1705840190.1135626]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [301 - 1705840190.1142128]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [301 - 1705840190.1158726]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [301 - 1705840190.117665]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [301 - 1705840190.1183782]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [301 - 1705840190.1190364]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [301 - 1705840190.119631]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [301 - 1705840190.1202002]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [301 - 1705840190.1218696]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [301 - 1705840190.1236484]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [301 - 1705840190.124356]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [301 - 1705840190.1250248]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [301 - 1705840190.1256838]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [301 - 1705840190.1263368]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [301 - 1705840190.1280046]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [301 - 1705840190.1298048]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [301 - 1705840190.1305332]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [301 - 1705840190.1312232]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [301 - 1705840190.131829]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [301 - 1705840190.1324022]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [301 - 1705840190.13414]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [301 - 1705840190.135919]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [301 - 1705840190.136662]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [301 - 1705840190.1373806]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [301 - 1705840190.138038]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [301 - 1705840190.1386552]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [301 - 1705840190.1403544]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [301 - 1705840190.1421707]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [301 - 1705840190.142915]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [301 - 1705840190.1436028]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [301 - 1705840190.1443205]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [301 - 1705840190.1449618]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [301 - 1705840190.1466384]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [301 - 1705840190.1484087]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [301 - 1705840190.1491187]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [301 - 1705840190.1497543]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [301 - 1705840190.150332]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [301 - 1705840190.1508892]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [301 - 1705840190.1525471]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [301 - 1705840190.1543188]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [301 - 1705840190.1549852]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [301 - 1705840190.1556246]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [301 - 1705840190.1561975]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [301 - 1705840190.1567602]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [301 - 1705840190.1584213]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [301 - 1705840190.1601713]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [301 - 1705840190.160859]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [301 - 1705840190.1614847]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [301 - 1705840190.162112]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [301 - 1705840190.1626978]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [301 - 1705840190.1643498]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [301 - 1705840190.1661005]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [301 - 1705840190.166814]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [301 - 1705840190.167451]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [301 - 1705840190.1680791]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [301 - 1705840190.168711]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [301 - 1705840190.170369]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [301 - 1705840190.1721108]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [301 - 1705840190.1728072]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [301 - 1705840190.1734407]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [301 - 1705840190.1740556]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [301 - 1705840190.1746325]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [301 - 1705840190.1762958]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [301 - 1705840190.1778386]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [301 - 1705840190.1785707]: 0.0
python ParamPruning/classifier.weight [301 - 1705840190.1790066]: 0.0
python DistillationModifier [308 - 1705840232.2703865]: Calling loss_update with:
args: 0.2313537895679474| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 4.888888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [308 - 1705840233.809294]: 
Returned: 0.3517983555793762| 

python LearningRateFunctionModifier [308 - 1705840236.7074714]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.888888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [308 - 1705840236.7076426]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [308 - 1705840236.7077043]: 9.358974358974357e-05
python LearningRateFunctionModifier/ParamGroup1 [308 - 1705840236.7079353]: 9.358974358974357e-05
python DistillationModifier/task_loss [308 - 1705840236.708225]: tensor(0.2314, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [308 - 1705840236.7092435]: tensor(0.3518, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [308 - 1705840236.709791]: tensor(0.3518, grad_fn=<AddBackward0>)
python ConstantPruningModifier [308 - 1705840236.710347]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 4.888888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [308 - 1705840236.9322257]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [308 - 1705840236.9330585]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [308 - 1705840236.9340382]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [308 - 1705840236.9347825]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [308 - 1705840236.9354267]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [308 - 1705840236.937219]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [308 - 1705840236.939135]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [308 - 1705840236.940033]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [308 - 1705840236.9408717]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [308 - 1705840236.9415784]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [308 - 1705840236.9422572]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [308 - 1705840236.944025]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [308 - 1705840236.9459429]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [308 - 1705840236.9467993]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [308 - 1705840236.9475448]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [308 - 1705840236.948193]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [308 - 1705840236.9488537]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [308 - 1705840236.950619]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [308 - 1705840236.9525092]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [308 - 1705840236.9533575]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [308 - 1705840236.9540656]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [308 - 1705840236.9546916]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [308 - 1705840236.9552495]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [308 - 1705840236.9570158]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [308 - 1705840236.9588964]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [308 - 1705840236.9597533]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [308 - 1705840236.9605103]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [308 - 1705840236.9612167]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [308 - 1705840236.9618857]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [308 - 1705840236.963715]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [308 - 1705840236.9656043]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [308 - 1705840236.9664648]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [308 - 1705840236.967195]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [308 - 1705840236.967832]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [308 - 1705840236.9684267]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [308 - 1705840236.9702315]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [308 - 1705840236.9721372]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [308 - 1705840236.97304]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [308 - 1705840236.9738066]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [308 - 1705840236.9745336]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [308 - 1705840236.9752123]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [308 - 1705840236.9770072]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [308 - 1705840236.9788873]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [308 - 1705840236.9797661]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [308 - 1705840236.9805257]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [308 - 1705840236.9812253]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [308 - 1705840236.9818702]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [308 - 1705840236.983652]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [308 - 1705840236.9855604]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [308 - 1705840236.9864612]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [308 - 1705840236.9872117]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [308 - 1705840236.9878793]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [308 - 1705840236.9885287]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [308 - 1705840236.9903407]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [308 - 1705840236.992287]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [308 - 1705840236.99319]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [308 - 1705840236.9939346]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [308 - 1705840236.9945726]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [308 - 1705840236.9951766]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [308 - 1705840236.9970245]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [308 - 1705840236.998935]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [308 - 1705840236.9998095]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [308 - 1705840237.0005777]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [308 - 1705840237.0012772]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [308 - 1705840237.0019157]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [308 - 1705840237.003726]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [308 - 1705840237.0055149]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [308 - 1705840237.0064132]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [308 - 1705840237.0071757]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [308 - 1705840237.0078533]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [308 - 1705840237.0084956]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [308 - 1705840237.01029]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [308 - 1705840237.01222]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [308 - 1705840237.0130963]: 0.0
python ParamPruning/classifier.weight [308 - 1705840237.0135875]: 0.0
python DistillationModifier [315 - 1705840275.1710296]: Calling loss_update with:
args: 0.6239970326423645| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840276.6034644]: 
Returned: 1.3069992065429688| 

python DistillationModifier [315 - 1705840278.0628073]: Calling loss_update with:
args: 1.0985441207885742| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840279.5091255]: 
Returned: 1.8252723217010498| 

python DistillationModifier [315 - 1705840280.974014]: Calling loss_update with:
args: 1.0244340896606445| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840282.4043593]: 
Returned: 1.1826122999191284| 

python DistillationModifier [315 - 1705840283.868782]: Calling loss_update with:
args: 0.5877032279968262| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840285.512848]: 
Returned: 0.7703993320465088| 

python DistillationModifier [315 - 1705840287.3492694]: Calling loss_update with:
args: 1.0315289497375488| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840288.790101]: 
Returned: 1.4463043212890625| 

python DistillationModifier [315 - 1705840290.253595]: Calling loss_update with:
args: 0.6719138622283936| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840291.6873767]: 
Returned: 1.2000203132629395| 

python DistillationModifier [315 - 1705840293.1465032]: Calling loss_update with:
args: 0.3907776176929474| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840294.5737076]: 
Returned: 1.0157887935638428| 

python DistillationModifier [315 - 1705840296.0355282]: Calling loss_update with:
args: 1.1322224140167236| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840297.4687393]: 
Returned: 1.45962655544281| 

python DistillationModifier [315 - 1705840298.9421735]: Calling loss_update with:
args: 0.5354134440422058| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840300.3734646]: 
Returned: 0.4470045268535614| 

python DistillationModifier [315 - 1705840301.8279088]: Calling loss_update with:
args: 0.7499581575393677| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840303.2544167]: 
Returned: 1.3174262046813965| 

python DistillationModifier [315 - 1705840304.7145162]: Calling loss_update with:
args: 0.7418279051780701| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840306.1405728]: 
Returned: 1.2102795839309692| 

python DistillationModifier [315 - 1705840307.600953]: Calling loss_update with:
args: 0.8844652771949768| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840309.041588]: 
Returned: 1.2676565647125244| 

python DistillationModifier [315 - 1705840310.5068197]: Calling loss_update with:
args: 0.9721853137016296| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840311.935616]: 
Returned: 1.7191485166549683| 

python DistillationModifier [315 - 1705840313.390761]: Calling loss_update with:
args: 0.6667994856834412| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840314.8163967]: 
Returned: 0.6647427082061768| 

python DistillationModifier [315 - 1705840316.3148186]: Calling loss_update with:
args: 0.9546217918395996| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840317.7340984]: 
Returned: 1.5223379135131836| 

python DistillationModifier [315 - 1705840319.190266]: Calling loss_update with:
args: 0.7138729691505432| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840320.620935]: 
Returned: 1.3071694374084473| 

python DistillationModifier [315 - 1705840322.4363186]: Calling loss_update with:
args: 0.6442727446556091| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840324.4671738]: 
Returned: 0.9830873608589172| 

python DistillationModifier [315 - 1705840326.0756586]: Calling loss_update with:
args: 0.9934359788894653| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840327.5009902]: 
Returned: 1.8113486766815186| 

python DistillationModifier [315 - 1705840328.9515903]: Calling loss_update with:
args: 1.1346207857131958| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840330.3839653]: 
Returned: 1.2232557535171509| 

python DistillationModifier [315 - 1705840331.8352563]: Calling loss_update with:
args: 0.6799811124801636| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840333.2565668]: 
Returned: 1.2183669805526733| 

python DistillationModifier [315 - 1705840334.7147822]: Calling loss_update with:
args: 0.5720240473747253| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840336.1393313]: 
Returned: 1.1480038166046143| 

python DistillationModifier [315 - 1705840337.599439]: Calling loss_update with:
args: 0.5029610395431519| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840339.0253305]: 
Returned: 0.8412001132965088| 

python DistillationModifier [315 - 1705840340.4868755]: Calling loss_update with:
args: 0.6547031998634338| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840341.9144237]: 
Returned: 1.018428921699524| 

python DistillationModifier [315 - 1705840343.3648665]: Calling loss_update with:
args: 0.7570682168006897| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840344.7915857]: 
Returned: 1.2777732610702515| 

python DistillationModifier [315 - 1705840346.298547]: Calling loss_update with:
args: 0.927425742149353| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840347.7418985]: 
Returned: 1.0701992511749268| 

python DistillationModifier [315 - 1705840349.1921103]: Calling loss_update with:
args: 0.5516287088394165| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840350.6169267]: 
Returned: 1.026498556137085| 

python DistillationModifier [315 - 1705840352.072302]: Calling loss_update with:
args: 0.8902174830436707| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840353.4940412]: 
Returned: 1.2324126958847046| 

python DistillationModifier [315 - 1705840354.948609]: Calling loss_update with:
args: 0.709347128868103| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840356.3810613]: 
Returned: 1.1711499691009521| 

python DistillationModifier [315 - 1705840358.4076052]: Calling loss_update with:
args: 0.6884287595748901| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840359.8366523]: 
Returned: 1.1100918054580688| 

python DistillationModifier [315 - 1705840361.3024325]: Calling loss_update with:
args: 0.49582016468048096| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840362.724446]: 
Returned: 0.975457489490509| 

python DistillationModifier [315 - 1705840364.1756163]: Calling loss_update with:
args: 1.0149470567703247| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840365.599416]: 
Returned: 1.8442761898040771| 

python DistillationModifier [315 - 1705840367.0572708]: Calling loss_update with:
args: 0.9156299829483032| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840368.485957]: 
Returned: 1.2976280450820923| 

python DistillationModifier [315 - 1705840369.9410934]: Calling loss_update with:
args: 0.8998898863792419| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840371.366865]: 
Returned: 1.5053038597106934| 

python DistillationModifier [315 - 1705840372.8217015]: Calling loss_update with:
args: 1.0644488334655762| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840374.2435322]: 
Returned: 1.6808195114135742| 

python DistillationModifier [315 - 1705840375.698898]: Calling loss_update with:
args: 1.3482409715652466| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840377.1672232]: 
Returned: 1.905788540840149| 

python DistillationModifier [315 - 1705840378.6242728]: Calling loss_update with:
args: 1.1679294109344482| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840380.04243]: 
Returned: 2.1515939235687256| 

python DistillationModifier [315 - 1705840381.6739528]: Calling loss_update with:
args: 0.9182459712028503| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840383.683572]: 
Returned: 0.8397999405860901| 

python DistillationModifier [315 - 1705840385.4804351]: Calling loss_update with:
args: 0.43157675862312317| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840386.9037895]: 
Returned: 0.9061065316200256| 

python DistillationModifier [315 - 1705840388.3627052]: Calling loss_update with:
args: 0.7341021299362183| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840389.780872]: 
Returned: 1.3814555406570435| 

python DistillationModifier [315 - 1705840391.2326546]: Calling loss_update with:
args: 0.9672845602035522| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840392.652847]: 
Returned: 1.911734938621521| 

python DistillationModifier [315 - 1705840394.1027021]: Calling loss_update with:
args: 0.4475928246974945| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840395.518629]: 
Returned: 1.1938402652740479| 

python DistillationModifier [315 - 1705840396.9739559]: Calling loss_update with:
args: 0.7354128360748291| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840398.3930655]: 
Returned: 1.526320219039917| 

python DistillationModifier [315 - 1705840399.858111]: Calling loss_update with:
args: 0.7022621631622314| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840401.2787151]: 
Returned: 0.771580696105957| 

python DistillationModifier [315 - 1705840402.7331207]: Calling loss_update with:
args: 0.6203910708427429| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840404.1479187]: 
Returned: 1.3223319053649902| 

python DistillationModifier [315 - 1705840405.6008382]: Calling loss_update with:
args: 0.8705719709396362| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840407.0620098]: 
Returned: 2.019155502319336| 

python DistillationModifier [315 - 1705840408.5056622]: Calling loss_update with:
args: 1.063964605331421| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840409.9250064]: 
Returned: 1.5871262550354004| 

python DistillationModifier [315 - 1705840411.3770714]: Calling loss_update with:
args: 0.44082051515579224| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840412.7976823]: 
Returned: 1.0877026319503784| 

python DistillationModifier [315 - 1705840414.2417207]: Calling loss_update with:
args: 0.43457120656967163| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840415.6576521]: 
Returned: 0.9066286683082581| 

python DistillationModifier [315 - 1705840417.1100528]: Calling loss_update with:
args: 1.3040398359298706| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840418.529658]: 
Returned: 2.36427903175354| 

python DistillationModifier [315 - 1705840420.551416]: Calling loss_update with:
args: 0.8347460031509399| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840421.9693716]: 
Returned: 1.5819021463394165| 

python DistillationModifier [315 - 1705840423.4154034]: Calling loss_update with:
args: 0.437839150428772| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840424.8290644]: 
Returned: 0.9146226048469543| 

python DistillationModifier [315 - 1705840426.272892]: Calling loss_update with:
args: 1.5228345394134521| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840427.6931024]: 
Returned: 1.397608995437622| 

python DistillationModifier [315 - 1705840429.1362271]: Calling loss_update with:
args: 0.6345768570899963| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840430.54555]: 
Returned: 0.7402117848396301| 

python DistillationModifier [315 - 1705840431.989565]: Calling loss_update with:
args: 0.5964384078979492| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840433.402515]: 
Returned: 0.9509600400924683| 

python DistillationModifier [315 - 1705840434.8427522]: Calling loss_update with:
args: 0.5593886971473694| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840436.25061]: 
Returned: 1.5698151588439941| 

python DistillationModifier [315 - 1705840437.7368236]: Calling loss_update with:
args: 0.8311241269111633| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840439.1492612]: 
Returned: 1.2038183212280273| 

python DistillationModifier [315 - 1705840440.5938776]: Calling loss_update with:
args: 0.8340916037559509| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840442.0561743]: 
Returned: 0.6862958073616028| 

python DistillationModifier [315 - 1705840444.0987058]: Calling loss_update with:
args: 0.8822314143180847| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840445.9431922]: 
Returned: 1.2300666570663452| 

python DistillationModifier [315 - 1705840447.3861692]: Calling loss_update with:
args: 0.9697453379631042| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840448.7988958]: 
Returned: 1.5159788131713867| 

python DistillationModifier [315 - 1705840450.2507296]: Calling loss_update with:
args: 1.011962652206421| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840451.6661208]: 
Returned: 1.8612265586853027| 

python DistillationModifier [315 - 1705840453.1064832]: Calling loss_update with:
args: 0.598962128162384| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840454.5192175]: 
Returned: 1.0771257877349854| 

python DistillationModifier [315 - 1705840455.960605]: Calling loss_update with:
args: 0.7709663510322571| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840457.3790653]: 
Returned: 1.3372589349746704| 

python DistillationModifier [315 - 1705840458.128784]: Calling loss_update with:
args: 0.6654610633850098| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840458.8538184]: 
Returned: 1.2699623107910156| 

python DistillationModifier [315 - 1705840460.890705]: Calling loss_update with:
args: 0.15427106618881226| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [315 - 1705840462.3078394]: 
Returned: 0.26593688130378723| 

python LearningRateFunctionModifier [315 - 1705840465.1659436]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [315 - 1705840465.1661298]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [315 - 1705840465.1661916]: 9.230769230769229e-05
python LearningRateFunctionModifier/ParamGroup1 [315 - 1705840465.1664336]: 9.230769230769229e-05
python DistillationModifier/task_loss [315 - 1705840465.166733]: tensor(0.1543, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [315 - 1705840465.1677554]: tensor(0.2659, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [315 - 1705840465.1683233]: tensor(0.2659, grad_fn=<AddBackward0>)
python ConstantPruningModifier [315 - 1705840465.1689098]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.0| steps_per_epoch: 63| 
python ConstantPruningModifier [315 - 1705840465.385823]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [315 - 1705840465.3865952]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [315 - 1705840465.387623]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [315 - 1705840465.38847]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [315 - 1705840465.3892248]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [315 - 1705840465.390958]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [315 - 1705840465.3925734]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [315 - 1705840465.3933692]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [315 - 1705840465.3940947]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [315 - 1705840465.394781]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [315 - 1705840465.3953927]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [315 - 1705840465.3970616]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [315 - 1705840465.3986378]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [315 - 1705840465.3994074]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [315 - 1705840465.4001362]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [315 - 1705840465.4007936]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [315 - 1705840465.4013965]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [315 - 1705840465.4030461]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [315 - 1705840465.4046502]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [315 - 1705840465.4055207]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [315 - 1705840465.4062307]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [315 - 1705840465.4068787]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [315 - 1705840465.4075363]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [315 - 1705840465.4092004]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [315 - 1705840465.4107172]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [315 - 1705840465.41147]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [315 - 1705840465.4122133]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [315 - 1705840465.4129074]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [315 - 1705840465.413508]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [315 - 1705840465.4151614]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [315 - 1705840465.416731]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [315 - 1705840465.4174619]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [315 - 1705840465.4181783]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [315 - 1705840465.418777]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [315 - 1705840465.4193382]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [315 - 1705840465.420994]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [315 - 1705840465.4226217]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [315 - 1705840465.423441]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [315 - 1705840465.4241774]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [315 - 1705840465.4248438]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [315 - 1705840465.425489]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [315 - 1705840465.4271603]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [315 - 1705840465.428753]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [315 - 1705840465.4295034]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [315 - 1705840465.4302132]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [315 - 1705840465.430872]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [315 - 1705840465.4314697]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [315 - 1705840465.4331524]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [315 - 1705840465.4347267]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [315 - 1705840465.435467]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [315 - 1705840465.436185]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [315 - 1705840465.4368086]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [315 - 1705840465.4373672]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [315 - 1705840465.4390128]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [315 - 1705840465.4406443]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [315 - 1705840465.4414291]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [315 - 1705840465.442122]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [315 - 1705840465.4427247]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [315 - 1705840465.4432843]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [315 - 1705840465.4449306]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [315 - 1705840465.4464476]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [315 - 1705840465.4471872]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [315 - 1705840465.447859]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [315 - 1705840465.448451]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [315 - 1705840465.449061]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [315 - 1705840465.4506924]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [315 - 1705840465.4522655]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [315 - 1705840465.4530866]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [315 - 1705840465.4537942]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [315 - 1705840465.4543695]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [315 - 1705840465.4549277]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [315 - 1705840465.4565647]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [315 - 1705840465.458153]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [315 - 1705840465.4589097]: 0.0
python ParamPruning/classifier.weight [315 - 1705840465.459373]: 0.0
python DistillationModifier [322 - 1705840506.8575287]: Calling loss_update with:
args: 0.2301800400018692| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.111111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [322 - 1705840508.2728438]: 
Returned: 0.3478792905807495| 

python LearningRateFunctionModifier [322 - 1705840511.120157]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.111111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [322 - 1705840511.1203313]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [322 - 1705840511.1203907]: 9.102564102564102e-05
python LearningRateFunctionModifier/ParamGroup1 [322 - 1705840511.1206245]: 9.102564102564102e-05
python DistillationModifier/task_loss [322 - 1705840511.1209424]: tensor(0.2302, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [322 - 1705840511.121917]: tensor(0.3479, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [322 - 1705840511.1224647]: tensor(0.3479, grad_fn=<AddBackward0>)
python ConstantPruningModifier [322 - 1705840511.122993]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.111111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [322 - 1705840511.3373919]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [322 - 1705840511.3381755]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [322 - 1705840511.339014]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [322 - 1705840511.3396282]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [322 - 1705840511.3401554]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [322 - 1705840511.3418186]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [322 - 1705840511.3433065]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [322 - 1705840511.3439596]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [322 - 1705840511.3445141]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [322 - 1705840511.345042]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [322 - 1705840511.3455563]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [322 - 1705840511.347054]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [322 - 1705840511.3487756]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [322 - 1705840511.349409]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [322 - 1705840511.3499575]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [322 - 1705840511.3504536]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [322 - 1705840511.35093]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [322 - 1705840511.3524914]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [322 - 1705840511.3541348]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [322 - 1705840511.3547301]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [322 - 1705840511.3552532]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [322 - 1705840511.355747]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [322 - 1705840511.356233]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [322 - 1705840511.3578482]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [322 - 1705840511.359323]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [322 - 1705840511.3599598]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [322 - 1705840511.3605337]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [322 - 1705840511.3610513]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [322 - 1705840511.3615363]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [322 - 1705840511.362986]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [322 - 1705840511.3646336]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [322 - 1705840511.3653088]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [322 - 1705840511.3658674]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [322 - 1705840511.3663683]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [322 - 1705840511.3668492]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [322 - 1705840511.3684165]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [322 - 1705840511.36989]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [322 - 1705840511.3705316]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [322 - 1705840511.3711138]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [322 - 1705840511.3716118]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [322 - 1705840511.3721502]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [322 - 1705840511.3736448]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [322 - 1705840511.3753147]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [322 - 1705840511.3759723]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [322 - 1705840511.3765335]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [322 - 1705840511.377055]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [322 - 1705840511.377556]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [322 - 1705840511.3791518]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [322 - 1705840511.3807917]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [322 - 1705840511.3813927]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [322 - 1705840511.3819344]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [322 - 1705840511.3824348]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [322 - 1705840511.382914]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [322 - 1705840511.3844936]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [322 - 1705840511.386139]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [322 - 1705840511.3867455]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [322 - 1705840511.3872738]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [322 - 1705840511.3877785]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [322 - 1705840511.388267]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [322 - 1705840511.389902]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [322 - 1705840511.39153]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [322 - 1705840511.3921494]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [322 - 1705840511.392683]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [322 - 1705840511.393181]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [322 - 1705840511.3936598]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [322 - 1705840511.395246]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [322 - 1705840511.3968742]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [322 - 1705840511.397485]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [322 - 1705840511.3980176]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [322 - 1705840511.398513]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [322 - 1705840511.3990016]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [322 - 1705840511.4005873]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [322 - 1705840511.4022167]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [322 - 1705840511.4028018]: 0.0
python ParamPruning/classifier.weight [322 - 1705840511.4031923]: 0.0
python DistillationModifier [329 - 1705840550.8292737]: Calling loss_update with:
args: 0.015833469107747078| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.222222222222222| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [329 - 1705840552.291134]: 
Returned: 0.043313607573509216| 

python LearningRateFunctionModifier [329 - 1705840555.6802857]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.222222222222222| steps_per_epoch: 63| 
python LearningRateFunctionModifier [329 - 1705840555.6804485]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [329 - 1705840555.6805215]: 8.974358974358974e-05
python LearningRateFunctionModifier/ParamGroup1 [329 - 1705840555.680777]: 8.974358974358974e-05
python DistillationModifier/task_loss [329 - 1705840555.68109]: tensor(0.0158, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [329 - 1705840555.6820376]: tensor(0.0433, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [329 - 1705840555.6825929]: tensor(0.0433, grad_fn=<AddBackward0>)
python ConstantPruningModifier [329 - 1705840555.68316]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.222222222222222| steps_per_epoch: 63| 
python ConstantPruningModifier [329 - 1705840555.897475]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [329 - 1705840555.8982975]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [329 - 1705840555.9077108]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [329 - 1705840555.9083443]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [329 - 1705840555.908934]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [329 - 1705840555.9106042]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [329 - 1705840555.9123323]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [329 - 1705840555.9129858]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [329 - 1705840555.9135194]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [329 - 1705840555.9140308]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [329 - 1705840555.914522]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [329 - 1705840555.9161358]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [329 - 1705840555.917786]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [329 - 1705840555.9183729]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [329 - 1705840555.9188733]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [329 - 1705840555.9193816]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [329 - 1705840555.9198725]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [329 - 1705840555.9215257]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [329 - 1705840555.9231713]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [329 - 1705840555.9237823]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [329 - 1705840555.924365]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [329 - 1705840555.9248939]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [329 - 1705840555.9253778]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [329 - 1705840555.9269636]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [329 - 1705840555.9286003]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [329 - 1705840555.9292357]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [329 - 1705840555.929792]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [329 - 1705840555.9303002]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [329 - 1705840555.9307928]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [329 - 1705840555.9324188]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [329 - 1705840555.9340818]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [329 - 1705840555.9346647]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [329 - 1705840555.9351637]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [329 - 1705840555.9356554]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [329 - 1705840555.9361522]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [329 - 1705840555.937718]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [329 - 1705840555.9393322]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [329 - 1705840555.9399579]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [329 - 1705840555.9404893]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [329 - 1705840555.9410217]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [329 - 1705840555.94152]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [329 - 1705840555.9431055]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [329 - 1705840555.9447532]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [329 - 1705840555.9453692]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [329 - 1705840555.9458919]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [329 - 1705840555.946411]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [329 - 1705840555.9469078]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [329 - 1705840555.9484258]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [329 - 1705840555.9500175]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [329 - 1705840555.9505813]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [329 - 1705840555.951085]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [329 - 1705840555.951584]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [329 - 1705840555.9520948]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [329 - 1705840555.9536624]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [329 - 1705840555.955278]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [329 - 1705840555.955856]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [329 - 1705840555.9563513]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [329 - 1705840555.956872]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [329 - 1705840555.9573593]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [329 - 1705840555.9589565]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [329 - 1705840555.960569]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [329 - 1705840555.9612107]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [329 - 1705840555.9617884]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [329 - 1705840555.962314]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [329 - 1705840555.9628036]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [329 - 1705840555.9643419]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [329 - 1705840555.9660375]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [329 - 1705840555.9666822]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [329 - 1705840555.9672673]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [329 - 1705840555.9677846]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [329 - 1705840555.96828]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [329 - 1705840555.96989]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [329 - 1705840555.9715147]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [329 - 1705840555.972116]: 0.0
python ParamPruning/classifier.weight [329 - 1705840555.972467]: 0.0
python DistillationModifier [336 - 1705840596.2985075]: Calling loss_update with:
args: 0.06388713419437408| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.333333333333333| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [336 - 1705840597.829594]: 
Returned: 0.17904828488826752| 

python LearningRateFunctionModifier [336 - 1705840600.66982]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.333333333333333| steps_per_epoch: 63| 
python LearningRateFunctionModifier [336 - 1705840600.6699805]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [336 - 1705840600.6700397]: 8.846153846153845e-05
python LearningRateFunctionModifier/ParamGroup1 [336 - 1705840600.6702695]: 8.846153846153845e-05
python DistillationModifier/task_loss [336 - 1705840600.670563]: tensor(0.0639, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [336 - 1705840600.6715145]: tensor(0.1790, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [336 - 1705840600.672056]: tensor(0.1790, grad_fn=<AddBackward0>)
python ConstantPruningModifier [336 - 1705840600.6725678]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.333333333333333| steps_per_epoch: 63| 
python ConstantPruningModifier [336 - 1705840600.885548]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [336 - 1705840600.8862813]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [336 - 1705840600.8872237]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [336 - 1705840600.8878748]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [336 - 1705840600.888449]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [336 - 1705840600.8900688]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [336 - 1705840600.89185]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [336 - 1705840600.892507]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [336 - 1705840600.8930607]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [336 - 1705840600.8936508]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [336 - 1705840600.8941498]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [336 - 1705840600.8957102]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [336 - 1705840600.8972054]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [336 - 1705840600.897883]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [336 - 1705840600.898532]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [336 - 1705840600.899076]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [336 - 1705840600.8996542]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [336 - 1705840600.9011922]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [336 - 1705840600.902667]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [336 - 1705840600.9033673]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [336 - 1705840600.9040196]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [336 - 1705840600.9046211]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [336 - 1705840600.9051733]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [336 - 1705840600.9067264]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [336 - 1705840600.9084432]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [336 - 1705840600.909146]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [336 - 1705840600.9096963]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [336 - 1705840600.9102805]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [336 - 1705840600.9107916]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [336 - 1705840600.9124095]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [336 - 1705840600.9138966]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [336 - 1705840600.9145756]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [336 - 1705840600.9151986]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [336 - 1705840600.9157517]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [336 - 1705840600.9162726]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [336 - 1705840600.917816]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [336 - 1705840600.9195032]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [336 - 1705840600.9202003]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [336 - 1705840600.92075]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [336 - 1705840600.9213798]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [336 - 1705840600.9219651]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [336 - 1705840600.9235616]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [336 - 1705840600.925237]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [336 - 1705840600.9258847]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [336 - 1705840600.9264667]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [336 - 1705840600.9269855]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [336 - 1705840600.9274747]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [336 - 1705840600.9290755]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [336 - 1705840600.9307837]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [336 - 1705840600.9314299]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [336 - 1705840600.9319754]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [336 - 1705840600.932571]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [336 - 1705840600.9331813]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [336 - 1705840600.9347928]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [336 - 1705840600.9364672]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [336 - 1705840600.9371364]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [336 - 1705840600.9377398]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [336 - 1705840600.9382465]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [336 - 1705840600.9388285]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [336 - 1705840600.9404278]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [336 - 1705840600.9421284]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [336 - 1705840600.9427955]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [336 - 1705840600.9433932]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [336 - 1705840600.9439783]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [336 - 1705840600.944492]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [336 - 1705840600.9461164]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [336 - 1705840600.9475534]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [336 - 1705840600.9482028]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [336 - 1705840600.948849]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [336 - 1705840600.9493823]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [336 - 1705840600.9499607]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [336 - 1705840600.951454]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [336 - 1705840600.953195]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [336 - 1705840600.953876]: 0.0
python ParamPruning/classifier.weight [336 - 1705840600.9542391]: 0.0
python DistillationModifier [343 - 1705840641.8513672]: Calling loss_update with:
args: 0.23757293820381165| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.444444444444445| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [343 - 1705840643.2654812]: 
Returned: 0.2190028727054596| 

python LearningRateFunctionModifier [343 - 1705840646.094882]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.444444444444445| steps_per_epoch: 63| 
python LearningRateFunctionModifier [343 - 1705840646.0950456]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [343 - 1705840646.0951047]: 8.717948717948717e-05
python LearningRateFunctionModifier/ParamGroup1 [343 - 1705840646.09533]: 8.717948717948717e-05
python DistillationModifier/task_loss [343 - 1705840646.0956473]: tensor(0.2376, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [343 - 1705840646.0966134]: tensor(0.2190, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [343 - 1705840646.0971556]: tensor(0.2190, grad_fn=<AddBackward0>)
python ConstantPruningModifier [343 - 1705840646.0976894]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.444444444444445| steps_per_epoch: 63| 
python ConstantPruningModifier [343 - 1705840646.3096921]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [343 - 1705840646.3104453]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [343 - 1705840646.3112602]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [343 - 1705840646.311925]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [343 - 1705840646.3124542]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [343 - 1705840646.3140764]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [343 - 1705840646.315721]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [343 - 1705840646.3164072]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [343 - 1705840646.3169801]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [343 - 1705840646.317489]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [343 - 1705840646.3179846]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [343 - 1705840646.3196404]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [343 - 1705840646.321272]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [343 - 1705840646.3219545]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [343 - 1705840646.3224926]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [343 - 1705840646.3229928]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [343 - 1705840646.3234704]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [343 - 1705840646.3250425]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [343 - 1705840646.326696]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [343 - 1705840646.3273325]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [343 - 1705840646.3278677]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [343 - 1705840646.3283474]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [343 - 1705840646.3288624]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [343 - 1705840646.330424]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [343 - 1705840646.3320715]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [343 - 1705840646.3327456]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [343 - 1705840646.3332956]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [343 - 1705840646.3337889]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [343 - 1705840646.3342884]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [343 - 1705840646.3358817]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [343 - 1705840646.3375235]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [343 - 1705840646.3381977]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [343 - 1705840646.338743]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [343 - 1705840646.3392382]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [343 - 1705840646.3397248]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [343 - 1705840646.3412995]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [343 - 1705840646.342928]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [343 - 1705840646.343592]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [343 - 1705840646.344132]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [343 - 1705840646.3446412]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [343 - 1705840646.34515]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [343 - 1705840646.3467143]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [343 - 1705840646.3481705]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [343 - 1705840646.3488088]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [343 - 1705840646.3493967]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [343 - 1705840646.3499067]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [343 - 1705840646.3503864]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [343 - 1705840646.352031]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [343 - 1705840646.3537638]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [343 - 1705840646.3544533]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [343 - 1705840646.355009]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [343 - 1705840646.3555079]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [343 - 1705840646.3559935]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [343 - 1705840646.3575943]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [343 - 1705840646.359214]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [343 - 1705840646.359874]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [343 - 1705840646.360426]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [343 - 1705840646.3609521]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [343 - 1705840646.3614378]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [343 - 1705840646.3629968]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [343 - 1705840646.3645787]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [343 - 1705840646.3652287]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [343 - 1705840646.3657503]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [343 - 1705840646.3662436]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [343 - 1705840646.3667116]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [343 - 1705840646.3682892]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [343 - 1705840646.3699424]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [343 - 1705840646.370648]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [343 - 1705840646.3712032]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [343 - 1705840646.3717198]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [343 - 1705840646.3722122]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [343 - 1705840646.3737879]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [343 - 1705840646.3754053]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [343 - 1705840646.3760734]: 0.0
python ParamPruning/classifier.weight [343 - 1705840646.376449]: 0.0
python DistillationModifier [350 - 1705840687.4199753]: Calling loss_update with:
args: 0.16197600960731506| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.555555555555555| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [350 - 1705840688.9489899]: 
Returned: 0.2469790130853653| 

python LearningRateFunctionModifier [350 - 1705840691.7891774]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.555555555555555| steps_per_epoch: 63| 
python LearningRateFunctionModifier [350 - 1705840691.7893374]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [350 - 1705840691.7893965]: 8.58974358974359e-05
python LearningRateFunctionModifier/ParamGroup1 [350 - 1705840691.7896223]: 8.58974358974359e-05
python DistillationModifier/task_loss [350 - 1705840691.789926]: tensor(0.1620, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [350 - 1705840691.7909415]: tensor(0.2470, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [350 - 1705840691.7914805]: tensor(0.2470, grad_fn=<AddBackward0>)
python ConstantPruningModifier [350 - 1705840691.7920375]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.555555555555555| steps_per_epoch: 63| 
python ConstantPruningModifier [350 - 1705840692.0040371]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [350 - 1705840692.0047934]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [350 - 1705840692.0056093]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [350 - 1705840692.0063052]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [350 - 1705840692.0069244]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [350 - 1705840692.0085585]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [350 - 1705840692.010289]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [350 - 1705840692.0109422]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [350 - 1705840692.011545]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [350 - 1705840692.0121775]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [350 - 1705840692.0127842]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [350 - 1705840692.0144095]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [350 - 1705840692.0160825]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [350 - 1705840692.0167227]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [350 - 1705840692.01734]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [350 - 1705840692.01786]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [350 - 1705840692.0184479]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [350 - 1705840692.0200424]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [350 - 1705840692.0217445]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [350 - 1705840692.022372]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [350 - 1705840692.0229533]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [350 - 1705840692.023474]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [350 - 1705840692.0239758]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [350 - 1705840692.0255644]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [350 - 1705840692.0272067]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [350 - 1705840692.0278087]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [350 - 1705840692.0283213]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [350 - 1705840692.028911]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [350 - 1705840692.0294867]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [350 - 1705840692.031071]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [350 - 1705840692.0327344]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [350 - 1705840692.033366]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [350 - 1705840692.033967]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [350 - 1705840692.0345001]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [350 - 1705840692.035013]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [350 - 1705840692.0365837]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [350 - 1705840692.0382664]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [350 - 1705840692.0388687]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [350 - 1705840692.0394428]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [350 - 1705840692.0400078]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [350 - 1705840692.0405285]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [350 - 1705840692.0421338]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [350 - 1705840692.0437443]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [350 - 1705840692.0443535]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [350 - 1705840692.0448904]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [350 - 1705840692.0454774]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [350 - 1705840692.0460434]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [350 - 1705840692.0476398]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [350 - 1705840692.0491009]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [350 - 1705840692.049707]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [350 - 1705840692.050311]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [350 - 1705840692.0508592]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [350 - 1705840692.0513856]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [350 - 1705840692.0528908]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [350 - 1705840692.054579]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [350 - 1705840692.05523]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [350 - 1705840692.055811]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [350 - 1705840692.0563984]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [350 - 1705840692.057021]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [350 - 1705840692.058626]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [350 - 1705840692.0602899]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [350 - 1705840692.0609164]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [350 - 1705840692.0615032]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [350 - 1705840692.0620155]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [350 - 1705840692.0625424]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [350 - 1705840692.064106]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [350 - 1705840692.065541]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [350 - 1705840692.0661745]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [350 - 1705840692.066783]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [350 - 1705840692.0673764]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [350 - 1705840692.0679142]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [350 - 1705840692.0694091]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [350 - 1705840692.0710583]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [350 - 1705840692.071733]: 0.0
python ParamPruning/classifier.weight [350 - 1705840692.0721035]: 0.0
python DistillationModifier [357 - 1705840731.5946105]: Calling loss_update with:
args: 0.3211815357208252| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.666666666666667| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [357 - 1705840733.0607002]: 
Returned: 0.3277263045310974| 

python LearningRateFunctionModifier [357 - 1705840735.8894353]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.666666666666667| steps_per_epoch: 63| 
python LearningRateFunctionModifier [357 - 1705840735.8896098]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [357 - 1705840735.8896685]: 8.46153846153846e-05
python LearningRateFunctionModifier/ParamGroup1 [357 - 1705840735.8898957]: 8.46153846153846e-05
python DistillationModifier/task_loss [357 - 1705840735.8901973]: tensor(0.3212, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [357 - 1705840735.8911593]: tensor(0.3277, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [357 - 1705840735.891723]: tensor(0.3277, grad_fn=<AddBackward0>)
python ConstantPruningModifier [357 - 1705840735.8922577]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.666666666666667| steps_per_epoch: 63| 
python ConstantPruningModifier [357 - 1705840736.1062346]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [357 - 1705840736.1069968]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [357 - 1705840736.1078775]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [357 - 1705840736.108515]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [357 - 1705840736.1091316]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [357 - 1705840736.1107793]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [357 - 1705840736.1124973]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [357 - 1705840736.1131957]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [357 - 1705840736.1138186]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [357 - 1705840736.1143644]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [357 - 1705840736.1148832]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [357 - 1705840736.116507]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [357 - 1705840736.1182072]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [357 - 1705840736.1188338]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [357 - 1705840736.1194618]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [357 - 1705840736.1200461]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [357 - 1705840736.1205838]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [357 - 1705840736.1221943]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [357 - 1705840736.1238394]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [357 - 1705840736.1244612]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [357 - 1705840736.1250265]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [357 - 1705840736.1256323]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [357 - 1705840736.1261194]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [357 - 1705840736.127671]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [357 - 1705840736.129325]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [357 - 1705840736.129942]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [357 - 1705840736.130572]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [357 - 1705840736.1311448]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [357 - 1705840736.1316879]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [357 - 1705840736.133314]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [357 - 1705840736.1349692]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [357 - 1705840736.1355822]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [357 - 1705840736.1361837]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [357 - 1705840736.136763]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [357 - 1705840736.1372836]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [357 - 1705840736.1388733]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [357 - 1705840736.140523]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [357 - 1705840736.141176]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [357 - 1705840736.1417873]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [357 - 1705840736.142366]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [357 - 1705840736.142897]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [357 - 1705840736.1445096]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [357 - 1705840736.1462154]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [357 - 1705840736.1468465]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [357 - 1705840736.147512]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [357 - 1705840736.148041]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [357 - 1705840736.1485603]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [357 - 1705840736.1501844]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [357 - 1705840736.1519024]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [357 - 1705840736.152541]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [357 - 1705840736.153165]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [357 - 1705840736.1537054]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [357 - 1705840736.1541977]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [357 - 1705840736.1557724]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [357 - 1705840736.1574543]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [357 - 1705840736.1580844]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [357 - 1705840736.1586905]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [357 - 1705840736.1592147]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [357 - 1705840736.159788]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [357 - 1705840736.1614149]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [357 - 1705840736.1628423]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [357 - 1705840736.163466]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [357 - 1705840736.1640842]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [357 - 1705840736.1646397]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [357 - 1705840736.1652389]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [357 - 1705840736.1667643]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [357 - 1705840736.1684437]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [357 - 1705840736.1691132]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [357 - 1705840736.169733]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [357 - 1705840736.1702778]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [357 - 1705840736.1707883]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [357 - 1705840736.172388]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [357 - 1705840736.173857]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [357 - 1705840736.1745224]: 0.0
python ParamPruning/classifier.weight [357 - 1705840736.1748962]: 0.0
python DistillationModifier [364 - 1705840777.3748894]: Calling loss_update with:
args: 0.397376149892807| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.777777777777778| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [364 - 1705840778.7813432]: 
Returned: 0.8748638033866882| 

python LearningRateFunctionModifier [364 - 1705840781.5997353]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.777777777777778| steps_per_epoch: 63| 
python LearningRateFunctionModifier [364 - 1705840781.5998983]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [364 - 1705840781.5999613]: 8.333333333333333e-05
python LearningRateFunctionModifier/ParamGroup1 [364 - 1705840781.6001885]: 8.333333333333333e-05
python DistillationModifier/task_loss [364 - 1705840781.6005025]: tensor(0.3974, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [364 - 1705840781.601496]: tensor(0.8749, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [364 - 1705840781.6021497]: tensor(0.8749, grad_fn=<AddBackward0>)
python ConstantPruningModifier [364 - 1705840781.6027784]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.777777777777778| steps_per_epoch: 63| 
python ConstantPruningModifier [364 - 1705840781.8153026]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [364 - 1705840781.8160512]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [364 - 1705840781.816898]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [364 - 1705840781.8175216]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [364 - 1705840781.8181672]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [364 - 1705840781.8197849]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [364 - 1705840781.8214364]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [364 - 1705840781.8220863]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [364 - 1705840781.8227446]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [364 - 1705840781.8233476]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [364 - 1705840781.8238916]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [364 - 1705840781.8255525]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [364 - 1705840781.8270051]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [364 - 1705840781.8276246]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [364 - 1705840781.8282306]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [364 - 1705840781.828779]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [364 - 1705840781.8293476]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [364 - 1705840781.8309531]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [364 - 1705840781.8325422]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [364 - 1705840781.8331416]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [364 - 1705840781.833758]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [364 - 1705840781.834274]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [364 - 1705840781.83485]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [364 - 1705840781.8363497]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [364 - 1705840781.8379982]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [364 - 1705840781.8386102]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [364 - 1705840781.8391936]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [364 - 1705840781.839712]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [364 - 1705840781.8402276]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [364 - 1705840781.8418076]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [364 - 1705840781.8435013]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [364 - 1705840781.8441277]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [364 - 1705840781.8447511]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [364 - 1705840781.8453526]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [364 - 1705840781.8458765]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [364 - 1705840781.84746]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [364 - 1705840781.84915]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [364 - 1705840781.8497455]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [364 - 1705840781.8503141]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [364 - 1705840781.8508346]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [364 - 1705840781.8513284]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [364 - 1705840781.852958]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [364 - 1705840781.8545825]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [364 - 1705840781.8551688]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [364 - 1705840781.8556798]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [364 - 1705840781.856172]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [364 - 1705840781.8567445]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [364 - 1705840781.8583028]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [364 - 1705840781.8597236]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [364 - 1705840781.8603835]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [364 - 1705840781.861036]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [364 - 1705840781.861638]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [364 - 1705840781.8621998]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [364 - 1705840781.8637466]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [364 - 1705840781.8653917]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [364 - 1705840781.8659718]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [364 - 1705840781.866546]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [364 - 1705840781.86706]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [364 - 1705840781.867556]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [364 - 1705840781.86912]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [364 - 1705840781.8707101]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [364 - 1705840781.8713033]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [364 - 1705840781.8719232]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [364 - 1705840781.872519]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [364 - 1705840781.873072]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [364 - 1705840781.874677]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [364 - 1705840781.876302]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [364 - 1705840781.8769011]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [364 - 1705840781.8774884]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [364 - 1705840781.8780084]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [364 - 1705840781.8785336]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [364 - 1705840781.8801155]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [364 - 1705840781.8815475]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [364 - 1705840781.8821707]: 0.0
python ParamPruning/classifier.weight [364 - 1705840781.8825479]: 0.0
python DistillationModifier [371 - 1705840822.7259388]: Calling loss_update with:
args: 0.10926757007837296| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 5.888888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [371 - 1705840824.2434015]: 
Returned: 0.22939816117286682| 

python LearningRateFunctionModifier [371 - 1705840827.070199]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.888888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [371 - 1705840827.0703633]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [371 - 1705840827.0704231]: 8.205128205128203e-05
python LearningRateFunctionModifier/ParamGroup1 [371 - 1705840827.070673]: 8.205128205128203e-05
python DistillationModifier/task_loss [371 - 1705840827.0709755]: tensor(0.1093, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [371 - 1705840827.0720327]: tensor(0.2294, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [371 - 1705840827.0726585]: tensor(0.2294, grad_fn=<AddBackward0>)
python ConstantPruningModifier [371 - 1705840827.0732734]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 5.888888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [371 - 1705840827.2869484]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [371 - 1705840827.2877002]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [371 - 1705840827.288734]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [371 - 1705840827.2895744]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [371 - 1705840827.2902052]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [371 - 1705840827.291879]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [371 - 1705840827.2933612]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [371 - 1705840827.2940753]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [371 - 1705840827.2946913]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [371 - 1705840827.2952425]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [371 - 1705840827.2957828]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [371 - 1705840827.2972405]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [371 - 1705840827.2986662]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [371 - 1705840827.2994266]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [371 - 1705840827.300137]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [371 - 1705840827.3007607]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [371 - 1705840827.3013294]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [371 - 1705840827.3028688]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [371 - 1705840827.3046036]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [371 - 1705840827.3052974]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [371 - 1705840827.3059323]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [371 - 1705840827.3065743]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [371 - 1705840827.307124]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [371 - 1705840827.3088105]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [371 - 1705840827.3103173]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [371 - 1705840827.3109612]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [371 - 1705840827.3115346]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [371 - 1705840827.312173]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [371 - 1705840827.3127177]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [371 - 1705840827.3141716]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [371 - 1705840827.315901]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [371 - 1705840827.3165932]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [371 - 1705840827.317194]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [371 - 1705840827.3178217]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [371 - 1705840827.318347]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [371 - 1705840827.3199482]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [371 - 1705840827.3213935]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [371 - 1705840827.3220723]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [371 - 1705840827.3226533]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [371 - 1705840827.3232298]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [371 - 1705840827.323749]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [371 - 1705840827.325182]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [371 - 1705840827.3268855]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [371 - 1705840827.3275871]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [371 - 1705840827.3282516]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [371 - 1705840827.3288383]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [371 - 1705840827.3294096]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [371 - 1705840827.3310094]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [371 - 1705840827.3327165]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [371 - 1705840827.3333616]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [371 - 1705840827.3339126]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [371 - 1705840827.3345356]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [371 - 1705840827.3350856]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [371 - 1705840827.3366857]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [371 - 1705840827.3380992]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [371 - 1705840827.3387806]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [371 - 1705840827.339417]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [371 - 1705840827.3399537]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [371 - 1705840827.3404627]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [371 - 1705840827.3419397]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [371 - 1705840827.34366]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [371 - 1705840827.3443189]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [371 - 1705840827.3449583]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [371 - 1705840827.345582]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [371 - 1705840827.3461459]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [371 - 1705840827.3477244]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [371 - 1705840827.349153]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [371 - 1705840827.3498173]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [371 - 1705840827.3503761]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [371 - 1705840827.3508945]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [371 - 1705840827.3514931]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [371 - 1705840827.3530262]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [371 - 1705840827.3547382]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [371 - 1705840827.3553975]: 0.0
python ParamPruning/classifier.weight [371 - 1705840827.3557923]: 0.0
python DistillationModifier [378 - 1705840864.890735]: Calling loss_update with:
args: 0.7838035225868225| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840866.7965999]: 
Returned: 1.453797698020935| 

python DistillationModifier [378 - 1705840868.2317276]: Calling loss_update with:
args: 1.5387752056121826| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840869.6400697]: 
Returned: 2.5174264907836914| 

python DistillationModifier [378 - 1705840871.0856352]: Calling loss_update with:
args: 1.1897724866867065| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840872.491714]: 
Returned: 1.306199312210083| 

python DistillationModifier [378 - 1705840873.9432712]: Calling loss_update with:
args: 0.652743399143219| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840875.3462017]: 
Returned: 0.8541479706764221| 

python DistillationModifier [378 - 1705840876.773983]: Calling loss_update with:
args: 1.1091930866241455| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840878.1732361]: 
Returned: 1.468475103378296| 

python DistillationModifier [378 - 1705840879.6091616]: Calling loss_update with:
args: 0.7977038621902466| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840881.0189734]: 
Returned: 1.5053462982177734| 

python DistillationModifier [378 - 1705840882.448624]: Calling loss_update with:
args: 0.3742193877696991| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840883.8509521]: 
Returned: 0.991334855556488| 

python DistillationModifier [378 - 1705840885.2900636]: Calling loss_update with:
args: 1.2663248777389526| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840886.6867025]: 
Returned: 1.794974684715271| 

python DistillationModifier [378 - 1705840888.119185]: Calling loss_update with:
args: 0.7572411298751831| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840889.5484407]: 
Returned: 0.6386488080024719| 

python DistillationModifier [378 - 1705840890.9802177]: Calling loss_update with:
args: 0.8841030597686768| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840892.4141867]: 
Returned: 1.4638478755950928| 

python DistillationModifier [378 - 1705840894.3574934]: Calling loss_update with:
args: 0.9353827834129333| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840895.7648797]: 
Returned: 1.5150060653686523| 

python DistillationModifier [378 - 1705840897.1971653]: Calling loss_update with:
args: 0.9854543209075928| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840898.5939438]: 
Returned: 1.587576985359192| 

python DistillationModifier [378 - 1705840900.0290546]: Calling loss_update with:
args: 0.9722579717636108| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840901.4280267]: 
Returned: 1.6628175973892212| 

python DistillationModifier [378 - 1705840902.8696406]: Calling loss_update with:
args: 0.6660428047180176| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840904.268855]: 
Returned: 0.5831080079078674| 

python DistillationModifier [378 - 1705840905.7022119]: Calling loss_update with:
args: 1.2629181146621704| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840907.0994325]: 
Returned: 2.013524055480957| 

python DistillationModifier [378 - 1705840908.5298407]: Calling loss_update with:
args: 1.1109205484390259| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840909.9417799]: 
Returned: 1.9772529602050781| 

python DistillationModifier [378 - 1705840911.370006]: Calling loss_update with:
args: 0.8636865615844727| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840912.7705004]: 
Returned: 1.220405101776123| 

python DistillationModifier [378 - 1705840914.2012482]: Calling loss_update with:
args: 1.1204441785812378| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840915.6034389]: 
Returned: 1.983349084854126| 

python DistillationModifier [378 - 1705840917.0480945]: Calling loss_update with:
args: 1.2769825458526611| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840918.4503706]: 
Returned: 1.418859839439392| 

python DistillationModifier [378 - 1705840919.8873174]: Calling loss_update with:
args: 0.8671079874038696| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840921.2867126]: 
Returned: 1.3767842054367065| 

python DistillationModifier [378 - 1705840923.2012966]: Calling loss_update with:
args: 0.7174429297447205| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840925.211892]: 
Returned: 1.425217628479004| 

python DistillationModifier [378 - 1705840926.663005]: Calling loss_update with:
args: 0.5651448369026184| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840928.063584]: 
Returned: 0.7669805288314819| 

python DistillationModifier [378 - 1705840929.4925277]: Calling loss_update with:
args: 0.8057160377502441| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840930.902808]: 
Returned: 1.2844802141189575| 

python DistillationModifier [378 - 1705840932.3333044]: Calling loss_update with:
args: 0.7823866009712219| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840933.732641]: 
Returned: 1.3590930700302124| 

python DistillationModifier [378 - 1705840935.1635218]: Calling loss_update with:
args: 1.0994659662246704| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840936.5728915]: 
Returned: 1.1472907066345215| 

python DistillationModifier [378 - 1705840938.0071366]: Calling loss_update with:
args: 0.6725719571113586| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840939.4048753]: 
Returned: 1.1729204654693604| 

python DistillationModifier [378 - 1705840940.845997]: Calling loss_update with:
args: 1.0831482410430908| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840942.2522511]: 
Returned: 1.4817136526107788| 

python DistillationModifier [378 - 1705840943.6806285]: Calling loss_update with:
args: 0.9107341170310974| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840945.0799906]: 
Returned: 1.4312820434570312| 

python DistillationModifier [378 - 1705840946.5139284]: Calling loss_update with:
args: 0.7987071871757507| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840947.9217277]: 
Returned: 1.3911139965057373| 

python DistillationModifier [378 - 1705840949.3524442]: Calling loss_update with:
args: 0.5887832641601562| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840950.7691946]: 
Returned: 1.0686206817626953| 

python DistillationModifier [378 - 1705840952.2017314]: Calling loss_update with:
args: 1.270231008529663| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840953.6455743]: 
Returned: 2.266017436981201| 

python DistillationModifier [378 - 1705840955.0765657]: Calling loss_update with:
args: 1.1332941055297852| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840956.4811873]: 
Returned: 1.540328860282898| 

python DistillationModifier [378 - 1705840957.912918]: Calling loss_update with:
args: 1.2143042087554932| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840959.3109975]: 
Returned: 2.0244412422180176| 

python DistillationModifier [378 - 1705840960.7482505]: Calling loss_update with:
args: 1.3720788955688477| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840962.1505642]: 
Returned: 2.0877158641815186| 

python DistillationModifier [378 - 1705840963.5835514]: Calling loss_update with:
args: 1.5765608549118042| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840964.9854224]: 
Returned: 2.0297069549560547| 

python DistillationModifier [378 - 1705840966.4191902]: Calling loss_update with:
args: 1.2904703617095947| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840967.8181334]: 
Returned: 2.4133124351501465| 

python DistillationModifier [378 - 1705840969.2497559]: Calling loss_update with:
args: 1.1133735179901123| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840970.6604242]: 
Returned: 0.9989601969718933| 

python DistillationModifier [378 - 1705840972.092614]: Calling loss_update with:
args: 0.48849987983703613| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840974.0533988]: 
Returned: 1.1070899963378906| 

python DistillationModifier [378 - 1705840975.4872456]: Calling loss_update with:
args: 0.9306119084358215| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840976.8927386]: 
Returned: 1.6057249307632446| 

python DistillationModifier [378 - 1705840978.3231454]: Calling loss_update with:
args: 1.1381194591522217| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840979.7199812]: 
Returned: 2.2386107444763184| 

python DistillationModifier [378 - 1705840981.1741922]: Calling loss_update with:
args: 0.6040214896202087| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840983.152375]: 
Returned: 1.2676541805267334| 

python DistillationModifier [378 - 1705840985.0955942]: Calling loss_update with:
args: 0.9442894458770752| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840986.5028605]: 
Returned: 2.019033193588257| 

python DistillationModifier [378 - 1705840987.9391458]: Calling loss_update with:
args: 0.7967393398284912| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840989.3358653]: 
Returned: 0.8594887256622314| 

python DistillationModifier [378 - 1705840990.7712648]: Calling loss_update with:
args: 0.7172533869743347| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840992.1737964]: 
Returned: 1.645732045173645| 

python DistillationModifier [378 - 1705840993.6145499]: Calling loss_update with:
args: 1.1432056427001953| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840995.017914]: 
Returned: 2.5043528079986572| 

python DistillationModifier [378 - 1705840996.4616127]: Calling loss_update with:
args: 1.2332448959350586| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705840997.8668876]: 
Returned: 1.705673336982727| 

python DistillationModifier [378 - 1705840999.3067355]: Calling loss_update with:
args: 0.5725468993186951| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705841000.7187123]: 
Returned: 1.3140171766281128| 

python DistillationModifier [378 - 1705841002.156469]: Calling loss_update with:
args: 0.4000476002693176| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705841003.558562]: 
Returned: 0.8462232947349548| 

python DistillationModifier [378 - 1705841004.9959586]: Calling loss_update with:
args: 1.437195062637329| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705841006.3943307]: 
Returned: 2.6419851779937744| 

python DistillationModifier [378 - 1705841007.8268583]: Calling loss_update with:
args: 0.9161648750305176| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705841009.2212813]: 
Returned: 1.7196648120880127| 

python DistillationModifier [378 - 1705841010.6494496]: Calling loss_update with:
args: 0.35892993211746216| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705841012.0602448]: 
Returned: 0.8280156254768372| 

python DistillationModifier [378 - 1705841013.4897673]: Calling loss_update with:
args: 1.8757245540618896| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705841014.937769]: 
Returned: 1.8644435405731201| 

python DistillationModifier [378 - 1705841016.373481]: Calling loss_update with:
args: 0.8735456466674805| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705841017.7783577]: 
Returned: 1.0609636306762695| 

python DistillationModifier [378 - 1705841019.2065704]: Calling loss_update with:
args: 0.774151623249054| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705841020.6072667]: 
Returned: 1.1915512084960938| 

python DistillationModifier [378 - 1705841022.0454803]: Calling loss_update with:
args: 0.4575515687465668| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705841023.4548838]: 
Returned: 1.3605856895446777| 

python DistillationModifier [378 - 1705841024.8892071]: Calling loss_update with:
args: 1.0038505792617798| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705841026.2861545]: 
Returned: 1.3978872299194336| 

python DistillationModifier [378 - 1705841027.7254093]: Calling loss_update with:
args: 1.0693475008010864| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705841029.1246228]: 
Returned: 0.8773103356361389| 

python DistillationModifier [378 - 1705841030.564282]: Calling loss_update with:
args: 1.1922320127487183| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705841031.9743636]: 
Returned: 1.422848105430603| 

python DistillationModifier [378 - 1705841033.4065902]: Calling loss_update with:
args: 1.0030380487442017| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705841034.807173]: 
Returned: 1.9203428030014038| 

python DistillationModifier [378 - 1705841036.238938]: Calling loss_update with:
args: 1.1968146562576294| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705841037.7821672]: 
Returned: 2.1558849811553955| 

python DistillationModifier [378 - 1705841039.6304708]: Calling loss_update with:
args: 0.683293879032135| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705841041.0304065]: 
Returned: 1.1376216411590576| 

python DistillationModifier [378 - 1705841042.7218807]: Calling loss_update with:
args: 0.9330255389213562| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705841044.741437]: 
Returned: 1.7201581001281738| 

python DistillationModifier [378 - 1705841045.7045655]: Calling loss_update with:
args: 0.669352650642395| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705841046.4246302]: 
Returned: 1.2993117570877075| 

python DistillationModifier [378 - 1705841048.4506195]: Calling loss_update with:
args: 0.1905820220708847| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [378 - 1705841049.8525653]: 
Returned: 0.09084199368953705| 

python LearningRateFunctionModifier [378 - 1705841052.6830509]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [378 - 1705841052.6832197]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [378 - 1705841052.683278]: 8.076923076923076e-05
python LearningRateFunctionModifier/ParamGroup1 [378 - 1705841052.683515]: 8.076923076923076e-05
python DistillationModifier/task_loss [378 - 1705841052.6838062]: tensor(0.1906, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [378 - 1705841052.6847596]: tensor(0.0908, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [378 - 1705841052.6853015]: tensor(0.0908, grad_fn=<AddBackward0>)
python ConstantPruningModifier [378 - 1705841052.6858165]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.0| steps_per_epoch: 63| 
python ConstantPruningModifier [378 - 1705841052.8996222]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [378 - 1705841052.9003298]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [378 - 1705841052.9012046]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [378 - 1705841052.901925]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [378 - 1705841052.9025636]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [378 - 1705841052.9041982]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [378 - 1705841052.9057193]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [378 - 1705841052.9064338]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [378 - 1705841052.9070487]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [378 - 1705841052.9076324]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [378 - 1705841052.9081256]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [378 - 1705841052.9097078]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [378 - 1705841052.9111829]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [378 - 1705841052.9118645]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [378 - 1705841052.912446]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [378 - 1705841052.9130464]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [378 - 1705841052.9135568]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [378 - 1705841052.915077]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [378 - 1705841052.916541]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [378 - 1705841052.9172196]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [378 - 1705841052.917863]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [378 - 1705841052.918412]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [378 - 1705841052.9190257]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [378 - 1705841052.9205775]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [378 - 1705841052.9220495]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [378 - 1705841052.922707]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [378 - 1705841052.923329]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [378 - 1705841052.9238398]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [378 - 1705841052.924323]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [378 - 1705841052.9258342]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [378 - 1705841052.9272811]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [378 - 1705841052.9279468]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [378 - 1705841052.9285998]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [378 - 1705841052.9292645]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [378 - 1705841052.929797]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [378 - 1705841052.9313622]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [378 - 1705841052.9328692]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [378 - 1705841052.933507]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [378 - 1705841052.9341247]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [378 - 1705841052.934652]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [378 - 1705841052.9352272]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [378 - 1705841052.9367828]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [378 - 1705841052.9382777]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [378 - 1705841052.938928]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [378 - 1705841052.9395232]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [378 - 1705841052.9400992]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [378 - 1705841052.9407141]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [378 - 1705841052.9422734]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [378 - 1705841052.943759]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [378 - 1705841052.9443967]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [378 - 1705841052.9450634]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [378 - 1705841052.9456708]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [378 - 1705841052.946207]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [378 - 1705841052.947754]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [378 - 1705841052.9492252]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [378 - 1705841052.9498684]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [378 - 1705841052.9504948]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [378 - 1705841052.951057]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [378 - 1705841052.9515786]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [378 - 1705841052.9531374]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [378 - 1705841052.9546125]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [378 - 1705841052.9552894]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [378 - 1705841052.9559307]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [378 - 1705841052.9564736]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [378 - 1705841052.9570231]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [378 - 1705841052.9585469]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [378 - 1705841052.9599915]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [378 - 1705841052.960683]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [378 - 1705841052.961268]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [378 - 1705841052.9618454]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [378 - 1705841052.9623466]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [378 - 1705841052.9638753]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [378 - 1705841052.965353]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [378 - 1705841052.966036]: 0.0
python ParamPruning/classifier.weight [378 - 1705841052.9664314]: 0.0
python DistillationModifier [385 - 1705841092.4188313]: Calling loss_update with:
args: 0.2710207998752594| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.111111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [385 - 1705841093.9356296]: 
Returned: 0.3286247253417969| 

python LearningRateFunctionModifier [385 - 1705841096.7637627]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.111111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [385 - 1705841096.7639253]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [385 - 1705841096.7639952]: 7.948717948717948e-05
python LearningRateFunctionModifier/ParamGroup1 [385 - 1705841096.7642267]: 7.948717948717948e-05
python DistillationModifier/task_loss [385 - 1705841096.764536]: tensor(0.2710, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [385 - 1705841096.7655394]: tensor(0.3286, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [385 - 1705841096.7660937]: tensor(0.3286, grad_fn=<AddBackward0>)
python ConstantPruningModifier [385 - 1705841096.7666233]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.111111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [385 - 1705841096.980342]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [385 - 1705841096.9811232]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [385 - 1705841096.981953]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [385 - 1705841096.982551]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [385 - 1705841096.9830737]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [385 - 1705841096.9847293]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [385 - 1705841096.9863772]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [385 - 1705841096.987012]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [385 - 1705841096.987558]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [385 - 1705841096.9880629]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [385 - 1705841096.9885535]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [385 - 1705841096.9901617]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [385 - 1705841096.9917955]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [385 - 1705841096.9924314]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [385 - 1705841096.9930017]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [385 - 1705841096.9935102]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [385 - 1705841096.9940145]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [385 - 1705841096.9956012]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [385 - 1705841096.9972224]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [385 - 1705841096.9978442]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [385 - 1705841096.9983814]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [385 - 1705841096.9988756]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [385 - 1705841096.9993646]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [385 - 1705841097.0009837]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [385 - 1705841097.0026765]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [385 - 1705841097.003326]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [385 - 1705841097.0038965]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [385 - 1705841097.0044062]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [385 - 1705841097.004916]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [385 - 1705841097.0064793]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [385 - 1705841097.008143]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [385 - 1705841097.0087717]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [385 - 1705841097.0093331]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [385 - 1705841097.009832]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [385 - 1705841097.0103238]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [385 - 1705841097.011859]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [385 - 1705841097.0135262]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [385 - 1705841097.0141559]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [385 - 1705841097.0146942]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [385 - 1705841097.0151951]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [385 - 1705841097.0157063]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [385 - 1705841097.017253]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [385 - 1705841097.0188882]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [385 - 1705841097.0194824]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [385 - 1705841097.0200107]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [385 - 1705841097.0205126]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [385 - 1705841097.0210154]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [385 - 1705841097.0225189]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [385 - 1705841097.0241237]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [385 - 1705841097.0247235]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [385 - 1705841097.025251]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [385 - 1705841097.0257368]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [385 - 1705841097.0262175]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [385 - 1705841097.0277264]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [385 - 1705841097.0293477]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [385 - 1705841097.029926]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [385 - 1705841097.0304556]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [385 - 1705841097.030956]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [385 - 1705841097.031434]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [385 - 1705841097.0330224]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [385 - 1705841097.0346212]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [385 - 1705841097.035197]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [385 - 1705841097.0357127]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [385 - 1705841097.0362003]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [385 - 1705841097.0366993]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [385 - 1705841097.0382204]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [385 - 1705841097.0398037]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [385 - 1705841097.0405586]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [385 - 1705841097.0412192]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [385 - 1705841097.0417836]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [385 - 1705841097.042323]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [385 - 1705841097.043994]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [385 - 1705841097.0455914]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [385 - 1705841097.0461798]: 0.0
python ParamPruning/classifier.weight [385 - 1705841097.0465662]: 0.0
python DistillationModifier [392 - 1705841137.8806655]: Calling loss_update with:
args: 0.26220938563346863| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.222222222222222| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [392 - 1705841139.3416753]: 
Returned: 0.10493435710668564| 

python LearningRateFunctionModifier [392 - 1705841142.176433]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.222222222222222| steps_per_epoch: 63| 
python LearningRateFunctionModifier [392 - 1705841142.176612]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [392 - 1705841142.1766846]: 7.82051282051282e-05
python LearningRateFunctionModifier/ParamGroup1 [392 - 1705841142.1769183]: 7.82051282051282e-05
python DistillationModifier/task_loss [392 - 1705841142.1772451]: tensor(0.2622, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [392 - 1705841142.178222]: tensor(0.1049, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [392 - 1705841142.1788626]: tensor(0.1049, grad_fn=<AddBackward0>)
python ConstantPruningModifier [392 - 1705841142.1794832]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.222222222222222| steps_per_epoch: 63| 
python ConstantPruningModifier [392 - 1705841142.3948839]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [392 - 1705841142.3955789]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [392 - 1705841142.3964732]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [392 - 1705841142.3972652]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [392 - 1705841142.397887]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [392 - 1705841142.399711]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [392 - 1705841142.40141]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [392 - 1705841142.4020855]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [392 - 1705841142.4027555]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [392 - 1705841142.4033372]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [392 - 1705841142.4039729]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [392 - 1705841142.4056337]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [392 - 1705841142.4072728]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [392 - 1705841142.4079053]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [392 - 1705841142.4085197]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [392 - 1705841142.4091563]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [392 - 1705841142.4097161]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [392 - 1705841142.4113824]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [392 - 1705841142.4130533]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [392 - 1705841142.4136999]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [392 - 1705841142.4143212]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [392 - 1705841142.4149053]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [392 - 1705841142.4154718]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [392 - 1705841142.4171126]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [392 - 1705841142.4187758]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [392 - 1705841142.419417]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [392 - 1705841142.4200535]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [392 - 1705841142.4206958]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [392 - 1705841142.4212523]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [392 - 1705841142.422876]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [392 - 1705841142.4245236]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [392 - 1705841142.4251869]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [392 - 1705841142.425802]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [392 - 1705841142.426408]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [392 - 1705841142.426953]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [392 - 1705841142.4285903]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [392 - 1705841142.4302347]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [392 - 1705841142.4308524]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [392 - 1705841142.4315104]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [392 - 1705841142.432083]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [392 - 1705841142.4326456]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [392 - 1705841142.43427]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [392 - 1705841142.4359088]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [392 - 1705841142.436537]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [392 - 1705841142.4371748]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [392 - 1705841142.4377744]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [392 - 1705841142.4383378]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [392 - 1705841142.4399729]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [392 - 1705841142.44163]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [392 - 1705841142.4422731]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [392 - 1705841142.4429088]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [392 - 1705841142.4435368]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [392 - 1705841142.4441633]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [392 - 1705841142.4458482]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [392 - 1705841142.4475229]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [392 - 1705841142.4481914]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [392 - 1705841142.4488473]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [392 - 1705841142.4495134]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [392 - 1705841142.4501443]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [392 - 1705841142.4518087]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [392 - 1705841142.4534864]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [392 - 1705841142.4541402]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [392 - 1705841142.4547822]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [392 - 1705841142.4553554]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [392 - 1705841142.4559271]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [392 - 1705841142.4575536]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [392 - 1705841142.4590147]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [392 - 1705841142.459707]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [392 - 1705841142.460355]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [392 - 1705841142.4609768]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [392 - 1705841142.461565]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [392 - 1705841142.4632165]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [392 - 1705841142.4649234]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [392 - 1705841142.4656138]: 0.0
python ParamPruning/classifier.weight [392 - 1705841142.4660242]: 0.0
python DistillationModifier [399 - 1705841183.2632074]: Calling loss_update with:
args: 0.18912290036678314| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.333333333333333| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [399 - 1705841184.801544]: 
Returned: 0.2948644757270813| 

python LearningRateFunctionModifier [399 - 1705841187.6206229]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.333333333333333| steps_per_epoch: 63| 
python LearningRateFunctionModifier [399 - 1705841187.620809]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [399 - 1705841187.62087]: 7.692307692307691e-05
python LearningRateFunctionModifier/ParamGroup1 [399 - 1705841187.6211054]: 7.692307692307691e-05
python DistillationModifier/task_loss [399 - 1705841187.621405]: tensor(0.1891, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [399 - 1705841187.622351]: tensor(0.2949, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [399 - 1705841187.622902]: tensor(0.2949, grad_fn=<AddBackward0>)
python ConstantPruningModifier [399 - 1705841187.6234381]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.333333333333333| steps_per_epoch: 63| 
python ConstantPruningModifier [399 - 1705841187.8359883]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [399 - 1705841187.8367462]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [399 - 1705841187.8376265]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [399 - 1705841187.8383036]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [399 - 1705841187.8389676]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [399 - 1705841187.8405983]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [399 - 1705841187.8422787]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [399 - 1705841187.8428912]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [399 - 1705841187.8435063]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [399 - 1705841187.8440418]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [399 - 1705841187.84457]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [399 - 1705841187.8462272]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [399 - 1705841187.8476746]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [399 - 1705841187.8483]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [399 - 1705841187.8489473]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [399 - 1705841187.8494983]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [399 - 1705841187.8500204]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [399 - 1705841187.851484]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [399 - 1705841187.8529701]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [399 - 1705841187.8536303]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [399 - 1705841187.85425]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [399 - 1705841187.8547852]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [399 - 1705841187.8552837]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [399 - 1705841187.8567634]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [399 - 1705841187.858142]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [399 - 1705841187.858776]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [399 - 1705841187.8593946]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [399 - 1705841187.8599477]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [399 - 1705841187.860493]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [399 - 1705841187.8620336]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [399 - 1705841187.863481]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [399 - 1705841187.8640978]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [399 - 1705841187.8646374]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [399 - 1705841187.865168]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [399 - 1705841187.865659]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [399 - 1705841187.8670924]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [399 - 1705841187.8687909]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [399 - 1705841187.8693905]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [399 - 1705841187.869997]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [399 - 1705841187.8706222]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [399 - 1705841187.8711586]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [399 - 1705841187.872768]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [399 - 1705841187.8742087]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [399 - 1705841187.874836]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [399 - 1705841187.8753693]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [399 - 1705841187.8758678]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [399 - 1705841187.8764455]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [399 - 1705841187.8779573]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [399 - 1705841187.8796024]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [399 - 1705841187.8802311]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [399 - 1705841187.8807743]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [399 - 1705841187.8813605]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [399 - 1705841187.8818495]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [399 - 1705841187.883444]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [399 - 1705841187.8849103]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [399 - 1705841187.8855214]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [399 - 1705841187.8861117]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [399 - 1705841187.8866477]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [399 - 1705841187.8871639]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [399 - 1705841187.888607]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [399 - 1705841187.8900373]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [399 - 1705841187.8906527]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [399 - 1705841187.891188]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [399 - 1705841187.891703]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [399 - 1705841187.892205]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [399 - 1705841187.8936143]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [399 - 1705841187.895024]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [399 - 1705841187.8956745]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [399 - 1705841187.8962283]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [399 - 1705841187.8968601]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [399 - 1705841187.8973596]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [399 - 1705841187.8988292]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [399 - 1705841187.900181]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [399 - 1705841187.9008474]: 0.0
python ParamPruning/classifier.weight [399 - 1705841187.9012337]: 0.0
python DistillationModifier [406 - 1705841228.168771]: Calling loss_update with:
args: 0.18459923565387726| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.444444444444445| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [406 - 1705841229.6092412]: 
Returned: 0.3283424377441406| 

python LearningRateFunctionModifier [406 - 1705841233.0611265]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.444444444444445| steps_per_epoch: 63| 
python LearningRateFunctionModifier [406 - 1705841233.0612981]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [406 - 1705841233.0613573]: 7.564102564102563e-05
python LearningRateFunctionModifier/ParamGroup1 [406 - 1705841233.0616043]: 7.564102564102563e-05
python DistillationModifier/task_loss [406 - 1705841233.061912]: tensor(0.1846, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [406 - 1705841233.0629363]: tensor(0.3283, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [406 - 1705841233.0635023]: tensor(0.3283, grad_fn=<AddBackward0>)
python ConstantPruningModifier [406 - 1705841233.0640495]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.444444444444445| steps_per_epoch: 63| 
python ConstantPruningModifier [406 - 1705841233.2804594]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [406 - 1705841233.2812982]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [406 - 1705841233.2922058]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [406 - 1705841233.2929666]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [406 - 1705841233.2935622]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [406 - 1705841233.2952836]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [406 - 1705841233.2968278]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [406 - 1705841233.2975304]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [406 - 1705841233.298138]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [406 - 1705841233.2986865]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [406 - 1705841233.2991934]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [406 - 1705841233.300762]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [406 - 1705841233.3024316]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [406 - 1705841233.303121]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [406 - 1705841233.303686]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [406 - 1705841233.304199]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [406 - 1705841233.3047295]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [406 - 1705841233.3063703]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [406 - 1705841233.3080072]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [406 - 1705841233.3086843]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [406 - 1705841233.3093092]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [406 - 1705841233.309874]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [406 - 1705841233.3103836]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [406 - 1705841233.3121183]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [406 - 1705841233.3138332]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [406 - 1705841233.3145258]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [406 - 1705841233.3150883]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [406 - 1705841233.315699]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [406 - 1705841233.316212]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [406 - 1705841233.317953]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [406 - 1705841233.3196228]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [406 - 1705841233.3202727]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [406 - 1705841233.3209176]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [406 - 1705841233.3214686]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [406 - 1705841233.3219738]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [406 - 1705841233.3237338]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [406 - 1705841233.325666]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [406 - 1705841233.3265324]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [406 - 1705841233.3271666]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [406 - 1705841233.3278246]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [406 - 1705841233.3284771]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [406 - 1705841233.3302677]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [406 - 1705841233.33202]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [406 - 1705841233.332777]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [406 - 1705841233.3335009]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [406 - 1705841233.3341193]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [406 - 1705841233.3346732]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [406 - 1705841233.336518]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [406 - 1705841233.3382442]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [406 - 1705841233.3389149]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [406 - 1705841233.3395503]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [406 - 1705841233.3401105]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [406 - 1705841233.3406634]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [406 - 1705841233.3424094]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [406 - 1705841233.344084]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [406 - 1705841233.3447978]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [406 - 1705841233.3453887]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [406 - 1705841233.3459187]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [406 - 1705841233.3464227]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [406 - 1705841233.348134]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [406 - 1705841233.3497694]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [406 - 1705841233.3504124]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [406 - 1705841233.3509638]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [406 - 1705841233.351568]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [406 - 1705841233.3521087]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [406 - 1705841233.3538575]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [406 - 1705841233.355385]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [406 - 1705841233.3560627]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [406 - 1705841233.3566835]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [406 - 1705841233.3572366]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [406 - 1705841233.35784]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [406 - 1705841233.3594124]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [406 - 1705841233.3611307]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [406 - 1705841233.36183]: 0.0
python ParamPruning/classifier.weight [406 - 1705841233.362229]: 0.0
python DistillationModifier [413 - 1705841272.9107764]: Calling loss_update with:
args: 0.4489801228046417| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.555555555555555| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [413 - 1705841274.317166]: 
Returned: 0.22000381350517273| 

python LearningRateFunctionModifier [413 - 1705841277.1562688]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.555555555555555| steps_per_epoch: 63| 
python LearningRateFunctionModifier [413 - 1705841277.1564329]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [413 - 1705841277.156496]: 7.435897435897436e-05
python LearningRateFunctionModifier/ParamGroup1 [413 - 1705841277.1567514]: 7.435897435897436e-05
python DistillationModifier/task_loss [413 - 1705841277.157075]: tensor(0.4490, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [413 - 1705841277.1580489]: tensor(0.2200, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [413 - 1705841277.1585839]: tensor(0.2200, grad_fn=<AddBackward0>)
python ConstantPruningModifier [413 - 1705841277.1591094]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.555555555555555| steps_per_epoch: 63| 
python ConstantPruningModifier [413 - 1705841277.3750134]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [413 - 1705841277.3757691]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [413 - 1705841277.3766592]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [413 - 1705841277.3774054]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [413 - 1705841277.3779855]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [413 - 1705841277.3796995]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [413 - 1705841277.3813727]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [413 - 1705841277.3820262]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [413 - 1705841277.3826883]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [413 - 1705841277.3832293]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [413 - 1705841277.3837695]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [413 - 1705841277.3854084]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [413 - 1705841277.387062]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [413 - 1705841277.3876975]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [413 - 1705841277.3882806]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [413 - 1705841277.388839]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [413 - 1705841277.3893602]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [413 - 1705841277.391012]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [413 - 1705841277.3926938]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [413 - 1705841277.3933163]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [413 - 1705841277.3939216]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [413 - 1705841277.3944554]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [413 - 1705841277.3949928]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [413 - 1705841277.3966243]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [413 - 1705841277.3982856]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [413 - 1705841277.3988895]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [413 - 1705841277.3994808]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [413 - 1705841277.400048]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [413 - 1705841277.4005768]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [413 - 1705841277.402215]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [413 - 1705841277.403898]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [413 - 1705841277.4045036]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [413 - 1705841277.4051192]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [413 - 1705841277.4056566]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [413 - 1705841277.4062557]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [413 - 1705841277.4078243]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [413 - 1705841277.409504]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [413 - 1705841277.410159]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [413 - 1705841277.4107842]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [413 - 1705841277.4113326]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [413 - 1705841277.4119058]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [413 - 1705841277.4135437]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [413 - 1705841277.4152277]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [413 - 1705841277.4158754]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [413 - 1705841277.4165149]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [413 - 1705841277.4171538]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [413 - 1705841277.4177065]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [413 - 1705841277.4192803]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [413 - 1705841277.4210258]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [413 - 1705841277.4216537]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [413 - 1705841277.4222739]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [413 - 1705841277.422823]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [413 - 1705841277.423363]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [413 - 1705841277.4249527]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [413 - 1705841277.4266489]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [413 - 1705841277.4273112]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [413 - 1705841277.427937]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [413 - 1705841277.4284725]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [413 - 1705841277.4290388]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [413 - 1705841277.4306133]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [413 - 1705841277.4322915]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [413 - 1705841277.432953]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [413 - 1705841277.4335785]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [413 - 1705841277.4341304]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [413 - 1705841277.4346719]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [413 - 1705841277.436252]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [413 - 1705841277.4380147]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [413 - 1705841277.4387033]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [413 - 1705841277.4393277]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [413 - 1705841277.4398975]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [413 - 1705841277.4405112]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [413 - 1705841277.4422472]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [413 - 1705841277.4439049]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [413 - 1705841277.4445188]: 0.0
python ParamPruning/classifier.weight [413 - 1705841277.4449334]: 0.0
python DistillationModifier [420 - 1705841318.3068717]: Calling loss_update with:
args: 0.21743081510066986| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.666666666666667| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [420 - 1705841319.7988768]: 
Returned: 0.3771369755268097| 

python LearningRateFunctionModifier [420 - 1705841322.6629171]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.666666666666667| steps_per_epoch: 63| 
python LearningRateFunctionModifier [420 - 1705841322.663088]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [420 - 1705841322.6631505]: 7.307692307692306e-05
python LearningRateFunctionModifier/ParamGroup1 [420 - 1705841322.6633954]: 7.307692307692306e-05
python DistillationModifier/task_loss [420 - 1705841322.6638305]: tensor(0.2174, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [420 - 1705841322.6648622]: tensor(0.3771, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [420 - 1705841322.665553]: tensor(0.3771, grad_fn=<AddBackward0>)
python ConstantPruningModifier [420 - 1705841322.6661918]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.666666666666667| steps_per_epoch: 63| 
python ConstantPruningModifier [420 - 1705841322.8829787]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [420 - 1705841322.88375]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [420 - 1705841322.8846068]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [420 - 1705841322.8853729]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [420 - 1705841322.8859832]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [420 - 1705841322.8877132]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [420 - 1705841322.889449]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [420 - 1705841322.8901312]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [420 - 1705841322.8907986]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [420 - 1705841322.8914719]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [420 - 1705841322.8920894]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [420 - 1705841322.8938177]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [420 - 1705841322.8955643]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [420 - 1705841322.8962412]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [420 - 1705841322.8969]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [420 - 1705841322.8974922]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [420 - 1705841322.8980918]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [420 - 1705841322.8997872]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [420 - 1705841322.9015772]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [420 - 1705841322.9022655]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [420 - 1705841322.90291]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [420 - 1705841322.9035602]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [420 - 1705841322.9041321]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [420 - 1705841322.90585]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [420 - 1705841322.9075685]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [420 - 1705841322.9082386]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [420 - 1705841322.9088962]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [420 - 1705841322.9094682]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [420 - 1705841322.9101033]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [420 - 1705841322.911825]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [420 - 1705841322.9135818]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [420 - 1705841322.9143057]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [420 - 1705841322.914882]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [420 - 1705841322.9155045]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [420 - 1705841322.9160745]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [420 - 1705841322.917761]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [420 - 1705841322.919278]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [420 - 1705841322.919959]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [420 - 1705841322.9206421]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [420 - 1705841322.9212928]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [420 - 1705841322.9219363]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [420 - 1705841322.923577]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [420 - 1705841322.9253392]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [420 - 1705841322.926036]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [420 - 1705841322.926695]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [420 - 1705841322.927281]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [420 - 1705841322.9278443]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [420 - 1705841322.929525]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [420 - 1705841322.9312413]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [420 - 1705841322.9319015]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [420 - 1705841322.9325483]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [420 - 1705841322.9331422]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [420 - 1705841322.9337795]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [420 - 1705841322.9354463]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [420 - 1705841322.9371696]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [420 - 1705841322.9378042]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [420 - 1705841322.9384513]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [420 - 1705841322.9390192]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [420 - 1705841322.9395509]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [420 - 1705841322.941234]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [420 - 1705841322.9429426]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [420 - 1705841322.9436328]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [420 - 1705841322.944255]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [420 - 1705841322.944855]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [420 - 1705841322.945478]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [420 - 1705841322.9471457]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [420 - 1705841322.9488728]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [420 - 1705841322.9495516]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [420 - 1705841322.9501898]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [420 - 1705841322.950767]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [420 - 1705841322.9513903]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [420 - 1705841322.9530935]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [420 - 1705841322.9548671]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [420 - 1705841322.95552]: 0.0
python ParamPruning/classifier.weight [420 - 1705841322.9559216]: 0.0
python DistillationModifier [427 - 1705841363.8134754]: Calling loss_update with:
args: 0.18938961625099182| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.777777777777778| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [427 - 1705841365.343249]: 
Returned: 0.291018545627594| 

python LearningRateFunctionModifier [427 - 1705841368.606573]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.777777777777778| steps_per_epoch: 63| 
python LearningRateFunctionModifier [427 - 1705841368.6067297]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [427 - 1705841368.6067889]: 7.179487179487178e-05
python LearningRateFunctionModifier/ParamGroup1 [427 - 1705841368.607031]: 7.179487179487178e-05
python DistillationModifier/task_loss [427 - 1705841368.6073334]: tensor(0.1894, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [427 - 1705841368.6084487]: tensor(0.2910, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [427 - 1705841368.6091397]: tensor(0.2910, grad_fn=<AddBackward0>)
python ConstantPruningModifier [427 - 1705841368.6098192]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.777777777777778| steps_per_epoch: 63| 
python ConstantPruningModifier [427 - 1705841368.9422925]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [427 - 1705841368.9430976]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [427 - 1705841368.9554281]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [427 - 1705841368.9561074]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [427 - 1705841368.9567716]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [427 - 1705841368.9584892]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [427 - 1705841368.960165]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [427 - 1705841368.960852]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [427 - 1705841368.961493]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [427 - 1705841368.9620986]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [427 - 1705841368.962654]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [427 - 1705841368.9642935]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [427 - 1705841368.9660256]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [427 - 1705841368.9666777]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [427 - 1705841368.967277]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [427 - 1705841368.9678562]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [427 - 1705841368.9683871]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [427 - 1705841368.970079]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [427 - 1705841368.9717295]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [427 - 1705841368.972342]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [427 - 1705841368.9729161]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [427 - 1705841368.9735086]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [427 - 1705841368.9739988]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [427 - 1705841368.9756563]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [427 - 1705841368.9773233]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [427 - 1705841368.977963]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [427 - 1705841368.978614]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [427 - 1705841368.9791937]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [427 - 1705841368.9797258]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [427 - 1705841368.981397]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [427 - 1705841368.9830492]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [427 - 1705841368.9836721]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [427 - 1705841368.9842663]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [427 - 1705841368.9848132]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [427 - 1705841368.98535]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [427 - 1705841368.9869747]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [427 - 1705841368.9886296]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [427 - 1705841368.9892514]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [427 - 1705841368.9898467]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [427 - 1705841368.9903865]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [427 - 1705841368.9909277]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [427 - 1705841368.9925756]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [427 - 1705841368.9942534]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [427 - 1705841368.9948797]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [427 - 1705841368.9955413]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [427 - 1705841368.9960845]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [427 - 1705841368.9966843]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [427 - 1705841368.9983416]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [427 - 1705841369.0000253]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [427 - 1705841369.000697]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [427 - 1705841369.001322]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [427 - 1705841369.0019014]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [427 - 1705841369.0025132]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [427 - 1705841369.0041995]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [427 - 1705841369.0059192]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [427 - 1705841369.0065813]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [427 - 1705841369.0072083]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [427 - 1705841369.0077696]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [427 - 1705841369.0083258]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [427 - 1705841369.0100129]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [427 - 1705841369.0117254]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [427 - 1705841369.0123832]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [427 - 1705841369.0130074]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [427 - 1705841369.0135462]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [427 - 1705841369.014085]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [427 - 1705841369.0157118]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [427 - 1705841369.017387]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [427 - 1705841369.0180206]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [427 - 1705841369.018616]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [427 - 1705841369.019193]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [427 - 1705841369.0197413]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [427 - 1705841369.0213947]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [427 - 1705841369.0230541]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [427 - 1705841369.023661]: 0.0
python ParamPruning/classifier.weight [427 - 1705841369.0240521]: 0.0
python DistillationModifier [434 - 1705841409.5644813]: Calling loss_update with:
args: 0.11904019117355347| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 6.888888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [434 - 1705841411.086359]: 
Returned: 0.3579201102256775| 

python LearningRateFunctionModifier [434 - 1705841413.9643517]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.888888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [434 - 1705841413.9645913]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [434 - 1705841413.96469]: 7.051282051282051e-05
python LearningRateFunctionModifier/ParamGroup1 [434 - 1705841413.9649277]: 7.051282051282051e-05
python DistillationModifier/task_loss [434 - 1705841413.965091]: tensor(0.1190, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [434 - 1705841413.9662313]: tensor(0.3579, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [434 - 1705841413.9668324]: tensor(0.3579, grad_fn=<AddBackward0>)
python ConstantPruningModifier [434 - 1705841413.9674358]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 6.888888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [434 - 1705841414.1876]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [434 - 1705841414.1883821]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [434 - 1705841414.189311]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [434 - 1705841414.1900036]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [434 - 1705841414.190628]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [434 - 1705841414.1923714]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [434 - 1705841414.193969]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [434 - 1705841414.1947384]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [434 - 1705841414.1953707]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [434 - 1705841414.1959167]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [434 - 1705841414.196529]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [434 - 1705841414.1981056]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [434 - 1705841414.1998813]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [434 - 1705841414.2005901]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [434 - 1705841414.2011712]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [434 - 1705841414.201782]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [434 - 1705841414.2022946]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [434 - 1705841414.2038946]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [434 - 1705841414.2053998]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [434 - 1705841414.20609]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [434 - 1705841414.2066638]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [434 - 1705841414.2072773]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [434 - 1705841414.2078905]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [434 - 1705841414.209562]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [434 - 1705841414.2112987]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [434 - 1705841414.2120173]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [434 - 1705841414.2126844]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [434 - 1705841414.2132545]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [434 - 1705841414.213865]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [434 - 1705841414.2155485]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [434 - 1705841414.2172828]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [434 - 1705841414.2179825]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [434 - 1705841414.2186491]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [434 - 1705841414.2192156]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [434 - 1705841414.219734]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [434 - 1705841414.221413]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [434 - 1705841414.2229397]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [434 - 1705841414.2236526]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [434 - 1705841414.2242734]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [434 - 1705841414.2249537]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [434 - 1705841414.2254982]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [434 - 1705841414.22712]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [434 - 1705841414.2286353]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [434 - 1705841414.2294288]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [434 - 1705841414.230081]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [434 - 1705841414.2306404]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [434 - 1705841414.2312415]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [434 - 1705841414.2329237]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [434 - 1705841414.2347534]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [434 - 1705841414.2354934]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [434 - 1705841414.2361045]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [434 - 1705841414.2366421]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [434 - 1705841414.2371693]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [434 - 1705841414.2387989]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [434 - 1705841414.2402976]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [434 - 1705841414.2409909]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [434 - 1705841414.241638]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [434 - 1705841414.2422543]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [434 - 1705841414.2428653]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [434 - 1705841414.244538]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [434 - 1705841414.2460446]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [434 - 1705841414.2467592]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [434 - 1705841414.2474277]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [434 - 1705841414.2480154]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [434 - 1705841414.2485306]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [434 - 1705841414.2501013]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [434 - 1705841414.2515655]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [434 - 1705841414.252316]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [434 - 1705841414.2530196]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [434 - 1705841414.2535727]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [434 - 1705841414.2541945]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [434 - 1705841414.2558494]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [434 - 1705841414.2573836]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [434 - 1705841414.2581441]: 0.0
python ParamPruning/classifier.weight [434 - 1705841414.258587]: 0.0
python DistillationModifier [441 - 1705841451.7365215]: Calling loss_update with:
args: 0.7083742022514343| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841453.1747382]: 
Returned: 1.320917010307312| 

python DistillationModifier [441 - 1705841454.643648]: Calling loss_update with:
args: 1.41531240940094| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841456.0750344]: 
Returned: 2.341442823410034| 

python DistillationModifier [441 - 1705841457.5420408]: Calling loss_update with:
args: 1.2877399921417236| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841458.978686]: 
Returned: 1.4896377325057983| 

python DistillationModifier [441 - 1705841460.4440901]: Calling loss_update with:
args: 0.6337738633155823| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841462.0103629]: 
Returned: 0.8961727023124695| 

python DistillationModifier [441 - 1705841464.0854554]: Calling loss_update with:
args: 1.0648101568222046| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841465.9594624]: 
Returned: 1.5655732154846191| 

python DistillationModifier [441 - 1705841467.432085]: Calling loss_update with:
args: 0.8480115532875061| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841468.8945153]: 
Returned: 1.5257420539855957| 

python DistillationModifier [441 - 1705841470.405161]: Calling loss_update with:
args: 0.27730146050453186| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841471.848106]: 
Returned: 0.8137990236282349| 

python DistillationModifier [441 - 1705841473.3184586]: Calling loss_update with:
args: 1.0778231620788574| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841474.7597528]: 
Returned: 1.5954521894454956| 

python DistillationModifier [441 - 1705841476.2335277]: Calling loss_update with:
args: 0.7165806293487549| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841477.672695]: 
Returned: 0.6419550180435181| 

python DistillationModifier [441 - 1705841479.1456175]: Calling loss_update with:
args: 0.7046878337860107| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841480.5927322]: 
Returned: 1.1493213176727295| 

python DistillationModifier [441 - 1705841482.0616732]: Calling loss_update with:
args: 0.8598542213439941| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841483.5042503]: 
Returned: 1.431532382965088| 

python DistillationModifier [441 - 1705841484.9724407]: Calling loss_update with:
args: 0.7716807723045349| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841486.4095597]: 
Returned: 1.4134948253631592| 

python DistillationModifier [441 - 1705841487.8843684]: Calling loss_update with:
args: 0.8078300952911377| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841489.3300865]: 
Returned: 1.4765963554382324| 

python DistillationModifier [441 - 1705841490.8008032]: Calling loss_update with:
args: 0.7371752858161926| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841492.2530982]: 
Returned: 0.6566291451454163| 

python DistillationModifier [441 - 1705841493.7275953]: Calling loss_update with:
args: 1.0928478240966797| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841495.1685271]: 
Returned: 1.5895915031433105| 

python DistillationModifier [441 - 1705841496.639014]: Calling loss_update with:
args: 0.9225497245788574| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841498.0859601]: 
Returned: 1.6300582885742188| 

python DistillationModifier [441 - 1705841499.5605106]: Calling loss_update with:
args: 0.6993960738182068| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841501.5746975]: 
Returned: 0.9951985478401184| 

python DistillationModifier [441 - 1705841503.0733068]: Calling loss_update with:
args: 1.0483111143112183| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841504.5070105]: 
Returned: 1.8476824760437012| 

python DistillationModifier [441 - 1705841505.9756424]: Calling loss_update with:
args: 1.0948444604873657| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841507.412009]: 
Returned: 1.298379898071289| 

python DistillationModifier [441 - 1705841508.8879876]: Calling loss_update with:
args: 0.8021694421768188| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841510.3378673]: 
Returned: 1.3096327781677246| 

python DistillationModifier [441 - 1705841511.8042274]: Calling loss_update with:
args: 0.7575007677078247| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841513.2425125]: 
Returned: 1.4301543235778809| 

python DistillationModifier [441 - 1705841514.7175741]: Calling loss_update with:
args: 0.6060876846313477| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841516.1588826]: 
Returned: 0.8866592645645142| 

python DistillationModifier [441 - 1705841517.6301222]: Calling loss_update with:
args: 0.7576792240142822| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841519.064195]: 
Returned: 1.2090495824813843| 

python DistillationModifier [441 - 1705841520.5299811]: Calling loss_update with:
args: 0.7042457461357117| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841522.1525378]: 
Returned: 1.3403505086898804| 

python DistillationModifier [441 - 1705841524.1993947]: Calling loss_update with:
args: 1.1145873069763184| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841526.0461974]: 
Returned: 1.3377560377120972| 

python DistillationModifier [441 - 1705841527.5264132]: Calling loss_update with:
args: 0.5842916369438171| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841528.9709196]: 
Returned: 1.0406646728515625| 

python DistillationModifier [441 - 1705841530.4961958]: Calling loss_update with:
args: 1.011534333229065| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841531.9468925]: 
Returned: 1.3546236753463745| 

python DistillationModifier [441 - 1705841533.4313881]: Calling loss_update with:
args: 0.7410734295845032| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841534.8772702]: 
Returned: 1.1503524780273438| 

python DistillationModifier [441 - 1705841536.3595507]: Calling loss_update with:
args: 0.6755962371826172| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841537.8075302]: 
Returned: 1.3437669277191162| 

python DistillationModifier [441 - 1705841539.2854052]: Calling loss_update with:
args: 0.5250423550605774| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841540.7406576]: 
Returned: 0.9184188842773438| 

python DistillationModifier [441 - 1705841542.2222047]: Calling loss_update with:
args: 1.0791875123977661| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841543.6709988]: 
Returned: 1.9522686004638672| 

python DistillationModifier [441 - 1705841545.1441429]: Calling loss_update with:
args: 0.8072755932807922| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841546.596073]: 
Returned: 0.9607288241386414| 

python DistillationModifier [441 - 1705841548.075831]: Calling loss_update with:
args: 1.3497605323791504| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841549.5193715]: 
Returned: 2.2240307331085205| 

python DistillationModifier [441 - 1705841551.0069394]: Calling loss_update with:
args: 1.1919949054718018| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841552.4544752]: 
Returned: 1.8274770975112915| 

python DistillationModifier [441 - 1705841553.9513574]: Calling loss_update with:
args: 1.2768782377243042| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841555.4067366]: 
Returned: 1.6123486757278442| 

python DistillationModifier [441 - 1705841556.8877928]: Calling loss_update with:
args: 1.0543906688690186| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841558.3393383]: 
Returned: 1.962874412536621| 

python DistillationModifier [441 - 1705841559.8195875]: Calling loss_update with:
args: 1.1234581470489502| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841561.3048723]: 
Returned: 1.1706807613372803| 

python DistillationModifier [441 - 1705841562.7866805]: Calling loss_update with:
args: 0.46052491664886475| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841564.597585]: 
Returned: 1.0117385387420654| 

python DistillationModifier [441 - 1705841566.3154197]: Calling loss_update with:
args: 0.8311412930488586| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841567.7641587]: 
Returned: 1.5275896787643433| 

python DistillationModifier [441 - 1705841569.2421064]: Calling loss_update with:
args: 0.9362083673477173| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841570.6895428]: 
Returned: 1.8676409721374512| 

python DistillationModifier [441 - 1705841572.1642182]: Calling loss_update with:
args: 0.5438323616981506| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841573.6189773]: 
Returned: 1.1566202640533447| 

python DistillationModifier [441 - 1705841575.107903]: Calling loss_update with:
args: 0.789729654788971| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841576.5581634]: 
Returned: 1.7361732721328735| 

python DistillationModifier [441 - 1705841578.0552504]: Calling loss_update with:
args: 0.7004202604293823| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841579.4960587]: 
Returned: 0.8422982096672058| 

python DistillationModifier [441 - 1705841580.9637494]: Calling loss_update with:
args: 0.6792469024658203| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841582.7490232]: 
Returned: 1.5293561220169067| 

python DistillationModifier [441 - 1705841584.810119]: Calling loss_update with:
args: 1.0001593828201294| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841586.485765]: 
Returned: 2.009995460510254| 

python DistillationModifier [441 - 1705841587.9751227]: Calling loss_update with:
args: 1.189239263534546| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841589.4182408]: 
Returned: 1.6622331142425537| 

python DistillationModifier [441 - 1705841590.8953302]: Calling loss_update with:
args: 0.5580728650093079| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841592.3827052]: 
Returned: 1.3677659034729004| 

python DistillationModifier [441 - 1705841593.8675454]: Calling loss_update with:
args: 0.189241424202919| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841595.3103793]: 
Returned: 0.48256203532218933| 

python DistillationModifier [441 - 1705841596.7860367]: Calling loss_update with:
args: 1.2745248079299927| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841598.2322457]: 
Returned: 2.336754322052002| 

python DistillationModifier [441 - 1705841599.7032316]: Calling loss_update with:
args: 0.6394134759902954| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841601.1578698]: 
Returned: 1.251449465751648| 

python DistillationModifier [441 - 1705841602.6497347]: Calling loss_update with:
args: 0.3402760326862335| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841604.1229055]: 
Returned: 0.7047825455665588| 

python DistillationModifier [441 - 1705841605.5943508]: Calling loss_update with:
args: 1.5748647451400757| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841607.0275087]: 
Returned: 1.457472324371338| 

python DistillationModifier [441 - 1705841608.5115988]: Calling loss_update with:
args: 0.7195621132850647| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841609.9465435]: 
Returned: 0.9994168877601624| 

python DistillationModifier [441 - 1705841611.424006]: Calling loss_update with:
args: 0.7826469540596008| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841612.8677979]: 
Returned: 1.2029452323913574| 

python DistillationModifier [441 - 1705841614.3511403]: Calling loss_update with:
args: 0.4482678174972534| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841615.7878902]: 
Returned: 1.0967003107070923| 

python DistillationModifier [441 - 1705841617.2517796]: Calling loss_update with:
args: 0.9906949996948242| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841618.6624553]: 
Returned: 1.4038413763046265| 

python DistillationModifier [441 - 1705841620.0945308]: Calling loss_update with:
args: 1.0444279909133911| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841621.5342941]: 
Returned: 0.9376184940338135| 

python DistillationModifier [441 - 1705841622.9813921]: Calling loss_update with:
args: 1.1201168298721313| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841624.3889403]: 
Returned: 1.2553575038909912| 

python DistillationModifier [441 - 1705841625.836906]: Calling loss_update with:
args: 1.2324410676956177| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841627.2478495]: 
Returned: 2.1959104537963867| 

python DistillationModifier [441 - 1705841628.6970613]: Calling loss_update with:
args: 1.009525179862976| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841630.1145504]: 
Returned: 1.828608512878418| 

python DistillationModifier [441 - 1705841631.5619037]: Calling loss_update with:
args: 0.7050095796585083| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841632.9782403]: 
Returned: 1.1115987300872803| 

python DistillationModifier [441 - 1705841634.424644]: Calling loss_update with:
args: 0.7394337058067322| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841635.876747]: 
Returned: 1.301894187927246| 

python DistillationModifier [441 - 1705841636.9409666]: Calling loss_update with:
args: 0.7843016982078552| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841637.9066272]: 
Returned: 1.496752381324768| 

python DistillationModifier [441 - 1705841639.956225]: Calling loss_update with:
args: 0.24224083125591278| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [441 - 1705841641.3924477]: 
Returned: 0.35315045714378357| 

python LearningRateFunctionModifier [441 - 1705841645.3985372]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [441 - 1705841645.3987067]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [441 - 1705841645.3987772]: 6.923076923076922e-05
python LearningRateFunctionModifier/ParamGroup1 [441 - 1705841645.3990195]: 6.923076923076922e-05
python DistillationModifier/task_loss [441 - 1705841645.3993392]: tensor(0.2422, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [441 - 1705841645.4005494]: tensor(0.3532, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [441 - 1705841645.4013546]: tensor(0.3532, grad_fn=<AddBackward0>)
python ConstantPruningModifier [441 - 1705841645.402014]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.0| steps_per_epoch: 63| 
python ConstantPruningModifier [441 - 1705841645.6333578]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [441 - 1705841645.634148]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [441 - 1705841645.635099]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [441 - 1705841645.6359773]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [441 - 1705841645.6367185]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [441 - 1705841645.6385472]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [441 - 1705841645.6404247]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [441 - 1705841645.6413262]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [441 - 1705841645.642105]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [441 - 1705841645.6428242]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [441 - 1705841645.6435032]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [441 - 1705841645.6452475]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [441 - 1705841645.6470635]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [441 - 1705841645.647847]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [441 - 1705841645.6485598]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [441 - 1705841645.6492364]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [441 - 1705841645.6499138]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [441 - 1705841645.6516588]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [441 - 1705841645.653478]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [441 - 1705841645.654318]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [441 - 1705841645.6550763]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [441 - 1705841645.6557276]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [441 - 1705841645.6565802]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [441 - 1705841645.659314]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [441 - 1705841645.6620913]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [441 - 1705841645.6631281]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [441 - 1705841645.664047]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [441 - 1705841645.6650295]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [441 - 1705841645.6658995]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [441 - 1705841645.6685753]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [441 - 1705841645.6712837]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [441 - 1705841645.6722846]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [441 - 1705841645.6732237]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [441 - 1705841645.674131]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [441 - 1705841645.6748955]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [441 - 1705841645.677569]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [441 - 1705841645.680267]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [441 - 1705841645.6812694]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [441 - 1705841645.6821914]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [441 - 1705841645.6830122]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [441 - 1705841645.6837776]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [441 - 1705841645.6864398]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [441 - 1705841645.6891718]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [441 - 1705841645.6900864]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [441 - 1705841645.690955]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [441 - 1705841645.6917431]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [441 - 1705841645.6926863]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [441 - 1705841645.6953638]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [441 - 1705841645.6981983]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [441 - 1705841645.6993034]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [441 - 1705841645.7002816]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [441 - 1705841645.701262]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [441 - 1705841645.7021914]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [441 - 1705841645.7048883]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [441 - 1705841645.707595]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [441 - 1705841645.708566]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [441 - 1705841645.7094882]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [441 - 1705841645.7102878]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [441 - 1705841645.7111876]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [441 - 1705841645.7138877]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [441 - 1705841645.7166016]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [441 - 1705841645.7173958]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [441 - 1705841645.7181056]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [441 - 1705841645.71872]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [441 - 1705841645.7192883]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [441 - 1705841645.7210348]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [441 - 1705841645.7228432]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [441 - 1705841645.7235885]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [441 - 1705841645.7242746]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [441 - 1705841645.7249374]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [441 - 1705841645.7255301]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [441 - 1705841645.7272422]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [441 - 1705841645.7288432]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [441 - 1705841645.7296355]: 0.0
python ParamPruning/classifier.weight [441 - 1705841645.7300885]: 0.0
python DistillationModifier [448 - 1705841686.3955534]: Calling loss_update with:
args: 0.22904092073440552| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.111111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [448 - 1705841687.8811514]: 
Returned: 0.3808799982070923| 

python LearningRateFunctionModifier [448 - 1705841690.7951038]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.111111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [448 - 1705841690.7952847]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [448 - 1705841690.7953725]: 6.794871794871795e-05
python LearningRateFunctionModifier/ParamGroup1 [448 - 1705841690.7956216]: 6.794871794871795e-05
python DistillationModifier/task_loss [448 - 1705841690.7959352]: tensor(0.2290, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [448 - 1705841690.7970853]: tensor(0.3809, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [448 - 1705841690.7977486]: tensor(0.3809, grad_fn=<AddBackward0>)
python ConstantPruningModifier [448 - 1705841690.7983644]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.111111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [448 - 1705841691.021422]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [448 - 1705841691.0222692]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [448 - 1705841691.023155]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [448 - 1705841691.0238225]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [448 - 1705841691.0244534]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [448 - 1705841691.0263054]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [448 - 1705841691.0282378]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [448 - 1705841691.0291274]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [448 - 1705841691.0298438]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [448 - 1705841691.030461]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [448 - 1705841691.031011]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [448 - 1705841691.0328066]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [448 - 1705841691.0347538]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [448 - 1705841691.035655]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [448 - 1705841691.0363379]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [448 - 1705841691.0369475]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [448 - 1705841691.037509]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [448 - 1705841691.0392807]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [448 - 1705841691.0411777]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [448 - 1705841691.0419912]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [448 - 1705841691.04267]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [448 - 1705841691.0432446]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [448 - 1705841691.043803]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [448 - 1705841691.0455577]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [448 - 1705841691.0473495]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [448 - 1705841691.0481071]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [448 - 1705841691.0487666]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [448 - 1705841691.0493379]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [448 - 1705841691.0498796]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [448 - 1705841691.0516272]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [448 - 1705841691.0535047]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [448 - 1705841691.054288]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [448 - 1705841691.0549107]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [448 - 1705841691.0554605]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [448 - 1705841691.0560024]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [448 - 1705841691.057747]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [448 - 1705841691.0593548]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [448 - 1705841691.0601473]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [448 - 1705841691.0608478]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [448 - 1705841691.061447]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [448 - 1705841691.062006]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [448 - 1705841691.0637074]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [448 - 1705841691.065589]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [448 - 1705841691.0663683]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [448 - 1705841691.067061]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [448 - 1705841691.0676634]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [448 - 1705841691.0682392]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [448 - 1705841691.070091]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [448 - 1705841691.0719993]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [448 - 1705841691.0728626]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [448 - 1705841691.0735354]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [448 - 1705841691.0741532]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [448 - 1705841691.074714]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [448 - 1705841691.076475]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [448 - 1705841691.0783267]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [448 - 1705841691.0791302]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [448 - 1705841691.0797768]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [448 - 1705841691.0803573]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [448 - 1705841691.0809133]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [448 - 1705841691.0826774]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [448 - 1705841691.084482]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [448 - 1705841691.0853248]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [448 - 1705841691.0859807]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [448 - 1705841691.0865767]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [448 - 1705841691.0871348]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [448 - 1705841691.088929]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [448 - 1705841691.0906854]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [448 - 1705841691.0915785]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [448 - 1705841691.0923176]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [448 - 1705841691.0929575]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [448 - 1705841691.0935419]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [448 - 1705841691.0952246]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [448 - 1705841691.0967913]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [448 - 1705841691.097565]: 0.0
python ParamPruning/classifier.weight [448 - 1705841691.0980108]: 0.0
python DistillationModifier [455 - 1705841733.0801907]: Calling loss_update with:
args: 0.013083573430776596| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.222222222222222| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [455 - 1705841734.6328902]: 
Returned: 0.1362764686346054| 

python LearningRateFunctionModifier [455 - 1705841737.5578587]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.222222222222222| steps_per_epoch: 63| 
python LearningRateFunctionModifier [455 - 1705841737.558033]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [455 - 1705841737.5580971]: 6.666666666666666e-05
python LearningRateFunctionModifier/ParamGroup1 [455 - 1705841737.5583365]: 6.666666666666666e-05
python DistillationModifier/task_loss [455 - 1705841737.5586576]: tensor(0.0131, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [455 - 1705841737.5596907]: tensor(0.1363, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [455 - 1705841737.5602326]: tensor(0.1363, grad_fn=<AddBackward0>)
python ConstantPruningModifier [455 - 1705841737.5607643]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.222222222222222| steps_per_epoch: 63| 
python ConstantPruningModifier [455 - 1705841737.7810018]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [455 - 1705841737.7818391]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [455 - 1705841737.782741]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [455 - 1705841737.7835588]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [455 - 1705841737.7842944]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [455 - 1705841737.786197]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [455 - 1705841737.787919]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [455 - 1705841737.7888136]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [455 - 1705841737.7895584]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [455 - 1705841737.790278]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [455 - 1705841737.790918]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [455 - 1705841737.7927074]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [455 - 1705841737.7945075]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [455 - 1705841737.7953715]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [455 - 1705841737.796198]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [455 - 1705841737.7969737]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [455 - 1705841737.7976358]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [455 - 1705841737.7994452]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [455 - 1705841737.8011384]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [455 - 1705841737.8019266]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [455 - 1705841737.8026907]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [455 - 1705841737.8034034]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [455 - 1705841737.8040917]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [455 - 1705841737.8058782]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [455 - 1705841737.8077285]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [455 - 1705841737.8085055]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [455 - 1705841737.8092852]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [455 - 1705841737.8099346]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [455 - 1705841737.810561]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [455 - 1705841737.812294]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [455 - 1705841737.814163]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [455 - 1705841737.814933]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [455 - 1705841737.815648]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [455 - 1705841737.8162806]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [455 - 1705841737.8169045]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [455 - 1705841737.8186457]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [455 - 1705841737.82043]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [455 - 1705841737.821216]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [455 - 1705841737.8219073]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [455 - 1705841737.8225582]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [455 - 1705841737.8232167]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [455 - 1705841737.8249474]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [455 - 1705841737.8265305]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [455 - 1705841737.8272889]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [455 - 1705841737.828011]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [455 - 1705841737.828682]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [455 - 1705841737.8293073]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [455 - 1705841737.8309608]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [455 - 1705841737.832786]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [455 - 1705841737.8334663]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [455 - 1705841737.834098]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [455 - 1705841737.8347404]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [455 - 1705841737.8353784]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [455 - 1705841737.8371372]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [455 - 1705841737.838925]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [455 - 1705841737.8396578]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [455 - 1705841737.8403537]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [455 - 1705841737.8409815]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [455 - 1705841737.8415797]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [455 - 1705841737.843303]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [455 - 1705841737.8449435]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [455 - 1705841737.845758]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [455 - 1705841737.8464873]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [455 - 1705841737.8471458]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [455 - 1705841737.8477268]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [455 - 1705841737.8494782]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [455 - 1705841737.851142]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [455 - 1705841737.8519762]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [455 - 1705841737.8527718]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [455 - 1705841737.8534515]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [455 - 1705841737.8540978]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [455 - 1705841737.85594]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [455 - 1705841737.857742]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [455 - 1705841737.858609]: 0.0
python ParamPruning/classifier.weight [455 - 1705841737.859136]: 0.0
python DistillationModifier [462 - 1705841779.9261882]: Calling loss_update with:
args: 0.07579223811626434| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.333333333333333| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [462 - 1705841781.466049]: 
Returned: 0.05752810090780258| 

python LearningRateFunctionModifier [462 - 1705841784.398304]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.333333333333333| steps_per_epoch: 63| 
python LearningRateFunctionModifier [462 - 1705841784.3984747]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [462 - 1705841784.3985362]: 6.538461538461539e-05
python LearningRateFunctionModifier/ParamGroup1 [462 - 1705841784.39879]: 6.538461538461539e-05
python DistillationModifier/task_loss [462 - 1705841784.399125]: tensor(0.0758, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [462 - 1705841784.4001842]: tensor(0.0575, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [462 - 1705841784.4009004]: tensor(0.0575, grad_fn=<AddBackward0>)
python ConstantPruningModifier [462 - 1705841784.4015436]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.333333333333333| steps_per_epoch: 63| 
python ConstantPruningModifier [462 - 1705841784.6230233]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [462 - 1705841784.6238346]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [462 - 1705841784.6247847]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [462 - 1705841784.6255486]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [462 - 1705841784.6262007]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [462 - 1705841784.6279852]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [462 - 1705841784.6297789]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [462 - 1705841784.6307042]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [462 - 1705841784.6314263]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [462 - 1705841784.6320837]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [462 - 1705841784.6326914]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [462 - 1705841784.6344247]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [462 - 1705841784.6362982]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [462 - 1705841784.6371715]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [462 - 1705841784.6378655]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [462 - 1705841784.6384847]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [462 - 1705841784.639055]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [462 - 1705841784.6408978]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [462 - 1705841784.6426811]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [462 - 1705841784.6435695]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [462 - 1705841784.6443193]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [462 - 1705841784.6449592]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [462 - 1705841784.6455379]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [462 - 1705841784.6472752]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [462 - 1705841784.6492298]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [462 - 1705841784.6500885]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [462 - 1705841784.6507857]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [462 - 1705841784.6513975]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [462 - 1705841784.6519775]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [462 - 1705841784.6537504]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [462 - 1705841784.6556022]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [462 - 1705841784.6564105]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [462 - 1705841784.6571038]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [462 - 1705841784.6577098]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [462 - 1705841784.6582668]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [462 - 1705841784.6600332]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [462 - 1705841784.6619906]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [462 - 1705841784.66283]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [462 - 1705841784.6635044]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [462 - 1705841784.6641626]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [462 - 1705841784.6647687]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [462 - 1705841784.666646]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [462 - 1705841784.6684554]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [462 - 1705841784.669418]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [462 - 1705841784.6701999]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [462 - 1705841784.670838]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [462 - 1705841784.6714284]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [462 - 1705841784.6732094]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [462 - 1705841784.675092]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [462 - 1705841784.6759086]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [462 - 1705841784.676588]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [462 - 1705841784.6771896]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [462 - 1705841784.6777449]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [462 - 1705841784.6794922]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [462 - 1705841784.681206]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [462 - 1705841784.6820662]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [462 - 1705841784.682785]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [462 - 1705841784.6834028]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [462 - 1705841784.6839664]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [462 - 1705841784.6857011]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [462 - 1705841784.6872928]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [462 - 1705841784.6880825]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [462 - 1705841784.688783]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [462 - 1705841784.68938]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [462 - 1705841784.6899283]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [462 - 1705841784.691682]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [462 - 1705841784.6934512]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [462 - 1705841784.6943042]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [462 - 1705841784.6950161]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [462 - 1705841784.6956341]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [462 - 1705841784.69619]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [462 - 1705841784.6979957]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [462 - 1705841784.6999695]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [462 - 1705841784.7008545]: 0.0
python ParamPruning/classifier.weight [462 - 1705841784.7013526]: 0.0
python DistillationModifier [469 - 1705841826.3673499]: Calling loss_update with:
args: 0.15293438732624054| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.444444444444445| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [469 - 1705841827.9133053]: 
Returned: 0.2753117084503174| 

python LearningRateFunctionModifier [469 - 1705841830.8378808]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.444444444444445| steps_per_epoch: 63| 
python LearningRateFunctionModifier [469 - 1705841830.8380697]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [469 - 1705841830.8381443]: 6.41025641025641e-05
python LearningRateFunctionModifier/ParamGroup1 [469 - 1705841830.8383813]: 6.41025641025641e-05
python DistillationModifier/task_loss [469 - 1705841830.8387053]: tensor(0.1529, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [469 - 1705841830.8397624]: tensor(0.2753, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [469 - 1705841830.8403525]: tensor(0.2753, grad_fn=<AddBackward0>)
python ConstantPruningModifier [469 - 1705841830.8409457]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.444444444444445| steps_per_epoch: 63| 
python ConstantPruningModifier [469 - 1705841831.0650942]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [469 - 1705841831.0659218]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [469 - 1705841831.0668612]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [469 - 1705841831.067573]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [469 - 1705841831.0681958]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [469 - 1705841831.069983]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [469 - 1705841831.0716257]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [469 - 1705841831.072438]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [469 - 1705841831.0732193]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [469 - 1705841831.0738585]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [469 - 1705841831.0744262]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [469 - 1705841831.0762458]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [469 - 1705841831.0779634]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [469 - 1705841831.078771]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [469 - 1705841831.079453]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [469 - 1705841831.0801284]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [469 - 1705841831.0807092]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [469 - 1705841831.082483]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [469 - 1705841831.0843637]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [469 - 1705841831.0851808]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [469 - 1705841831.085847]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [469 - 1705841831.086431]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [469 - 1705841831.08708]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [469 - 1705841831.0888503]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [469 - 1705841831.0905282]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [469 - 1705841831.0912948]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [469 - 1705841831.0920196]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [469 - 1705841831.0926604]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [469 - 1705841831.0932832]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [469 - 1705841831.0950737]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [469 - 1705841831.0969265]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [469 - 1705841831.0977008]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [469 - 1705841831.0984073]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [469 - 1705841831.0990684]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [469 - 1705841831.0996778]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [469 - 1705841831.1013806]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [469 - 1705841831.1029959]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [469 - 1705841831.1037884]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [469 - 1705841831.1044521]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [469 - 1705841831.1051505]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [469 - 1705841831.1057348]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [469 - 1705841831.1075113]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [469 - 1705841831.1092403]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [469 - 1705841831.1100934]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [469 - 1705841831.1108787]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [469 - 1705841831.1115594]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [469 - 1705841831.1121597]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [469 - 1705841831.1139646]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [469 - 1705841831.1158185]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [469 - 1705841831.1166193]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [469 - 1705841831.1173139]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [469 - 1705841831.1179702]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [469 - 1705841831.1185408]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [469 - 1705841831.1203105]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [469 - 1705841831.122147]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [469 - 1705841831.1228878]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [469 - 1705841831.1236067]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [469 - 1705841831.1242104]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [469 - 1705841831.1248035]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [469 - 1705841831.1265855]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [469 - 1705841831.128397]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [469 - 1705841831.1291637]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [469 - 1705841831.1297746]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [469 - 1705841831.1304214]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [469 - 1705841831.130975]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [469 - 1705841831.132793]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [469 - 1705841831.134512]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [469 - 1705841831.1353252]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [469 - 1705841831.1360586]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [469 - 1705841831.1366994]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [469 - 1705841831.1372752]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [469 - 1705841831.1390905]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [469 - 1705841831.1409223]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [469 - 1705841831.1417012]: 0.0
python ParamPruning/classifier.weight [469 - 1705841831.1421726]: 0.0
python DistillationModifier [476 - 1705841872.1282442]: Calling loss_update with:
args: 0.21579451858997345| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.555555555555555| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [476 - 1705841873.6475565]: 
Returned: 0.14174626767635345| 

python LearningRateFunctionModifier [476 - 1705841876.519591]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.555555555555555| steps_per_epoch: 63| 
python LearningRateFunctionModifier [476 - 1705841876.5197554]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [476 - 1705841876.5198147]: 6.282051282051282e-05
python LearningRateFunctionModifier/ParamGroup1 [476 - 1705841876.5200505]: 6.282051282051282e-05
python DistillationModifier/task_loss [476 - 1705841876.5203576]: tensor(0.2158, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [476 - 1705841876.5213847]: tensor(0.1417, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [476 - 1705841876.5219653]: tensor(0.1417, grad_fn=<AddBackward0>)
python ConstantPruningModifier [476 - 1705841876.5225036]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.555555555555555| steps_per_epoch: 63| 
python ConstantPruningModifier [476 - 1705841876.7384756]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [476 - 1705841876.7392619]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [476 - 1705841876.7401505]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [476 - 1705841876.7408688]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [476 - 1705841876.7415693]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [476 - 1705841876.743326]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [476 - 1705841876.745155]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [476 - 1705841876.7458615]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [476 - 1705841876.7465575]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [476 - 1705841876.747173]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [476 - 1705841876.7477336]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [476 - 1705841876.7494166]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [476 - 1705841876.75123]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [476 - 1705841876.7519417]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [476 - 1705841876.7526016]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [476 - 1705841876.753192]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [476 - 1705841876.753713]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [476 - 1705841876.7553854]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [476 - 1705841876.756986]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [476 - 1705841876.7576902]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [476 - 1705841876.7583473]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [476 - 1705841876.758996]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [476 - 1705841876.7595587]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [476 - 1705841876.761237]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [476 - 1705841876.762981]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [476 - 1705841876.7636652]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [476 - 1705841876.764285]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [476 - 1705841876.764857]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [476 - 1705841876.7653716]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [476 - 1705841876.7670045]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [476 - 1705841876.768767]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [476 - 1705841876.7693954]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [476 - 1705841876.7700021]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [476 - 1705841876.7706115]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [476 - 1705841876.7712011]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [476 - 1705841876.772938]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [476 - 1705841876.7746313]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [476 - 1705841876.7752912]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [476 - 1705841876.775914]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [476 - 1705841876.7764654]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [476 - 1705841876.7770803]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [476 - 1705841876.7787461]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [476 - 1705841876.7804847]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [476 - 1705841876.7812169]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [476 - 1705841876.7818608]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [476 - 1705841876.782415]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [476 - 1705841876.782944]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [476 - 1705841876.784584]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [476 - 1705841876.7863839]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [476 - 1705841876.787048]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [476 - 1705841876.7876468]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [476 - 1705841876.7881885]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [476 - 1705841876.7887247]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [476 - 1705841876.7904131]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [476 - 1705841876.7922428]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [476 - 1705841876.792936]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [476 - 1705841876.793583]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [476 - 1705841876.7941284]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [476 - 1705841876.7947314]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [476 - 1705841876.796418]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [476 - 1705841876.7981691]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [476 - 1705841876.7988405]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [476 - 1705841876.7995331]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [476 - 1705841876.8001378]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [476 - 1705841876.8007007]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [476 - 1705841876.8024092]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [476 - 1705841876.8042672]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [476 - 1705841876.8049686]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [476 - 1705841876.8056073]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [476 - 1705841876.8061826]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [476 - 1705841876.8067834]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [476 - 1705841876.8084807]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [476 - 1705841876.8102512]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [476 - 1705841876.8109205]: 0.0
python ParamPruning/classifier.weight [476 - 1705841876.811309]: 0.0
python DistillationModifier [483 - 1705841918.8189702]: Calling loss_update with:
args: 0.17938974499702454| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.666666666666667| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [483 - 1705841920.355599]: 
Returned: 0.17095142602920532| 

python LearningRateFunctionModifier [483 - 1705841923.2818143]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.666666666666667| steps_per_epoch: 63| 
python LearningRateFunctionModifier [483 - 1705841923.2819936]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [483 - 1705841923.2820683]: 6.153846153846154e-05
python LearningRateFunctionModifier/ParamGroup1 [483 - 1705841923.2822976]: 6.153846153846154e-05
python DistillationModifier/task_loss [483 - 1705841923.282632]: tensor(0.1794, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [483 - 1705841923.2836921]: tensor(0.1710, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [483 - 1705841923.284343]: tensor(0.1710, grad_fn=<AddBackward0>)
python ConstantPruningModifier [483 - 1705841923.2849858]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.666666666666667| steps_per_epoch: 63| 
python ConstantPruningModifier [483 - 1705841923.5054758]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [483 - 1705841923.5062947]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [483 - 1705841923.507288]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [483 - 1705841923.5081446]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [483 - 1705841923.5089588]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [483 - 1705841923.5108495]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [483 - 1705841923.5127966]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [483 - 1705841923.513654]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [483 - 1705841923.514433]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [483 - 1705841923.5151234]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [483 - 1705841923.5158303]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [483 - 1705841923.5176358]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [483 - 1705841923.5194497]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [483 - 1705841923.5202403]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [483 - 1705841923.5209982]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [483 - 1705841923.5216637]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [483 - 1705841923.522385]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [483 - 1705841923.5241737]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [483 - 1705841923.526005]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [483 - 1705841923.526769]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [483 - 1705841923.5274959]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [483 - 1705841923.5281339]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [483 - 1705841923.52871]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [483 - 1705841923.5304804]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [483 - 1705841923.5323288]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [483 - 1705841923.5331423]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [483 - 1705841923.533888]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [483 - 1705841923.5346189]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [483 - 1705841923.5352778]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [483 - 1705841923.5371168]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [483 - 1705841923.5389678]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [483 - 1705841923.5397644]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [483 - 1705841923.5405078]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [483 - 1705841923.541181]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [483 - 1705841923.541878]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [483 - 1705841923.5437446]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [483 - 1705841923.5456629]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [483 - 1705841923.5465453]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [483 - 1705841923.5473413]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [483 - 1705841923.548076]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [483 - 1705841923.548767]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [483 - 1705841923.5506003]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [483 - 1705841923.552461]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [483 - 1705841923.5532632]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [483 - 1705841923.5539377]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [483 - 1705841923.5546372]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [483 - 1705841923.5553076]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [483 - 1705841923.5571024]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [483 - 1705841923.558724]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [483 - 1705841923.5594862]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [483 - 1705841923.5601509]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [483 - 1705841923.5607622]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [483 - 1705841923.5614557]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [483 - 1705841923.56319]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [483 - 1705841923.5650227]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [483 - 1705841923.5657835]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [483 - 1705841923.5665]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [483 - 1705841923.567198]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [483 - 1705841923.5678198]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [483 - 1705841923.5696297]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [483 - 1705841923.5714793]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [483 - 1705841923.5723243]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [483 - 1705841923.5730934]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [483 - 1705841923.5738158]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [483 - 1705841923.5744824]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [483 - 1705841923.5762951]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [483 - 1705841923.5781894]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [483 - 1705841923.579062]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [483 - 1705841923.5798378]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [483 - 1705841923.5805182]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [483 - 1705841923.5811603]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [483 - 1705841923.5829184]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [483 - 1705841923.5847373]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [483 - 1705841923.5855129]: 0.0
python ParamPruning/classifier.weight [483 - 1705841923.5859814]: 0.0
python DistillationModifier [490 - 1705841965.196729]: Calling loss_update with:
args: 0.04617166146636009| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.777777777777778| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [490 - 1705841966.7114158]: 
Returned: 0.09549898654222488| 

python LearningRateFunctionModifier [490 - 1705841969.616411]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.777777777777778| steps_per_epoch: 63| 
python LearningRateFunctionModifier [490 - 1705841969.6165924]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [490 - 1705841969.6166658]: 6.025641025641025e-05
python LearningRateFunctionModifier/ParamGroup1 [490 - 1705841969.616923]: 6.025641025641025e-05
python DistillationModifier/task_loss [490 - 1705841969.617235]: tensor(0.0462, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [490 - 1705841969.6183472]: tensor(0.0955, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [490 - 1705841969.618989]: tensor(0.0955, grad_fn=<AddBackward0>)
python ConstantPruningModifier [490 - 1705841969.619631]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.777777777777778| steps_per_epoch: 63| 
python ConstantPruningModifier [490 - 1705841969.8418176]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [490 - 1705841969.842645]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [490 - 1705841969.8435764]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [490 - 1705841969.8444023]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [490 - 1705841969.845216]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [490 - 1705841969.847045]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [490 - 1705841969.8490074]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [490 - 1705841969.8498263]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [490 - 1705841969.8505821]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [490 - 1705841969.8512387]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [490 - 1705841969.8519192]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [490 - 1705841969.8537178]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [490 - 1705841969.8555918]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [490 - 1705841969.8564186]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [490 - 1705841969.8571248]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [490 - 1705841969.8577316]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [490 - 1705841969.8583975]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [490 - 1705841969.8601437]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [490 - 1705841969.861786]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [490 - 1705841969.8625908]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [490 - 1705841969.8632827]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [490 - 1705841969.8639927]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [490 - 1705841969.8645551]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [490 - 1705841969.8661985]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [490 - 1705841969.8680193]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [490 - 1705841969.8687253]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [490 - 1705841969.8693252]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [490 - 1705841969.8698792]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [490 - 1705841969.8704088]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [490 - 1705841969.8721082]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [490 - 1705841969.8737206]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [490 - 1705841969.874495]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [490 - 1705841969.8751602]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [490 - 1705841969.875844]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [490 - 1705841969.8764203]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [490 - 1705841969.8782187]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [490 - 1705841969.8800573]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [490 - 1705841969.8808544]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [490 - 1705841969.8815253]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [490 - 1705841969.8821926]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [490 - 1705841969.8827507]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [490 - 1705841969.8844848]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [490 - 1705841969.8861332]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [490 - 1705841969.886927]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [490 - 1705841969.887563]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [490 - 1705841969.888176]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [490 - 1705841969.8888726]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [490 - 1705841969.8906534]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [490 - 1705841969.8923223]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [490 - 1705841969.893165]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [490 - 1705841969.8938649]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [490 - 1705841969.8944793]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [490 - 1705841969.8950431]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [490 - 1705841969.896849]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [490 - 1705841969.8986292]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [490 - 1705841969.8995385]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [490 - 1705841969.9003189]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [490 - 1705841969.9009638]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [490 - 1705841969.9015474]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [490 - 1705841969.9033058]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [490 - 1705841969.9050457]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [490 - 1705841969.9059207]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [490 - 1705841969.9066463]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [490 - 1705841969.9073706]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [490 - 1705841969.9079747]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [490 - 1705841969.909756]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [490 - 1705841969.9115572]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [490 - 1705841969.9123294]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [490 - 1705841969.9130108]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [490 - 1705841969.9137175]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [490 - 1705841969.9143276]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [490 - 1705841969.9161103]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [490 - 1705841969.9180353]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [490 - 1705841969.9188297]: 0.0
python ParamPruning/classifier.weight [490 - 1705841969.9193034]: 0.0
python DistillationModifier [497 - 1705842011.8170817]: Calling loss_update with:
args: 0.22037231922149658| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 7.888888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [497 - 1705842013.3478515]: 
Returned: 0.24113509058952332| 

python LearningRateFunctionModifier [497 - 1705842016.2805936]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.888888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [497 - 1705842016.280787]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [497 - 1705842016.280857]: 5.8974358974358954e-05
python LearningRateFunctionModifier/ParamGroup1 [497 - 1705842016.2810924]: 5.8974358974358954e-05
python DistillationModifier/task_loss [497 - 1705842016.2814274]: tensor(0.2204, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [497 - 1705842016.282467]: tensor(0.2411, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [497 - 1705842016.2831578]: tensor(0.2411, grad_fn=<AddBackward0>)
python ConstantPruningModifier [497 - 1705842016.28382]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 7.888888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [497 - 1705842016.539862]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [497 - 1705842016.5406952]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [497 - 1705842016.5416589]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [497 - 1705842016.5425081]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [497 - 1705842016.5432673]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [497 - 1705842016.5452063]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [497 - 1705842016.547053]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [497 - 1705842016.5479665]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [497 - 1705842016.5487833]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [497 - 1705842016.5494463]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [497 - 1705842016.5500607]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [497 - 1705842016.5518966]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [497 - 1705842016.5538878]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [497 - 1705842016.5547733]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [497 - 1705842016.555543]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [497 - 1705842016.5562894]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [497 - 1705842016.5570097]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [497 - 1705842016.558818]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [497 - 1705842016.5606625]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [497 - 1705842016.5615098]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [497 - 1705842016.5622897]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [497 - 1705842016.5630085]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [497 - 1705842016.5637143]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [497 - 1705842016.5655816]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [497 - 1705842016.567507]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [497 - 1705842016.568365]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [497 - 1705842016.5690937]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [497 - 1705842016.5698214]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [497 - 1705842016.5704937]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [497 - 1705842016.5723643]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [497 - 1705842016.5741622]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [497 - 1705842016.5750618]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [497 - 1705842016.575791]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [497 - 1705842016.5765226]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [497 - 1705842016.577169]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [497 - 1705842016.5790057]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [497 - 1705842016.5809617]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [497 - 1705842016.5818589]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [497 - 1705842016.5826743]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [497 - 1705842016.5833547]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [497 - 1705842016.5840354]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [497 - 1705842016.585866]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [497 - 1705842016.587627]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [497 - 1705842016.588494]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [497 - 1705842016.5893102]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [497 - 1705842016.590005]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [497 - 1705842016.5906641]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [497 - 1705842016.5925093]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [497 - 1705842016.5944283]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [497 - 1705842016.5952861]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [497 - 1705842016.5960543]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [497 - 1705842016.596773]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [497 - 1705842016.597435]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [497 - 1705842016.5993042]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [497 - 1705842016.6012273]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [497 - 1705842016.602072]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [497 - 1705842016.6028469]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [497 - 1705842016.603607]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [497 - 1705842016.604296]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [497 - 1705842016.6061497]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [497 - 1705842016.6078331]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [497 - 1705842016.6086981]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [497 - 1705842016.6094415]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [497 - 1705842016.6100824]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [497 - 1705842016.6106672]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [497 - 1705842016.6124399]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [497 - 1705842016.6141274]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [497 - 1705842016.6149569]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [497 - 1705842016.615778]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [497 - 1705842016.6165252]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [497 - 1705842016.6172316]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [497 - 1705842016.6190891]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [497 - 1705842016.620909]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [497 - 1705842016.622008]: 0.0
python ParamPruning/classifier.weight [497 - 1705842016.6227036]: 0.0
python DistillationModifier [504 - 1705842054.3377802]: Calling loss_update with:
args: 0.7026830315589905| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842055.8009138]: 
Returned: 1.198216438293457| 

python DistillationModifier [504 - 1705842057.2706013]: Calling loss_update with:
args: 1.233690619468689| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842058.704198]: 
Returned: 1.9898208379745483| 

python DistillationModifier [504 - 1705842060.1731026]: Calling loss_update with:
args: 1.0197745561599731| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842062.1069698]: 
Returned: 1.0034080743789673| 

python DistillationModifier [504 - 1705842064.2618814]: Calling loss_update with:
args: 0.6088816523551941| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842065.7203445]: 
Returned: 0.9148132801055908| 

python DistillationModifier [504 - 1705842067.193499]: Calling loss_update with:
args: 1.098103642463684| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842068.6499343]: 
Returned: 1.4861242771148682| 

python DistillationModifier [504 - 1705842070.121194]: Calling loss_update with:
args: 0.6142236590385437| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842071.556801]: 
Returned: 1.1762117147445679| 

python DistillationModifier [504 - 1705842073.0284917]: Calling loss_update with:
args: 0.3016129434108734| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842074.470696]: 
Returned: 0.7682439088821411| 

python DistillationModifier [504 - 1705842075.9476547]: Calling loss_update with:
args: 1.1510602235794067| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842077.4147718]: 
Returned: 1.301356315612793| 

python DistillationModifier [504 - 1705842078.890744]: Calling loss_update with:
args: 0.6650588512420654| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842080.3339012]: 
Returned: 0.5579575300216675| 

python DistillationModifier [504 - 1705842081.81653]: Calling loss_update with:
args: 0.736015260219574| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842083.2550313]: 
Returned: 1.112581491470337| 

python DistillationModifier [504 - 1705842084.724375]: Calling loss_update with:
args: 0.4560557007789612| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842086.1750355]: 
Returned: 0.829093337059021| 

python DistillationModifier [504 - 1705842087.6436498]: Calling loss_update with:
args: 0.972184419631958| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842089.0766878]: 
Returned: 1.1757255792617798| 

python DistillationModifier [504 - 1705842090.5415068]: Calling loss_update with:
args: 0.941450834274292| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842091.9775217]: 
Returned: 1.4143174886703491| 

python DistillationModifier [504 - 1705842093.4435675]: Calling loss_update with:
args: 0.6587321758270264| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842094.8833413]: 
Returned: 0.6579561233520508| 

python DistillationModifier [504 - 1705842096.3573842]: Calling loss_update with:
args: 1.034892201423645| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842097.787749]: 
Returned: 1.67644202709198| 

python DistillationModifier [504 - 1705842099.2520993]: Calling loss_update with:
args: 0.8212482333183289| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842100.683492]: 
Returned: 1.3665320873260498| 

python DistillationModifier [504 - 1705842102.1541204]: Calling loss_update with:
args: 0.7315587997436523| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842103.5931575]: 
Returned: 1.0346248149871826| 

python DistillationModifier [504 - 1705842105.0640247]: Calling loss_update with:
args: 0.8957180380821228| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842106.5078373]: 
Returned: 1.6123417615890503| 

python DistillationModifier [504 - 1705842108.0208027]: Calling loss_update with:
args: 0.8778668642044067| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842109.4659548]: 
Returned: 0.9910168051719666| 

python DistillationModifier [504 - 1705842110.9380934]: Calling loss_update with:
args: 0.4537578225135803| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842112.3770316]: 
Returned: 0.9860805869102478| 

python DistillationModifier [504 - 1705842113.845309]: Calling loss_update with:
args: 0.5396515727043152| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842115.2882316]: 
Returned: 1.2271705865859985| 

python DistillationModifier [504 - 1705842116.7635744]: Calling loss_update with:
args: 0.7064516544342041| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842118.1935785]: 
Returned: 0.9817309379577637| 

python DistillationModifier [504 - 1705842119.666476]: Calling loss_update with:
args: 0.6228637099266052| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842121.0937657]: 
Returned: 0.946465253829956| 

python DistillationModifier [504 - 1705842122.9730926]: Calling loss_update with:
args: 0.6588010191917419| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842125.0408812]: 
Returned: 0.9180668592453003| 

python DistillationModifier [504 - 1705842126.7883022]: Calling loss_update with:
args: 0.799096941947937| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842128.2264216]: 
Returned: 0.6504198908805847| 

python DistillationModifier [504 - 1705842129.698876]: Calling loss_update with:
args: 0.2960691750049591| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842131.1381235]: 
Returned: 0.5516241192817688| 

python DistillationModifier [504 - 1705842132.6092734]: Calling loss_update with:
args: 0.8098587393760681| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842134.0576353]: 
Returned: 1.090468168258667| 

python DistillationModifier [504 - 1705842135.5289865]: Calling loss_update with:
args: 0.6988101601600647| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842136.9765394]: 
Returned: 1.0700815916061401| 

python DistillationModifier [504 - 1705842138.4950328]: Calling loss_update with:
args: 0.8127647042274475| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842139.9345975]: 
Returned: 0.9555169939994812| 

python DistillationModifier [504 - 1705842141.4148657]: Calling loss_update with:
args: 0.5901994109153748| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842142.8577275]: 
Returned: 0.7913417816162109| 

python DistillationModifier [504 - 1705842144.3299096]: Calling loss_update with:
args: 0.9735379815101624| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842145.781902]: 
Returned: 1.79932701587677| 

python DistillationModifier [504 - 1705842147.2757733]: Calling loss_update with:
args: 0.9023879170417786| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842148.7091966]: 
Returned: 1.0595935583114624| 

python DistillationModifier [504 - 1705842150.1785269]: Calling loss_update with:
args: 0.7729464769363403| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842151.6179218]: 
Returned: 1.2724148035049438| 

python DistillationModifier [504 - 1705842153.0915878]: Calling loss_update with:
args: 1.2924180030822754| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842154.5323439]: 
Returned: 2.0776994228363037| 

python DistillationModifier [504 - 1705842156.0218983]: Calling loss_update with:
args: 1.3836582899093628| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842157.4610553]: 
Returned: 1.6744800806045532| 

python DistillationModifier [504 - 1705842158.9255736]: Calling loss_update with:
args: 0.9569834470748901| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842160.36314]: 
Returned: 1.8522218465805054| 

python DistillationModifier [504 - 1705842161.8379486]: Calling loss_update with:
args: 1.0801165103912354| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842163.2795866]: 
Returned: 1.260801076889038| 

python DistillationModifier [504 - 1705842164.7577684]: Calling loss_update with:
args: 0.4970749020576477| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842166.200508]: 
Returned: 0.7709113955497742| 

python DistillationModifier [504 - 1705842167.6706297]: Calling loss_update with:
args: 0.6869878768920898| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842169.1456754]: 
Returned: 1.3266522884368896| 

python DistillationModifier [504 - 1705842170.6143036]: Calling loss_update with:
args: 1.1640586853027344| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842172.061211]: 
Returned: 2.0971248149871826| 

python DistillationModifier [504 - 1705842173.5278893]: Calling loss_update with:
args: 0.5064888000488281| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842174.9681172]: 
Returned: 1.014918327331543| 

python DistillationModifier [504 - 1705842176.443167]: Calling loss_update with:
args: 0.727505624294281| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842177.8816564]: 
Returned: 1.4199856519699097| 

python DistillationModifier [504 - 1705842179.3461933]: Calling loss_update with:
args: 0.6226181387901306| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842180.7843275]: 
Returned: 0.59320068359375| 

python DistillationModifier [504 - 1705842182.5588474]: Calling loss_update with:
args: 0.6820992231369019| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842184.5843468]: 
Returned: 1.4108706712722778| 

python DistillationModifier [504 - 1705842186.6330202]: Calling loss_update with:
args: 0.8742737770080566| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842188.3363464]: 
Returned: 1.5810267925262451| 

python DistillationModifier [504 - 1705842189.8024945]: Calling loss_update with:
args: 1.122910737991333| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842191.2340422]: 
Returned: 1.8278306722640991| 

python DistillationModifier [504 - 1705842192.703633]: Calling loss_update with:
args: 0.5264116525650024| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842194.140603]: 
Returned: 1.0273945331573486| 

python DistillationModifier [504 - 1705842195.6135392]: Calling loss_update with:
args: 0.2979656457901001| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842197.0568802]: 
Returned: 0.6035322546958923| 

python DistillationModifier [504 - 1705842198.5264995]: Calling loss_update with:
args: 1.3147326707839966| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842200.0072374]: 
Returned: 2.405860424041748| 

python DistillationModifier [504 - 1705842201.4898746]: Calling loss_update with:
args: 1.083396553993225| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842202.9293563]: 
Returned: 2.0550169944763184| 

python DistillationModifier [504 - 1705842204.3994215]: Calling loss_update with:
args: 0.38569724559783936| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842205.837912]: 
Returned: 0.8259902000427246| 

python DistillationModifier [504 - 1705842207.3241177]: Calling loss_update with:
args: 1.7248389720916748| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842208.7620544]: 
Returned: 1.6659820079803467| 

python DistillationModifier [504 - 1705842210.2338355]: Calling loss_update with:
args: 0.7862294316291809| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842211.6880517]: 
Returned: 1.0957354307174683| 

python DistillationModifier [504 - 1705842213.1667058]: Calling loss_update with:
args: 0.6698228716850281| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842214.619196]: 
Returned: 0.8807236552238464| 

python DistillationModifier [504 - 1705842216.1033857]: Calling loss_update with:
args: 0.6960061192512512| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842217.5496192]: 
Returned: 1.4518650770187378| 

python DistillationModifier [504 - 1705842219.0211334]: Calling loss_update with:
args: 0.7177592515945435| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842220.440874]: 
Returned: 1.022230625152588| 

python DistillationModifier [504 - 1705842221.8738434]: Calling loss_update with:
args: 0.9989608526229858| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842223.286777]: 
Returned: 0.8603090047836304| 

python DistillationModifier [504 - 1705842224.7350912]: Calling loss_update with:
args: 1.0694847106933594| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842226.155615]: 
Returned: 1.3085825443267822| 

python DistillationModifier [504 - 1705842227.6131442]: Calling loss_update with:
args: 0.8309736847877502| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842229.0453415]: 
Returned: 1.498339056968689| 

python DistillationModifier [504 - 1705842230.5418317]: Calling loss_update with:
args: 0.7676409482955933| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842231.9698124]: 
Returned: 1.3126453161239624| 

python DistillationModifier [504 - 1705842233.4362276]: Calling loss_update with:
args: 0.6279150247573853| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842234.8603182]: 
Returned: 1.0845832824707031| 

python DistillationModifier [504 - 1705842236.3174825]: Calling loss_update with:
args: 0.7536327838897705| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842237.756534]: 
Returned: 1.370409369468689| 

python DistillationModifier [504 - 1705842238.5197518]: Calling loss_update with:
args: 0.6262757182121277| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842239.2558799]: 
Returned: 1.0934685468673706| 

python DistillationModifier [504 - 1705842241.329491]: Calling loss_update with:
args: 0.14123722910881042| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [504 - 1705842243.334444]: 
Returned: 0.07542044669389725| 

python LearningRateFunctionModifier [504 - 1705842246.8708532]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [504 - 1705842246.8710449]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [504 - 1705842246.8711176]: 5.7692307692307684e-05
python LearningRateFunctionModifier/ParamGroup1 [504 - 1705842246.8713496]: 5.7692307692307684e-05
python DistillationModifier/task_loss [504 - 1705842246.871711]: tensor(0.1412, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [504 - 1705842246.8727572]: tensor(0.0754, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [504 - 1705842246.873444]: tensor(0.0754, grad_fn=<AddBackward0>)
python ConstantPruningModifier [504 - 1705842246.874103]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.0| steps_per_epoch: 63| 
python ConstantPruningModifier [504 - 1705842247.0982985]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [504 - 1705842247.0990856]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [504 - 1705842247.1000595]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [504 - 1705842247.100843]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [504 - 1705842247.1015992]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [504 - 1705842247.103403]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [504 - 1705842247.1051097]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [504 - 1705842247.10598]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [504 - 1705842247.10671]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [504 - 1705842247.1073313]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [504 - 1705842247.1079004]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [504 - 1705842247.1096206]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [504 - 1705842247.1114488]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [504 - 1705842247.112258]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [504 - 1705842247.1129236]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [504 - 1705842247.1135302]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [504 - 1705842247.1140954]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [504 - 1705842247.1158354]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [504 - 1705842247.1176035]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [504 - 1705842247.1184971]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [504 - 1705842247.1192238]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [504 - 1705842247.1199567]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [504 - 1705842247.1205966]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [504 - 1705842247.1223948]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [504 - 1705842247.1241257]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [504 - 1705842247.1250231]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [504 - 1705842247.1257634]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [504 - 1705842247.1264324]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [504 - 1705842247.1270206]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [504 - 1705842247.1287725]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [504 - 1705842247.1304832]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [504 - 1705842247.131347]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [504 - 1705842247.132152]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [504 - 1705842247.1328745]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [504 - 1705842247.1335487]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [504 - 1705842247.135353]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [504 - 1705842247.1370928]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [504 - 1705842247.1380327]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [504 - 1705842247.138797]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [504 - 1705842247.1394436]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [504 - 1705842247.1400383]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [504 - 1705842247.1417787]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [504 - 1705842247.143443]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [504 - 1705842247.1442852]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [504 - 1705842247.1450734]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [504 - 1705842247.1457458]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [504 - 1705842247.1463394]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [504 - 1705842247.1481361]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [504 - 1705842247.150047]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [504 - 1705842247.150901]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [504 - 1705842247.1516943]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [504 - 1705842247.1523688]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [504 - 1705842247.1529877]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [504 - 1705842247.1547456]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [504 - 1705842247.1564143]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [504 - 1705842247.1572752]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [504 - 1705842247.1580534]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [504 - 1705842247.1587076]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [504 - 1705842247.1593974]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [504 - 1705842247.161238]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [504 - 1705842247.1629682]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [504 - 1705842247.1638534]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [504 - 1705842247.164591]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [504 - 1705842247.1653411]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [504 - 1705842247.1659808]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [504 - 1705842247.1677842]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [504 - 1705842247.1696863]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [504 - 1705842247.1705592]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [504 - 1705842247.1712644]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [504 - 1705842247.1720178]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [504 - 1705842247.172659]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [504 - 1705842247.174492]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [504 - 1705842247.1763628]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [504 - 1705842247.1772366]: 0.0
python ParamPruning/classifier.weight [504 - 1705842247.1777282]: 0.0
python DistillationModifier [511 - 1705842300.753821]: Calling loss_update with:
args: 0.3151845335960388| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.11111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [511 - 1705842302.57035]: 
Returned: 0.4706364572048187| 

python LearningRateFunctionModifier [511 - 1705842306.662735]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.11111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [511 - 1705842306.6629214]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [511 - 1705842306.6827621]: 5.64102564102564e-05
python LearningRateFunctionModifier/ParamGroup1 [511 - 1705842306.6830385]: 5.64102564102564e-05
python DistillationModifier/task_loss [511 - 1705842306.6833534]: tensor(0.3152, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [511 - 1705842306.684384]: tensor(0.4706, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [511 - 1705842306.6849952]: tensor(0.4706, grad_fn=<AddBackward0>)
python ConstantPruningModifier [511 - 1705842306.6855597]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.11111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [511 - 1705842306.9158492]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [511 - 1705842306.9167345]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [511 - 1705842306.9176536]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [511 - 1705842306.9184735]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [511 - 1705842306.919162]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [511 - 1705842306.920969]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [511 - 1705842306.9227958]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [511 - 1705842306.92352]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [511 - 1705842306.924203]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [511 - 1705842306.924836]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [511 - 1705842306.9254224]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [511 - 1705842306.9271889]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [511 - 1705842306.929049]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [511 - 1705842306.929801]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [511 - 1705842306.9304807]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [511 - 1705842306.9310794]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [511 - 1705842306.9316459]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [511 - 1705842306.9334648]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [511 - 1705842306.9352746]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [511 - 1705842306.9359746]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [511 - 1705842306.936619]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [511 - 1705842306.937304]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [511 - 1705842306.937856]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [511 - 1705842306.939581]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [511 - 1705842306.9412735]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [511 - 1705842306.942009]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [511 - 1705842306.9426868]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [511 - 1705842306.9432843]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [511 - 1705842306.9438655]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [511 - 1705842306.945656]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [511 - 1705842306.9474785]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [511 - 1705842306.9482265]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [511 - 1705842306.9489481]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [511 - 1705842306.9495752]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [511 - 1705842306.9501693]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [511 - 1705842306.9519546]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [511 - 1705842306.9538422]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [511 - 1705842306.9546313]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [511 - 1705842306.9552424]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [511 - 1705842306.955879]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [511 - 1705842306.9564264]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [511 - 1705842306.958222]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [511 - 1705842306.9599123]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [511 - 1705842306.9606643]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [511 - 1705842306.9614227]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [511 - 1705842306.9620686]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [511 - 1705842306.9626288]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [511 - 1705842306.9643466]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [511 - 1705842306.9661038]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [511 - 1705842306.966795]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [511 - 1705842306.9674451]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [511 - 1705842306.9679966]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [511 - 1705842306.9685555]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [511 - 1705842306.9703133]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [511 - 1705842306.9721072]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [511 - 1705842306.9728222]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [511 - 1705842306.9734838]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [511 - 1705842306.974062]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [511 - 1705842306.9746408]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [511 - 1705842306.976419]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [511 - 1705842306.9782686]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [511 - 1705842306.9790087]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [511 - 1705842306.979713]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [511 - 1705842306.9803185]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [511 - 1705842306.9809504]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [511 - 1705842306.9827356]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [511 - 1705842306.9845812]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [511 - 1705842306.9853678]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [511 - 1705842306.986054]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [511 - 1705842306.9867127]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [511 - 1705842306.9873219]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [511 - 1705842306.9892015]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [511 - 1705842306.991116]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [511 - 1705842306.9919121]: 0.0
python ParamPruning/classifier.weight [511 - 1705842306.9924014]: 0.0
python DistillationModifier [518 - 1705842359.0487797]: Calling loss_update with:
args: 0.17253847420215607| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.222222222222221| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [518 - 1705842360.4989743]: 
Returned: 0.07295869290828705| 

python LearningRateFunctionModifier [518 - 1705842364.7251825]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.222222222222221| steps_per_epoch: 63| 
python LearningRateFunctionModifier [518 - 1705842364.7253754]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [518 - 1705842364.72544]: 5.512820512820513e-05
python LearningRateFunctionModifier/ParamGroup1 [518 - 1705842364.7256684]: 5.512820512820513e-05
python DistillationModifier/task_loss [518 - 1705842364.725984]: tensor(0.1725, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [518 - 1705842364.7270374]: tensor(0.0730, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [518 - 1705842364.7276409]: tensor(0.0730, grad_fn=<AddBackward0>)
python ConstantPruningModifier [518 - 1705842364.7282126]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.222222222222221| steps_per_epoch: 63| 
python ConstantPruningModifier [518 - 1705842365.0250883]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [518 - 1705842365.025991]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [518 - 1705842365.0270922]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [518 - 1705842365.0280483]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [518 - 1705842365.0288525]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [518 - 1705842365.031012]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [518 - 1705842365.033061]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [518 - 1705842365.0339644]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [518 - 1705842365.034738]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [518 - 1705842365.0355232]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [518 - 1705842365.0362382]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [518 - 1705842365.038325]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [518 - 1705842365.0402482]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [518 - 1705842365.041111]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [518 - 1705842365.041882]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [518 - 1705842365.0426366]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [518 - 1705842365.0433643]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [518 - 1705842365.0452852]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [518 - 1705842365.047311]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [518 - 1705842365.0482388]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [518 - 1705842365.0490744]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [518 - 1705842365.049796]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [518 - 1705842365.0505624]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [518 - 1705842365.052703]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [518 - 1705842365.0548542]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [518 - 1705842365.0557141]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [518 - 1705842365.0564141]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [518 - 1705842365.0570517]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [518 - 1705842365.0576327]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [518 - 1705842365.0594125]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [518 - 1705842365.0612822]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [518 - 1705842365.062107]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [518 - 1705842365.0628653]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [518 - 1705842365.0635912]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [518 - 1705842365.0642772]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [518 - 1705842365.0663118]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [518 - 1705842365.0684874]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [518 - 1705842365.0693521]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [518 - 1705842365.0701272]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [518 - 1705842365.0708756]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [518 - 1705842365.071597]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [518 - 1705842365.0736957]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [518 - 1705842365.075835]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [518 - 1705842365.0767002]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [518 - 1705842365.0774286]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [518 - 1705842365.078042]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [518 - 1705842365.078613]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [518 - 1705842365.080721]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [518 - 1705842365.0828001]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [518 - 1705842365.0837162]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [518 - 1705842365.0846395]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [518 - 1705842365.085514]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [518 - 1705842365.0863194]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [518 - 1705842365.0884614]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [518 - 1705842365.0908043]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [518 - 1705842365.0918696]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [518 - 1705842365.092771]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [518 - 1705842365.0936415]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [518 - 1705842365.0943942]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [518 - 1705842365.096181]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [518 - 1705842365.0980375]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [518 - 1705842365.0988448]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [518 - 1705842365.0995336]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [518 - 1705842365.100316]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [518 - 1705842365.1010966]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [518 - 1705842365.1030898]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [518 - 1705842365.105145]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [518 - 1705842365.1059802]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [518 - 1705842365.1066427]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [518 - 1705842365.107233]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [518 - 1705842365.107903]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [518 - 1705842365.109934]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [518 - 1705842365.1120107]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [518 - 1705842365.1128762]: 0.0
python ParamPruning/classifier.weight [518 - 1705842365.1133645]: 0.0
python DistillationModifier [525 - 1705842417.174119]: Calling loss_update with:
args: 0.1573750227689743| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.333333333333334| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [525 - 1705842418.6175842]: 
Returned: 0.12646226584911346| 

python LearningRateFunctionModifier [525 - 1705842422.186358]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.333333333333334| steps_per_epoch: 63| 
python LearningRateFunctionModifier [525 - 1705842422.1865463]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [525 - 1705842422.1975975]: 5.384615384615383e-05
python LearningRateFunctionModifier/ParamGroup1 [525 - 1705842422.1978495]: 5.384615384615383e-05
python DistillationModifier/task_loss [525 - 1705842422.1981614]: tensor(0.1574, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [525 - 1705842422.1992042]: tensor(0.1265, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [525 - 1705842422.1997912]: tensor(0.1265, grad_fn=<AddBackward0>)
python ConstantPruningModifier [525 - 1705842422.2003582]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.333333333333334| steps_per_epoch: 63| 
python ConstantPruningModifier [525 - 1705842422.4994245]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [525 - 1705842422.500339]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [525 - 1705842422.5013998]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [525 - 1705842422.5021722]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [525 - 1705842422.502801]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [525 - 1705842422.5048707]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [525 - 1705842422.5069911]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [525 - 1705842422.5078928]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [525 - 1705842422.5087295]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [525 - 1705842422.5094848]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [525 - 1705842422.5101705]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [525 - 1705842422.512306]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [525 - 1705842422.5144908]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [525 - 1705842422.515358]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [525 - 1705842422.5161448]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [525 - 1705842422.5169032]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [525 - 1705842422.5175667]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [525 - 1705842422.519647]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [525 - 1705842422.5218759]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [525 - 1705842422.5227664]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [525 - 1705842422.523585]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [525 - 1705842422.5243056]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [525 - 1705842422.5250332]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [525 - 1705842422.5270896]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [525 - 1705842422.529189]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [525 - 1705842422.5300057]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [525 - 1705842422.5307393]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [525 - 1705842422.5314512]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [525 - 1705842422.532114]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [525 - 1705842422.534165]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [525 - 1705842422.536266]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [525 - 1705842422.5371294]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [525 - 1705842422.5377836]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [525 - 1705842422.5384932]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [525 - 1705842422.5390754]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [525 - 1705842422.54107]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [525 - 1705842422.5431566]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [525 - 1705842422.543973]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [525 - 1705842422.544717]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [525 - 1705842422.5453975]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [525 - 1705842422.5460331]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [525 - 1705842422.5480986]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [525 - 1705842422.5501978]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [525 - 1705842422.5509977]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [525 - 1705842422.5517855]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [525 - 1705842422.5524545]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [525 - 1705842422.5530899]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [525 - 1705842422.5551167]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [525 - 1705842422.5572114]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [525 - 1705842422.5580144]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [525 - 1705842422.5587559]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [525 - 1705842422.5594604]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [525 - 1705842422.5601358]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [525 - 1705842422.562317]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [525 - 1705842422.5644696]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [525 - 1705842422.5653985]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [525 - 1705842422.5661902]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [525 - 1705842422.566847]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [525 - 1705842422.5674348]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [525 - 1705842422.569503]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [525 - 1705842422.571742]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [525 - 1705842422.5727012]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [525 - 1705842422.573464]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [525 - 1705842422.5740993]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [525 - 1705842422.5747209]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [525 - 1705842422.5767589]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [525 - 1705842422.578899]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [525 - 1705842422.579711]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [525 - 1705842422.58048]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [525 - 1705842422.5811794]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [525 - 1705842422.5818899]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [525 - 1705842422.5839334]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [525 - 1705842422.5860362]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [525 - 1705842422.5868902]: 0.0
python ParamPruning/classifier.weight [525 - 1705842422.5873842]: 0.0
python DistillationModifier [532 - 1705842475.0650308]: Calling loss_update with:
args: 0.040059201419353485| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.444444444444445| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [532 - 1705842476.569186]: 
Returned: 0.1318286508321762| 

python LearningRateFunctionModifier [532 - 1705842479.8488173]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.444444444444445| steps_per_epoch: 63| 
python LearningRateFunctionModifier [532 - 1705842479.8490043]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [532 - 1705842479.8490624]: 5.256410256410255e-05
python LearningRateFunctionModifier/ParamGroup1 [532 - 1705842479.8492997]: 5.256410256410255e-05
python DistillationModifier/task_loss [532 - 1705842479.8496215]: tensor(0.0401, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [532 - 1705842479.8506646]: tensor(0.1318, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [532 - 1705842479.8512366]: tensor(0.1318, grad_fn=<AddBackward0>)
python ConstantPruningModifier [532 - 1705842479.851825]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.444444444444445| steps_per_epoch: 63| 
python ConstantPruningModifier [532 - 1705842480.07423]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [532 - 1705842480.0750906]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [532 - 1705842480.0890574]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [532 - 1705842480.089928]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [532 - 1705842480.0906153]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [532 - 1705842480.0924132]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [532 - 1705842480.0942557]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [532 - 1705842480.0951245]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [532 - 1705842480.0958867]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [532 - 1705842480.0965588]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [532 - 1705842480.0972888]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [532 - 1705842480.099093]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [532 - 1705842480.1008837]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [532 - 1705842480.1016638]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [532 - 1705842480.1023788]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [532 - 1705842480.1029875]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [532 - 1705842480.1035495]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [532 - 1705842480.1052413]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [532 - 1705842480.1070127]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [532 - 1705842480.1077332]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [532 - 1705842480.108388]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [532 - 1705842480.108991]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [532 - 1705842480.1095338]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [532 - 1705842480.1112692]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [532 - 1705842480.1129367]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [532 - 1705842480.1136842]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [532 - 1705842480.11436]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [532 - 1705842480.1150146]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [532 - 1705842480.1156652]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [532 - 1705842480.117443]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [532 - 1705842480.1192431]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [532 - 1705842480.1199858]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [532 - 1705842480.1206915]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [532 - 1705842480.1213603]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [532 - 1705842480.1219578]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [532 - 1705842480.1237144]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [532 - 1705842480.1255267]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [532 - 1705842480.126307]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [532 - 1705842480.1269748]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [532 - 1705842480.1276493]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [532 - 1705842480.1282349]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [532 - 1705842480.129992]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [532 - 1705842480.1315851]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [532 - 1705842480.132304]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [532 - 1705842480.1329322]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [532 - 1705842480.1335776]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [532 - 1705842480.134123]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [532 - 1705842480.135799]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [532 - 1705842480.1375718]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [532 - 1705842480.138308]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [532 - 1705842480.1389499]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [532 - 1705842480.139591]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [532 - 1705842480.1402094]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [532 - 1705842480.141922]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [532 - 1705842480.143665]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [532 - 1705842480.1443746]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [532 - 1705842480.1450374]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [532 - 1705842480.1456876]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [532 - 1705842480.1462739]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [532 - 1705842480.1479468]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [532 - 1705842480.149603]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [532 - 1705842480.1503222]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [532 - 1705842480.150926]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [532 - 1705842480.1515682]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [532 - 1705842480.1522248]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [532 - 1705842480.1539073]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [532 - 1705842480.1556242]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [532 - 1705842480.156299]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [532 - 1705842480.1569414]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [532 - 1705842480.157506]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [532 - 1705842480.1580205]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [532 - 1705842480.1596036]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [532 - 1705842480.161315]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [532 - 1705842480.1619742]: 0.0
python ParamPruning/classifier.weight [532 - 1705842480.16237]: 0.0
python DistillationModifier [539 - 1705842533.4751942]: Calling loss_update with:
args: 0.03267795220017433| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.555555555555555| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [539 - 1705842535.4210994]: 
Returned: 0.16585423052310944| 

python LearningRateFunctionModifier [539 - 1705842538.716992]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.555555555555555| steps_per_epoch: 63| 
python LearningRateFunctionModifier [539 - 1705842538.7171853]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [539 - 1705842538.7172427]: 5.128205128205128e-05
python LearningRateFunctionModifier/ParamGroup1 [539 - 1705842538.7174695]: 5.128205128205128e-05
python DistillationModifier/task_loss [539 - 1705842538.717793]: tensor(0.0327, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [539 - 1705842538.718914]: tensor(0.1659, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [539 - 1705842538.7194867]: tensor(0.1659, grad_fn=<AddBackward0>)
python ConstantPruningModifier [539 - 1705842538.7200396]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.555555555555555| steps_per_epoch: 63| 
python ConstantPruningModifier [539 - 1705842538.9445865]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [539 - 1705842538.9454446]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [539 - 1705842538.946289]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [539 - 1705842538.9470088]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [539 - 1705842538.947602]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [539 - 1705842538.9493668]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [539 - 1705842538.9511318]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [539 - 1705842538.9519196]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [539 - 1705842538.952634]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [539 - 1705842538.953327]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [539 - 1705842538.9539244]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [539 - 1705842538.9556744]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [539 - 1705842538.9574838]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [539 - 1705842538.9582114]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [539 - 1705842538.9588838]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [539 - 1705842538.9595468]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [539 - 1705842538.9601545]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [539 - 1705842538.9619663]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [539 - 1705842538.9637532]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [539 - 1705842538.9644728]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [539 - 1705842538.9651856]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [539 - 1705842538.9658148]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [539 - 1705842538.9663591]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [539 - 1705842538.9681165]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [539 - 1705842538.9699554]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [539 - 1705842538.9707203]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [539 - 1705842538.9713984]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [539 - 1705842538.9721029]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [539 - 1705842538.9727862]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [539 - 1705842538.9746635]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [539 - 1705842538.9765716]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [539 - 1705842538.9774208]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [539 - 1705842538.978183]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [539 - 1705842538.9788315]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [539 - 1705842538.9794374]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [539 - 1705842538.9812684]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [539 - 1705842538.9831593]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [539 - 1705842538.9840076]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [539 - 1705842538.9848306]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [539 - 1705842538.9855804]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [539 - 1705842538.9861875]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [539 - 1705842538.9879465]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [539 - 1705842538.9897518]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [539 - 1705842538.9905112]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [539 - 1705842538.99121]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [539 - 1705842538.9918637]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [539 - 1705842538.9924245]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [539 - 1705842538.994188]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [539 - 1705842538.9959478]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [539 - 1705842538.9967256]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [539 - 1705842538.9974144]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [539 - 1705842538.9980295]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [539 - 1705842538.998682]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [539 - 1705842539.0004537]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [539 - 1705842539.0023043]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [539 - 1705842539.003107]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [539 - 1705842539.0038462]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [539 - 1705842539.0044775]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [539 - 1705842539.0051098]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [539 - 1705842539.0068774]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [539 - 1705842539.008615]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [539 - 1705842539.0095024]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [539 - 1705842539.0103066]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [539 - 1705842539.0110326]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [539 - 1705842539.0117202]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [539 - 1705842539.0134878]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [539 - 1705842539.015254]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [539 - 1705842539.0160031]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [539 - 1705842539.0167465]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [539 - 1705842539.0173602]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [539 - 1705842539.0179093]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [539 - 1705842539.0196705]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [539 - 1705842539.0214775]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [539 - 1705842539.0222342]: 0.0
python ParamPruning/classifier.weight [539 - 1705842539.0226567]: 0.0
python DistillationModifier [546 - 1705842591.3747919]: Calling loss_update with:
args: 0.014533020555973053| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.666666666666666| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [546 - 1705842592.9350662]: 
Returned: 0.09917900711297989| 

python LearningRateFunctionModifier [546 - 1705842596.271862]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.666666666666666| steps_per_epoch: 63| 
python LearningRateFunctionModifier [546 - 1705842596.27206]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [546 - 1705842596.272118]: 4.9999999999999996e-05
python LearningRateFunctionModifier/ParamGroup1 [546 - 1705842596.272351]: 4.9999999999999996e-05
python DistillationModifier/task_loss [546 - 1705842596.272693]: tensor(0.0145, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [546 - 1705842596.2737722]: tensor(0.0992, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [546 - 1705842596.2743614]: tensor(0.0992, grad_fn=<AddBackward0>)
python ConstantPruningModifier [546 - 1705842596.2749114]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.666666666666666| steps_per_epoch: 63| 
python ConstantPruningModifier [546 - 1705842596.4991567]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [546 - 1705842596.5000339]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [546 - 1705842596.514437]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [546 - 1705842596.5152287]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [546 - 1705842596.515933]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [546 - 1705842596.5178204]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [546 - 1705842596.519742]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [546 - 1705842596.520553]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [546 - 1705842596.5212908]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [546 - 1705842596.5219717]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [546 - 1705842596.5225716]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [546 - 1705842596.5243535]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [546 - 1705842596.526155]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [546 - 1705842596.5268862]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [546 - 1705842596.5275865]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [546 - 1705842596.5282369]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [546 - 1705842596.5288453]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [546 - 1705842596.5305924]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [546 - 1705842596.5323663]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [546 - 1705842596.5331352]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [546 - 1705842596.5337267]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [546 - 1705842596.5342548]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [546 - 1705842596.5348568]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [546 - 1705842596.5365393]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [546 - 1705842596.538168]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [546 - 1705842596.5389109]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [546 - 1705842596.5396295]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [546 - 1705842596.5402396]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [546 - 1705842596.540836]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [546 - 1705842596.5426388]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [546 - 1705842596.5444584]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [546 - 1705842596.5452163]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [546 - 1705842596.545883]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [546 - 1705842596.5464935]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [546 - 1705842596.5470822]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [546 - 1705842596.548876]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [546 - 1705842596.550794]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [546 - 1705842596.5515828]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [546 - 1705842596.5523176]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [546 - 1705842596.5529573]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [546 - 1705842596.5536137]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [546 - 1705842596.5554168]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [546 - 1705842596.5571296]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [546 - 1705842596.557968]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [546 - 1705842596.5586665]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [546 - 1705842596.5592186]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [546 - 1705842596.5598328]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [546 - 1705842596.5615938]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [546 - 1705842596.5632849]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [546 - 1705842596.5641012]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [546 - 1705842596.5648627]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [546 - 1705842596.5655563]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [546 - 1705842596.5661755]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [546 - 1705842596.5679367]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [546 - 1705842596.5697632]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [546 - 1705842596.5705047]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [546 - 1705842596.5711694]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [546 - 1705842596.5717921]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [546 - 1705842596.572366]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [546 - 1705842596.5741258]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [546 - 1705842596.5758412]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [546 - 1705842596.5765274]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [546 - 1705842596.5771823]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [546 - 1705842596.5777617]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [546 - 1705842596.5783052]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [546 - 1705842596.5800385]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [546 - 1705842596.581681]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [546 - 1705842596.5824637]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [546 - 1705842596.5831335]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [546 - 1705842596.5837097]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [546 - 1705842596.5843527]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [546 - 1705842596.586125]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [546 - 1705842596.5878723]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [546 - 1705842596.588766]: 0.0
python ParamPruning/classifier.weight [546 - 1705842596.5892937]: 0.0
python DistillationModifier [553 - 1705842649.7484927]: Calling loss_update with:
args: 0.1650402992963791| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.777777777777779| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [553 - 1705842651.198375]: 
Returned: 0.19269342720508575| 

python LearningRateFunctionModifier [553 - 1705842654.483326]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.777777777777779| steps_per_epoch: 63| 
python LearningRateFunctionModifier [553 - 1705842654.4835284]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [553 - 1705842654.4835868]: 4.871794871794871e-05
python LearningRateFunctionModifier/ParamGroup1 [553 - 1705842654.4838252]: 4.871794871794871e-05
python DistillationModifier/task_loss [553 - 1705842654.484143]: tensor(0.1650, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [553 - 1705842654.485307]: tensor(0.1927, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [553 - 1705842654.4859815]: tensor(0.1927, grad_fn=<AddBackward0>)
python ConstantPruningModifier [553 - 1705842654.4866157]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.777777777777779| steps_per_epoch: 63| 
python ConstantPruningModifier [553 - 1705842654.7458506]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [553 - 1705842654.7467728]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [553 - 1705842654.7488616]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [553 - 1705842654.7496455]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [553 - 1705842654.7503793]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [553 - 1705842654.752249]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [553 - 1705842654.7542236]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [553 - 1705842654.7551825]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [553 - 1705842654.75608]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [553 - 1705842654.756942]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [553 - 1705842654.7577212]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [553 - 1705842654.7595453]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [553 - 1705842654.7613304]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [553 - 1705842654.7623444]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [553 - 1705842654.7631426]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [553 - 1705842654.7639747]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [553 - 1705842654.764738]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [553 - 1705842654.7665963]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [553 - 1705842654.768334]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [553 - 1705842654.7692559]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [553 - 1705842654.7700372]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [553 - 1705842654.7706819]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [553 - 1705842654.7713904]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [553 - 1705842654.7732592]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [553 - 1705842654.7749538]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [553 - 1705842654.7758203]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [553 - 1705842654.776643]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [553 - 1705842654.7773666]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [553 - 1705842654.7780933]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [553 - 1705842654.7798889]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [553 - 1705842654.7815795]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [553 - 1705842654.7824304]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [553 - 1705842654.783234]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [553 - 1705842654.7839806]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [553 - 1705842654.7847323]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [553 - 1705842654.7866411]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [553 - 1705842654.7882943]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [553 - 1705842654.789107]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [553 - 1705842654.7897837]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [553 - 1705842654.7905223]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [553 - 1705842654.7912455]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [553 - 1705842654.7933512]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [553 - 1705842654.7952545]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [553 - 1705842654.7960756]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [553 - 1705842654.7968457]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [553 - 1705842654.7975314]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [553 - 1705842654.7982163]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [553 - 1705842654.799924]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [553 - 1705842654.8016455]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [553 - 1705842654.8025017]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [553 - 1705842654.8032987]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [553 - 1705842654.8040104]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [553 - 1705842654.8046932]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [553 - 1705842654.8065665]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [553 - 1705842654.8082345]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [553 - 1705842654.809109]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [553 - 1705842654.8099895]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [553 - 1705842654.8107946]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [553 - 1705842654.811539]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [553 - 1705842654.8136551]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [553 - 1705842654.815667]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [553 - 1705842654.8166118]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [553 - 1705842654.8174691]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [553 - 1705842654.8181353]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [553 - 1705842654.8188486]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [553 - 1705842654.8208263]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [553 - 1705842654.8225734]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [553 - 1705842654.8234222]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [553 - 1705842654.8242023]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [553 - 1705842654.825041]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [553 - 1705842654.8257525]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [553 - 1705842654.8276563]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [553 - 1705842654.8295631]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [553 - 1705842654.8306072]: 0.0
python ParamPruning/classifier.weight [553 - 1705842654.831143]: 0.0
python DistillationModifier [560 - 1705842707.9533892]: Calling loss_update with:
args: 0.022283848375082016| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 8.88888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [560 - 1705842709.4758804]: 
Returned: 0.1560862809419632| 

python LearningRateFunctionModifier [560 - 1705842712.719868]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.88888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [560 - 1705842712.7200415]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [560 - 1705842712.720103]: 4.743589743589743e-05
python LearningRateFunctionModifier/ParamGroup1 [560 - 1705842712.7203436]: 4.743589743589743e-05
python DistillationModifier/task_loss [560 - 1705842712.720685]: tensor(0.0223, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [560 - 1705842712.721697]: tensor(0.1561, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [560 - 1705842712.7223518]: tensor(0.1561, grad_fn=<AddBackward0>)
python ConstantPruningModifier [560 - 1705842712.7229705]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 8.88888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [560 - 1705842712.9405663]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [560 - 1705842712.9414082]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [560 - 1705842712.9423606]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [560 - 1705842712.9430788]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [560 - 1705842712.943713]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [560 - 1705842712.9454658]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [560 - 1705842712.947033]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [560 - 1705842712.9477854]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [560 - 1705842712.948438]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [560 - 1705842712.9490209]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [560 - 1705842712.9495642]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [560 - 1705842712.951103]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [560 - 1705842712.9526625]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [560 - 1705842712.9534934]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [560 - 1705842712.9662352]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [560 - 1705842712.9670274]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [560 - 1705842712.9677033]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [560 - 1705842712.96944]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [560 - 1705842712.9710083]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [560 - 1705842712.9717815]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [560 - 1705842712.9724936]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [560 - 1705842712.973191]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [560 - 1705842712.973828]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [560 - 1705842712.9754872]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [560 - 1705842712.977028]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [560 - 1705842712.977813]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [560 - 1705842712.9785523]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [560 - 1705842712.9792647]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [560 - 1705842712.9798725]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [560 - 1705842712.981628]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [560 - 1705842712.983228]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [560 - 1705842712.984012]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [560 - 1705842712.98478]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [560 - 1705842712.9854007]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [560 - 1705842712.9859579]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [560 - 1705842712.9875882]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [560 - 1705842712.9891758]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [560 - 1705842712.9900064]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [560 - 1705842712.9907658]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [560 - 1705842712.9914892]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [560 - 1705842712.9921222]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [560 - 1705842712.9938326]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [560 - 1705842712.9953835]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [560 - 1705842712.9961557]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [560 - 1705842712.9969218]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [560 - 1705842712.9975781]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [560 - 1705842712.9981368]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [560 - 1705842712.9997194]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [560 - 1705842713.001346]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [560 - 1705842713.0021148]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [560 - 1705842713.0027888]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [560 - 1705842713.003393]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [560 - 1705842713.0039845]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [560 - 1705842713.0059056]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [560 - 1705842713.0077882]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [560 - 1705842713.008696]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [560 - 1705842713.0095124]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [560 - 1705842713.01014]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [560 - 1705842713.0107818]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [560 - 1705842713.0124176]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [560 - 1705842713.013896]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [560 - 1705842713.0145676]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [560 - 1705842713.0151997]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [560 - 1705842713.0158298]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [560 - 1705842713.01638]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [560 - 1705842713.0179937]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [560 - 1705842713.0194612]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [560 - 1705842713.0201333]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [560 - 1705842713.0208042]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [560 - 1705842713.0213807]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [560 - 1705842713.0219007]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [560 - 1705842713.0234883]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [560 - 1705842713.024926]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [560 - 1705842713.025579]: 0.0
python ParamPruning/classifier.weight [560 - 1705842713.0259883]: 0.0
python DistillationModifier [567 - 1705842762.3964055]: Calling loss_update with:
args: 0.952208936214447| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842763.8418593]: 
Returned: 1.7231876850128174| 

python DistillationModifier [567 - 1705842766.5534158]: Calling loss_update with:
args: 1.5523523092269897| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842768.0011933]: 
Returned: 2.542203187942505| 

python DistillationModifier [567 - 1705842770.7231152]: Calling loss_update with:
args: 1.3105928897857666| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842772.1705177]: 
Returned: 1.4614813327789307| 

python DistillationModifier [567 - 1705842774.8756962]: Calling loss_update with:
args: 0.6306924223899841| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842776.3209662]: 
Returned: 0.7599693536758423| 

python DistillationModifier [567 - 1705842779.0811286]: Calling loss_update with:
args: 0.9399638772010803| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842780.526759]: 
Returned: 1.7471132278442383| 

python DistillationModifier [567 - 1705842784.1552048]: Calling loss_update with:
args: 0.7050371766090393| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842785.7947872]: 
Returned: 1.403601050376892| 

python DistillationModifier [567 - 1705842788.505561]: Calling loss_update with:
args: 0.560213565826416| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842789.9488797]: 
Returned: 1.3111567497253418| 

python DistillationModifier [567 - 1705842792.6505415]: Calling loss_update with:
args: 1.363627552986145| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842794.0895607]: 
Returned: 1.4580962657928467| 

python DistillationModifier [567 - 1705842796.7933884]: Calling loss_update with:
args: 0.7751181721687317| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842798.2311208]: 
Returned: 0.6006671786308289| 

python DistillationModifier [567 - 1705842800.9337482]: Calling loss_update with:
args: 0.6771759986877441| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842802.3749232]: 
Returned: 1.0682518482208252| 

python DistillationModifier [567 - 1705842805.0687127]: Calling loss_update with:
args: 0.7201594710350037| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842806.5057218]: 
Returned: 1.0739400386810303| 

python DistillationModifier [567 - 1705842809.7622464]: Calling loss_update with:
args: 1.3422900438308716| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842811.200638]: 
Returned: 1.9318475723266602| 

python DistillationModifier [567 - 1705842813.8924205]: Calling loss_update with:
args: 1.2840911149978638| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842815.3254144]: 
Returned: 1.9902074337005615| 

python DistillationModifier [567 - 1705842818.01521]: Calling loss_update with:
args: 0.7327337861061096| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842819.4535403]: 
Returned: 0.9348636865615845| 

python DistillationModifier [567 - 1705842822.1686366]: Calling loss_update with:
args: 1.2257002592086792| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842823.609586]: 
Returned: 2.0630335807800293| 

python DistillationModifier [567 - 1705842826.2929685]: Calling loss_update with:
args: 1.0735670328140259| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842827.7225208]: 
Returned: 1.9032948017120361| 

python DistillationModifier [567 - 1705842830.4087214]: Calling loss_update with:
args: 0.7542033791542053| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842831.8405373]: 
Returned: 1.0844348669052124| 

python DistillationModifier [567 - 1705842834.5227966]: Calling loss_update with:
args: 1.1997629404067993| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842835.9511216]: 
Returned: 2.0907180309295654| 

python DistillationModifier [567 - 1705842838.6729233]: Calling loss_update with:
args: 1.5075634717941284| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842840.1120284]: 
Returned: 1.6662952899932861| 

python DistillationModifier [567 - 1705842843.2893806]: Calling loss_update with:
args: 0.875515878200531| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842845.3204157]: 
Returned: 1.4781968593597412| 

python DistillationModifier [567 - 1705842848.0617542]: Calling loss_update with:
args: 0.9303939938545227| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842849.4941983]: 
Returned: 1.7914432287216187| 

python DistillationModifier [567 - 1705842852.1961708]: Calling loss_update with:
args: 0.6233325600624084| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842853.636854]: 
Returned: 0.8541212677955627| 

python DistillationModifier [567 - 1705842856.3233087]: Calling loss_update with:
args: 1.055094838142395| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842857.7519944]: 
Returned: 1.3093918561935425| 

python DistillationModifier [567 - 1705842860.4409947]: Calling loss_update with:
args: 0.8295029401779175| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842861.8692043]: 
Returned: 1.4419283866882324| 

python DistillationModifier [567 - 1705842864.551812]: Calling loss_update with:
args: 1.3889875411987305| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842865.979828]: 
Returned: 1.6384878158569336| 

python DistillationModifier [567 - 1705842868.7059965]: Calling loss_update with:
args: 0.8188723921775818| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842870.13346]: 
Returned: 1.4468859434127808| 

python DistillationModifier [567 - 1705842872.810762]: Calling loss_update with:
args: 1.2705042362213135| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842874.244934]: 
Returned: 1.7824000120162964| 

python DistillationModifier [567 - 1705842877.4685001]: Calling loss_update with:
args: 0.9474592208862305| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842878.9144456]: 
Returned: 1.4116506576538086| 

python DistillationModifier [567 - 1705842881.5990818]: Calling loss_update with:
args: 0.9986841678619385| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842883.0261974]: 
Returned: 1.6391128301620483| 

python DistillationModifier [567 - 1705842885.7052124]: Calling loss_update with:
args: 0.5750451683998108| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842887.1412039]: 
Returned: 1.3622206449508667| 

python DistillationModifier [567 - 1705842889.8368683]: Calling loss_update with:
args: 1.3700839281082153| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842891.2689543]: 
Returned: 2.423161268234253| 

python DistillationModifier [567 - 1705842893.9517078]: Calling loss_update with:
args: 1.0974544286727905| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842895.3823018]: 
Returned: 1.4492602348327637| 

python DistillationModifier [567 - 1705842898.1036391]: Calling loss_update with:
args: 1.2172884941101074| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842899.5528157]: 
Returned: 2.0151500701904297| 

python DistillationModifier [567 - 1705842902.7675722]: Calling loss_update with:
args: 1.4174625873565674| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842904.643321]: 
Returned: 2.1539149284362793| 

python DistillationModifier [567 - 1705842907.3579528]: Calling loss_update with:
args: 1.7054998874664307| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842908.7998352]: 
Returned: 2.2037289142608643| 

python DistillationModifier [567 - 1705842911.5025787]: Calling loss_update with:
args: 1.3877955675125122| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842912.9410145]: 
Returned: 2.5860795974731445| 

python DistillationModifier [567 - 1705842915.6412816]: Calling loss_update with:
args: 1.1317697763442993| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842917.0731177]: 
Returned: 1.4010519981384277| 

python DistillationModifier [567 - 1705842919.7806485]: Calling loss_update with:
args: 0.4848368167877197| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842921.2194228]: 
Returned: 1.1410330533981323| 

python DistillationModifier [567 - 1705842923.916608]: Calling loss_update with:
args: 0.9338375329971313| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842925.350641]: 
Returned: 1.672675609588623| 

python DistillationModifier [567 - 1705842928.046763]: Calling loss_update with:
args: 1.309844732284546| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842929.5270698]: 
Returned: 2.531132221221924| 

python DistillationModifier [567 - 1705842932.2273233]: Calling loss_update with:
args: 0.5600184798240662| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842933.6670928]: 
Returned: 1.3542529344558716| 

python DistillationModifier [567 - 1705842936.3596923]: Calling loss_update with:
args: 1.0750445127487183| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842937.7962837]: 
Returned: 2.2355947494506836| 

python DistillationModifier [567 - 1705842940.5035732]: Calling loss_update with:
args: 0.8337535262107849| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842941.9416854]: 
Returned: 0.8816581964492798| 

python DistillationModifier [567 - 1705842944.6112242]: Calling loss_update with:
args: 0.6663824319839478| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842946.0183117]: 
Returned: 1.601308822631836| 

python DistillationModifier [567 - 1705842948.6716187]: Calling loss_update with:
args: 1.0762052536010742| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842950.0979204]: 
Returned: 2.3906564712524414| 

python DistillationModifier [567 - 1705842952.753292]: Calling loss_update with:
args: 1.4727070331573486| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842954.169471]: 
Returned: 1.9539839029312134| 

python DistillationModifier [567 - 1705842956.831816]: Calling loss_update with:
args: 0.6969156265258789| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842958.249187]: 
Returned: 1.5629099607467651| 

python DistillationModifier [567 - 1705842961.4881184]: Calling loss_update with:
args: 0.29853886365890503| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842963.466733]: 
Returned: 0.6930273175239563| 

python DistillationModifier [567 - 1705842966.7010734]: Calling loss_update with:
args: 1.2690459489822388| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842968.1387832]: 
Returned: 2.3554792404174805| 

python DistillationModifier [567 - 1705842970.8457332]: Calling loss_update with:
args: 1.0631022453308105| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842972.286974]: 
Returned: 1.9840501546859741| 

python DistillationModifier [567 - 1705842974.992267]: Calling loss_update with:
args: 0.3946053683757782| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842976.4384096]: 
Returned: 0.9863258004188538| 

python DistillationModifier [567 - 1705842979.1422734]: Calling loss_update with:
args: 1.5689524412155151| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842980.6044693]: 
Returned: 1.57527756690979| 

python DistillationModifier [567 - 1705842983.308524]: Calling loss_update with:
args: 1.0260292291641235| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842984.765551]: 
Returned: 1.3617743253707886| 

python DistillationModifier [567 - 1705842987.4724329]: Calling loss_update with:
args: 0.8444141149520874| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842988.9150727]: 
Returned: 1.3787686824798584| 

python DistillationModifier [567 - 1705842991.7070885]: Calling loss_update with:
args: 0.6599239706993103| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842993.1572433]: 
Returned: 1.546556830406189| 

python DistillationModifier [567 - 1705842995.8687563]: Calling loss_update with:
args: 1.0396010875701904| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705842997.3188043]: 
Returned: 1.430799126625061| 

python DistillationModifier [567 - 1705843000.0412252]: Calling loss_update with:
args: 1.0660051107406616| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705843001.4938855]: 
Returned: 0.9317165017127991| 

python DistillationModifier [567 - 1705843004.2023296]: Calling loss_update with:
args: 1.1906079053878784| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705843005.6497235]: 
Returned: 1.5178864002227783| 

python DistillationModifier [567 - 1705843008.3544104]: Calling loss_update with:
args: 1.2183363437652588| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705843009.8055599]: 
Returned: 2.015498638153076| 

python DistillationModifier [567 - 1705843012.5131977]: Calling loss_update with:
args: 1.1123460531234741| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705843013.9537194]: 
Returned: 2.117934465408325| 

python DistillationModifier [567 - 1705843016.651801]: Calling loss_update with:
args: 1.1208361387252808| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705843018.0850015]: 
Returned: 1.834721565246582| 

python DistillationModifier [567 - 1705843020.8321502]: Calling loss_update with:
args: 1.249837040901184| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705843022.7780626]: 
Returned: 2.208491325378418| 

python DistillationModifier [567 - 1705843024.9971824]: Calling loss_update with:
args: 0.30170735716819763| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705843026.0481982]: 
Returned: 0.6496586799621582| 

python DistillationModifier [567 - 1705843029.4126413]: Calling loss_update with:
args: 0.009681453928351402| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [567 - 1705843030.8664844]: 
Returned: 0.07335823774337769| 

python LearningRateFunctionModifier [567 - 1705843034.1618102]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [567 - 1705843034.1619918]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [567 - 1705843034.1620867]: 4.615384615384616e-05
python LearningRateFunctionModifier/ParamGroup1 [567 - 1705843034.1623318]: 4.615384615384616e-05
python DistillationModifier/task_loss [567 - 1705843034.162633]: tensor(0.0097, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [567 - 1705843034.1637807]: tensor(0.0734, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [567 - 1705843034.1644475]: tensor(0.0734, grad_fn=<AddBackward0>)
python ConstantPruningModifier [567 - 1705843034.165123]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.0| steps_per_epoch: 63| 
python ConstantPruningModifier [567 - 1705843034.3871713]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [567 - 1705843034.3880005]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [567 - 1705843034.3888886]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [567 - 1705843034.3896117]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [567 - 1705843034.3902285]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [567 - 1705843034.391962]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [567 - 1705843034.3938334]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [567 - 1705843034.3946483]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [567 - 1705843034.3953145]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [567 - 1705843034.3959022]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [567 - 1705843034.396449]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [567 - 1705843034.3982413]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [567 - 1705843034.4000897]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [567 - 1705843034.4008977]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [567 - 1705843034.4015563]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [567 - 1705843034.4021235]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [567 - 1705843034.4026494]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [567 - 1705843034.4043791]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [567 - 1705843034.406224]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [567 - 1705843034.4070044]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [567 - 1705843034.4076726]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [567 - 1705843034.408261]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [567 - 1705843034.4088275]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [567 - 1705843034.4105997]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [567 - 1705843034.412538]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [567 - 1705843034.4135752]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [567 - 1705843034.4142845]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [567 - 1705843034.4148762]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [567 - 1705843034.4154115]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [567 - 1705843034.4170818]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [567 - 1705843034.4188812]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [567 - 1705843034.4196618]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [567 - 1705843034.420301]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [567 - 1705843034.4208922]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [567 - 1705843034.4214396]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [567 - 1705843034.4230874]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [567 - 1705843034.4248376]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [567 - 1705843034.4255753]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [567 - 1705843034.4262009]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [567 - 1705843034.4267511]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [567 - 1705843034.4272697]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [567 - 1705843034.4290183]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [567 - 1705843034.4308112]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [567 - 1705843034.4315503]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [567 - 1705843034.4322069]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [567 - 1705843034.4327729]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [567 - 1705843034.4332936]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [567 - 1705843034.4350321]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [567 - 1705843034.4368665]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [567 - 1705843034.4376493]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [567 - 1705843034.4383109]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [567 - 1705843034.4389522]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [567 - 1705843034.4395413]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [567 - 1705843034.4413035]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [567 - 1705843034.4431117]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [567 - 1705843034.4438934]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [567 - 1705843034.4445734]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [567 - 1705843034.4451885]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [567 - 1705843034.4457362]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [567 - 1705843034.4474614]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [567 - 1705843034.4492927]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [567 - 1705843034.4500585]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [567 - 1705843034.4506962]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [567 - 1705843034.4512732]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [567 - 1705843034.4518223]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [567 - 1705843034.4536095]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [567 - 1705843034.455413]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [567 - 1705843034.4561987]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [567 - 1705843034.4568808]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [567 - 1705843034.4574833]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [567 - 1705843034.4582026]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [567 - 1705843034.4609609]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [567 - 1705843034.4628656]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [567 - 1705843034.4636939]: 0.0
python ParamPruning/classifier.weight [567 - 1705843034.4641807]: 0.0
python DistillationModifier [574 - 1705843086.964918]: Calling loss_update with:
args: 0.10030283033847809| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.11111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [574 - 1705843088.5041077]: 
Returned: 0.06733711808919907| 

python LearningRateFunctionModifier [574 - 1705843091.8185835]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.11111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [574 - 1705843091.8187673]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [574 - 1705843091.818835]: 4.4871794871794874e-05
python LearningRateFunctionModifier/ParamGroup1 [574 - 1705843091.819071]: 4.4871794871794874e-05
python DistillationModifier/task_loss [574 - 1705843091.8193817]: tensor(0.1003, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [574 - 1705843091.8204978]: tensor(0.0673, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [574 - 1705843091.8211908]: tensor(0.0673, grad_fn=<AddBackward0>)
python ConstantPruningModifier [574 - 1705843091.8218105]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.11111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [574 - 1705843092.04677]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [574 - 1705843092.0476081]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [574 - 1705843092.0485349]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [574 - 1705843092.049381]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [574 - 1705843092.0501301]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [574 - 1705843092.0521052]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [574 - 1705843092.053948]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [574 - 1705843092.0548694]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [574 - 1705843092.055678]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [574 - 1705843092.05637]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [574 - 1705843092.0570095]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [574 - 1705843092.0588603]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [574 - 1705843092.0609443]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [574 - 1705843092.0617895]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [574 - 1705843092.0625591]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [574 - 1705843092.0632472]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [574 - 1705843092.0639355]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [574 - 1705843092.0658202]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [574 - 1705843092.067759]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [574 - 1705843092.0685477]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [574 - 1705843092.0693173]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [574 - 1705843092.0699751]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [574 - 1705843092.0706005]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [574 - 1705843092.0724285]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [574 - 1705843092.0743885]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [574 - 1705843092.0751832]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [574 - 1705843092.075895]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [574 - 1705843092.0765655]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [574 - 1705843092.0772514]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [574 - 1705843092.079138]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [574 - 1705843092.0809639]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [574 - 1705843092.0819082]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [574 - 1705843092.0826588]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [574 - 1705843092.0833836]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [574 - 1705843092.0840082]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [574 - 1705843092.0859048]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [574 - 1705843092.0878863]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [574 - 1705843092.0887344]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [574 - 1705843092.0894923]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [574 - 1705843092.0902185]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [574 - 1705843092.0908759]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [574 - 1705843092.0927682]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [574 - 1705843092.0946608]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [574 - 1705843092.0954592]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [574 - 1705843092.0962124]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [574 - 1705843092.0968993]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [574 - 1705843092.0975542]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [574 - 1705843092.099448]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [574 - 1705843092.101384]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [574 - 1705843092.1022446]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [574 - 1705843092.1030042]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [574 - 1705843092.1037002]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [574 - 1705843092.1043978]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [574 - 1705843092.1063168]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [574 - 1705843092.108224]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [574 - 1705843092.1091275]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [574 - 1705843092.1099193]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [574 - 1705843092.1106126]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [574 - 1705843092.1112607]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [574 - 1705843092.11314]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [574 - 1705843092.1149516]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [574 - 1705843092.1157591]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [574 - 1705843092.1165025]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [574 - 1705843092.1171856]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [574 - 1705843092.117813]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [574 - 1705843092.1196709]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [574 - 1705843092.1215942]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [574 - 1705843092.1223845]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [574 - 1705843092.1231291]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [574 - 1705843092.1237674]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [574 - 1705843092.124381]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [574 - 1705843092.1263037]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [574 - 1705843092.128293]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [574 - 1705843092.1291656]: 0.0
python ParamPruning/classifier.weight [574 - 1705843092.1296778]: 0.0
python DistillationModifier [581 - 1705843145.003843]: Calling loss_update with:
args: 0.05547147989273071| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.222222222222221| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [581 - 1705843146.5023282]: 
Returned: 0.10187150537967682| 

python LearningRateFunctionModifier [581 - 1705843149.789299]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.222222222222221| steps_per_epoch: 63| 
python LearningRateFunctionModifier [581 - 1705843149.789467]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [581 - 1705843149.7895393]: 4.3589743589743604e-05
python LearningRateFunctionModifier/ParamGroup1 [581 - 1705843149.7897742]: 4.3589743589743604e-05
python DistillationModifier/task_loss [581 - 1705843149.790122]: tensor(0.0555, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [581 - 1705843149.791173]: tensor(0.1019, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [581 - 1705843149.7918787]: tensor(0.1019, grad_fn=<AddBackward0>)
python ConstantPruningModifier [581 - 1705843149.7925382]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.222222222222221| steps_per_epoch: 63| 
python ConstantPruningModifier [581 - 1705843150.0125124]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [581 - 1705843150.013414]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [581 - 1705843150.0243545]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [581 - 1705843150.025211]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [581 - 1705843150.025956]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [581 - 1705843150.0277889]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [581 - 1705843150.0297544]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [581 - 1705843150.030627]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [581 - 1705843150.0313394]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [581 - 1705843150.0319955]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [581 - 1705843150.0325956]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [581 - 1705843150.0344284]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [581 - 1705843150.0362706]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [581 - 1705843150.037185]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [581 - 1705843150.0380206]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [581 - 1705843150.0387459]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [581 - 1705843150.0394678]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [581 - 1705843150.0413392]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [581 - 1705843150.0432878]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [581 - 1705843150.0441306]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [581 - 1705843150.0449002]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [581 - 1705843150.0455961]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [581 - 1705843150.0462458]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [581 - 1705843150.0480778]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [581 - 1705843150.0499775]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [581 - 1705843150.0508049]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [581 - 1705843150.0515387]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [581 - 1705843150.0522325]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [581 - 1705843150.0529468]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [581 - 1705843150.0547745]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [581 - 1705843150.0566833]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [581 - 1705843150.057563]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [581 - 1705843150.0583348]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [581 - 1705843150.0590138]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [581 - 1705843150.0596306]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [581 - 1705843150.0614288]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [581 - 1705843150.063352]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [581 - 1705843150.0641496]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [581 - 1705843150.064822]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [581 - 1705843150.0654035]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [581 - 1705843150.0660675]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [581 - 1705843150.0677743]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [581 - 1705843150.0696228]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [581 - 1705843150.0703957]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [581 - 1705843150.0711577]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [581 - 1705843150.0718334]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [581 - 1705843150.0724661]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [581 - 1705843150.0742598]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [581 - 1705843150.0761502]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [581 - 1705843150.0769784]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [581 - 1705843150.0777295]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [581 - 1705843150.0783937]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [581 - 1705843150.0790136]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [581 - 1705843150.0807972]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [581 - 1705843150.0827045]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [581 - 1705843150.0835438]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [581 - 1705843150.0843256]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [581 - 1705843150.0850494]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [581 - 1705843150.0857615]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [581 - 1705843150.0875897]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [581 - 1705843150.0893276]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [581 - 1705843150.0902083]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [581 - 1705843150.0910022]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [581 - 1705843150.0917213]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [581 - 1705843150.092323]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [581 - 1705843150.094065]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [581 - 1705843150.0956984]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [581 - 1705843150.096545]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [581 - 1705843150.0973473]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [581 - 1705843150.0980375]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [581 - 1705843150.0986712]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [581 - 1705843150.1003993]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [581 - 1705843150.1022575]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [581 - 1705843150.1029942]: 0.0
python ParamPruning/classifier.weight [581 - 1705843150.1034503]: 0.0
python DistillationModifier [588 - 1705843202.7956934]: Calling loss_update with:
args: 0.275892972946167| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.333333333333334| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [588 - 1705843204.9246972]: 
Returned: 0.10614177584648132| 

python LearningRateFunctionModifier [588 - 1705843208.3319635]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.333333333333334| steps_per_epoch: 63| 
python LearningRateFunctionModifier [588 - 1705843208.3321762]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [588 - 1705843208.3508544]: 4.230769230769231e-05
python LearningRateFunctionModifier/ParamGroup1 [588 - 1705843208.3511362]: 4.230769230769231e-05
python DistillationModifier/task_loss [588 - 1705843208.3514543]: tensor(0.2759, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [588 - 1705843208.3525631]: tensor(0.1061, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [588 - 1705843208.3532908]: tensor(0.1061, grad_fn=<AddBackward0>)
python ConstantPruningModifier [588 - 1705843208.353965]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.333333333333334| steps_per_epoch: 63| 
python ConstantPruningModifier [588 - 1705843208.575571]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [588 - 1705843208.5764427]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [588 - 1705843208.5773666]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [588 - 1705843208.5780993]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [588 - 1705843208.5787039]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [588 - 1705843208.580492]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [588 - 1705843208.5824497]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [588 - 1705843208.5833051]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [588 - 1705843208.5840898]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [588 - 1705843208.5848463]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [588 - 1705843208.5855262]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [588 - 1705843208.587412]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [588 - 1705843208.5893996]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [588 - 1705843208.590267]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [588 - 1705843208.5910673]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [588 - 1705843208.5917754]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [588 - 1705843208.5924094]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [588 - 1705843208.5942576]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [588 - 1705843208.5962045]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [588 - 1705843208.5970325]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [588 - 1705843208.5976985]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [588 - 1705843208.5982811]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [588 - 1705843208.599584]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [588 - 1705843208.6014662]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [588 - 1705843208.603339]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [588 - 1705843208.6041355]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [588 - 1705843208.6048818]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [588 - 1705843208.605538]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [588 - 1705843208.606172]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [588 - 1705843208.6080008]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [588 - 1705843208.6099463]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [588 - 1705843208.6108024]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [588 - 1705843208.6115582]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [588 - 1705843208.6123078]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [588 - 1705843208.6130242]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [588 - 1705843208.6148658]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [588 - 1705843208.6167934]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [588 - 1705843208.6176162]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [588 - 1705843208.6183717]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [588 - 1705843208.6191347]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [588 - 1705843208.6197882]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [588 - 1705843208.6216068]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [588 - 1705843208.6234207]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [588 - 1705843208.6241744]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [588 - 1705843208.624828]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [588 - 1705843208.6254942]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [588 - 1705843208.6260607]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [588 - 1705843208.6278183]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [588 - 1705843208.6297016]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [588 - 1705843208.630468]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [588 - 1705843208.6312141]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [588 - 1705843208.631878]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [588 - 1705843208.6325665]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [588 - 1705843208.6343951]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [588 - 1705843208.6362698]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [588 - 1705843208.6370778]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [588 - 1705843208.6378386]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [588 - 1705843208.6385515]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [588 - 1705843208.6391761]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [588 - 1705843208.6410096]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [588 - 1705843208.6427026]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [588 - 1705843208.6435785]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [588 - 1705843208.6444063]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [588 - 1705843208.6451764]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [588 - 1705843208.6458437]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [588 - 1705843208.647664]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [588 - 1705843208.6493611]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [588 - 1705843208.650198]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [588 - 1705843208.6509807]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [588 - 1705843208.6516857]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [588 - 1705843208.6523747]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [588 - 1705843208.6541836]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [588 - 1705843208.6558585]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [588 - 1705843208.6567545]: 0.0
python ParamPruning/classifier.weight [588 - 1705843208.6573176]: 0.0
python DistillationModifier [595 - 1705843262.3521764]: Calling loss_update with:
args: 0.029927600175142288| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.444444444444445| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [595 - 1705843264.397078]: 
Returned: 0.08001720160245895| 

python LearningRateFunctionModifier [595 - 1705843267.814368]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.444444444444445| steps_per_epoch: 63| 
python LearningRateFunctionModifier [595 - 1705843267.8145537]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [595 - 1705843267.8146245]: 4.1025641025641023e-05
python LearningRateFunctionModifier/ParamGroup1 [595 - 1705843267.814877]: 4.1025641025641023e-05
python DistillationModifier/task_loss [595 - 1705843267.815204]: tensor(0.0299, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [595 - 1705843267.8163698]: tensor(0.0800, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [595 - 1705843267.8171291]: tensor(0.0800, grad_fn=<AddBackward0>)
python ConstantPruningModifier [595 - 1705843267.8178022]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.444444444444445| steps_per_epoch: 63| 
python ConstantPruningModifier [595 - 1705843268.0423229]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [595 - 1705843268.043184]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [595 - 1705843268.0440695]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [595 - 1705843268.0447547]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [595 - 1705843268.045388]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [595 - 1705843268.0472279]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [595 - 1705843268.0490496]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [595 - 1705843268.0499847]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [595 - 1705843268.050768]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [595 - 1705843268.051424]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [595 - 1705843268.05204]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [595 - 1705843268.0538063]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [595 - 1705843268.0556192]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [595 - 1705843268.0564103]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [595 - 1705843268.0571206]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [595 - 1705843268.057735]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [595 - 1705843268.058311]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [595 - 1705843268.060064]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [595 - 1705843268.0618143]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [595 - 1705843268.0626864]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [595 - 1705843268.0634067]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [595 - 1705843268.064048]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [595 - 1705843268.0646057]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [595 - 1705843268.0663438]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [595 - 1705843268.0680609]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [595 - 1705843268.0689242]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [595 - 1705843268.06966]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [595 - 1705843268.0702827]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [595 - 1705843268.070854]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [595 - 1705843268.0727232]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [595 - 1705843268.0744917]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [595 - 1705843268.075377]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [595 - 1705843268.076119]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [595 - 1705843268.0767877]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [595 - 1705843268.0773702]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [595 - 1705843268.0791352]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [595 - 1705843268.0811307]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [595 - 1705843268.082056]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [595 - 1705843268.0827975]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [595 - 1705843268.0834558]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [595 - 1705843268.0840425]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [595 - 1705843268.0858333]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [595 - 1705843268.0877008]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [595 - 1705843268.088542]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [595 - 1705843268.0892358]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [595 - 1705843268.089852]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [595 - 1705843268.090434]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [595 - 1705843268.0922565]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [595 - 1705843268.0941713]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [595 - 1705843268.0950139]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [595 - 1705843268.0957296]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [595 - 1705843268.0963597]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [595 - 1705843268.0969539]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [595 - 1705843268.098811]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [595 - 1705843268.1006262]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [595 - 1705843268.101563]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [595 - 1705843268.1023066]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [595 - 1705843268.1029382]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [595 - 1705843268.1035407]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [595 - 1705843268.1053498]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [595 - 1705843268.1072507]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [595 - 1705843268.108119]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [595 - 1705843268.1088333]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [595 - 1705843268.1094515]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [595 - 1705843268.1100059]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [595 - 1705843268.1118279]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [595 - 1705843268.1136756]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [595 - 1705843268.114482]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [595 - 1705843268.1151938]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [595 - 1705843268.1158078]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [595 - 1705843268.1163838]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [595 - 1705843268.118199]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [595 - 1705843268.1201324]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [595 - 1705843268.1210907]: 0.0
python ParamPruning/classifier.weight [595 - 1705843268.121644]: 0.0
python DistillationModifier [602 - 1705843320.8668714]: Calling loss_update with:
args: 0.0995987132191658| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.555555555555555| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [602 - 1705843322.5602713]: 
Returned: 0.2229568064212799| 

python LearningRateFunctionModifier [602 - 1705843326.8417504]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.555555555555555| steps_per_epoch: 63| 
python LearningRateFunctionModifier [602 - 1705843326.841928]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [602 - 1705843326.841997]: 3.974358974358975e-05
python LearningRateFunctionModifier/ParamGroup1 [602 - 1705843326.842239]: 3.974358974358975e-05
python DistillationModifier/task_loss [602 - 1705843326.8425586]: tensor(0.0996, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [602 - 1705843326.8436654]: tensor(0.2230, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [602 - 1705843326.8443048]: tensor(0.2230, grad_fn=<AddBackward0>)
python ConstantPruningModifier [602 - 1705843326.8449786]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.555555555555555| steps_per_epoch: 63| 
python ConstantPruningModifier [602 - 1705843327.0701988]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [602 - 1705843327.071046]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [602 - 1705843327.071963]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [602 - 1705843327.072786]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [602 - 1705843327.073463]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [602 - 1705843327.0753126]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [602 - 1705843327.0770833]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [602 - 1705843327.0779927]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [602 - 1705843327.0787401]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [602 - 1705843327.0794384]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [602 - 1705843327.0801077]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [602 - 1705843327.0819042]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [602 - 1705843327.0837495]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [602 - 1705843327.08454]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [602 - 1705843327.0852153]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [602 - 1705843327.0857892]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [602 - 1705843327.086368]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [602 - 1705843327.0881073]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [602 - 1705843327.089947]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [602 - 1705843327.0906997]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [602 - 1705843327.0913448]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [602 - 1705843327.0919557]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [602 - 1705843327.0925102]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [602 - 1705843327.0942824]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [602 - 1705843327.0961115]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [602 - 1705843327.0968852]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [602 - 1705843327.0975304]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [602 - 1705843327.0982158]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [602 - 1705843327.0988104]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [602 - 1705843327.1006377]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [602 - 1705843327.1024065]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [602 - 1705843327.1032689]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [602 - 1705843327.1040494]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [602 - 1705843327.1047752]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [602 - 1705843327.1053915]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [602 - 1705843327.107205]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [602 - 1705843327.1091032]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [602 - 1705843327.1098742]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [602 - 1705843327.1105425]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [602 - 1705843327.111174]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [602 - 1705843327.1117485]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [602 - 1705843327.113539]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [602 - 1705843327.1154275]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [602 - 1705843327.1162336]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [602 - 1705843327.1169221]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [602 - 1705843327.1175263]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [602 - 1705843327.1180747]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [602 - 1705843327.1198435]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [602 - 1705843327.1217368]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [602 - 1705843327.1225686]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [602 - 1705843327.123241]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [602 - 1705843327.1238444]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [602 - 1705843327.124388]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [602 - 1705843327.1262043]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [602 - 1705843327.1279564]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [602 - 1705843327.1288464]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [602 - 1705843327.1296613]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [602 - 1705843327.1303701]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [602 - 1705843327.1310158]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [602 - 1705843327.1328769]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [602 - 1705843327.1347651]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [602 - 1705843327.1355858]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [602 - 1705843327.1362562]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [602 - 1705843327.1368737]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [602 - 1705843327.137426]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [602 - 1705843327.1392143]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [602 - 1705843327.1410916]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [602 - 1705843327.1419258]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [602 - 1705843327.1426048]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [602 - 1705843327.1432033]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [602 - 1705843327.143757]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [602 - 1705843327.1455843]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [602 - 1705843327.147483]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [602 - 1705843327.148333]: 0.0
python ParamPruning/classifier.weight [602 - 1705843327.1488705]: 0.0
python DistillationModifier [609 - 1705843380.278534]: Calling loss_update with:
args: 0.009591205976903439| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.666666666666666| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [609 - 1705843381.7309394]: 
Returned: 0.06044688820838928| 

python LearningRateFunctionModifier [609 - 1705843386.2986896]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.666666666666666| steps_per_epoch: 63| 
python LearningRateFunctionModifier [609 - 1705843386.2988684]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [609 - 1705843386.298939]: 3.846153846153847e-05
python LearningRateFunctionModifier/ParamGroup1 [609 - 1705843386.2991834]: 3.846153846153847e-05
python DistillationModifier/task_loss [609 - 1705843386.2994964]: tensor(0.0096, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [609 - 1705843386.3005576]: tensor(0.0604, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [609 - 1705843386.301195]: tensor(0.0604, grad_fn=<AddBackward0>)
python ConstantPruningModifier [609 - 1705843386.3017874]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.666666666666666| steps_per_epoch: 63| 
python ConstantPruningModifier [609 - 1705843386.5989702]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [609 - 1705843386.600041]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [609 - 1705843386.6011212]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [609 - 1705843386.6019018]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [609 - 1705843386.6025403]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [609 - 1705843386.6045983]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [609 - 1705843386.6069186]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [609 - 1705843386.6078174]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [609 - 1705843386.6085808]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [609 - 1705843386.6093206]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [609 - 1705843386.60996]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [609 - 1705843386.6118565]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [609 - 1705843386.6137633]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [609 - 1705843386.6146326]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [609 - 1705843386.615427]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [609 - 1705843386.6161141]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [609 - 1705843386.616725]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [609 - 1705843386.6188383]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [609 - 1705843386.6211462]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [609 - 1705843386.6220217]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [609 - 1705843386.6227486]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [609 - 1705843386.6233888]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [609 - 1705843386.6239865]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [609 - 1705843386.6261435]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [609 - 1705843386.6284416]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [609 - 1705843386.6293645]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [609 - 1705843386.6300406]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [609 - 1705843386.6306221]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [609 - 1705843386.631159]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [609 - 1705843386.6332152]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [609 - 1705843386.63552]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [609 - 1705843386.6366148]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [609 - 1705843386.637537]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [609 - 1705843386.6385944]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [609 - 1705843386.6393955]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [609 - 1705843386.6416566]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [609 - 1705843386.6438344]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [609 - 1705843386.6449041]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [609 - 1705843386.6457918]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [609 - 1705843386.6465268]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [609 - 1705843386.647259]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [609 - 1705843386.6494558]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [609 - 1705843386.6518104]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [609 - 1705843386.6527796]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [609 - 1705843386.6535678]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [609 - 1705843386.6542928]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [609 - 1705843386.654902]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [609 - 1705843386.6570213]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [609 - 1705843386.6592329]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [609 - 1705843386.6600995]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [609 - 1705843386.6608274]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [609 - 1705843386.6614997]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [609 - 1705843386.662105]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [609 - 1705843386.664151]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [609 - 1705843386.666452]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [609 - 1705843386.6674166]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [609 - 1705843386.668176]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [609 - 1705843386.668875]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [609 - 1705843386.669513]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [609 - 1705843386.6715672]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [609 - 1705843386.6739109]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [609 - 1705843386.674779]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [609 - 1705843386.6755059]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [609 - 1705843386.6761642]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [609 - 1705843386.6767886]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [609 - 1705843386.6788175]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [609 - 1705843386.6810102]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [609 - 1705843386.681864]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [609 - 1705843386.6825674]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [609 - 1705843386.6832192]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [609 - 1705843386.6838427]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [609 - 1705843386.685985]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [609 - 1705843386.6881943]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [609 - 1705843386.6891108]: 0.0
python ParamPruning/classifier.weight [609 - 1705843386.6896346]: 0.0
python DistillationModifier [616 - 1705843439.7332573]: Calling loss_update with:
args: 0.10453332215547562| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.777777777777779| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [616 - 1705843441.2346303]: 
Returned: 0.25179654359817505| 

python LearningRateFunctionModifier [616 - 1705843445.6020188]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.777777777777779| steps_per_epoch: 63| 
python LearningRateFunctionModifier [616 - 1705843445.6022248]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [616 - 1705843445.6198227]: 3.717948717948716e-05
python LearningRateFunctionModifier/ParamGroup1 [616 - 1705843445.6201048]: 3.717948717948716e-05
python DistillationModifier/task_loss [616 - 1705843445.620404]: tensor(0.1045, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [616 - 1705843445.6214507]: tensor(0.2518, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [616 - 1705843445.6220634]: tensor(0.2518, grad_fn=<AddBackward0>)
python ConstantPruningModifier [616 - 1705843445.6226263]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.777777777777779| steps_per_epoch: 63| 
python ConstantPruningModifier [616 - 1705843445.8469977]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [616 - 1705843445.847865]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [616 - 1705843445.848812]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [616 - 1705843445.849521]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [616 - 1705843445.8501422]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [616 - 1705843445.8519692]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [616 - 1705843445.8539224]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [616 - 1705843445.8547516]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [616 - 1705843445.8554373]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [616 - 1705843445.8560314]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [616 - 1705843445.856582]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [616 - 1705843445.8583715]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [616 - 1705843445.8602042]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [616 - 1705843445.8610015]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [616 - 1705843445.86166]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [616 - 1705843445.8622203]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [616 - 1705843445.8627388]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [616 - 1705843445.8645415]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [616 - 1705843445.8664668]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [616 - 1705843445.867284]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [616 - 1705843445.867931]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [616 - 1705843445.8685062]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [616 - 1705843445.8690574]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [616 - 1705843445.8707917]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [616 - 1705843445.872613]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [616 - 1705843445.873353]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [616 - 1705843445.8739538]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [616 - 1705843445.874505]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [616 - 1705843445.8750248]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [616 - 1705843445.876816]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [616 - 1705843445.8786936]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [616 - 1705843445.8794153]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [616 - 1705843445.880008]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [616 - 1705843445.8805473]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [616 - 1705843445.8810906]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [616 - 1705843445.8827558]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [616 - 1705843445.884353]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [616 - 1705843445.885164]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [616 - 1705843445.8858202]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [616 - 1705843445.8864064]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [616 - 1705843445.8869245]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [616 - 1705843445.8886423]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [616 - 1705843445.8904953]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [616 - 1705843445.8912344]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [616 - 1705843445.891875]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [616 - 1705843445.8924208]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [616 - 1705843445.8929648]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [616 - 1705843445.8946931]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [616 - 1705843445.8964446]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [616 - 1705843445.8971796]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [616 - 1705843445.8977766]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [616 - 1705843445.898317]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [616 - 1705843445.8988237]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [616 - 1705843445.9005804]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [616 - 1705843445.902363]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [616 - 1705843445.9030738]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [616 - 1705843445.9036684]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [616 - 1705843445.9041946]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [616 - 1705843445.9047186]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [616 - 1705843445.9064775]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [616 - 1705843445.908207]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [616 - 1705843445.9090247]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [616 - 1705843445.9097087]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [616 - 1705843445.910273]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [616 - 1705843445.9108007]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [616 - 1705843445.9125607]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [616 - 1705843445.9144077]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [616 - 1705843445.9151447]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [616 - 1705843445.9157565]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [616 - 1705843445.9163122]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [616 - 1705843445.9168515]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [616 - 1705843445.9185808]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [616 - 1705843445.9204075]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [616 - 1705843445.921133]: 0.0
python ParamPruning/classifier.weight [616 - 1705843445.921564]: 0.0
python DistillationModifier [623 - 1705843499.5750167]: Calling loss_update with:
args: 0.20181725919246674| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 9.88888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [623 - 1705843501.101903]: 
Returned: 0.08291259407997131| 

python LearningRateFunctionModifier [623 - 1705843505.5394316]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.88888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [623 - 1705843505.539599]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [623 - 1705843505.5396724]: 3.589743589743589e-05
python LearningRateFunctionModifier/ParamGroup1 [623 - 1705843505.5399034]: 3.589743589743589e-05
python DistillationModifier/task_loss [623 - 1705843505.5402427]: tensor(0.2018, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [623 - 1705843505.5414414]: tensor(0.0829, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [623 - 1705843505.5421956]: tensor(0.0829, grad_fn=<AddBackward0>)
python ConstantPruningModifier [623 - 1705843505.5428765]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 9.88888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [623 - 1705843505.7693133]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [623 - 1705843505.770184]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [623 - 1705843505.7710836]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [623 - 1705843505.7718725]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [623 - 1705843505.7724786]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [623 - 1705843505.7743335]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [623 - 1705843505.7761707]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [623 - 1705843505.777088]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [623 - 1705843505.7778032]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [623 - 1705843505.778399]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [623 - 1705843505.7789562]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [623 - 1705843505.780729]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [623 - 1705843505.7825859]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [623 - 1705843505.7833776]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [623 - 1705843505.7840009]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [623 - 1705843505.7845607]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [623 - 1705843505.7851453]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [623 - 1705843505.7868636]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [623 - 1705843505.7887104]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [623 - 1705843505.7894726]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [623 - 1705843505.7900994]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [623 - 1705843505.7906659]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [623 - 1705843505.7911904]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [623 - 1705843505.7930095]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [623 - 1705843505.7948875]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [623 - 1705843505.7957065]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [623 - 1705843505.7963622]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [623 - 1705843505.7969708]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [623 - 1705843505.7975237]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [623 - 1705843505.799243]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [623 - 1705843505.801082]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [623 - 1705843505.8018565]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [623 - 1705843505.8024917]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [623 - 1705843505.803066]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [623 - 1705843505.8036137]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [623 - 1705843505.8053482]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [623 - 1705843505.807062]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [623 - 1705843505.8079367]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [623 - 1705843505.808684]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [623 - 1705843505.8092983]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [623 - 1705843505.809842]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [623 - 1705843505.8116157]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [623 - 1705843505.8134668]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [623 - 1705843505.8142486]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [623 - 1705843505.8149025]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [623 - 1705843505.8154395]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [623 - 1705843505.8159595]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [623 - 1705843505.8176372]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [623 - 1705843505.8194504]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [623 - 1705843505.8202133]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [623 - 1705843505.8208327]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [623 - 1705843505.8213797]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [623 - 1705843505.8219113]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [623 - 1705843505.8237107]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [623 - 1705843505.8254983]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [623 - 1705843505.826275]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [623 - 1705843505.8268914]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [623 - 1705843505.8274498]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [623 - 1705843505.827979]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [623 - 1705843505.8297756]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [623 - 1705843505.8315165]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [623 - 1705843505.8323736]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [623 - 1705843505.8331177]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [623 - 1705843505.8337219]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [623 - 1705843505.8342612]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [623 - 1705843505.836078]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [623 - 1705843505.8380103]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [623 - 1705843505.8388147]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [623 - 1705843505.8394504]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [623 - 1705843505.8400164]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [623 - 1705843505.840544]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [623 - 1705843505.842324]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [623 - 1705843505.8440325]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [623 - 1705843505.8448844]: 0.0
python ParamPruning/classifier.weight [623 - 1705843505.8453739]: 0.0
python DistillationModifier [630 - 1705843555.7825081]: Calling loss_update with:
args: 0.857816219329834| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843557.2231934]: 
Returned: 1.6731313467025757| 

python DistillationModifier [630 - 1705843559.920866]: Calling loss_update with:
args: 1.39666748046875| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843561.3561847]: 
Returned: 2.2303507328033447| 

python DistillationModifier [630 - 1705843565.0646315]: Calling loss_update with:
args: 1.3647630214691162| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843566.5417075]: 
Returned: 1.5562366247177124| 

python DistillationModifier [630 - 1705843569.2435565]: Calling loss_update with:
args: 0.7858058214187622| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843570.6858191]: 
Returned: 1.0045785903930664| 

python DistillationModifier [630 - 1705843573.3996997]: Calling loss_update with:
args: 0.8321287035942078| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843574.8408237]: 
Returned: 1.3564074039459229| 

python DistillationModifier [630 - 1705843577.5464985]: Calling loss_update with:
args: 0.8441239595413208| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843578.9848526]: 
Returned: 1.58995521068573| 

python DistillationModifier [630 - 1705843581.690388]: Calling loss_update with:
args: 0.511181116104126| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843583.1248543]: 
Returned: 1.1705927848815918| 

python DistillationModifier [630 - 1705843585.8325975]: Calling loss_update with:
args: 1.3517509698867798| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843587.2699087]: 
Returned: 1.4517730474472046| 

python DistillationModifier [630 - 1705843589.9668174]: Calling loss_update with:
args: 0.7367550730705261| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843591.4063306]: 
Returned: 0.5636632442474365| 

python DistillationModifier [630 - 1705843594.1004386]: Calling loss_update with:
args: 0.8574866056442261| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843595.5365856]: 
Returned: 1.3773448467254639| 

python DistillationModifier [630 - 1705843598.734294]: Calling loss_update with:
args: 0.8627728819847107| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843600.1777492]: 
Returned: 1.3849505186080933| 

python DistillationModifier [630 - 1705843602.8888223]: Calling loss_update with:
args: 0.9728588461875916| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843604.324908]: 
Returned: 1.5652973651885986| 

python DistillationModifier [630 - 1705843607.01221]: Calling loss_update with:
args: 1.1850770711898804| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843608.441022]: 
Returned: 2.017310857772827| 

python DistillationModifier [630 - 1705843611.1386046]: Calling loss_update with:
args: 0.8526023626327515| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843612.5773003]: 
Returned: 1.0273154973983765| 

python DistillationModifier [630 - 1705843615.2554448]: Calling loss_update with:
args: 1.1480207443237305| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843616.6821122]: 
Returned: 1.9696314334869385| 

python DistillationModifier [630 - 1705843619.3635979]: Calling loss_update with:
args: 1.1216109991073608| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843620.7979655]: 
Returned: 1.9545204639434814| 

python DistillationModifier [630 - 1705843624.5530972]: Calling loss_update with:
args: 0.9101749062538147| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843625.998372]: 
Returned: 1.3315528631210327| 

python DistillationModifier [630 - 1705843628.7373054]: Calling loss_update with:
args: 1.158450961112976| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843630.1721132]: 
Returned: 2.059288263320923| 

python DistillationModifier [630 - 1705843632.8752623]: Calling loss_update with:
args: 1.3832578659057617| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843634.3131685]: 
Returned: 1.586236596107483| 

python DistillationModifier [630 - 1705843637.005792]: Calling loss_update with:
args: 0.9124354720115662| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843638.4387197]: 
Returned: 1.439461588859558| 

python DistillationModifier [630 - 1705843641.133098]: Calling loss_update with:
args: 1.0131797790527344| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843642.5702062]: 
Returned: 1.7318320274353027| 

python DistillationModifier [630 - 1705843645.2684207]: Calling loss_update with:
args: 0.7645703554153442| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843646.7062833]: 
Returned: 1.1562800407409668| 

python DistillationModifier [630 - 1705843649.413775]: Calling loss_update with:
args: 0.9803333282470703| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843650.8499484]: 
Returned: 1.5009119510650635| 

python DistillationModifier [630 - 1705843653.5536337]: Calling loss_update with:
args: 0.8593482971191406| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843654.9905162]: 
Returned: 1.4636372327804565| 

python DistillationModifier [630 - 1705843658.0086102]: Calling loss_update with:
args: 1.338085412979126| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843659.7027576]: 
Returned: 1.5237882137298584| 

python DistillationModifier [630 - 1705843662.411939]: Calling loss_update with:
args: 0.6949000954627991| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843663.8607247]: 
Returned: 1.229103684425354| 

python DistillationModifier [630 - 1705843666.5604632]: Calling loss_update with:
args: 1.1816778182983398| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843667.972914]: 
Returned: 1.6233959197998047| 

python DistillationModifier [630 - 1705843670.60557]: Calling loss_update with:
args: 0.8580384850502014| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843672.013689]: 
Returned: 1.3132463693618774| 

python DistillationModifier [630 - 1705843674.669909]: Calling loss_update with:
args: 0.7333663702011108| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843676.0804617]: 
Returned: 1.5676915645599365| 

python DistillationModifier [630 - 1705843678.7294495]: Calling loss_update with:
args: 0.6254773139953613| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843680.145174]: 
Returned: 1.2646929025650024| 

python DistillationModifier [630 - 1705843683.4366095]: Calling loss_update with:
args: 1.405990719795227| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843685.3449702]: 
Returned: 2.4622418880462646| 

python DistillationModifier [630 - 1705843688.0685148]: Calling loss_update with:
args: 1.0930421352386475| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843689.5034075]: 
Returned: 1.411589503288269| 

python DistillationModifier [630 - 1705843692.2140229]: Calling loss_update with:
args: 1.075019359588623| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843693.6632943]: 
Returned: 1.7425082921981812| 

python DistillationModifier [630 - 1705843696.3772957]: Calling loss_update with:
args: 1.3238941431045532| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843697.8188074]: 
Returned: 1.990131139755249| 

python DistillationModifier [630 - 1705843700.5253906]: Calling loss_update with:
args: 1.6004607677459717| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843701.980199]: 
Returned: 1.9853872060775757| 

python DistillationModifier [630 - 1705843704.6931348]: Calling loss_update with:
args: 1.3165946006774902| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843706.1363742]: 
Returned: 2.4241178035736084| 

python DistillationModifier [630 - 1705843708.8413968]: Calling loss_update with:
args: 1.188298225402832| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843710.2844176]: 
Returned: 1.3118748664855957| 

python DistillationModifier [630 - 1705843712.993973]: Calling loss_update with:
args: 0.3919870853424072| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843714.4462025]: 
Returned: 0.951653778553009| 

python DistillationModifier [630 - 1705843717.1571546]: Calling loss_update with:
args: 0.7949070334434509| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843718.64448]: 
Returned: 1.413730502128601| 

python DistillationModifier [630 - 1705843721.3524177]: Calling loss_update with:
args: 1.085314154624939| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843722.8179193]: 
Returned: 2.0996923446655273| 

python DistillationModifier [630 - 1705843725.521156]: Calling loss_update with:
args: 0.6100566983222961| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843726.9588451]: 
Returned: 1.7417486906051636| 

python DistillationModifier [630 - 1705843729.6712503]: Calling loss_update with:
args: 0.9250355362892151| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843731.116074]: 
Returned: 1.9501502513885498| 

python DistillationModifier [630 - 1705843733.837454]: Calling loss_update with:
args: 0.800578236579895| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843735.285199]: 
Returned: 0.8884739875793457| 

python DistillationModifier [630 - 1705843738.5367222]: Calling loss_update with:
args: 0.7920290231704712| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843739.9812427]: 
Returned: 1.6434727907180786| 

python DistillationModifier [630 - 1705843743.6378758]: Calling loss_update with:
args: 1.367073893547058| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843745.077378]: 
Returned: 2.8755362033843994| 

python DistillationModifier [630 - 1705843747.784429]: Calling loss_update with:
args: 1.4395570755004883| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843749.2691047]: 
Returned: 1.9638975858688354| 

python DistillationModifier [630 - 1705843751.9937916]: Calling loss_update with:
args: 0.7613043189048767| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843753.4456124]: 
Returned: 1.6908752918243408| 

python DistillationModifier [630 - 1705843756.1550345]: Calling loss_update with:
args: 0.3468991219997406| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843757.5921273]: 
Returned: 0.7622974514961243| 

python DistillationModifier [630 - 1705843760.277201]: Calling loss_update with:
args: 1.4467060565948486| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843761.718681]: 
Returned: 2.6030936241149902| 

python DistillationModifier [630 - 1705843764.4158938]: Calling loss_update with:
args: 0.9971542358398438| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843765.8450158]: 
Returned: 1.8276140689849854| 

python DistillationModifier [630 - 1705843768.5338326]: Calling loss_update with:
args: 0.6322236061096191| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843769.9671435]: 
Returned: 1.1607898473739624| 

python DistillationModifier [630 - 1705843772.6606553]: Calling loss_update with:
args: 1.6294384002685547| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843774.109071]: 
Returned: 1.5423015356063843| 

python DistillationModifier [630 - 1705843776.8004372]: Calling loss_update with:
args: 1.0983866453170776| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843778.2245026]: 
Returned: 1.5049617290496826| 

python DistillationModifier [630 - 1705843780.9569561]: Calling loss_update with:
args: 0.8051166534423828| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843782.3915746]: 
Returned: 1.2654695510864258| 

python DistillationModifier [630 - 1705843785.091192]: Calling loss_update with:
args: 0.6350061893463135| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843786.5223756]: 
Returned: 1.3497668504714966| 

python DistillationModifier [630 - 1705843789.2034087]: Calling loss_update with:
args: 0.8617258667945862| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843790.6336524]: 
Returned: 1.2636817693710327| 

python DistillationModifier [630 - 1705843793.3331583]: Calling loss_update with:
args: 1.0004510879516602| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843794.760993]: 
Returned: 0.8738532662391663| 

python DistillationModifier [630 - 1705843797.4397247]: Calling loss_update with:
args: 1.0996944904327393| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843798.8659222]: 
Returned: 1.319278597831726| 

python DistillationModifier [630 - 1705843801.6533983]: Calling loss_update with:
args: 1.1463921070098877| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843803.673444]: 
Returned: 1.9992260932922363| 

python DistillationModifier [630 - 1705843807.334733]: Calling loss_update with:
args: 1.176102876663208| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843808.7658203]: 
Returned: 2.1360907554626465| 

python DistillationModifier [630 - 1705843811.4900546]: Calling loss_update with:
args: 0.9814660549163818| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843812.9248745]: 
Returned: 1.606127381324768| 

python DistillationModifier [630 - 1705843815.6186728]: Calling loss_update with:
args: 1.2089567184448242| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843817.053057]: 
Returned: 2.292734384536743| 

python DistillationModifier [630 - 1705843818.5782433]: Calling loss_update with:
args: 0.5866391062736511| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843819.3229659]: 
Returned: 1.1502488851547241| 

python DistillationModifier [630 - 1705843822.6951463]: Calling loss_update with:
args: 0.09773357957601547| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [630 - 1705843824.1491318]: 
Returned: 0.21336941421031952| 

python LearningRateFunctionModifier [630 - 1705843827.4457324]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [630 - 1705843827.4459162]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [630 - 1705843827.445984]: 3.4615384615384605e-05
python LearningRateFunctionModifier/ParamGroup1 [630 - 1705843827.446226]: 3.4615384615384605e-05
python DistillationModifier/task_loss [630 - 1705843827.4465694]: tensor(0.0977, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [630 - 1705843827.4476953]: tensor(0.2134, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [630 - 1705843827.4484644]: tensor(0.2134, grad_fn=<AddBackward0>)
python ConstantPruningModifier [630 - 1705843827.4492247]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.0| steps_per_epoch: 63| 
python ConstantPruningModifier [630 - 1705843827.6712594]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [630 - 1705843827.672118]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [630 - 1705843827.6730971]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [630 - 1705843827.673929]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [630 - 1705843827.6746862]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [630 - 1705843827.6765676]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [630 - 1705843827.6785388]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [630 - 1705843827.6794271]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [630 - 1705843827.6802537]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [630 - 1705843827.681001]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [630 - 1705843827.681698]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [630 - 1705843827.683531]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [630 - 1705843827.6854455]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [630 - 1705843827.6862822]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [630 - 1705843827.6870573]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [630 - 1705843827.687733]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [630 - 1705843827.6883357]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [630 - 1705843827.6901608]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [630 - 1705843827.6921196]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [630 - 1705843827.6929984]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [630 - 1705843827.6937873]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [630 - 1705843827.694432]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [630 - 1705843827.6950665]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [630 - 1705843827.6968899]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [630 - 1705843827.6985903]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [630 - 1705843827.6994479]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [630 - 1705843827.7002356]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [630 - 1705843827.7009208]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [630 - 1705843827.701544]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [630 - 1705843827.7033246]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [630 - 1705843827.705245]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [630 - 1705843827.7061288]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [630 - 1705843827.7069027]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [630 - 1705843827.7076552]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [630 - 1705843827.7083683]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [630 - 1705843827.7101245]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [630 - 1705843827.711968]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [630 - 1705843827.712777]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [630 - 1705843827.7135122]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [630 - 1705843827.7141747]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [630 - 1705843827.7148447]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [630 - 1705843827.7166166]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [630 - 1705843827.7185314]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [630 - 1705843827.7193832]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [630 - 1705843827.7201493]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [630 - 1705843827.7208061]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [630 - 1705843827.7214105]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [630 - 1705843827.7231157]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [630 - 1705843827.7249153]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [630 - 1705843827.7257166]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [630 - 1705843827.7264552]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [630 - 1705843827.7271478]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [630 - 1705843827.727818]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [630 - 1705843827.7295828]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [630 - 1705843827.7313962]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [630 - 1705843827.7321892]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [630 - 1705843827.7329624]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [630 - 1705843827.7336779]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [630 - 1705843827.734289]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [630 - 1705843827.736057]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [630 - 1705843827.7379582]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [630 - 1705843827.738788]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [630 - 1705843827.7395182]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [630 - 1705843827.7401612]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [630 - 1705843827.740778]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [630 - 1705843827.7425528]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [630 - 1705843827.744476]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [630 - 1705843827.745355]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [630 - 1705843827.746135]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [630 - 1705843827.74685]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [630 - 1705843827.7475379]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [630 - 1705843827.7493758]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [630 - 1705843827.7510908]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [630 - 1705843827.752031]: 0.0
python ParamPruning/classifier.weight [630 - 1705843827.7525778]: 0.0
python DistillationModifier [637 - 1705843882.2877157]: Calling loss_update with:
args: 0.3222760856151581| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.11111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [637 - 1705843883.8082228]: 
Returned: 0.2880162298679352| 

python LearningRateFunctionModifier [637 - 1705843887.1011343]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.11111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [637 - 1705843887.1013103]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [637 - 1705843887.1013777]: 3.3333333333333335e-05
python LearningRateFunctionModifier/ParamGroup1 [637 - 1705843887.1016064]: 3.3333333333333335e-05
python DistillationModifier/task_loss [637 - 1705843887.1019187]: tensor(0.3223, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [637 - 1705843887.1029556]: tensor(0.2880, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [637 - 1705843887.1034951]: tensor(0.2880, grad_fn=<AddBackward0>)
python ConstantPruningModifier [637 - 1705843887.1040256]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.11111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [637 - 1705843887.3255556]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [637 - 1705843887.326425]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [637 - 1705843887.3372586]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [637 - 1705843887.338154]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [637 - 1705843887.3388937]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [637 - 1705843887.3407593]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [637 - 1705843887.3426373]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [637 - 1705843887.343422]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [637 - 1705843887.3441806]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [637 - 1705843887.3449051]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [637 - 1705843887.345549]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [637 - 1705843887.3473606]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [637 - 1705843887.3492653]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [637 - 1705843887.350165]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [637 - 1705843887.3509107]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [637 - 1705843887.351602]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [637 - 1705843887.3522313]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [637 - 1705843887.3539896]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [637 - 1705843887.3557727]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [637 - 1705843887.3564806]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [637 - 1705843887.3571653]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [637 - 1705843887.357756]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [637 - 1705843887.3583279]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [637 - 1705843887.3600628]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [637 - 1705843887.3618975]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [637 - 1705843887.362675]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [637 - 1705843887.3633838]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [637 - 1705843887.3640647]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [637 - 1705843887.3647091]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [637 - 1705843887.3665044]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [637 - 1705843887.368365]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [637 - 1705843887.369171]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [637 - 1705843887.369902]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [637 - 1705843887.3705904]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [637 - 1705843887.371231]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [637 - 1705843887.373078]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [637 - 1705843887.3747368]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [637 - 1705843887.3755243]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [637 - 1705843887.3762755]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [637 - 1705843887.3770106]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [637 - 1705843887.3776977]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [637 - 1705843887.3794718]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [637 - 1705843887.3813593]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [637 - 1705843887.382146]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [637 - 1705843887.382831]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [637 - 1705843887.3834386]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [637 - 1705843887.3840222]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [637 - 1705843887.385784]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [637 - 1705843887.3876388]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [637 - 1705843887.3884187]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [637 - 1705843887.3891728]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [637 - 1705843887.3898432]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [637 - 1705843887.3904808]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [637 - 1705843887.3922412]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [637 - 1705843887.3940928]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [637 - 1705843887.3948283]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [637 - 1705843887.3955219]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [637 - 1705843887.3961833]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [637 - 1705843887.3967998]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [637 - 1705843887.398563]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [637 - 1705843887.4003897]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [637 - 1705843887.4011433]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [637 - 1705843887.4017525]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [637 - 1705843887.4023201]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [637 - 1705843887.4029417]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [637 - 1705843887.4048152]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [637 - 1705843887.4066362]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [637 - 1705843887.4073749]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [637 - 1705843887.4080462]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [637 - 1705843887.4086363]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [637 - 1705843887.409232]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [637 - 1705843887.410939]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [637 - 1705843887.4127896]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [637 - 1705843887.4135385]: 0.0
python ParamPruning/classifier.weight [637 - 1705843887.413955]: 0.0
python DistillationModifier [644 - 1705843941.6767292]: Calling loss_update with:
args: 0.1503954529762268| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.222222222222221| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [644 - 1705843943.2271252]: 
Returned: 0.11298269033432007| 

python LearningRateFunctionModifier [644 - 1705843946.5296235]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.222222222222221| steps_per_epoch: 63| 
python LearningRateFunctionModifier [644 - 1705843946.529809]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [644 - 1705843946.5298748]: 3.205128205128205e-05
python LearningRateFunctionModifier/ParamGroup1 [644 - 1705843946.530116]: 3.205128205128205e-05
python DistillationModifier/task_loss [644 - 1705843946.5304308]: tensor(0.1504, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [644 - 1705843946.5314891]: tensor(0.1130, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [644 - 1705843946.5321255]: tensor(0.1130, grad_fn=<AddBackward0>)
python ConstantPruningModifier [644 - 1705843946.5327044]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.222222222222221| steps_per_epoch: 63| 
python ConstantPruningModifier [644 - 1705843946.7545807]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [644 - 1705843946.7554522]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [644 - 1705843946.7667646]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [644 - 1705843946.767567]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [644 - 1705843946.7682009]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [644 - 1705843946.7700453]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [644 - 1705843946.771908]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [644 - 1705843946.772725]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [644 - 1705843946.773391]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [644 - 1705843946.7739847]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [644 - 1705843946.774526]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [644 - 1705843946.7763302]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [644 - 1705843946.778119]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [644 - 1705843946.7788138]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [644 - 1705843946.7793941]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [644 - 1705843946.77992]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [644 - 1705843946.7804308]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [644 - 1705843946.7820716]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [644 - 1705843946.783837]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [644 - 1705843946.7845354]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [644 - 1705843946.7851262]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [644 - 1705843946.785636]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [644 - 1705843946.7861671]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [644 - 1705843946.7878368]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [644 - 1705843946.7896261]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [644 - 1705843946.7903655]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [644 - 1705843946.7909827]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [644 - 1705843946.7915134]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [644 - 1705843946.7920473]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [644 - 1705843946.793824]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [644 - 1705843946.7956486]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [644 - 1705843946.7964325]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [644 - 1705843946.7971015]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [644 - 1705843946.7976687]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [644 - 1705843946.7981827]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [644 - 1705843946.7998595]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [644 - 1705843946.8015764]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [644 - 1705843946.8022547]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [644 - 1705843946.8028283]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [644 - 1705843946.8033812]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [644 - 1705843946.8038833]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [644 - 1705843946.805604]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [644 - 1705843946.8073509]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [644 - 1705843946.8080745]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [644 - 1705843946.8086975]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [644 - 1705843946.8092418]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [644 - 1705843946.8097384]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [644 - 1705843946.8114583]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [644 - 1705843946.8131537]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [644 - 1705843946.8139145]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [644 - 1705843946.8145368]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [644 - 1705843946.8150806]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [644 - 1705843946.815579]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [644 - 1705843946.8172946]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [644 - 1705843946.8191695]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [644 - 1705843946.8199825]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [644 - 1705843946.8206306]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [644 - 1705843946.821228]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [644 - 1705843946.8217466]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [644 - 1705843946.823512]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [644 - 1705843946.825253]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [644 - 1705843946.8260736]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [644 - 1705843946.8267386]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [644 - 1705843946.8272958]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [644 - 1705843946.8278072]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [644 - 1705843946.8294659]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [644 - 1705843946.8312426]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [644 - 1705843946.831975]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [644 - 1705843946.8325896]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [644 - 1705843946.8331654]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [644 - 1705843946.8336782]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [644 - 1705843946.8353963]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [644 - 1705843946.8370528]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [644 - 1705843946.8378022]: 0.0
python ParamPruning/classifier.weight [644 - 1705843946.8382342]: 0.0
python DistillationModifier [651 - 1705844001.1414819]: Calling loss_update with:
args: 0.04935495927929878| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.333333333333334| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [651 - 1705844002.660112]: 
Returned: 0.10535430908203125| 

python LearningRateFunctionModifier [651 - 1705844005.9677012]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.333333333333334| steps_per_epoch: 63| 
python LearningRateFunctionModifier [651 - 1705844005.9678736]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [651 - 1705844005.9679413]: 3.0769230769230754e-05
python LearningRateFunctionModifier/ParamGroup1 [651 - 1705844005.968181]: 3.0769230769230754e-05
python DistillationModifier/task_loss [651 - 1705844005.9685056]: tensor(0.0494, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [651 - 1705844005.9696777]: tensor(0.1054, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [651 - 1705844005.970272]: tensor(0.1054, grad_fn=<AddBackward0>)
python ConstantPruningModifier [651 - 1705844005.9708483]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.333333333333334| steps_per_epoch: 63| 
python ConstantPruningModifier [651 - 1705844006.1958482]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [651 - 1705844006.1967506]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [651 - 1705844006.2097044]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [651 - 1705844006.2104783]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [651 - 1705844006.2112575]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [651 - 1705844006.21312]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [651 - 1705844006.2149346]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [651 - 1705844006.2157292]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [651 - 1705844006.2164536]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [651 - 1705844006.217153]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [651 - 1705844006.217782]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [651 - 1705844006.219521]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [651 - 1705844006.2213871]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [651 - 1705844006.2222037]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [651 - 1705844006.2229133]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [651 - 1705844006.2235465]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [651 - 1705844006.2241154]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [651 - 1705844006.2259016]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [651 - 1705844006.2274995]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [651 - 1705844006.228255]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [651 - 1705844006.2288961]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [651 - 1705844006.229557]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [651 - 1705844006.2301083]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [651 - 1705844006.2319007]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [651 - 1705844006.2337615]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [651 - 1705844006.2345302]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [651 - 1705844006.2352228]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [651 - 1705844006.2358732]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [651 - 1705844006.23649]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [651 - 1705844006.2383225]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [651 - 1705844006.240201]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [651 - 1705844006.241056]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [651 - 1705844006.2417877]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [651 - 1705844006.2424152]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [651 - 1705844006.2430928]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [651 - 1705844006.244894]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [651 - 1705844006.2465339]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [651 - 1705844006.2473366]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [651 - 1705844006.248006]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [651 - 1705844006.2485797]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [651 - 1705844006.2491472]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [651 - 1705844006.2508528]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [651 - 1705844006.2525368]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [651 - 1705844006.2533975]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [651 - 1705844006.254196]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [651 - 1705844006.2549067]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [651 - 1705844006.2555194]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [651 - 1705844006.2573395]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [651 - 1705844006.2591875]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [651 - 1705844006.2599845]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [651 - 1705844006.2607403]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [651 - 1705844006.2613783]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [651 - 1705844006.2621334]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [651 - 1705844006.263942]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [651 - 1705844006.2658925]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [651 - 1705844006.2666893]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [651 - 1705844006.2673762]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [651 - 1705844006.2680404]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [651 - 1705844006.268685]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [651 - 1705844006.2705421]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [651 - 1705844006.2722812]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [651 - 1705844006.2731857]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [651 - 1705844006.2739294]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [651 - 1705844006.274539]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [651 - 1705844006.275098]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [651 - 1705844006.2768972]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [651 - 1705844006.2788153]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [651 - 1705844006.2796435]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [651 - 1705844006.2803771]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [651 - 1705844006.2811055]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [651 - 1705844006.281759]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [651 - 1705844006.2836096]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [651 - 1705844006.2855027]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [651 - 1705844006.286338]: 0.0
python ParamPruning/classifier.weight [651 - 1705844006.2868216]: 0.0
python DistillationModifier [658 - 1705844060.7827673]: Calling loss_update with:
args: 0.26529842615127563| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.444444444444445| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [658 - 1705844062.2264001]: 
Returned: 0.1188034787774086| 

python LearningRateFunctionModifier [658 - 1705844065.5186017]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.444444444444445| steps_per_epoch: 63| 
python LearningRateFunctionModifier [658 - 1705844065.5187812]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [658 - 1705844065.5188513]: 2.9487179487179484e-05
python LearningRateFunctionModifier/ParamGroup1 [658 - 1705844065.5190952]: 2.9487179487179484e-05
python DistillationModifier/task_loss [658 - 1705844065.5194235]: tensor(0.2653, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [658 - 1705844065.5204935]: tensor(0.1188, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [658 - 1705844065.5211082]: tensor(0.1188, grad_fn=<AddBackward0>)
python ConstantPruningModifier [658 - 1705844065.521693]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.444444444444445| steps_per_epoch: 63| 
python ConstantPruningModifier [658 - 1705844065.7447765]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [658 - 1705844065.7456272]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [658 - 1705844065.746562]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [658 - 1705844065.7473953]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [658 - 1705844065.7481222]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [658 - 1705844065.7499778]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [658 - 1705844065.7517738]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [658 - 1705844065.752664]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [658 - 1705844065.7534077]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [658 - 1705844065.7541125]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [658 - 1705844065.7546988]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [658 - 1705844065.7565331]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [658 - 1705844065.75846]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [658 - 1705844065.7592914]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [658 - 1705844065.7599833]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [658 - 1705844065.7605815]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [658 - 1705844065.761253]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [658 - 1705844065.7630298]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [658 - 1705844065.7647338]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [658 - 1705844065.765538]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [658 - 1705844065.7662494]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [658 - 1705844065.7668765]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [658 - 1705844065.7674346]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [658 - 1705844065.7692218]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [658 - 1705844065.7710633]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [658 - 1705844065.7718673]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [658 - 1705844065.7726107]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [658 - 1705844065.7732878]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [658 - 1705844065.7738647]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [658 - 1705844065.7756164]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [658 - 1705844065.7773566]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [658 - 1705844065.7781937]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [658 - 1705844065.7788935]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [658 - 1705844065.7794788]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [658 - 1705844065.780127]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [658 - 1705844065.7818675]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [658 - 1705844065.7837021]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [658 - 1705844065.793276]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [658 - 1705844065.7940052]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [658 - 1705844065.7947228]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [658 - 1705844065.7952964]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [658 - 1705844065.7970538]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [658 - 1705844065.798835]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [658 - 1705844065.7995298]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [658 - 1705844065.8001232]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [658 - 1705844065.8006628]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [658 - 1705844065.80131]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [658 - 1705844065.803032]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [658 - 1705844065.8046994]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [658 - 1705844065.805494]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [658 - 1705844065.806223]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [658 - 1705844065.8069167]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [658 - 1705844065.8075728]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [658 - 1705844065.8093956]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [658 - 1705844065.8110895]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [658 - 1705844065.811944]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [658 - 1705844065.81272]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [658 - 1705844065.8133605]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [658 - 1705844065.8139124]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [658 - 1705844065.8156073]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [658 - 1705844065.8174562]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [658 - 1705844065.8182156]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [658 - 1705844065.8188527]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [658 - 1705844065.8194127]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [658 - 1705844065.820062]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [658 - 1705844065.8218567]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [658 - 1705844065.823526]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [658 - 1705844065.8243225]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [658 - 1705844065.8250813]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [658 - 1705844065.8257244]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [658 - 1705844065.8262887]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [658 - 1705844065.8280487]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [658 - 1705844065.8298955]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [658 - 1705844065.8306506]: 0.0
python ParamPruning/classifier.weight [658 - 1705844065.8310869]: 0.0
python DistillationModifier [665 - 1705844120.1416879]: Calling loss_update with:
args: 0.19058094918727875| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.555555555555555| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [665 - 1705844121.6975224]: 
Returned: 0.25310495495796204| 

python LearningRateFunctionModifier [665 - 1705844124.9804487]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.555555555555555| steps_per_epoch: 63| 
python LearningRateFunctionModifier [665 - 1705844124.9806366]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [665 - 1705844124.9807286]: 2.82051282051282e-05
python LearningRateFunctionModifier/ParamGroup1 [665 - 1705844124.9809573]: 2.82051282051282e-05
python DistillationModifier/task_loss [665 - 1705844124.9812999]: tensor(0.1906, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [665 - 1705844124.9823465]: tensor(0.2531, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [665 - 1705844124.9830306]: tensor(0.2531, grad_fn=<AddBackward0>)
python ConstantPruningModifier [665 - 1705844124.9836857]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.555555555555555| steps_per_epoch: 63| 
python ConstantPruningModifier [665 - 1705844125.2065594]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [665 - 1705844125.207437]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [665 - 1705844125.227503]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [665 - 1705844125.2282574]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [665 - 1705844125.2289157]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [665 - 1705844125.2307093]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [665 - 1705844125.2326393]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [665 - 1705844125.2334592]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [665 - 1705844125.2341216]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [665 - 1705844125.234705]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [665 - 1705844125.2352605]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [665 - 1705844125.237002]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [665 - 1705844125.2386556]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [665 - 1705844125.2394505]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [665 - 1705844125.240135]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [665 - 1705844125.2407393]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [665 - 1705844125.2413065]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [665 - 1705844125.242972]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [665 - 1705844125.2448344]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [665 - 1705844125.2456558]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [665 - 1705844125.2463121]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [665 - 1705844125.246889]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [665 - 1705844125.247427]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [665 - 1705844125.2491605]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [665 - 1705844125.2509995]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [665 - 1705844125.25179]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [665 - 1705844125.2524264]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [665 - 1705844125.253015]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [665 - 1705844125.253565]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [665 - 1705844125.2552695]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [665 - 1705844125.2571464]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [665 - 1705844125.2579424]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [665 - 1705844125.258611]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [665 - 1705844125.25919]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [665 - 1705844125.2597466]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [665 - 1705844125.2615025]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [665 - 1705844125.2631779]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [665 - 1705844125.2639675]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [665 - 1705844125.264646]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [665 - 1705844125.2652671]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [665 - 1705844125.2658358]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [665 - 1705844125.2675745]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [665 - 1705844125.2695012]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [665 - 1705844125.2703347]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [665 - 1705844125.271112]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [665 - 1705844125.2717433]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [665 - 1705844125.272322]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [665 - 1705844125.2740474]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [665 - 1705844125.2758923]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [665 - 1705844125.2766829]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [665 - 1705844125.2773461]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [665 - 1705844125.2779257]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [665 - 1705844125.2784789]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [665 - 1705844125.2801838]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [665 - 1705844125.2820675]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [665 - 1705844125.2828362]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [665 - 1705844125.2834802]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [665 - 1705844125.2840667]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [665 - 1705844125.2846234]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [665 - 1705844125.2863953]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [665 - 1705844125.2880857]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [665 - 1705844125.288919]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [665 - 1705844125.289636]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [665 - 1705844125.2902472]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [665 - 1705844125.2908282]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [665 - 1705844125.2926433]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [665 - 1705844125.2945554]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [665 - 1705844125.2953408]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [665 - 1705844125.2960536]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [665 - 1705844125.2967572]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [665 - 1705844125.2973533]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [665 - 1705844125.2990372]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [665 - 1705844125.3010638]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [665 - 1705844125.302414]: 0.0
python ParamPruning/classifier.weight [665 - 1705844125.3029296]: 0.0
python DistillationModifier [672 - 1705844179.4912393]: Calling loss_update with:
args: 0.20586979389190674| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.666666666666666| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [672 - 1705844180.9428916]: 
Returned: 0.15828892588615417| 

python LearningRateFunctionModifier [672 - 1705844184.2370691]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.666666666666666| steps_per_epoch: 63| 
python LearningRateFunctionModifier [672 - 1705844184.2372427]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [672 - 1705844184.2373118]: 2.6923076923076917e-05
python LearningRateFunctionModifier/ParamGroup1 [672 - 1705844184.237559]: 2.6923076923076917e-05
python DistillationModifier/task_loss [672 - 1705844184.2378712]: tensor(0.2059, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [672 - 1705844184.2389944]: tensor(0.1583, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [672 - 1705844184.239644]: tensor(0.1583, grad_fn=<AddBackward0>)
python ConstantPruningModifier [672 - 1705844184.2402668]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.666666666666666| steps_per_epoch: 63| 
python ConstantPruningModifier [672 - 1705844184.463386]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [672 - 1705844184.4642396]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [672 - 1705844184.46512]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [672 - 1705844184.4657445]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [672 - 1705844184.4663925]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [672 - 1705844184.468125]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [672 - 1705844184.4699292]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [672 - 1705844184.4706993]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [672 - 1705844184.4713833]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [672 - 1705844184.4720318]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [672 - 1705844184.472647]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [672 - 1705844184.4744906]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [672 - 1705844184.4763768]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [672 - 1705844184.477173]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [672 - 1705844184.4778802]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [672 - 1705844184.4784858]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [672 - 1705844184.479076]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [672 - 1705844184.4808402]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [672 - 1705844184.4826396]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [672 - 1705844184.4833875]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [672 - 1705844184.4840157]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [672 - 1705844184.484576]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [672 - 1705844184.4852178]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [672 - 1705844184.4869533]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [672 - 1705844184.4886243]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [672 - 1705844184.4894774]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [672 - 1705844184.4902291]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [672 - 1705844184.4908566]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [672 - 1705844184.491411]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [672 - 1705844184.4932213]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [672 - 1705844184.4950767]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [672 - 1705844184.4958465]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [672 - 1705844184.496574]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [672 - 1705844184.4972198]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [672 - 1705844184.4978757]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [672 - 1705844184.4996445]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [672 - 1705844184.501548]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [672 - 1705844184.5023851]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [672 - 1705844184.5031273]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [672 - 1705844184.5038211]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [672 - 1705844184.504496]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [672 - 1705844184.506309]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [672 - 1705844184.5079505]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [672 - 1705844184.508804]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [672 - 1705844184.5094976]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [672 - 1705844184.5101821]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [672 - 1705844184.5108256]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [672 - 1705844184.5126152]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [672 - 1705844184.5143168]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [672 - 1705844184.51515]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [672 - 1705844184.5159001]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [672 - 1705844184.5166047]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [672 - 1705844184.5172849]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [672 - 1705844184.5190532]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [672 - 1705844184.5208921]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [672 - 1705844184.5216978]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [672 - 1705844184.5224392]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [672 - 1705844184.5231488]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [672 - 1705844184.5237586]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [672 - 1705844184.525516]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [672 - 1705844184.5273795]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [672 - 1705844184.5281758]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [672 - 1705844184.5288963]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [672 - 1705844184.529577]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [672 - 1705844184.530233]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [672 - 1705844184.5320842]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [672 - 1705844184.533834]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [672 - 1705844184.5346825]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [672 - 1705844184.5354626]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [672 - 1705844184.5361285]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [672 - 1705844184.5367782]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [672 - 1705844184.538511]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [672 - 1705844184.5401776]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [672 - 1705844184.540989]: 0.0
python ParamPruning/classifier.weight [672 - 1705844184.541466]: 0.0
python DistillationModifier [679 - 1705844239.6037862]: Calling loss_update with:
args: 0.1678820103406906| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.777777777777779| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [679 - 1705844241.1757927]: 
Returned: 0.17198219895362854| 

python LearningRateFunctionModifier [679 - 1705844244.4772]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.777777777777779| steps_per_epoch: 63| 
python LearningRateFunctionModifier [679 - 1705844244.4773736]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [679 - 1705844244.4774427]: 2.564102564102562e-05
python LearningRateFunctionModifier/ParamGroup1 [679 - 1705844244.477676]: 2.564102564102562e-05
python DistillationModifier/task_loss [679 - 1705844244.4779947]: tensor(0.1679, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [679 - 1705844244.4790487]: tensor(0.1720, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [679 - 1705844244.4796257]: tensor(0.1720, grad_fn=<AddBackward0>)
python ConstantPruningModifier [679 - 1705844244.4801893]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.777777777777779| steps_per_epoch: 63| 
python ConstantPruningModifier [679 - 1705844244.7036483]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [679 - 1705844244.70455]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [679 - 1705844244.7166455]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [679 - 1705844244.7173421]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [679 - 1705844244.7180345]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [679 - 1705844244.719828]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [679 - 1705844244.72172]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [679 - 1705844244.7224975]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [679 - 1705844244.7231429]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [679 - 1705844244.7238033]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [679 - 1705844244.7244396]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [679 - 1705844244.7262878]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [679 - 1705844244.7279742]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [679 - 1705844244.7287927]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [679 - 1705844244.7295406]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [679 - 1705844244.7301753]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [679 - 1705844244.7307475]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [679 - 1705844244.732586]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [679 - 1705844244.734541]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [679 - 1705844244.7353454]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [679 - 1705844244.7360053]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [679 - 1705844244.7366643]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [679 - 1705844244.737305]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [679 - 1705844244.7390912]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [679 - 1705844244.7408557]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [679 - 1705844244.7415895]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [679 - 1705844244.742241]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [679 - 1705844244.742805]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [679 - 1705844244.7433732]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [679 - 1705844244.7450955]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [679 - 1705844244.746896]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [679 - 1705844244.7476199]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [679 - 1705844244.7482708]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [679 - 1705844244.748868]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [679 - 1705844244.749435]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [679 - 1705844244.7511678]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [679 - 1705844244.7528162]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [679 - 1705844244.7536068]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [679 - 1705844244.7542453]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [679 - 1705844244.7548902]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [679 - 1705844244.7554505]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [679 - 1705844244.757184]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [679 - 1705844244.7590325]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [679 - 1705844244.7597744]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [679 - 1705844244.760486]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [679 - 1705844244.7611237]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [679 - 1705844244.761672]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [679 - 1705844244.7634692]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [679 - 1705844244.765215]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [679 - 1705844244.766052]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [679 - 1705844244.7667212]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [679 - 1705844244.7674243]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [679 - 1705844244.7681022]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [679 - 1705844244.769984]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [679 - 1705844244.7716982]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [679 - 1705844244.7725098]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [679 - 1705844244.7732286]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [679 - 1705844244.773904]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [679 - 1705844244.7744796]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [679 - 1705844244.776218]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [679 - 1705844244.7778428]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [679 - 1705844244.7786164]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [679 - 1705844244.7793431]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [679 - 1705844244.7799723]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [679 - 1705844244.780537]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [679 - 1705844244.7822435]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [679 - 1705844244.7840872]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [679 - 1705844244.7848384]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [679 - 1705844244.7854404]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [679 - 1705844244.7860723]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [679 - 1705844244.7866328]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [679 - 1705844244.7883573]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [679 - 1705844244.7901466]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [679 - 1705844244.7908602]: 0.0
python ParamPruning/classifier.weight [679 - 1705844244.7913055]: 0.0
python DistillationModifier [686 - 1705844299.474188]: Calling loss_update with:
args: 0.0331420935690403| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 10.88888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [686 - 1705844300.9804163]: 
Returned: 0.10653515160083771| 

python LearningRateFunctionModifier [686 - 1705844304.294022]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.88888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [686 - 1705844304.2942317]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [686 - 1705844304.2942924]: 2.435897435897435e-05
python LearningRateFunctionModifier/ParamGroup1 [686 - 1705844304.2945454]: 2.435897435897435e-05
python DistillationModifier/task_loss [686 - 1705844304.2948956]: tensor(0.0331, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [686 - 1705844304.2960348]: tensor(0.1065, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [686 - 1705844304.2968261]: tensor(0.1065, grad_fn=<AddBackward0>)
python ConstantPruningModifier [686 - 1705844304.2976]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 10.88888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [686 - 1705844304.5230448]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [686 - 1705844304.5345926]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [686 - 1705844304.5354877]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [686 - 1705844304.536155]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [686 - 1705844304.5367694]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [686 - 1705844304.5386314]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [686 - 1705844304.5405931]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [686 - 1705844304.541519]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [686 - 1705844304.5422664]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [686 - 1705844304.5429156]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [686 - 1705844304.5435026]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [686 - 1705844304.5453308]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [686 - 1705844304.5470939]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [686 - 1705844304.5479982]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [686 - 1705844304.548776]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [686 - 1705844304.5494235]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [686 - 1705844304.5500085]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [686 - 1705844304.5518374]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [686 - 1705844304.5535982]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [686 - 1705844304.5544813]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [686 - 1705844304.5552237]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [686 - 1705844304.5558622]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [686 - 1705844304.5565188]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [686 - 1705844304.558305]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [686 - 1705844304.5601616]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [686 - 1705844304.5610023]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [686 - 1705844304.561675]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [686 - 1705844304.5622776]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [686 - 1705844304.5628872]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [686 - 1705844304.5646024]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [686 - 1705844304.566441]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [686 - 1705844304.5672767]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [686 - 1705844304.5679753]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [686 - 1705844304.5685863]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [686 - 1705844304.5691726]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [686 - 1705844304.5709877]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [686 - 1705844304.5727584]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [686 - 1705844304.573626]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [686 - 1705844304.5743527]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [686 - 1705844304.5750146]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [686 - 1705844304.575601]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [686 - 1705844304.5773795]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [686 - 1705844304.5792356]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [686 - 1705844304.5800378]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [686 - 1705844304.5807266]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [686 - 1705844304.5813339]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [686 - 1705844304.5818913]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [686 - 1705844304.583721]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [686 - 1705844304.5856035]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [686 - 1705844304.5864272]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [686 - 1705844304.5871093]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [686 - 1705844304.5877042]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [686 - 1705844304.5882704]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [686 - 1705844304.5901835]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [686 - 1705844304.592228]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [686 - 1705844304.5931573]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [686 - 1705844304.5938811]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [686 - 1705844304.59452]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [686 - 1705844304.5951009]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [686 - 1705844304.5969875]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [686 - 1705844304.5988805]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [686 - 1705844304.5997124]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [686 - 1705844304.6004124]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [686 - 1705844304.6010602]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [686 - 1705844304.6016204]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [686 - 1705844304.6034553]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [686 - 1705844304.6053438]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [686 - 1705844304.6062045]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [686 - 1705844304.6069298]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [686 - 1705844304.6075883]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [686 - 1705844304.6081889]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [686 - 1705844304.6101046]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [686 - 1705844304.6121037]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [686 - 1705844304.6130595]: 0.0
python ParamPruning/classifier.weight [686 - 1705844304.613607]: 0.0
python DistillationModifier [693 - 1705844356.0943658]: Calling loss_update with:
args: 0.7497793436050415| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844357.5461648]: 
Returned: 1.5281540155410767| 

python DistillationModifier [693 - 1705844360.247066]: Calling loss_update with:
args: 1.2414847612380981| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844361.6867077]: 
Returned: 2.017810583114624| 

python DistillationModifier [693 - 1705844364.3851485]: Calling loss_update with:
args: 1.183686375617981| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844365.8238776]: 
Returned: 1.2808053493499756| 

python DistillationModifier [693 - 1705844368.5305338]: Calling loss_update with:
args: 0.5187302827835083| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844369.9665225]: 
Returned: 0.8562997579574585| 

python DistillationModifier [693 - 1705844372.6769302]: Calling loss_update with:
args: 0.7925490736961365| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844374.1183276]: 
Returned: 1.2567013502120972| 

python DistillationModifier [693 - 1705844376.8172915]: Calling loss_update with:
args: 0.6642031073570251| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844378.2547784]: 
Returned: 1.270617961883545| 

python DistillationModifier [693 - 1705844380.952629]: Calling loss_update with:
args: 0.40934574604034424| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844382.388847]: 
Returned: 1.0589107275009155| 

python DistillationModifier [693 - 1705844385.0945992]: Calling loss_update with:
args: 1.2229360342025757| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844386.5922718]: 
Returned: 1.4368975162506104| 

python DistillationModifier [693 - 1705844389.5236335]: Calling loss_update with:
args: 0.7260076403617859| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844391.3118587]: 
Returned: 0.49733904004096985| 

python DistillationModifier [693 - 1705844393.947607]: Calling loss_update with:
args: 0.7285692095756531| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844395.358046]: 
Returned: 1.1726630926132202| 

python DistillationModifier [693 - 1705844398.0117934]: Calling loss_update with:
args: 0.6492645740509033| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844399.4154665]: 
Returned: 1.0377449989318848| 

python DistillationModifier [693 - 1705844402.4221601]: Calling loss_update with:
args: 1.041462779045105| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844404.4527135]: 
Returned: 1.3913384675979614| 

python DistillationModifier [693 - 1705844407.2819808]: Calling loss_update with:
args: 0.8499680161476135| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844408.7014928]: 
Returned: 1.7201104164123535| 

python DistillationModifier [693 - 1705844411.3751392]: Calling loss_update with:
args: 0.6513733267784119| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844412.7967556]: 
Returned: 0.7443568110466003| 

python DistillationModifier [693 - 1705844415.4773161]: Calling loss_update with:
args: 1.217352271080017| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844416.9482749]: 
Returned: 2.0450525283813477| 

python DistillationModifier [693 - 1705844419.645302]: Calling loss_update with:
args: 1.018856406211853| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844421.0800507]: 
Returned: 1.78862726688385| 

python DistillationModifier [693 - 1705844423.7690406]: Calling loss_update with:
args: 0.5455131530761719| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844425.1972198]: 
Returned: 0.7368479371070862| 

python DistillationModifier [693 - 1705844427.8947608]: Calling loss_update with:
args: 1.0036839246749878| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844429.3288841]: 
Returned: 1.7972255945205688| 

python DistillationModifier [693 - 1705844432.0167491]: Calling loss_update with:
args: 1.0824358463287354| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844433.4482937]: 
Returned: 1.288178563117981| 

python DistillationModifier [693 - 1705844436.1362226]: Calling loss_update with:
args: 0.8873732686042786| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844437.5724578]: 
Returned: 1.3448681831359863| 

python DistillationModifier [693 - 1705844440.2489007]: Calling loss_update with:
args: 0.7273758053779602| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844441.6812217]: 
Returned: 1.3034462928771973| 

python DistillationModifier [693 - 1705844444.3802571]: Calling loss_update with:
args: 0.5048786401748657| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844445.8392587]: 
Returned: 0.721500813961029| 

python DistillationModifier [693 - 1705844448.591802]: Calling loss_update with:
args: 0.819107711315155| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844450.032931]: 
Returned: 1.1486551761627197| 

python DistillationModifier [693 - 1705844453.2727365]: Calling loss_update with:
args: 0.7289561033248901| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844454.7282686]: 
Returned: 1.213609218597412| 

python DistillationModifier [693 - 1705844457.4325776]: Calling loss_update with:
args: 1.3169041872024536| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844458.8631308]: 
Returned: 1.5372645854949951| 

python DistillationModifier [693 - 1705844461.558562]: Calling loss_update with:
args: 0.3992557227611542| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844463.5254982]: 
Returned: 0.7572081089019775| 

python DistillationModifier [693 - 1705844466.7776384]: Calling loss_update with:
args: 1.0253568887710571| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844468.2171116]: 
Returned: 1.4414641857147217| 

python DistillationModifier [693 - 1705844470.9147477]: Calling loss_update with:
args: 0.7426931858062744| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844472.3477771]: 
Returned: 1.2041491270065308| 

python DistillationModifier [693 - 1705844475.0604122]: Calling loss_update with:
args: 0.6099936366081238| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844476.509227]: 
Returned: 1.071183443069458| 

python DistillationModifier [693 - 1705844479.242112]: Calling loss_update with:
args: 0.666233241558075| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844480.6746473]: 
Returned: 1.302317500114441| 

python DistillationModifier [693 - 1705844483.3724196]: Calling loss_update with:
args: 1.224292516708374| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844484.8149188]: 
Returned: 2.18381929397583| 

python DistillationModifier [693 - 1705844487.5169]: Calling loss_update with:
args: 0.965323805809021| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844488.9511647]: 
Returned: 1.2621601819992065| 

python DistillationModifier [693 - 1705844491.651118]: Calling loss_update with:
args: 1.0605748891830444| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844493.09174]: 
Returned: 1.7305917739868164| 

python DistillationModifier [693 - 1705844495.7979677]: Calling loss_update with:
args: 1.3469983339309692| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844497.2414334]: 
Returned: 2.0715115070343018| 

python DistillationModifier [693 - 1705844499.9397333]: Calling loss_update with:
args: 1.3570882081985474| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844501.3756287]: 
Returned: 1.626781940460205| 

python DistillationModifier [693 - 1705844504.0658975]: Calling loss_update with:
args: 1.0776346921920776| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844505.5109317]: 
Returned: 2.0950677394866943| 

python DistillationModifier [693 - 1705844508.266486]: Calling loss_update with:
args: 1.1953383684158325| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844509.7071908]: 
Returned: 1.3083255290985107| 

python DistillationModifier [693 - 1705844512.4120092]: Calling loss_update with:
args: 0.4714055061340332| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844513.8514056]: 
Returned: 0.9494031071662903| 

python DistillationModifier [693 - 1705844516.5567546]: Calling loss_update with:
args: 0.7864112854003906| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844518.3722172]: 
Returned: 1.4389431476593018| 

python DistillationModifier [693 - 1705844521.2550695]: Calling loss_update with:
args: 1.2110949754714966| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844523.0949771]: 
Returned: 2.29398250579834| 

python DistillationModifier [693 - 1705844526.4743888]: Calling loss_update with:
args: 0.4597479999065399| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844527.9241536]: 
Returned: 1.367724061012268| 

python DistillationModifier [693 - 1705844530.6211448]: Calling loss_update with:
args: 0.898944616317749| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844532.060761]: 
Returned: 1.8916093111038208| 

python DistillationModifier [693 - 1705844534.7689338]: Calling loss_update with:
args: 0.8557416796684265| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844536.2192285]: 
Returned: 0.9057428240776062| 

python DistillationModifier [693 - 1705844538.9633772]: Calling loss_update with:
args: 0.7305301427841187| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844540.4028203]: 
Returned: 1.5026576519012451| 

python DistillationModifier [693 - 1705844543.1274357]: Calling loss_update with:
args: 1.0517717599868774| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844544.5702388]: 
Returned: 2.234365463256836| 

python DistillationModifier [693 - 1705844547.271799]: Calling loss_update with:
args: 1.1916637420654297| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844548.7146266]: 
Returned: 1.7859164476394653| 

python DistillationModifier [693 - 1705844551.4108965]: Calling loss_update with:
args: 0.6492499113082886| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844552.8504868]: 
Returned: 1.47145676612854| 

python DistillationModifier [693 - 1705844555.5703607]: Calling loss_update with:
args: 0.31276482343673706| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844557.0164454]: 
Returned: 0.6447708010673523| 

python DistillationModifier [693 - 1705844559.7086656]: Calling loss_update with:
args: 1.0928369760513306| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844561.1467097]: 
Returned: 2.0134503841400146| 

python DistillationModifier [693 - 1705844563.8519087]: Calling loss_update with:
args: 1.1776896715164185| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844565.2964525]: 
Returned: 2.104323148727417| 

python DistillationModifier [693 - 1705844568.0075943]: Calling loss_update with:
args: 0.41753676533699036| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844569.4917908]: 
Returned: 0.9002565741539001| 

python DistillationModifier [693 - 1705844572.1857822]: Calling loss_update with:
args: 1.4265931844711304| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844573.6187267]: 
Returned: 1.380627989768982| 

python DistillationModifier [693 - 1705844576.3114507]: Calling loss_update with:
args: 0.8636842370033264| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844577.7643478]: 
Returned: 1.1828569173812866| 

python DistillationModifier [693 - 1705844580.4515831]: Calling loss_update with:
args: 0.6353834867477417| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844581.950986]: 
Returned: 0.9827820658683777| 

python DistillationModifier [693 - 1705844585.9675364]: Calling loss_update with:
args: 0.6032372117042542| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844587.4080575]: 
Returned: 1.4638432264328003| 

python DistillationModifier [693 - 1705844590.1010776]: Calling loss_update with:
args: 0.9384939670562744| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844591.5375004]: 
Returned: 1.3584011793136597| 

python DistillationModifier [693 - 1705844594.2495074]: Calling loss_update with:
args: 0.8454821705818176| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844595.683936]: 
Returned: 0.8582507371902466| 

python DistillationModifier [693 - 1705844598.3749595]: Calling loss_update with:
args: 0.9631154537200928| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844599.8530793]: 
Returned: 1.2203656435012817| 

python DistillationModifier [693 - 1705844602.549093]: Calling loss_update with:
args: 1.084492564201355| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844603.9817226]: 
Returned: 1.8046393394470215| 

python DistillationModifier [693 - 1705844606.6709983]: Calling loss_update with:
args: 1.1614630222320557| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844608.1102777]: 
Returned: 2.093777656555176| 

python DistillationModifier [693 - 1705844610.793171]: Calling loss_update with:
args: 0.929988443851471| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844612.2206407]: 
Returned: 1.4462467432022095| 

python DistillationModifier [693 - 1705844614.910669]: Calling loss_update with:
args: 0.9637870192527771| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844616.3440857]: 
Returned: 1.8311277627944946| 

python DistillationModifier [693 - 1705844617.859051]: Calling loss_update with:
args: 0.4297199249267578| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844618.5924637]: 
Returned: 0.83559650182724| 

python DistillationModifier [693 - 1705844621.9408484]: Calling loss_update with:
args: 0.09675490111112595| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [693 - 1705844623.3928022]: 
Returned: 0.19541211426258087| 

python LearningRateFunctionModifier [693 - 1705844626.6834261]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [693 - 1705844626.683607]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [693 - 1705844626.6836765]: 2.307692307692308e-05
python LearningRateFunctionModifier/ParamGroup1 [693 - 1705844626.683911]: 2.307692307692308e-05
python DistillationModifier/task_loss [693 - 1705844626.6842184]: tensor(0.0968, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [693 - 1705844626.6853113]: tensor(0.1954, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [693 - 1705844626.6859128]: tensor(0.1954, grad_fn=<AddBackward0>)
python ConstantPruningModifier [693 - 1705844626.6865137]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.0| steps_per_epoch: 63| 
python ConstantPruningModifier [693 - 1705844626.9065318]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [693 - 1705844626.907343]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [693 - 1705844626.9084098]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [693 - 1705844626.9092996]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [693 - 1705844626.9099972]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [693 - 1705844626.9118114]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [693 - 1705844626.9137356]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [693 - 1705844626.9145443]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [693 - 1705844626.9152257]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [693 - 1705844626.9158084]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [693 - 1705844626.9164836]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [693 - 1705844626.918251]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [693 - 1705844626.9200776]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [693 - 1705844626.9208941]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [693 - 1705844626.9215453]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [693 - 1705844626.922119]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [693 - 1705844626.9226537]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [693 - 1705844626.9243822]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [693 - 1705844626.9262598]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [693 - 1705844626.9270627]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [693 - 1705844626.927832]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [693 - 1705844626.9285202]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [693 - 1705844626.9291935]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [693 - 1705844626.9308975]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [693 - 1705844626.9327111]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [693 - 1705844626.9334803]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [693 - 1705844626.9342127]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [693 - 1705844626.9348073]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [693 - 1705844626.9353485]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [693 - 1705844626.9370704]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [693 - 1705844626.938796]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [693 - 1705844626.9394789]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [693 - 1705844626.9401314]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [693 - 1705844626.9407268]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [693 - 1705844626.9414067]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [693 - 1705844626.9435399]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [693 - 1705844626.9453762]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [693 - 1705844626.946174]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [693 - 1705844626.946813]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [693 - 1705844626.9473827]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [693 - 1705844626.947917]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [693 - 1705844626.9496834]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [693 - 1705844626.9514616]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [693 - 1705844626.9521835]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [693 - 1705844626.952871]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [693 - 1705844626.953458]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [693 - 1705844626.95413]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [693 - 1705844626.956034]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [693 - 1705844626.9578526]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [693 - 1705844626.9586267]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [693 - 1705844626.9593267]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [693 - 1705844626.9600465]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [693 - 1705844626.9607196]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [693 - 1705844626.9624064]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [693 - 1705844626.9641635]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [693 - 1705844626.9648993]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [693 - 1705844626.96552]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [693 - 1705844626.9660757]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [693 - 1705844626.9666078]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [693 - 1705844626.968352]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [693 - 1705844626.9701915]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [693 - 1705844626.9709966]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [693 - 1705844626.971744]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [693 - 1705844626.9723518]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [693 - 1705844626.9729202]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [693 - 1705844626.9746149]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [693 - 1705844626.9764516]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [693 - 1705844626.9772875]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [693 - 1705844626.9780483]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [693 - 1705844626.9786751]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [693 - 1705844626.9792397]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [693 - 1705844626.9809816]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [693 - 1705844626.9827127]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [693 - 1705844626.983429]: 0.0
python ParamPruning/classifier.weight [693 - 1705844626.983844]: 0.0
python DistillationModifier [700 - 1705844681.6078813]: Calling loss_update with:
args: 0.05867833271622658| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.11111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [700 - 1705844683.1483457]: 
Returned: 0.11141176521778107| 

python LearningRateFunctionModifier [700 - 1705844686.4424167]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.11111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [700 - 1705844686.4425983]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [700 - 1705844686.442679]: 2.179487179487181e-05
python LearningRateFunctionModifier/ParamGroup1 [700 - 1705844686.4429123]: 2.179487179487181e-05
python DistillationModifier/task_loss [700 - 1705844686.4432547]: tensor(0.0587, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [700 - 1705844686.4443276]: tensor(0.1114, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [700 - 1705844686.445053]: tensor(0.1114, grad_fn=<AddBackward0>)
python ConstantPruningModifier [700 - 1705844686.4457247]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.11111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [700 - 1705844686.6786602]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [700 - 1705844686.6794982]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [700 - 1705844686.680371]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [700 - 1705844686.681086]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [700 - 1705844686.6816988]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [700 - 1705844686.6834815]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [700 - 1705844686.6852677]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [700 - 1705844686.686162]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [700 - 1705844686.6868927]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [700 - 1705844686.6875308]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [700 - 1705844686.6881127]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [700 - 1705844686.6899123]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [700 - 1705844686.691827]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [700 - 1705844686.6926205]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [700 - 1705844686.6932847]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [700 - 1705844686.6938572]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [700 - 1705844686.6944077]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [700 - 1705844686.696169]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [700 - 1705844686.6981425]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [700 - 1705844686.6989741]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [700 - 1705844686.6996577]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [700 - 1705844686.7002475]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [700 - 1705844686.700818]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [700 - 1705844686.702542]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [700 - 1705844686.7043512]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [700 - 1705844686.7050705]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [700 - 1705844686.705672]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [700 - 1705844686.706246]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [700 - 1705844686.706799]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [700 - 1705844686.7085829]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [700 - 1705844686.7103612]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [700 - 1705844686.7112477]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [700 - 1705844686.7119877]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [700 - 1705844686.7126172]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [700 - 1705844686.7132006]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [700 - 1705844686.7149658]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [700 - 1705844686.7168524]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [700 - 1705844686.717634]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [700 - 1705844686.718285]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [700 - 1705844686.718898]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [700 - 1705844686.719455]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [700 - 1705844686.7212524]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [700 - 1705844686.7230859]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [700 - 1705844686.723916]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [700 - 1705844686.7245758]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [700 - 1705844686.7251654]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [700 - 1705844686.7257187]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [700 - 1705844686.727502]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [700 - 1705844686.7292607]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [700 - 1705844686.73014]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [700 - 1705844686.7308583]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [700 - 1705844686.7314882]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [700 - 1705844686.7320848]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [700 - 1705844686.733893]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [700 - 1705844686.7357073]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [700 - 1705844686.7365496]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [700 - 1705844686.7372675]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [700 - 1705844686.7379186]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [700 - 1705844686.7384984]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [700 - 1705844686.7403085]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [700 - 1705844686.7422473]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [700 - 1705844686.7431135]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [700 - 1705844686.7438135]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [700 - 1705844686.7444358]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [700 - 1705844686.7450578]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [700 - 1705844686.746921]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [700 - 1705844686.748847]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [700 - 1705844686.7496717]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [700 - 1705844686.7503495]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [700 - 1705844686.7509377]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [700 - 1705844686.7515016]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [700 - 1705844686.753319]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [700 - 1705844686.7550204]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [700 - 1705844686.7558596]: 0.0
python ParamPruning/classifier.weight [700 - 1705844686.7563708]: 0.0
python DistillationModifier [707 - 1705844742.1353889]: Calling loss_update with:
args: 0.03473280742764473| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.222222222222221| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [707 - 1705844743.6623259]: 
Returned: 0.06316094100475311| 

python LearningRateFunctionModifier [707 - 1705844746.9754632]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.222222222222221| steps_per_epoch: 63| 
python LearningRateFunctionModifier [707 - 1705844746.975652]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [707 - 1705844746.9757204]: 2.0512820512820512e-05
python LearningRateFunctionModifier/ParamGroup1 [707 - 1705844746.9759574]: 2.0512820512820512e-05
python DistillationModifier/task_loss [707 - 1705844746.976288]: tensor(0.0347, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [707 - 1705844746.9774082]: tensor(0.0632, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [707 - 1705844746.9781568]: tensor(0.0632, grad_fn=<AddBackward0>)
python ConstantPruningModifier [707 - 1705844746.9788597]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.222222222222221| steps_per_epoch: 63| 
python ConstantPruningModifier [707 - 1705844747.2020204]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [707 - 1705844747.2029588]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [707 - 1705844747.2276943]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [707 - 1705844747.2284613]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [707 - 1705844747.229126]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [707 - 1705844747.2309077]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [707 - 1705844747.2326286]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [707 - 1705844747.2334716]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [707 - 1705844747.2341683]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [707 - 1705844747.234762]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [707 - 1705844747.2353375]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [707 - 1705844747.2370768]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [707 - 1705844747.23906]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [707 - 1705844747.2398932]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [707 - 1705844747.2405858]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [707 - 1705844747.2412071]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [707 - 1705844747.2417824]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [707 - 1705844747.2435486]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [707 - 1705844747.2454815]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [707 - 1705844747.2462997]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [707 - 1705844747.2469816]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [707 - 1705844747.2475936]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [707 - 1705844747.2481675]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [707 - 1705844747.250012]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [707 - 1705844747.2520063]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [707 - 1705844747.2528596]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [707 - 1705844747.253563]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [707 - 1705844747.2541583]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [707 - 1705844747.2547348]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [707 - 1705844747.2565322]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [707 - 1705844747.2583163]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [707 - 1705844747.2591748]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [707 - 1705844747.2599015]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [707 - 1705844747.2605007]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [707 - 1705844747.261097]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [707 - 1705844747.2628853]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [707 - 1705844747.264606]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [707 - 1705844747.2654192]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [707 - 1705844747.2661304]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [707 - 1705844747.2667346]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [707 - 1705844747.2673051]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [707 - 1705844747.2690995]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [707 - 1705844747.2709846]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [707 - 1705844747.2718704]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [707 - 1705844747.2726326]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [707 - 1705844747.2732723]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [707 - 1705844747.2738512]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [707 - 1705844747.2756581]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [707 - 1705844747.2776027]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [707 - 1705844747.2784643]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [707 - 1705844747.2791798]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [707 - 1705844747.2797737]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [707 - 1705844747.280325]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [707 - 1705844747.2820587]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [707 - 1705844747.283844]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [707 - 1705844747.2845876]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [707 - 1705844747.2852328]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [707 - 1705844747.285796]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [707 - 1705844747.2863371]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [707 - 1705844747.2880526]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [707 - 1705844747.2899175]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [707 - 1705844747.2907314]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [707 - 1705844747.291405]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [707 - 1705844747.292004]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [707 - 1705844747.2925682]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [707 - 1705844747.2943265]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [707 - 1705844747.2961812]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [707 - 1705844747.2969828]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [707 - 1705844747.2976587]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [707 - 1705844747.2982335]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [707 - 1705844747.2987921]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [707 - 1705844747.3005707]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [707 - 1705844747.302462]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [707 - 1705844747.303282]: 0.0
python ParamPruning/classifier.weight [707 - 1705844747.3037739]: 0.0
python DistillationModifier [714 - 1705844802.9399421]: Calling loss_update with:
args: 0.16474774479866028| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.333333333333334| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [714 - 1705844804.4930387]: 
Returned: 0.28266167640686035| 

python LearningRateFunctionModifier [714 - 1705844807.8040922]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.333333333333334| steps_per_epoch: 63| 
python LearningRateFunctionModifier [714 - 1705844807.8042712]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [714 - 1705844807.8043387]: 1.9230769230769214e-05
python LearningRateFunctionModifier/ParamGroup1 [714 - 1705844807.8045852]: 1.9230769230769214e-05
python DistillationModifier/task_loss [714 - 1705844807.8049319]: tensor(0.1647, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [714 - 1705844807.8059943]: tensor(0.2827, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [714 - 1705844807.8066063]: tensor(0.2827, grad_fn=<AddBackward0>)
python ConstantPruningModifier [714 - 1705844807.8071964]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.333333333333334| steps_per_epoch: 63| 
python ConstantPruningModifier [714 - 1705844808.0314038]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [714 - 1705844808.032281]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [714 - 1705844808.0331619]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [714 - 1705844808.0337932]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [714 - 1705844808.034338]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [714 - 1705844808.0361288]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [714 - 1705844808.0380504]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [714 - 1705844808.0388293]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [714 - 1705844808.0394797]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [714 - 1705844808.040039]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [714 - 1705844808.0405538]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [714 - 1705844808.0422883]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [714 - 1705844808.043951]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [714 - 1705844808.0447958]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [714 - 1705844808.045502]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [714 - 1705844808.0460753]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [714 - 1705844808.0465922]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [714 - 1705844808.048382]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [714 - 1705844808.0500798]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [714 - 1705844808.0509362]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [714 - 1705844808.0516503]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [714 - 1705844808.0522819]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [714 - 1705844808.0528347]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [714 - 1705844808.0545475]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [714 - 1705844808.056213]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [714 - 1705844808.057011]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [714 - 1705844808.0576892]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [714 - 1705844808.0582626]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [714 - 1705844808.0587764]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [714 - 1705844808.0605166]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [714 - 1705844808.0622022]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [714 - 1705844808.0629802]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [714 - 1705844808.0636325]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [714 - 1705844808.0641959]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [714 - 1705844808.0647252]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [714 - 1705844808.0663862]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [714 - 1705844808.068008]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [714 - 1705844808.068786]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [714 - 1705844808.069428]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [714 - 1705844808.0700166]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [714 - 1705844808.070531]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [714 - 1705844808.0722117]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [714 - 1705844808.0740306]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [714 - 1705844808.0747674]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [714 - 1705844808.0753813]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [714 - 1705844808.0759099]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [714 - 1705844808.0764139]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [714 - 1705844808.0781555]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [714 - 1705844808.0800176]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [714 - 1705844808.080853]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [714 - 1705844808.081526]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [714 - 1705844808.082095]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [714 - 1705844808.08262]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [714 - 1705844808.0843403]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [714 - 1705844808.0860226]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [714 - 1705844808.0868087]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [714 - 1705844808.0874352]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [714 - 1705844808.0879815]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [714 - 1705844808.0885026]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [714 - 1705844808.0902538]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [714 - 1705844808.0919356]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [714 - 1705844808.092767]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [714 - 1705844808.0934408]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [714 - 1705844808.094012]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [714 - 1705844808.094529]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [714 - 1705844808.0962472]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [714 - 1705844808.0981286]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [714 - 1705844808.0989275]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [714 - 1705844808.0995748]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [714 - 1705844808.1001394]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [714 - 1705844808.100663]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [714 - 1705844808.1024346]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [714 - 1705844808.1042922]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [714 - 1705844808.1050868]: 0.0
python ParamPruning/classifier.weight [714 - 1705844808.1055598]: 0.0
python DistillationModifier [721 - 1705844863.5807877]: Calling loss_update with:
args: 0.16502366960048676| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.444444444444445| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [721 - 1705844865.0355504]: 
Returned: 0.19556790590286255| 

python LearningRateFunctionModifier [721 - 1705844868.3367589]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.444444444444445| steps_per_epoch: 63| 
python LearningRateFunctionModifier [721 - 1705844868.336946]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [721 - 1705844868.3370016]: 1.7948717948717944e-05
python LearningRateFunctionModifier/ParamGroup1 [721 - 1705844868.3372455]: 1.7948717948717944e-05
python DistillationModifier/task_loss [721 - 1705844868.3375816]: tensor(0.1650, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [721 - 1705844868.3386505]: tensor(0.1956, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [721 - 1705844868.3392382]: tensor(0.1956, grad_fn=<AddBackward0>)
python ConstantPruningModifier [721 - 1705844868.339814]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.444444444444445| steps_per_epoch: 63| 
python ConstantPruningModifier [721 - 1705844868.5640552]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [721 - 1705844868.564953]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [721 - 1705844868.5659757]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [721 - 1705844868.5666325]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [721 - 1705844868.5672328]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [721 - 1705844868.5690198]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [721 - 1705844868.5707288]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [721 - 1705844868.5715194]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [721 - 1705844868.5722282]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [721 - 1705844868.5728326]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [721 - 1705844868.5733764]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [721 - 1705844868.5750878]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [721 - 1705844868.5769753]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [721 - 1705844868.5777454]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [721 - 1705844868.5784054]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [721 - 1705844868.5789864]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [721 - 1705844868.5795329]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [721 - 1705844868.5813696]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [721 - 1705844868.5833142]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [721 - 1705844868.6087308]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [721 - 1705844868.609553]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [721 - 1705844868.6101913]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [721 - 1705844868.6107755]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [721 - 1705844868.6125786]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [721 - 1705844868.6143863]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [721 - 1705844868.615204]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [721 - 1705844868.6158788]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [721 - 1705844868.616464]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [721 - 1705844868.617037]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [721 - 1705844868.6187694]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [721 - 1705844868.6206079]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [721 - 1705844868.6213822]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [721 - 1705844868.6220152]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [721 - 1705844868.6225963]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [721 - 1705844868.6231337]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [721 - 1705844868.624866]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [721 - 1705844868.626496]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [721 - 1705844868.6272533]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [721 - 1705844868.627899]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [721 - 1705844868.6284719]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [721 - 1705844868.6290305]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [721 - 1705844868.6307533]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [721 - 1705844868.6326246]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [721 - 1705844868.6334412]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [721 - 1705844868.6341488]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [721 - 1705844868.6347191]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [721 - 1705844868.63526]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [721 - 1705844868.636991]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [721 - 1705844868.6387925]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [721 - 1705844868.6395125]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [721 - 1705844868.640158]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [721 - 1705844868.6407301]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [721 - 1705844868.6412618]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [721 - 1705844868.642987]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [721 - 1705844868.6448443]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [721 - 1705844868.6456056]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [721 - 1705844868.6462238]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [721 - 1705844868.6467822]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [721 - 1705844868.647319]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [721 - 1705844868.6491356]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [721 - 1705844868.6509879]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [721 - 1705844868.6518114]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [721 - 1705844868.6524472]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [721 - 1705844868.6530437]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [721 - 1705844868.653585]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [721 - 1705844868.6553288]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [721 - 1705844868.657069]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [721 - 1705844868.6578693]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [721 - 1705844868.658547]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [721 - 1705844868.6591449]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [721 - 1705844868.6596909]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [721 - 1705844868.661448]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [721 - 1705844868.6632564]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [721 - 1705844868.6640298]: 0.0
python ParamPruning/classifier.weight [721 - 1705844868.6644866]: 0.0
python DistillationModifier [728 - 1705844922.9934952]: Calling loss_update with:
args: 0.04929030314087868| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.555555555555555| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [728 - 1705844924.4386911]: 
Returned: 0.11286483705043793| 

python LearningRateFunctionModifier [728 - 1705844927.7352839]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.555555555555555| steps_per_epoch: 63| 
python LearningRateFunctionModifier [728 - 1705844927.7354603]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [728 - 1705844927.7355332]: 1.6666666666666674e-05
python LearningRateFunctionModifier/ParamGroup1 [728 - 1705844927.7357635]: 1.6666666666666674e-05
python DistillationModifier/task_loss [728 - 1705844927.7360818]: tensor(0.0493, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [728 - 1705844927.7371564]: tensor(0.1129, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [728 - 1705844927.7377555]: tensor(0.1129, grad_fn=<AddBackward0>)
python ConstantPruningModifier [728 - 1705844927.7383301]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.555555555555555| steps_per_epoch: 63| 
python ConstantPruningModifier [728 - 1705844927.966195]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [728 - 1705844927.9670303]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [728 - 1705844927.9679139]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [728 - 1705844927.9685955]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [728 - 1705844927.9692044]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [728 - 1705844927.9709187]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [728 - 1705844927.9727468]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [728 - 1705844927.973469]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [728 - 1705844927.9740841]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [728 - 1705844927.974665]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [728 - 1705844927.9752219]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [728 - 1705844927.976914]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [728 - 1705844927.9787738]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [728 - 1705844927.979573]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [728 - 1705844927.9802322]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [728 - 1705844927.9808254]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [728 - 1705844927.9813864]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [728 - 1705844927.983121]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [728 - 1705844927.9849706]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [728 - 1705844927.9858325]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [728 - 1705844927.986517]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [728 - 1705844927.9871373]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [728 - 1705844927.9877164]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [728 - 1705844927.989463]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [728 - 1705844927.991166]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [728 - 1705844927.9920166]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [728 - 1705844927.992734]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [728 - 1705844927.993349]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [728 - 1705844927.993932]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [728 - 1705844927.995651]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [728 - 1705844927.9975512]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [728 - 1705844927.998363]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [728 - 1705844927.9990344]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [728 - 1705844927.9996316]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [728 - 1705844928.0001934]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [728 - 1705844928.0019438]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [728 - 1705844928.0038104]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [728 - 1705844928.0046031]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [728 - 1705844928.0052779]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [728 - 1705844928.0059066]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [728 - 1705844928.0064633]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [728 - 1705844928.0081782]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [728 - 1705844928.0100465]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [728 - 1705844928.0108309]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [728 - 1705844928.0114954]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [728 - 1705844928.0121257]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [728 - 1705844928.012709]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [728 - 1705844928.014421]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [728 - 1705844928.0162308]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [728 - 1705844928.0169914]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [728 - 1705844928.0176334]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [728 - 1705844928.0181987]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [728 - 1705844928.0187578]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [728 - 1705844928.0204155]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [728 - 1705844928.0222645]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [728 - 1705844928.0229926]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [728 - 1705844928.0235996]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [728 - 1705844928.0241609]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [728 - 1705844928.024717]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [728 - 1705844928.026429]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [728 - 1705844928.028339]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [728 - 1705844928.029238]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [728 - 1705844928.0299675]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [728 - 1705844928.0305817]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [728 - 1705844928.03114]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [728 - 1705844928.032915]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [728 - 1705844928.0347905]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [728 - 1705844928.0355573]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [728 - 1705844928.0361981]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [728 - 1705844928.036806]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [728 - 1705844928.0373683]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [728 - 1705844928.0390315]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [728 - 1705844928.0406003]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [728 - 1705844928.0413709]: 0.0
python ParamPruning/classifier.weight [728 - 1705844928.0418358]: 0.0
python DistillationModifier [735 - 1705844983.5711052]: Calling loss_update with:
args: 0.04183628037571907| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.666666666666666| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [735 - 1705844985.0983808]: 
Returned: 0.07551289349794388| 

python LearningRateFunctionModifier [735 - 1705844988.4127495]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.666666666666666| steps_per_epoch: 63| 
python LearningRateFunctionModifier [735 - 1705844988.4129403]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [735 - 1705844988.413001]: 1.5384615384615404e-05
python LearningRateFunctionModifier/ParamGroup1 [735 - 1705844988.4132276]: 1.5384615384615404e-05
python DistillationModifier/task_loss [735 - 1705844988.4135683]: tensor(0.0418, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [735 - 1705844988.4146583]: tensor(0.0755, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [735 - 1705844988.4153764]: tensor(0.0755, grad_fn=<AddBackward0>)
python ConstantPruningModifier [735 - 1705844988.4160583]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.666666666666666| steps_per_epoch: 63| 
python ConstantPruningModifier [735 - 1705844988.6407626]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [735 - 1705844988.641625]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [735 - 1705844988.6677454]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [735 - 1705844988.66854]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [735 - 1705844988.669226]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [735 - 1705844988.6710577]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [735 - 1705844988.6730013]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [735 - 1705844988.6738377]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [735 - 1705844988.6745284]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [735 - 1705844988.675116]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [735 - 1705844988.6757026]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [735 - 1705844988.6774542]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [735 - 1705844988.6791148]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [735 - 1705844988.6799335]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [735 - 1705844988.680614]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [735 - 1705844988.6812406]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [735 - 1705844988.6818202]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [735 - 1705844988.683565]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [735 - 1705844988.685466]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [735 - 1705844988.6862853]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [735 - 1705844988.6869922]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [735 - 1705844988.6876295]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [735 - 1705844988.6882226]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [735 - 1705844988.6900566]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [735 - 1705844988.6919765]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [735 - 1705844988.6928372]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [735 - 1705844988.6935291]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [735 - 1705844988.694135]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [735 - 1705844988.6947036]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [735 - 1705844988.6964867]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [735 - 1705844988.6983423]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [735 - 1705844988.699148]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [735 - 1705844988.6998117]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [735 - 1705844988.7004123]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [735 - 1705844988.701011]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [735 - 1705844988.7028282]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [735 - 1705844988.7047894]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [735 - 1705844988.7056475]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [735 - 1705844988.7063487]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [735 - 1705844988.7069728]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [735 - 1705844988.7075498]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [735 - 1705844988.7093623]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [735 - 1705844988.7113469]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [735 - 1705844988.7122612]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [735 - 1705844988.7129817]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [735 - 1705844988.7136397]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [735 - 1705844988.7142406]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [735 - 1705844988.7160122]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [735 - 1705844988.7179003]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [735 - 1705844988.7187068]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [735 - 1705844988.719372]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [735 - 1705844988.7199996]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [735 - 1705844988.7205794]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [735 - 1705844988.7224035]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [735 - 1705844988.7243092]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [735 - 1705844988.7251503]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [735 - 1705844988.7258492]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [735 - 1705844988.7264724]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [735 - 1705844988.7270339]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [735 - 1705844988.7287986]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [735 - 1705844988.7307365]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [735 - 1705844988.7315543]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [735 - 1705844988.732263]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [735 - 1705844988.7328892]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [735 - 1705844988.733483]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [735 - 1705844988.7352734]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [735 - 1705844988.7373335]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [735 - 1705844988.7382474]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [735 - 1705844988.739024]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [735 - 1705844988.7396684]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [735 - 1705844988.7402802]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [735 - 1705844988.7421634]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [735 - 1705844988.7441545]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [735 - 1705844988.7450955]: 0.0
python ParamPruning/classifier.weight [735 - 1705844988.7456634]: 0.0
python DistillationModifier [742 - 1705845044.3245137]: Calling loss_update with:
args: 0.09376068413257599| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.777777777777779| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [742 - 1705845045.7785609]: 
Returned: 0.14289723336696625| 

python LearningRateFunctionModifier [742 - 1705845049.0943959]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.777777777777779| steps_per_epoch: 63| 
python LearningRateFunctionModifier [742 - 1705845049.0945842]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [742 - 1705845049.0946515]: 1.410256410256408e-05
python LearningRateFunctionModifier/ParamGroup1 [742 - 1705845049.0948858]: 1.410256410256408e-05
python DistillationModifier/task_loss [742 - 1705845049.0952263]: tensor(0.0938, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [742 - 1705845049.0963385]: tensor(0.1429, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [742 - 1705845049.0971153]: tensor(0.1429, grad_fn=<AddBackward0>)
python ConstantPruningModifier [742 - 1705845049.0978396]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.777777777777779| steps_per_epoch: 63| 
python ConstantPruningModifier [742 - 1705845049.3228703]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [742 - 1705845049.32372]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [742 - 1705845049.3247795]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [742 - 1705845049.325519]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [742 - 1705845049.326195]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [742 - 1705845049.328019]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [742 - 1705845049.330004]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [742 - 1705845049.3310127]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [742 - 1705845049.331876]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [742 - 1705845049.3326163]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [742 - 1705845049.3333066]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [742 - 1705845049.3352656]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [742 - 1705845049.3373232]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [742 - 1705845049.3383048]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [742 - 1705845049.3391232]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [742 - 1705845049.339833]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [742 - 1705845049.340477]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [742 - 1705845049.3423777]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [742 - 1705845049.3441834]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [742 - 1705845049.3451815]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [742 - 1705845049.3460588]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [742 - 1705845049.346834]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [742 - 1705845049.3476071]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [742 - 1705845049.349512]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [742 - 1705845049.351333]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [742 - 1705845049.352239]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [742 - 1705845049.3530717]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [742 - 1705845049.3537796]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [742 - 1705845049.3544018]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [742 - 1705845049.356202]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [742 - 1705845049.3581493]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [742 - 1705845049.359022]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [742 - 1705845049.3597486]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [742 - 1705845049.36039]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [742 - 1705845049.3610067]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [742 - 1705845049.3627992]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [742 - 1705845049.3645432]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [742 - 1705845049.3654108]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [742 - 1705845049.3661597]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [742 - 1705845049.3668385]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [742 - 1705845049.3674104]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [742 - 1705845049.3692055]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [742 - 1705845049.3711052]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [742 - 1705845049.3719993]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [742 - 1705845049.3727453]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [742 - 1705845049.3733897]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [742 - 1705845049.3740005]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [742 - 1705845049.3758218]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [742 - 1705845049.377656]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [742 - 1705845049.3785844]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [742 - 1705845049.3794384]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [742 - 1705845049.3802028]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [742 - 1705845049.3808656]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [742 - 1705845049.3826785]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [742 - 1705845049.3845327]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [742 - 1705845049.3853526]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [742 - 1705845049.386033]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [742 - 1705845049.3866367]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [742 - 1705845049.3872046]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [742 - 1705845049.3890276]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [742 - 1705845049.3909814]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [742 - 1705845049.3918734]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [742 - 1705845049.3926044]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [742 - 1705845049.3932483]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [742 - 1705845049.3938317]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [742 - 1705845049.3956268]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [742 - 1705845049.3974817]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [742 - 1705845049.3982887]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [742 - 1705845049.3990464]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [742 - 1705845049.3997242]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [742 - 1705845049.4003887]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [742 - 1705845049.4021964]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [742 - 1705845049.4039693]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [742 - 1705845049.4048965]: 0.0
python ParamPruning/classifier.weight [742 - 1705845049.4054558]: 0.0
python DistillationModifier [749 - 1705845104.6557822]: Calling loss_update with:
args: 0.011240906082093716| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 11.88888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [749 - 1705845106.2070827]: 
Returned: 0.06742262840270996| 

python LearningRateFunctionModifier [749 - 1705845109.5253553]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.88888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [749 - 1705845109.5255325]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [749 - 1705845109.5256038]: 1.282051282051281e-05
python LearningRateFunctionModifier/ParamGroup1 [749 - 1705845109.5258353]: 1.282051282051281e-05
python DistillationModifier/task_loss [749 - 1705845109.5261817]: tensor(0.0112, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [749 - 1705845109.5272458]: tensor(0.0674, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [749 - 1705845109.5279498]: tensor(0.0674, grad_fn=<AddBackward0>)
python ConstantPruningModifier [749 - 1705845109.5286257]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 11.88888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [749 - 1705845109.7546666]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [749 - 1705845109.7555335]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [749 - 1705845109.7700493]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [749 - 1705845109.7710536]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [749 - 1705845109.7719085]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [749 - 1705845109.7747374]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [749 - 1705845109.7775636]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [749 - 1705845109.7787235]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [749 - 1705845109.77968]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [749 - 1705845109.7805107]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [749 - 1705845109.7813947]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [749 - 1705845109.7841437]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [749 - 1705845109.786991]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [749 - 1705845109.788079]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [749 - 1705845109.78897]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [749 - 1705845109.7897792]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [749 - 1705845109.7905488]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [749 - 1705845109.7933679]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [749 - 1705845109.796173]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [749 - 1705845109.7970545]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [749 - 1705845109.797886]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [749 - 1705845109.798574]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [749 - 1705845109.79919]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [749 - 1705845109.8010824]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [749 - 1705845109.8031034]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [749 - 1705845109.8040864]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [749 - 1705845109.8048563]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [749 - 1705845109.8055165]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [749 - 1705845109.8061302]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [749 - 1705845109.8079138]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [749 - 1705845109.8096845]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [749 - 1705845109.8105946]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [749 - 1705845109.8113708]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [749 - 1705845109.8120723]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [749 - 1705845109.8127117]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [749 - 1705845109.8145232]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [749 - 1705845109.8164387]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [749 - 1705845109.8173373]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [749 - 1705845109.8180766]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [749 - 1705845109.8187354]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [749 - 1705845109.8193479]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [749 - 1705845109.8211832]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [749 - 1705845109.8230639]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [749 - 1705845109.8238783]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [749 - 1705845109.8246186]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [749 - 1705845109.82525]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [749 - 1705845109.8258405]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [749 - 1705845109.827652]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [749 - 1705845109.8295698]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [749 - 1705845109.8304393]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [749 - 1705845109.831187]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [749 - 1705845109.8318822]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [749 - 1705845109.832514]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [749 - 1705845109.8344016]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [749 - 1705845109.8362517]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [749 - 1705845109.8372397]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [749 - 1705845109.838066]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [749 - 1705845109.8387697]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [749 - 1705845109.8393936]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [749 - 1705845109.8412323]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [749 - 1705845109.8431997]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [749 - 1705845109.8440852]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [749 - 1705845109.844842]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [749 - 1705845109.8454723]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [749 - 1705845109.846078]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [749 - 1705845109.8479013]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [749 - 1705845109.8498828]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [749 - 1705845109.8508115]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [749 - 1705845109.85157]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [749 - 1705845109.8522484]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [749 - 1705845109.8528843]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [749 - 1705845109.8547192]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [749 - 1705845109.8564928]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [749 - 1705845109.857419]: 0.0
python ParamPruning/classifier.weight [749 - 1705845109.857974]: 0.0
python DistillationModifier [756 - 1705845160.4065142]: Calling loss_update with:
args: 1.0068225860595703| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845161.848504]: 
Returned: 1.9088777303695679| 

python DistillationModifier [756 - 1705845165.1148365]: Calling loss_update with:
args: 1.2712948322296143| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845166.5562224]: 
Returned: 2.073385238647461| 

python DistillationModifier [756 - 1705845169.2899358]: Calling loss_update with:
args: 1.386160135269165| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845170.7372968]: 
Returned: 1.5912208557128906| 

python DistillationModifier [756 - 1705845173.4451692]: Calling loss_update with:
args: 0.48531463742256165| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845174.8886554]: 
Returned: 0.5706960558891296| 

python DistillationModifier [756 - 1705845177.6410997]: Calling loss_update with:
args: 1.06928551197052| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845179.0981417]: 
Returned: 1.6111797094345093| 

python DistillationModifier [756 - 1705845182.1312356]: Calling loss_update with:
args: 0.7558907866477966| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845184.1640875]: 
Returned: 1.473760724067688| 

python DistillationModifier [756 - 1705845187.056237]: Calling loss_update with:
args: 0.46534463763237| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845188.501701]: 
Returned: 1.088883638381958| 

python DistillationModifier [756 - 1705845191.2208285]: Calling loss_update with:
args: 1.2793759107589722| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845192.6733527]: 
Returned: 1.4771065711975098| 

python DistillationModifier [756 - 1705845195.3757396]: Calling loss_update with:
args: 0.6878018379211426| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845196.8256013]: 
Returned: 0.6640519499778748| 

python DistillationModifier [756 - 1705845199.5518537]: Calling loss_update with:
args: 0.8424814343452454| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845200.9979978]: 
Returned: 1.4009112119674683| 

python DistillationModifier [756 - 1705845203.704663]: Calling loss_update with:
args: 0.8096186518669128| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845205.149712]: 
Returned: 1.1606528759002686| 

python DistillationModifier [756 - 1705845207.900293]: Calling loss_update with:
args: 1.068755030632019| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845209.351153]: 
Returned: 1.477366328239441| 

python DistillationModifier [756 - 1705845212.0511065]: Calling loss_update with:
args: 0.9843918681144714| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845213.4888656]: 
Returned: 1.7385505437850952| 

python DistillationModifier [756 - 1705845216.1885307]: Calling loss_update with:
args: 0.7282671928405762| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845217.62903]: 
Returned: 0.7756175994873047| 

python DistillationModifier [756 - 1705845220.3324442]: Calling loss_update with:
args: 1.2250926494598389| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845221.7722812]: 
Returned: 1.995694637298584| 

python DistillationModifier [756 - 1705845224.4745653]: Calling loss_update with:
args: 0.9313803315162659| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845225.9185708]: 
Returned: 1.61081862449646| 

python DistillationModifier [756 - 1705845228.616905]: Calling loss_update with:
args: 0.8704829216003418| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845230.0696735]: 
Returned: 1.2490508556365967| 

python DistillationModifier [756 - 1705845233.3314612]: Calling loss_update with:
args: 1.0249897241592407| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845234.768469]: 
Returned: 1.9146097898483276| 

python DistillationModifier [756 - 1705845237.5171275]: Calling loss_update with:
args: 1.406907081604004| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845238.9598439]: 
Returned: 1.544942855834961| 

python DistillationModifier [756 - 1705845241.6786866]: Calling loss_update with:
args: 0.8062496781349182| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845243.7163346]: 
Returned: 1.3566910028457642| 

python DistillationModifier [756 - 1705845246.987026]: Calling loss_update with:
args: 0.86545330286026| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845248.4274693]: 
Returned: 1.468382477760315| 

python DistillationModifier [756 - 1705845251.1538486]: Calling loss_update with:
args: 0.705069363117218| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845252.5967495]: 
Returned: 1.032423496246338| 

python DistillationModifier [756 - 1705845255.3015366]: Calling loss_update with:
args: 0.8902616500854492| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845256.740806]: 
Returned: 1.1797614097595215| 

python DistillationModifier [756 - 1705845259.4514453]: Calling loss_update with:
args: 0.8045726418495178| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845260.909847]: 
Returned: 1.483771800994873| 

python DistillationModifier [756 - 1705845263.618383]: Calling loss_update with:
args: 1.2279481887817383| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845265.0604954]: 
Returned: 1.403434157371521| 

python DistillationModifier [756 - 1705845267.8049877]: Calling loss_update with:
args: 0.6312134265899658| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845269.2493324]: 
Returned: 1.0655401945114136| 

python DistillationModifier [756 - 1705845271.9598308]: Calling loss_update with:
args: 1.2082099914550781| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845273.4065974]: 
Returned: 1.6847566366195679| 

python DistillationModifier [756 - 1705845276.1099157]: Calling loss_update with:
args: 0.8721600770950317| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845277.5490642]: 
Returned: 1.3417115211486816| 

python DistillationModifier [756 - 1705845280.2517102]: Calling loss_update with:
args: 0.7101399898529053| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845281.7086568]: 
Returned: 1.4419615268707275| 

python DistillationModifier [756 - 1705845284.4108937]: Calling loss_update with:
args: 0.6290900111198425| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845285.855814]: 
Returned: 1.288170337677002| 

python DistillationModifier [756 - 1705845288.5597816]: Calling loss_update with:
args: 1.3652033805847168| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845290.0370824]: 
Returned: 2.3666393756866455| 

python DistillationModifier [756 - 1705845292.7465677]: Calling loss_update with:
args: 1.083642601966858| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845294.1839757]: 
Returned: 1.4052752256393433| 

python DistillationModifier [756 - 1705845296.939714]: Calling loss_update with:
args: 1.2513777017593384| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845298.9331355]: 
Returned: 2.0796875953674316| 

python DistillationModifier [756 - 1705845301.7651992]: Calling loss_update with:
args: 1.1716924905776978| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845303.7877748]: 
Returned: 1.8633464574813843| 

python DistillationModifier [756 - 1705845306.8540664]: Calling loss_update with:
args: 1.3996556997299194| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845308.2981253]: 
Returned: 1.8306113481521606| 

python DistillationModifier [756 - 1705845311.0067568]: Calling loss_update with:
args: 1.2019593715667725| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845312.4460459]: 
Returned: 2.259589910507202| 

python DistillationModifier [756 - 1705845315.1442604]: Calling loss_update with:
args: 1.0026720762252808| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845316.5883362]: 
Returned: 1.1032713651657104| 

python DistillationModifier [756 - 1705845319.2904747]: Calling loss_update with:
args: 0.3750823140144348| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845320.74494]: 
Returned: 0.9709725379943848| 

python DistillationModifier [756 - 1705845323.4627593]: Calling loss_update with:
args: 0.9581547379493713| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845324.9040527]: 
Returned: 1.7367745637893677| 

python DistillationModifier [756 - 1705845327.6333249]: Calling loss_update with:
args: 1.408082365989685| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845329.0834856]: 
Returned: 2.6823015213012695| 

python DistillationModifier [756 - 1705845331.790217]: Calling loss_update with:
args: 0.6074177622795105| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845333.2235773]: 
Returned: 1.750622034072876| 

python DistillationModifier [756 - 1705845335.9280314]: Calling loss_update with:
args: 0.9182281494140625| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845337.368079]: 
Returned: 1.8615663051605225| 

python DistillationModifier [756 - 1705845340.0721807]: Calling loss_update with:
args: 1.0424306392669678| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845341.5386753]: 
Returned: 1.1086994409561157| 

python DistillationModifier [756 - 1705845344.2502878]: Calling loss_update with:
args: 0.7698543071746826| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845345.6940935]: 
Returned: 1.694638729095459| 

python DistillationModifier [756 - 1705845348.400507]: Calling loss_update with:
args: 0.9433924555778503| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845349.838705]: 
Returned: 2.216688394546509| 

python DistillationModifier [756 - 1705845352.562172]: Calling loss_update with:
args: 1.365685224533081| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845354.0040696]: 
Returned: 1.8991577625274658| 

python DistillationModifier [756 - 1705845356.658285]: Calling loss_update with:
args: 0.7214144468307495| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845358.1982918]: 
Returned: 1.5885568857192993| 

python DistillationModifier [756 - 1705845361.258923]: Calling loss_update with:
args: 0.4783215820789337| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845363.1317105]: 
Returned: 1.0481436252593994| 

python DistillationModifier [756 - 1705845366.3798327]: Calling loss_update with:
args: 1.4621658325195312| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845367.805636]: 
Returned: 2.6895041465759277| 

python DistillationModifier [756 - 1705845370.5036998]: Calling loss_update with:
args: 1.007827639579773| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845371.933612]: 
Returned: 1.8476613759994507| 

python DistillationModifier [756 - 1705845374.6166635]: Calling loss_update with:
args: 0.5760467052459717| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845376.0479317]: 
Returned: 1.176040768623352| 

python DistillationModifier [756 - 1705845378.7431097]: Calling loss_update with:
args: 1.4282004833221436| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845380.1815612]: 
Returned: 1.401560664176941| 

python DistillationModifier [756 - 1705845382.8932521]: Calling loss_update with:
args: 0.9486192464828491| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845384.3247766]: 
Returned: 1.2951253652572632| 

python DistillationModifier [756 - 1705845387.0148005]: Calling loss_update with:
args: 0.7233227491378784| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845388.4972298]: 
Returned: 1.160791277885437| 

python DistillationModifier [756 - 1705845391.198247]: Calling loss_update with:
args: 0.5035721659660339| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845392.6379926]: 
Returned: 1.2823419570922852| 

python DistillationModifier [756 - 1705845395.3277414]: Calling loss_update with:
args: 0.8261365294456482| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845396.760789]: 
Returned: 1.2126374244689941| 

python DistillationModifier [756 - 1705845399.44816]: Calling loss_update with:
args: 0.9826130867004395| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845400.8854525]: 
Returned: 0.9637448191642761| 

python DistillationModifier [756 - 1705845403.5877433]: Calling loss_update with:
args: 0.9917071461677551| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845405.0329902]: 
Returned: 1.2969785928726196| 

python DistillationModifier [756 - 1705845407.7285264]: Calling loss_update with:
args: 1.115597128868103| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845409.1783752]: 
Returned: 1.9653894901275635| 

python DistillationModifier [756 - 1705845411.9026647]: Calling loss_update with:
args: 1.0883408784866333| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845413.3455281]: 
Returned: 2.0096139907836914| 

python DistillationModifier [756 - 1705845416.0597308]: Calling loss_update with:
args: 0.9949607253074646| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845417.5031939]: 
Returned: 1.6618363857269287| 

python DistillationModifier [756 - 1705845420.7562637]: Calling loss_update with:
args: 1.0996297597885132| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845422.567429]: 
Returned: 2.0483317375183105| 

python DistillationModifier [756 - 1705845424.835704]: Calling loss_update with:
args: 0.5559521317481995| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845425.605683]: 
Returned: 1.1258947849273682| 

python DistillationModifier [756 - 1705845429.0219564]: Calling loss_update with:
args: 0.008397134020924568| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [756 - 1705845430.4710872]: 
Returned: 0.04045410454273224| 

python LearningRateFunctionModifier [756 - 1705845433.7914736]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| 
python LearningRateFunctionModifier [756 - 1705845433.7916932]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [756 - 1705845433.7917573]: 1.153846153846154e-05
python LearningRateFunctionModifier/ParamGroup1 [756 - 1705845433.7920055]: 1.153846153846154e-05
python DistillationModifier/task_loss [756 - 1705845433.792341]: tensor(0.0084, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [756 - 1705845433.793463]: tensor(0.0405, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [756 - 1705845433.7941806]: tensor(0.0405, grad_fn=<AddBackward0>)
python ConstantPruningModifier [756 - 1705845433.7948601]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.0| steps_per_epoch: 63| 
python ConstantPruningModifier [756 - 1705845434.0187924]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [756 - 1705845434.0196435]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [756 - 1705845434.020555]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [756 - 1705845434.0212898]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [756 - 1705845434.021983]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [756 - 1705845434.024166]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [756 - 1705845434.025988]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [756 - 1705845434.0269175]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [756 - 1705845434.0276976]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [756 - 1705845434.0283396]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [756 - 1705845434.0289772]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [756 - 1705845434.030714]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [756 - 1705845434.0324328]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [756 - 1705845434.0333123]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [756 - 1705845434.0340521]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [756 - 1705845434.0346708]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [756 - 1705845434.0352325]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [756 - 1705845434.037032]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [756 - 1705845434.0388222]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [756 - 1705845434.0397193]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [756 - 1705845434.0404553]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [756 - 1705845434.0411053]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [756 - 1705845434.0416806]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [756 - 1705845434.043435]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [756 - 1705845434.045218]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [756 - 1705845434.046134]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [756 - 1705845434.0468903]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [756 - 1705845434.0475512]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [756 - 1705845434.0481408]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [756 - 1705845434.0499535]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [756 - 1705845434.0516155]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [756 - 1705845434.0524702]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [756 - 1705845434.0532107]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [756 - 1705845434.0538275]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [756 - 1705845434.054396]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [756 - 1705845434.0561953]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [756 - 1705845434.0580387]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [756 - 1705845434.0589428]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [756 - 1705845434.0596802]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [756 - 1705845434.0602882]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [756 - 1705845434.0608673]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [756 - 1705845434.0625968]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [756 - 1705845434.0642128]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [756 - 1705845434.0650403]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [756 - 1705845434.0657485]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [756 - 1705845434.0663586]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [756 - 1705845434.0669224]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [756 - 1705845434.068707]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [756 - 1705845434.0703917]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [756 - 1705845434.0711942]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [756 - 1705845434.0718853]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [756 - 1705845434.0724974]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [756 - 1705845434.0730693]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [756 - 1705845434.0747595]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [756 - 1705845434.0764563]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [756 - 1705845434.077355]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [756 - 1705845434.0780928]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [756 - 1705845434.0787115]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [756 - 1705845434.0792766]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [756 - 1705845434.081077]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [756 - 1705845434.0827947]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [756 - 1705845434.08366]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [756 - 1705845434.0843596]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [756 - 1705845434.0849957]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [756 - 1705845434.085548]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [756 - 1705845434.0872936]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [756 - 1705845434.0890148]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [756 - 1705845434.089874]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [756 - 1705845434.090613]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [756 - 1705845434.0912378]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [756 - 1705845434.0918157]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [756 - 1705845434.0936062]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [756 - 1705845434.0953355]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [756 - 1705845434.096222]: 0.0
python ParamPruning/classifier.weight [756 - 1705845434.0967808]: 0.0
python DistillationModifier [763 - 1705845487.7997]: Calling loss_update with:
args: 0.1927770972251892| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.11111111111111| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [763 - 1705845489.347673]: 
Returned: 0.08085913956165314| 

python LearningRateFunctionModifier [763 - 1705845492.6602643]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.11111111111111| steps_per_epoch: 63| 
python LearningRateFunctionModifier [763 - 1705845492.660461]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [763 - 1705845492.6605332]: 1.0256410256410242e-05
python LearningRateFunctionModifier/ParamGroup1 [763 - 1705845492.6607974]: 1.0256410256410242e-05
python DistillationModifier/task_loss [763 - 1705845492.6611168]: tensor(0.1928, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [763 - 1705845492.6621413]: tensor(0.0809, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [763 - 1705845492.662723]: tensor(0.0809, grad_fn=<AddBackward0>)
python ConstantPruningModifier [763 - 1705845492.663287]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.11111111111111| steps_per_epoch: 63| 
python ConstantPruningModifier [763 - 1705845492.8959131]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [763 - 1705845492.8968048]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [763 - 1705845492.8976839]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [763 - 1705845492.8983243]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [763 - 1705845492.8988829]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [763 - 1705845492.9006443]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [763 - 1705845492.9025273]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [763 - 1705845492.9033]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [763 - 1705845492.9038997]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [763 - 1705845492.904429]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [763 - 1705845492.9049594]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [763 - 1705845492.9067025]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [763 - 1705845492.9085288]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [763 - 1705845492.909348]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [763 - 1705845492.9099967]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [763 - 1705845492.9105449]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [763 - 1705845492.9110544]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [763 - 1705845492.9128106]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [763 - 1705845492.9146142]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [763 - 1705845492.9153419]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [763 - 1705845492.9159524]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [763 - 1705845492.9165008]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [763 - 1705845492.9170384]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [763 - 1705845492.9188406]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [763 - 1705845492.9206445]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [763 - 1705845492.9214091]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [763 - 1705845492.9220252]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [763 - 1705845492.9225717]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [763 - 1705845492.9230814]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [763 - 1705845492.924831]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [763 - 1705845492.9266934]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [763 - 1705845492.9274788]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [763 - 1705845492.9280896]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [763 - 1705845492.928633]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [763 - 1705845492.9291725]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [763 - 1705845492.930945]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [763 - 1705845492.9328566]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [763 - 1705845492.933675]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [763 - 1705845492.934297]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [763 - 1705845492.9348862]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [763 - 1705845492.9354134]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [763 - 1705845492.9371784]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [763 - 1705845492.9389687]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [763 - 1705845492.9397373]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [763 - 1705845492.9403753]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [763 - 1705845492.9409459]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [763 - 1705845492.9414663]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [763 - 1705845492.9431121]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [763 - 1705845492.944695]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [763 - 1705845492.945486]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [763 - 1705845492.9461324]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [763 - 1705845492.946687]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [763 - 1705845492.9472003]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [763 - 1705845492.948907]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [763 - 1705845492.9507334]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [763 - 1705845492.951473]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [763 - 1705845492.9520993]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [763 - 1705845492.952645]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [763 - 1705845492.9531858]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [763 - 1705845492.954872]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [763 - 1705845492.9567103]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [763 - 1705845492.957396]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [763 - 1705845492.9579709]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [763 - 1705845492.9584994]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [763 - 1705845492.9590015]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [763 - 1705845492.9607687]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [763 - 1705845492.9626706]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [763 - 1705845492.9634893]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [763 - 1705845492.9641342]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [763 - 1705845492.9647222]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [763 - 1705845492.9652646]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [763 - 1705845492.96707]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [763 - 1705845492.968951]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [763 - 1705845492.9697511]: 0.0
python ParamPruning/classifier.weight [763 - 1705845492.9702308]: 0.0
python DistillationModifier [770 - 1705845546.3063502]: Calling loss_update with:
args: 0.027833061292767525| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.222222222222221| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [770 - 1705845547.8750296]: 
Returned: 0.1117318868637085| 

python LearningRateFunctionModifier [770 - 1705845551.1835248]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.222222222222221| steps_per_epoch: 63| 
python LearningRateFunctionModifier [770 - 1705845551.1837065]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [770 - 1705845551.1837742]: 8.974358974358972e-06
python LearningRateFunctionModifier/ParamGroup1 [770 - 1705845551.1840076]: 8.974358974358972e-06
python DistillationModifier/task_loss [770 - 1705845551.1843228]: tensor(0.0278, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [770 - 1705845551.1854873]: tensor(0.1117, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [770 - 1705845551.1860697]: tensor(0.1117, grad_fn=<AddBackward0>)
python ConstantPruningModifier [770 - 1705845551.1866367]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.222222222222221| steps_per_epoch: 63| 
python ConstantPruningModifier [770 - 1705845551.4109652]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [770 - 1705845551.4118822]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [770 - 1705845551.412978]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [770 - 1705845551.4137301]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [770 - 1705845551.4144268]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [770 - 1705845551.4162898]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [770 - 1705845551.4182236]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [770 - 1705845551.419111]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [770 - 1705845551.4199038]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [770 - 1705845551.420642]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [770 - 1705845551.4212801]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [770 - 1705845551.4230585]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [770 - 1705845551.424798]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [770 - 1705845551.4256341]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [770 - 1705845551.4263968]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [770 - 1705845551.4270685]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [770 - 1705845551.4276874]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [770 - 1705845551.4294584]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [770 - 1705845551.4311626]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [770 - 1705845551.4319754]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [770 - 1705845551.4327526]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [770 - 1705845551.433409]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [770 - 1705845551.434085]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [770 - 1705845551.4358275]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [770 - 1705845551.437681]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [770 - 1705845551.4384842]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [770 - 1705845551.4392233]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [770 - 1705845551.4399278]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [770 - 1705845551.4406018]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [770 - 1705845551.4424233]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [770 - 1705845551.444234]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [770 - 1705845551.4450374]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [770 - 1705845551.4457629]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [770 - 1705845551.4463773]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [770 - 1705845551.446994]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [770 - 1705845551.4487922]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [770 - 1705845551.450658]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [770 - 1705845551.4514742]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [770 - 1705845551.4522567]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [770 - 1705845551.4530075]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [770 - 1705845551.4536936]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [770 - 1705845551.4554987]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [770 - 1705845551.4573252]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [770 - 1705845551.4582]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [770 - 1705845551.4589703]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [770 - 1705845551.4596624]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [770 - 1705845551.4603066]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [770 - 1705845551.4621613]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [770 - 1705845551.4640265]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [770 - 1705845551.4649003]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [770 - 1705845551.4656756]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [770 - 1705845551.4663541]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [770 - 1705845551.4669774]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [770 - 1705845551.4688222]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [770 - 1705845551.4705968]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [770 - 1705845551.471426]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [770 - 1705845551.472232]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [770 - 1705845551.4729447]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [770 - 1705845551.4736075]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [770 - 1705845551.475359]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [770 - 1705845551.477101]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [770 - 1705845551.477946]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [770 - 1705845551.478733]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [770 - 1705845551.4795165]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [770 - 1705845551.4802248]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [770 - 1705845551.4821055]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [770 - 1705845551.4839997]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [770 - 1705845551.4848266]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [770 - 1705845551.4855623]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [770 - 1705845551.486225]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [770 - 1705845551.4869099]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [770 - 1705845551.4886992]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [770 - 1705845551.490372]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [770 - 1705845551.4911907]: 0.0
python ParamPruning/classifier.weight [770 - 1705845551.4917216]: 0.0
python DistillationModifier [777 - 1705845604.0756867]: Calling loss_update with:
args: 0.018709881231188774| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.333333333333334| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [777 - 1705845606.1844292]: 
Returned: 0.12027531862258911| 

python LearningRateFunctionModifier [777 - 1705845609.6516309]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.333333333333334| steps_per_epoch: 63| 
python LearningRateFunctionModifier [777 - 1705845609.65186]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [777 - 1705845609.662193]: 7.692307692307675e-06
python LearningRateFunctionModifier/ParamGroup1 [777 - 1705845609.6624541]: 7.692307692307675e-06
python DistillationModifier/task_loss [777 - 1705845609.6627688]: tensor(0.0187, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [777 - 1705845609.6638906]: tensor(0.1203, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [777 - 1705845609.664631]: tensor(0.1203, grad_fn=<AddBackward0>)
python ConstantPruningModifier [777 - 1705845609.665329]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.333333333333334| steps_per_epoch: 63| 
python ConstantPruningModifier [777 - 1705845609.8909245]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [777 - 1705845609.891798]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [777 - 1705845609.8927484]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [777 - 1705845609.8935452]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [777 - 1705845609.8941672]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [777 - 1705845609.895949]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [777 - 1705845609.8977659]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [777 - 1705845609.898496]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [777 - 1705845609.899127]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [777 - 1705845609.8997052]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [777 - 1705845609.9002547]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [777 - 1705845609.9020238]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [777 - 1705845609.9037716]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [777 - 1705845609.9044657]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [777 - 1705845609.9051738]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [777 - 1705845609.9058418]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [777 - 1705845609.9064908]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [777 - 1705845609.9082298]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [777 - 1705845609.9100459]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [777 - 1705845609.9107783]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [777 - 1705845609.9114633]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [777 - 1705845609.9121842]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [777 - 1705845609.9128876]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [777 - 1705845609.9146366]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [777 - 1705845609.9164176]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [777 - 1705845609.9171574]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [777 - 1705845609.9177735]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [777 - 1705845609.9184253]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [777 - 1705845609.9190025]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [777 - 1705845609.9207308]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [777 - 1705845609.9225273]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [777 - 1705845609.9232693]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [777 - 1705845609.923979]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [777 - 1705845609.924622]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [777 - 1705845609.925243]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [777 - 1705845609.9269583]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [777 - 1705845609.9287834]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [777 - 1705845609.929579]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [777 - 1705845609.9303205]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [777 - 1705845609.9310102]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [777 - 1705845609.9317153]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [777 - 1705845609.9334779]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [777 - 1705845609.9353247]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [777 - 1705845609.9360826]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [777 - 1705845609.9368181]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [777 - 1705845609.937445]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [777 - 1705845609.9381254]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [777 - 1705845609.9399064]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [777 - 1705845609.9417515]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [777 - 1705845609.9425287]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [777 - 1705845609.943247]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [777 - 1705845609.9438708]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [777 - 1705845609.9445548]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [777 - 1705845609.9463618]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [777 - 1705845609.9482524]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [777 - 1705845609.9490812]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [777 - 1705845609.9497561]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [777 - 1705845609.9504654]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [777 - 1705845609.9511573]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [777 - 1705845609.9529486]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [777 - 1705845609.9547284]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [777 - 1705845609.9554539]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [777 - 1705845609.956149]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [777 - 1705845609.9567926]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [777 - 1705845609.957399]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [777 - 1705845609.959133]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [777 - 1705845609.9610093]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [777 - 1705845609.961774]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [777 - 1705845609.9624848]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [777 - 1705845609.9631271]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [777 - 1705845609.9637072]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [777 - 1705845609.965501]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [777 - 1705845609.9672916]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [777 - 1705845609.9680498]: 0.0
python ParamPruning/classifier.weight [777 - 1705845609.968492]: 0.0
python DistillationModifier [784 - 1705845661.0995257]: Calling loss_update with:
args: 0.23599956929683685| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.444444444444445| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [784 - 1705845662.9754755]: 
Returned: 0.15298867225646973| 

python LearningRateFunctionModifier [784 - 1705845667.2699835]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.444444444444445| steps_per_epoch: 63| 
python LearningRateFunctionModifier [784 - 1705845667.2701604]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [784 - 1705845667.2702296]: 6.410256410256405e-06
python LearningRateFunctionModifier/ParamGroup1 [784 - 1705845667.2704697]: 6.410256410256405e-06
python DistillationModifier/task_loss [784 - 1705845667.270792]: tensor(0.2360, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [784 - 1705845667.2719684]: tensor(0.1530, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [784 - 1705845667.2726855]: tensor(0.1530, grad_fn=<AddBackward0>)
python ConstantPruningModifier [784 - 1705845667.2733514]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.444444444444445| steps_per_epoch: 63| 
python ConstantPruningModifier [784 - 1705845667.5724711]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [784 - 1705845667.573434]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [784 - 1705845667.574465]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [784 - 1705845667.5751665]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [784 - 1705845667.5758195]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [784 - 1705845667.5779104]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [784 - 1705845667.580049]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [784 - 1705845667.5809932]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [784 - 1705845667.5817869]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [784 - 1705845667.5824482]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [784 - 1705845667.583071]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [784 - 1705845667.5851874]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [784 - 1705845667.5872872]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [784 - 1705845667.5882103]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [784 - 1705845667.5890424]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [784 - 1705845667.589722]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [784 - 1705845667.5903554]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [784 - 1705845667.5924437]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [784 - 1705845667.5945039]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [784 - 1705845667.595393]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [784 - 1705845667.596151]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [784 - 1705845667.5968127]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [784 - 1705845667.597431]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [784 - 1705845667.5995214]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [784 - 1705845667.601719]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [784 - 1705845667.60269]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [784 - 1705845667.6034882]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [784 - 1705845667.6041775]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [784 - 1705845667.604822]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [784 - 1705845667.6069148]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [784 - 1705845667.6088862]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [784 - 1705845667.609761]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [784 - 1705845667.6104913]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [784 - 1705845667.6111395]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [784 - 1705845667.6117659]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [784 - 1705845667.6138482]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [784 - 1705845667.615877]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [784 - 1705845667.616841]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [784 - 1705845667.6176322]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [784 - 1705845667.6183112]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [784 - 1705845667.6189504]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [784 - 1705845667.6210399]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [784 - 1705845667.6230614]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [784 - 1705845667.623977]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [784 - 1705845667.624799]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [784 - 1705845667.6254814]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [784 - 1705845667.626105]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [784 - 1705845667.6281805]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [784 - 1705845667.6301768]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [784 - 1705845667.631062]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [784 - 1705845667.6318133]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [784 - 1705845667.6324532]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [784 - 1705845667.6330924]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [784 - 1705845667.6351428]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [784 - 1705845667.6373143]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [784 - 1705845667.6383045]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [784 - 1705845667.6391435]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [784 - 1705845667.639869]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [784 - 1705845667.640512]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [784 - 1705845667.642627]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [784 - 1705845667.64469]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [784 - 1705845667.645573]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [784 - 1705845667.6463256]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [784 - 1705845667.6470077]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [784 - 1705845667.64763]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [784 - 1705845667.6497498]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [784 - 1705845667.6519053]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [784 - 1705845667.652985]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [784 - 1705845667.6538]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [784 - 1705845667.6544735]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [784 - 1705845667.6550999]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [784 - 1705845667.6571715]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [784 - 1705845667.659239]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [784 - 1705845667.660193]: 0.0
python ParamPruning/classifier.weight [784 - 1705845667.6607864]: 0.0
python DistillationModifier [791 - 1705845719.2403228]: Calling loss_update with:
args: 0.025128237903118134| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.555555555555555| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [791 - 1705845720.695941]: 
Returned: 0.11047881096601486| 

python LearningRateFunctionModifier [791 - 1705845724.8422112]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.555555555555555| steps_per_epoch: 63| 
python LearningRateFunctionModifier [791 - 1705845724.8424134]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [791 - 1705845724.8424742]: 5.128205128205135e-06
python LearningRateFunctionModifier/ParamGroup1 [791 - 1705845724.8427296]: 5.128205128205135e-06
python DistillationModifier/task_loss [791 - 1705845724.843021]: tensor(0.0251, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [791 - 1705845724.8442214]: tensor(0.1105, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [791 - 1705845724.8448803]: tensor(0.1105, grad_fn=<AddBackward0>)
python ConstantPruningModifier [791 - 1705845724.8454962]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.555555555555555| steps_per_epoch: 63| 
python ConstantPruningModifier [791 - 1705845725.2198794]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [791 - 1705845725.2210252]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [791 - 1705845725.222368]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [791 - 1705845725.2234282]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [791 - 1705845725.224407]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [791 - 1705845725.2271912]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [791 - 1705845725.2299104]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [791 - 1705845725.2309859]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [791 - 1705845725.2320433]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [791 - 1705845725.233032]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [791 - 1705845725.2339363]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [791 - 1705845725.2367227]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [791 - 1705845725.2394266]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [791 - 1705845725.2405486]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [791 - 1705845725.241575]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [791 - 1705845725.2425132]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [791 - 1705845725.2434378]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [791 - 1705845725.2463112]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [791 - 1705845725.2490406]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [791 - 1705845725.2501967]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [791 - 1705845725.2512367]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [791 - 1705845725.2522686]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [791 - 1705845725.2531724]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [791 - 1705845725.255924]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [791 - 1705845725.2586293]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [791 - 1705845725.2597811]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [791 - 1705845725.2608669]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [791 - 1705845725.2617564]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [791 - 1705845725.2625594]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [791 - 1705845725.265287]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [791 - 1705845725.2679694]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [791 - 1705845725.2690718]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [791 - 1705845725.2700489]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [791 - 1705845725.271056]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [791 - 1705845725.2719886]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [791 - 1705845725.2747328]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [791 - 1705845725.2773094]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [791 - 1705845725.2783606]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [791 - 1705845725.2793152]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [791 - 1705845725.2801535]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [791 - 1705845725.281103]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [791 - 1705845725.2837727]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [791 - 1705845725.2863922]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [791 - 1705845725.2874622]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [791 - 1705845725.2884538]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [791 - 1705845725.2893007]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [791 - 1705845725.290167]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [791 - 1705845725.293]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [791 - 1705845725.2957714]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [791 - 1705845725.2969368]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [791 - 1705845725.2978113]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [791 - 1705845725.2986095]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [791 - 1705845725.299527]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [791 - 1705845725.3022814]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [791 - 1705845725.3048987]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [791 - 1705845725.3059433]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [791 - 1705845725.3069284]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [791 - 1705845725.307861]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [791 - 1705845725.308709]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [791 - 1705845725.3114338]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [791 - 1705845725.3141437]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [791 - 1705845725.315328]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [791 - 1705845725.3163419]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [791 - 1705845725.3173115]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [791 - 1705845725.31813]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [791 - 1705845725.3208482]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [791 - 1705845725.3235826]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [791 - 1705845725.3247552]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [791 - 1705845725.325777]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [791 - 1705845725.326641]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [791 - 1705845725.3274267]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [791 - 1705845725.330175]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [791 - 1705845725.332819]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [791 - 1705845725.3338237]: 0.0
python ParamPruning/classifier.weight [791 - 1705845725.3343325]: 0.0
python DistillationModifier [798 - 1705845777.7913043]: Calling loss_update with:
args: 0.044886816293001175| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.666666666666666| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [798 - 1705845779.2356906]: 
Returned: 0.05794628709554672| 

python LearningRateFunctionModifier [798 - 1705845783.0068645]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.666666666666666| steps_per_epoch: 63| 
python LearningRateFunctionModifier [798 - 1705845783.0070612]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [798 - 1705845783.0077145]: 3.8461538461538375e-06
python LearningRateFunctionModifier/ParamGroup1 [798 - 1705845783.007947]: 3.8461538461538375e-06
python DistillationModifier/task_loss [798 - 1705845783.0082629]: tensor(0.0449, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [798 - 1705845783.009352]: tensor(0.0579, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [798 - 1705845783.0100644]: tensor(0.0579, grad_fn=<AddBackward0>)
python ConstantPruningModifier [798 - 1705845783.0107715]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.666666666666666| steps_per_epoch: 63| 
python ConstantPruningModifier [798 - 1705845783.3212621]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [798 - 1705845783.3221776]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [798 - 1705845783.3231926]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [798 - 1705845783.3239176]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [798 - 1705845783.3245378]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [798 - 1705845783.3266582]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [798 - 1705845783.3287067]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [798 - 1705845783.3295958]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [798 - 1705845783.3302958]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [798 - 1705845783.3309574]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [798 - 1705845783.3315804]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [798 - 1705845783.3338532]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [798 - 1705845783.3360236]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [798 - 1705845783.3369548]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [798 - 1705845783.3377042]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [798 - 1705845783.338302]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [798 - 1705845783.3388693]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [798 - 1705845783.3408623]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [798 - 1705845783.342831]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [798 - 1705845783.3437233]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [798 - 1705845783.3444362]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [798 - 1705845783.3450785]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [798 - 1705845783.3456576]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [798 - 1705845783.347529]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [798 - 1705845783.3494234]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [798 - 1705845783.3502958]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [798 - 1705845783.351023]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [798 - 1705845783.3516865]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [798 - 1705845783.352288]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [798 - 1705845783.3544366]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [798 - 1705845783.3565154]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [798 - 1705845783.3574421]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [798 - 1705845783.358179]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [798 - 1705845783.3587832]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [798 - 1705845783.359359]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [798 - 1705845783.3614085]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [798 - 1705845783.3634315]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [798 - 1705845783.3643475]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [798 - 1705845783.3651397]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [798 - 1705845783.3657575]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [798 - 1705845783.366321]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [798 - 1705845783.368276]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [798 - 1705845783.3702953]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [798 - 1705845783.3711836]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [798 - 1705845783.3719268]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [798 - 1705845783.3725548]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [798 - 1705845783.3731425]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [798 - 1705845783.3750093]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [798 - 1705845783.3769026]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [798 - 1705845783.3776987]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [798 - 1705845783.3783808]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [798 - 1705845783.3789775]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [798 - 1705845783.379529]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [798 - 1705845783.3815444]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [798 - 1705845783.3835168]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [798 - 1705845783.3843222]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [798 - 1705845783.3850186]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [798 - 1705845783.3856142]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [798 - 1705845783.3861818]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [798 - 1705845783.3881872]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [798 - 1705845783.3901744]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [798 - 1705845783.3910654]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [798 - 1705845783.391804]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [798 - 1705845783.392441]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [798 - 1705845783.3930657]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [798 - 1705845783.3951216]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [798 - 1705845783.3971827]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [798 - 1705845783.3980231]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [798 - 1705845783.3986866]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [798 - 1705845783.399301]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [798 - 1705845783.399838]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [798 - 1705845783.4017313]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [798 - 1705845783.403604]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [798 - 1705845783.4045005]: 0.0
python ParamPruning/classifier.weight [798 - 1705845783.405016]: 0.0
python DistillationModifier [805 - 1705845836.0722003]: Calling loss_update with:
args: 0.03417214751243591| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.777777777777779| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [805 - 1705845837.5878813]: 
Returned: 0.08514167368412018| 

python LearningRateFunctionModifier [805 - 1705845840.8004377]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.777777777777779| steps_per_epoch: 63| 
python LearningRateFunctionModifier [805 - 1705845840.8006055]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [805 - 1705845840.8006856]: 2.5641025641025674e-06
python LearningRateFunctionModifier/ParamGroup1 [805 - 1705845840.800939]: 2.5641025641025674e-06
python DistillationModifier/task_loss [805 - 1705845840.8012605]: tensor(0.0342, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [805 - 1705845840.8023365]: tensor(0.0851, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [805 - 1705845840.8029678]: tensor(0.0851, grad_fn=<AddBackward0>)
python ConstantPruningModifier [805 - 1705845840.8035622]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.777777777777779| steps_per_epoch: 63| 
python ConstantPruningModifier [805 - 1705845841.0243177]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [805 - 1705845841.025148]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [805 - 1705845841.0261781]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [805 - 1705845841.0269294]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [805 - 1705845841.0275195]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [805 - 1705845841.029209]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [805 - 1705845841.0307236]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [805 - 1705845841.0314198]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [805 - 1705845841.0320606]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [805 - 1705845841.0326123]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [805 - 1705845841.0331674]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [805 - 1705845841.0347354]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [805 - 1705845841.0362077]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [805 - 1705845841.0369482]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [805 - 1705845841.03758]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [805 - 1705845841.0381494]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [805 - 1705845841.038684]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [805 - 1705845841.0402443]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [805 - 1705845841.0417018]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [805 - 1705845841.042364]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [805 - 1705845841.0429578]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [805 - 1705845841.043506]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [805 - 1705845841.0440414]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [805 - 1705845841.0455968]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [805 - 1705845841.0470474]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [805 - 1705845841.0477278]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [805 - 1705845841.0483403]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [805 - 1705845841.0489085]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [805 - 1705845841.0494525]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [805 - 1705845841.0510492]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [805 - 1705845841.0525777]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [805 - 1705845841.0533226]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [805 - 1705845841.0539513]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [805 - 1705845841.0545065]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [805 - 1705845841.055044]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [805 - 1705845841.0565586]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [805 - 1705845841.0580156]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [805 - 1705845841.0587513]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [805 - 1705845841.0593908]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [805 - 1705845841.0599537]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [805 - 1705845841.0604858]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [805 - 1705845841.0620806]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [805 - 1705845841.0635421]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [805 - 1705845841.0642474]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [805 - 1705845841.0649035]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [805 - 1705845841.0654805]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [805 - 1705845841.0660248]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [805 - 1705845841.0676289]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [805 - 1705845841.069164]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [805 - 1705845841.069936]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [805 - 1705845841.0796516]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [805 - 1705845841.080249]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [805 - 1705845841.0808077]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [805 - 1705845841.0824542]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [805 - 1705845841.083967]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [805 - 1705845841.0846565]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [805 - 1705845841.0852907]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [805 - 1705845841.0858426]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [805 - 1705845841.0863905]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [805 - 1705845841.0879037]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [805 - 1705845841.0893624]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [805 - 1705845841.0900831]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [805 - 1705845841.0907102]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [805 - 1705845841.0912695]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [805 - 1705845841.0918381]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [805 - 1705845841.0934403]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [805 - 1705845841.0949094]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [805 - 1705845841.0956337]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [805 - 1705845841.096262]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [805 - 1705845841.0968554]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [805 - 1705845841.0973988]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [805 - 1705845841.0990264]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [805 - 1705845841.1005213]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [805 - 1705845841.1012373]: 0.0
python ParamPruning/classifier.weight [805 - 1705845841.1016698]: 0.0
python DistillationModifier [812 - 1705845894.4209569]: Calling loss_update with:
args: 0.012559553608298302| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 12.88888888888889| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [812 - 1705845895.9811218]: 
Returned: 0.1037469208240509| 

python LearningRateFunctionModifier [812 - 1705845899.3072302]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.88888888888889| steps_per_epoch: 63| 
python LearningRateFunctionModifier [812 - 1705845899.3074067]: 
Returned: <class 'NoneType'>| 

python LearningRateFunctionModifier/ParamGroup0 [812 - 1705845899.30748]: 1.2820512820512701e-06
python LearningRateFunctionModifier/ParamGroup1 [812 - 1705845899.3077216]: 1.2820512820512701e-06
python DistillationModifier/task_loss [812 - 1705845899.3081608]: tensor(0.0126, grad_fn=<NllLossBackward0>)
python DistillationModifier/distillation_loss [812 - 1705845899.3093123]: tensor(0.1037, grad_fn=<DivBackward0>)
python DistillationModifier/total_loss [812 - 1705845899.3099585]: tensor(0.1037, grad_fn=<AddBackward0>)
python ConstantPruningModifier [812 - 1705845899.3105886]: Calling update with:
args: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'torch.optim.adamw.AdamW'>| 
kwargs: epoch: 12.88888888888889| steps_per_epoch: 63| 
python ConstantPruningModifier [812 - 1705845899.5343757]: 
Returned: <class 'NoneType'>| 

python ParamPruning/bert.encoder.layer.0.attention.self.query.weight [812 - 1705845899.535249]: 0.8644917607307434
python ParamPruning/bert.encoder.layer.0.attention.self.key.weight [812 - 1705845899.536316]: 0.8680216670036316
python ParamPruning/bert.encoder.layer.0.attention.self.value.weight [812 - 1705845899.537186]: 0.9312150478363037
python ParamPruning/bert.encoder.layer.0.attention.output.dense.weight [812 - 1705845899.5379345]: 0.9232262372970581
python ParamPruning/bert.encoder.layer.0.intermediate.dense.weight [812 - 1705845899.5397682]: 0.9153103232383728
python ParamPruning/bert.encoder.layer.0.output.dense.weight [812 - 1705845899.541555]: 0.9356380105018616
python ParamPruning/bert.encoder.layer.1.attention.self.query.weight [812 - 1705845899.5424461]: 0.8620554804801941
python ParamPruning/bert.encoder.layer.1.attention.self.key.weight [812 - 1705845899.5432167]: 0.8625064492225647
python ParamPruning/bert.encoder.layer.1.attention.self.value.weight [812 - 1705845899.543856]: 0.9279242753982544
python ParamPruning/bert.encoder.layer.1.attention.output.dense.weight [812 - 1705845899.5444303]: 0.9245097041130066
python ParamPruning/bert.encoder.layer.1.intermediate.dense.weight [812 - 1705845899.5462165]: 0.8945091962814331
python ParamPruning/bert.encoder.layer.1.output.dense.weight [812 - 1705845899.5478947]: 0.9265751242637634
python ParamPruning/bert.encoder.layer.2.attention.self.query.weight [812 - 1705845899.548787]: 0.8451063632965088
python ParamPruning/bert.encoder.layer.2.attention.self.key.weight [812 - 1705845899.5495253]: 0.8532799482345581
python ParamPruning/bert.encoder.layer.2.attention.self.value.weight [812 - 1705845899.5501397]: 0.9295670986175537
python ParamPruning/bert.encoder.layer.2.attention.output.dense.weight [812 - 1705845899.5507965]: 0.9288228154182434
python ParamPruning/bert.encoder.layer.2.intermediate.dense.weight [812 - 1705845899.5525851]: 0.8895581364631653
python ParamPruning/bert.encoder.layer.2.output.dense.weight [812 - 1705845899.5543349]: 0.9237624406814575
python ParamPruning/bert.encoder.layer.3.attention.self.query.weight [812 - 1705845899.555237]: 0.8711106777191162
python ParamPruning/bert.encoder.layer.3.attention.self.key.weight [812 - 1705845899.555997]: 0.8704121708869934
python ParamPruning/bert.encoder.layer.3.attention.self.value.weight [812 - 1705845899.5566454]: 0.9085676670074463
python ParamPruning/bert.encoder.layer.3.attention.output.dense.weight [812 - 1705845899.5572503]: 0.9130028486251831
python ParamPruning/bert.encoder.layer.3.intermediate.dense.weight [812 - 1705845899.5590005]: 0.8868213295936584
python ParamPruning/bert.encoder.layer.3.output.dense.weight [812 - 1705845899.560696]: 0.9209082126617432
python ParamPruning/bert.encoder.layer.4.attention.self.query.weight [812 - 1705845899.561552]: 0.8635711669921875
python ParamPruning/bert.encoder.layer.4.attention.self.key.weight [812 - 1705845899.5622878]: 0.8665059208869934
python ParamPruning/bert.encoder.layer.4.attention.self.value.weight [812 - 1705845899.5628872]: 0.8824039101600647
python ParamPruning/bert.encoder.layer.4.attention.output.dense.weight [812 - 1705845899.5634341]: 0.8957400918006897
python ParamPruning/bert.encoder.layer.4.intermediate.dense.weight [812 - 1705845899.56521]: 0.8822059631347656
python ParamPruning/bert.encoder.layer.4.output.dense.weight [812 - 1705845899.5669255]: 0.9172935485839844
python ParamPruning/bert.encoder.layer.5.attention.self.query.weight [812 - 1705845899.567796]: 0.8685166835784912
python ParamPruning/bert.encoder.layer.5.attention.self.key.weight [812 - 1705845899.5685956]: 0.8675944209098816
python ParamPruning/bert.encoder.layer.5.attention.self.value.weight [812 - 1705845899.5693235]: 0.8843333125114441
python ParamPruning/bert.encoder.layer.5.attention.output.dense.weight [812 - 1705845899.5699232]: 0.8958756923675537
python ParamPruning/bert.encoder.layer.5.intermediate.dense.weight [812 - 1705845899.571757]: 0.8838331699371338
python ParamPruning/bert.encoder.layer.5.output.dense.weight [812 - 1705845899.5735047]: 0.91917884349823
python ParamPruning/bert.encoder.layer.6.attention.self.query.weight [812 - 1705845899.5743942]: 0.8699256181716919
python ParamPruning/bert.encoder.layer.6.attention.self.key.weight [812 - 1705845899.5751388]: 0.8717482089996338
python ParamPruning/bert.encoder.layer.6.attention.self.value.weight [812 - 1705845899.575742]: 0.8924729824066162
python ParamPruning/bert.encoder.layer.6.attention.output.dense.weight [812 - 1705845899.576313]: 0.9078572392463684
python ParamPruning/bert.encoder.layer.6.intermediate.dense.weight [812 - 1705845899.5780919]: 0.8827946782112122
python ParamPruning/bert.encoder.layer.6.output.dense.weight [812 - 1705845899.5798461]: 0.9222526550292969
python ParamPruning/bert.encoder.layer.7.attention.self.query.weight [812 - 1705845899.5807438]: 0.8792538046836853
python ParamPruning/bert.encoder.layer.7.attention.self.key.weight [812 - 1705845899.5814924]: 0.8780839443206787
python ParamPruning/bert.encoder.layer.7.attention.self.value.weight [812 - 1705845899.5821161]: 0.8773871660232544
python ParamPruning/bert.encoder.layer.7.attention.output.dense.weight [812 - 1705845899.5826788]: 0.8886498212814331
python ParamPruning/bert.encoder.layer.7.intermediate.dense.weight [812 - 1705845899.5843704]: 0.8991622924804688
python ParamPruning/bert.encoder.layer.7.output.dense.weight [812 - 1705845899.5860286]: 0.9271066188812256
python ParamPruning/bert.encoder.layer.8.attention.self.query.weight [812 - 1705845899.5868282]: 0.8583950400352478
python ParamPruning/bert.encoder.layer.8.attention.self.key.weight [812 - 1705845899.587523]: 0.856842041015625
python ParamPruning/bert.encoder.layer.8.attention.self.value.weight [812 - 1705845899.588147]: 0.8692152500152588
python ParamPruning/bert.encoder.layer.8.attention.output.dense.weight [812 - 1705845899.5887358]: 0.8843451738357544
python ParamPruning/bert.encoder.layer.8.intermediate.dense.weight [812 - 1705845899.590531]: 0.9019758701324463
python ParamPruning/bert.encoder.layer.8.output.dense.weight [812 - 1705845899.5923]: 0.9253442287445068
python ParamPruning/bert.encoder.layer.9.attention.self.query.weight [812 - 1705845899.5932038]: 0.8518574833869934
python ParamPruning/bert.encoder.layer.9.attention.self.key.weight [812 - 1705845899.593922]: 0.8523983359336853
python ParamPruning/bert.encoder.layer.9.attention.self.value.weight [812 - 1705845899.5945375]: 0.8783705234527588
python ParamPruning/bert.encoder.layer.9.attention.output.dense.weight [812 - 1705845899.5950968]: 0.8867306113243103
python ParamPruning/bert.encoder.layer.9.intermediate.dense.weight [812 - 1705845899.596871]: 0.9011484980583191
python ParamPruning/bert.encoder.layer.9.output.dense.weight [812 - 1705845899.5985398]: 0.91860032081604
python ParamPruning/bert.encoder.layer.10.attention.self.query.weight [812 - 1705845899.5993485]: 0.8570064902305603
python ParamPruning/bert.encoder.layer.10.attention.self.key.weight [812 - 1705845899.6000946]: 0.8588087558746338
python ParamPruning/bert.encoder.layer.10.attention.self.value.weight [812 - 1705845899.6007173]: 0.8733994960784912
python ParamPruning/bert.encoder.layer.10.attention.output.dense.weight [812 - 1705845899.601294]: 0.8768836259841919
python ParamPruning/bert.encoder.layer.10.intermediate.dense.weight [812 - 1705845899.6030915]: 0.9072990417480469
python ParamPruning/bert.encoder.layer.10.output.dense.weight [812 - 1705845899.6048782]: 0.9249801635742188
python ParamPruning/bert.encoder.layer.11.attention.self.query.weight [812 - 1705845899.6058266]: 0.8541886806488037
python ParamPruning/bert.encoder.layer.11.attention.self.key.weight [812 - 1705845899.6066043]: 0.8596123456954956
python ParamPruning/bert.encoder.layer.11.attention.self.value.weight [812 - 1705845899.607246]: 0.8792198896408081
python ParamPruning/bert.encoder.layer.11.attention.output.dense.weight [812 - 1705845899.6078303]: 0.8855014443397522
python ParamPruning/bert.encoder.layer.11.intermediate.dense.weight [812 - 1705845899.6096573]: 0.8986706137657166
python ParamPruning/bert.encoder.layer.11.output.dense.weight [812 - 1705845899.6114175]: 0.9309417009353638
python ParamPruning/bert.pooler.dense.weight [812 - 1705845899.6123211]: 0.0
python ParamPruning/classifier.weight [812 - 1705845899.612889]: 0.0
python DistillationModifier [819 - 1705845949.200002]: Calling loss_update with:
args: 0.7124918103218079| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705845950.6536107]: 
Returned: 1.3712681531906128| 

python DistillationModifier [819 - 1705845953.1628928]: Calling loss_update with:
args: 1.2905980348587036| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705845954.6244135]: 
Returned: 2.0820209980010986| 

python DistillationModifier [819 - 1705845957.134229]: Calling loss_update with:
args: 1.2177355289459229| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705845958.5754375]: 
Returned: 1.2551960945129395| 

python DistillationModifier [819 - 1705845961.0733054]: Calling loss_update with:
args: 0.5044678449630737| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705845963.0451312]: 
Returned: 0.7663607597351074| 

python DistillationModifier [819 - 1705845966.157797]: Calling loss_update with:
args: 0.7963809967041016| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705845967.5968983]: 
Returned: 1.3258873224258423| 

python DistillationModifier [819 - 1705845970.1048806]: Calling loss_update with:
args: 0.7401059865951538| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705845971.5425363]: 
Returned: 1.4106148481369019| 

python DistillationModifier [819 - 1705845974.0789456]: Calling loss_update with:
args: 0.4471615254878998| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705845975.5213504]: 
Returned: 1.0298051834106445| 

python DistillationModifier [819 - 1705845978.0242538]: Calling loss_update with:
args: 1.224131464958191| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705845979.4610445]: 
Returned: 1.380776286125183| 

python DistillationModifier [819 - 1705845981.9706917]: Calling loss_update with:
args: 0.6788507699966431| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705845983.4029362]: 
Returned: 0.45136702060699463| 

python DistillationModifier [819 - 1705845985.9221723]: Calling loss_update with:
args: 0.8341873288154602| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705845987.934606]: 
Returned: 1.3310002088546753| 

python DistillationModifier [819 - 1705845990.4362223]: Calling loss_update with:
args: 0.6107773780822754| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705845991.8771496]: 
Returned: 1.0900766849517822| 

python DistillationModifier [819 - 1705845994.3852808]: Calling loss_update with:
args: 0.9856857061386108| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705845995.8578765]: 
Returned: 1.3884494304656982| 

python DistillationModifier [819 - 1705845998.3580902]: Calling loss_update with:
args: 0.7880709171295166| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705845999.7853467]: 
Returned: 1.5060811042785645| 

python DistillationModifier [819 - 1705846002.2903912]: Calling loss_update with:
args: 0.6257631778717041| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846003.7274745]: 
Returned: 0.5171569585800171| 

python DistillationModifier [819 - 1705846006.2350898]: Calling loss_update with:
args: 1.254634141921997| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846007.6680603]: 
Returned: 2.036529541015625| 

python DistillationModifier [819 - 1705846010.1755207]: Calling loss_update with:
args: 0.9462183713912964| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846011.6319072]: 
Returned: 1.612190842628479| 

python DistillationModifier [819 - 1705846014.1554515]: Calling loss_update with:
args: 0.6957600116729736| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846015.5866597]: 
Returned: 1.0026872158050537| 

python DistillationModifier [819 - 1705846018.0940635]: Calling loss_update with:
args: 0.9811602234840393| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846019.529114]: 
Returned: 1.7617440223693848| 

python DistillationModifier [819 - 1705846022.463349]: Calling loss_update with:
args: 1.2011091709136963| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846024.5112095]: 
Returned: 1.3883469104766846| 

python DistillationModifier [819 - 1705846027.1977277]: Calling loss_update with:
args: 0.6287158727645874| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846028.6585896]: 
Returned: 1.155879259109497| 

python DistillationModifier [819 - 1705846031.1688745]: Calling loss_update with:
args: 0.8865614533424377| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846032.612068]: 
Returned: 1.6662178039550781| 

python DistillationModifier [819 - 1705846035.1258929]: Calling loss_update with:
args: 0.5571365356445312| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846036.5666547]: 
Returned: 0.8401514291763306| 

python DistillationModifier [819 - 1705846039.0832808]: Calling loss_update with:
args: 0.7547932863235474| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846040.5253768]: 
Returned: 1.0335263013839722| 

python DistillationModifier [819 - 1705846043.0432434]: Calling loss_update with:
args: 0.7953892946243286| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846044.490195]: 
Returned: 1.2659534215927124| 

python DistillationModifier [819 - 1705846047.0129092]: Calling loss_update with:
args: 1.129113793373108| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846048.4590173]: 
Returned: 1.2759989500045776| 

python DistillationModifier [819 - 1705846051.5183117]: Calling loss_update with:
args: 0.3781200349330902| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846052.9640272]: 
Returned: 0.6838683485984802| 

python DistillationModifier [819 - 1705846055.478126]: Calling loss_update with:
args: 1.0207699537277222| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846056.9693952]: 
Returned: 1.4540519714355469| 

python DistillationModifier [819 - 1705846059.4838338]: Calling loss_update with:
args: 0.8091841340065002| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846060.9332647]: 
Returned: 1.2522441148757935| 

python DistillationModifier [819 - 1705846063.447435]: Calling loss_update with:
args: 0.5386637449264526| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846064.899673]: 
Returned: 0.9057222008705139| 

python DistillationModifier [819 - 1705846067.4151511]: Calling loss_update with:
args: 0.6605322957038879| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846068.8585026]: 
Returned: 1.2023255825042725| 

python DistillationModifier [819 - 1705846071.3750196]: Calling loss_update with:
args: 1.1706678867340088| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846072.825808]: 
Returned: 1.9554853439331055| 

python DistillationModifier [819 - 1705846075.3562253]: Calling loss_update with:
args: 1.019182562828064| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846076.805301]: 
Returned: 1.2090269327163696| 

python DistillationModifier [819 - 1705846079.3039734]: Calling loss_update with:
args: 0.9455083608627319| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846080.706427]: 
Returned: 1.5541348457336426| 

python DistillationModifier [819 - 1705846084.1471891]: Calling loss_update with:
args: 1.081775188446045| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846085.5891638]: 
Returned: 1.7734850645065308| 

python DistillationModifier [819 - 1705846088.130806]: Calling loss_update with:
args: 1.3760656118392944| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846089.5593095]: 
Returned: 1.7424336671829224| 

python DistillationModifier [819 - 1705846092.0556056]: Calling loss_update with:
args: 1.1104142665863037| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846093.4936678]: 
Returned: 2.1234230995178223| 

python DistillationModifier [819 - 1705846095.9977272]: Calling loss_update with:
args: 0.9312979578971863| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846097.4401977]: 
Returned: 1.0820999145507812| 

python DistillationModifier [819 - 1705846099.9478486]: Calling loss_update with:
args: 0.3953728973865509| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846101.4002395]: 
Returned: 0.9305902123451233| 

python DistillationModifier [819 - 1705846103.9156775]: Calling loss_update with:
args: 0.909160852432251| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846105.3799524]: 
Returned: 1.591556191444397| 

python DistillationModifier [819 - 1705846107.8952045]: Calling loss_update with:
args: 1.3902010917663574| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846109.3347325]: 
Returned: 2.447610855102539| 

python DistillationModifier [819 - 1705846111.8534272]: Calling loss_update with:
args: 0.39902210235595703| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846113.3706715]: 
Returned: 1.1145209074020386| 

python DistillationModifier [819 - 1705846116.3246102]: Calling loss_update with:
args: 0.9025577902793884| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846117.8180037]: 
Returned: 1.7919285297393799| 

python DistillationModifier [819 - 1705846120.3233135]: Calling loss_update with:
args: 0.769846498966217| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846121.7693467]: 
Returned: 0.7826249599456787| 

python DistillationModifier [819 - 1705846124.2753844]: Calling loss_update with:
args: 0.8174515962600708| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846125.7226415]: 
Returned: 1.680487036705017| 

python DistillationModifier [819 - 1705846128.2408829]: Calling loss_update with:
args: 0.9856151938438416| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846129.6868343]: 
Returned: 2.133929491043091| 

python DistillationModifier [819 - 1705846132.2035983]: Calling loss_update with:
args: 1.2905489206314087| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846133.6727154]: 
Returned: 1.933701515197754| 

python DistillationModifier [819 - 1705846136.1918275]: Calling loss_update with:
args: 0.6950937509536743| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846137.6426983]: 
Returned: 1.3767714500427246| 

python DistillationModifier [819 - 1705846140.1532946]: Calling loss_update with:
args: 0.2593838572502136| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846141.6000686]: 
Returned: 0.570967972278595| 

python DistillationModifier [819 - 1705846145.2678733]: Calling loss_update with:
args: 1.24784517288208| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846146.7236934]: 
Returned: 2.320465564727783| 

python DistillationModifier [819 - 1705846149.2868621]: Calling loss_update with:
args: 1.053046703338623| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846150.730207]: 
Returned: 1.9858741760253906| 

python DistillationModifier [819 - 1705846153.2507753]: Calling loss_update with:
args: 0.364531546831131| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846154.7002635]: 
Returned: 0.837617039680481| 

python DistillationModifier [819 - 1705846157.2278624]: Calling loss_update with:
args: 1.873270034790039| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846158.6774867]: 
Returned: 1.804514765739441| 

python DistillationModifier [819 - 1705846161.1935937]: Calling loss_update with:
args: 0.8997842073440552| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846162.6456542]: 
Returned: 1.1570813655853271| 

python DistillationModifier [819 - 1705846165.1702814]: Calling loss_update with:
args: 0.6443566679954529| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846166.6139379]: 
Returned: 0.9498952031135559| 

python DistillationModifier [819 - 1705846169.1283758]: Calling loss_update with:
args: 0.6649063229560852| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846170.5637422]: 
Returned: 1.5194573402404785| 

python DistillationModifier [819 - 1705846173.0642133]: Calling loss_update with:
args: 0.7374601364135742| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846174.5177011]: 
Returned: 1.0769731998443604| 

python DistillationModifier [819 - 1705846177.0190275]: Calling loss_update with:
args: 0.8050124645233154| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846178.7458982]: 
Returned: 0.8938874006271362| 

python DistillationModifier [819 - 1705846181.502667]: Calling loss_update with:
args: 1.0671831369400024| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846182.9427292]: 
Returned: 1.412339687347412| 

python DistillationModifier [819 - 1705846185.4567578]: Calling loss_update with:
args: 1.018036127090454| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846186.9025257]: 
Returned: 1.626051425933838| 

python DistillationModifier [819 - 1705846189.4158783]: Calling loss_update with:
args: 0.8689170479774475| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846190.8559926]: 
Returned: 1.6051764488220215| 

python DistillationModifier [819 - 1705846193.3621361]: Calling loss_update with:
args: 0.7626402974128723| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846194.8140862]: 
Returned: 1.292956829071045| 

python DistillationModifier [819 - 1705846197.3130515]: Calling loss_update with:
args: 0.7923025488853455| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846198.752702]: 
Returned: 1.5692944526672363| 

python DistillationModifier [819 - 1705846200.1430926]: Calling loss_update with:
args: 0.6579767465591431| <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>| <class 'accelerate.optimizer.AcceleratedOptimizer'>| 
kwargs: epoch: 13.0| steps_per_epoch: 63| student_outputs: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>| student_inputs: <class 'dict'>| teacher_inputs: <class 'dict'>| 
python DistillationModifier [819 - 1705846200.8880746]: 
Returned: 1.2195768356323242| 

