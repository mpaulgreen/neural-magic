{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a6333c-3705-4ac1-8367-b51e97e13c6e",
   "metadata": {},
   "source": [
    "# Run this notebook on CUDA and with GPUs as it involves Finetuning and Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52db4b91-6174-4294-b0c3-753e662101a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note: CUDA® is a parallel computing platform and programming model developed by NVIDIA for general computing on graphical processing units (GPUs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f81505-54ea-4e19-99b1-beebb8b9241d",
   "metadata": {},
   "source": [
    "## Sparse Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e148d58b-1f6f-44e1-a0c4-00d2b15d4074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install sparseml\n",
    "# !pip install sparseml[transformers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d6e52-a39d-4d1c-9e79-f48ef2890699",
   "metadata": {
    "tags": []
   },
   "source": [
    "### create a teacher for the desired text classification dataset, we will fine-tune a dense BERT model from the SparseZoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c05d73-810b-4a72-a6bb-257aaa6fc69b",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69242ee0-e466-428c-9bea-18e1afd1c6a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# models are saved locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87d257ea-2111-4fd3-b9e5-1d8348e1f2f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading (…)ng/pytorch_model.bin: 100%|███| 418M/418M [00:06<00:00, 69.1MB/s]\n",
      "Downloading (…)g/train_results.json: 100%|██████| 462/462 [00:00<00:00, 206kB/s]\n",
      "Downloading (…)training/config.json: 100%|██████| 605/605 [00:00<00:00, 285kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████| 285/285 [00:00<00:00, 130kB/s]\n",
      "Downloading (…)ining/tokenizer.json: 100%|███| 455k/455k [00:00<00:00, 12.1MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|█████| 112/112 [00:00<00:00, 49.2kB/s]\n",
      "Downloading (…)g/trainer_state.json: 100%|█| 91.9k/91.9k [00:00<00:00, 6.11MB/s]\n",
      "Downloading (…)ng/eval_results.json: 100%|██████| 355/355 [00:00<00:00, 145kB/s]\n",
      "Downloading (…)e/training/vocab.txt: 100%|███| 226k/226k [00:00<00:00, 6.60MB/s]\n",
      "Downloading (…)ng/training_args.bin: 100%|██| 2.30k/2.30k [00:00<00:00, 760kB/s]\n",
      "Downloading (…)ing/all_results.json: 100%|██████| 796/796 [00:00<00:00, 334kB/s]\n",
      "2024-01-17 18:28:50 sparseml.transformers.text_classification WARNING  Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "Downloading readme: 100%|██████████████████| 31.9k/31.9k [00:00<00:00, 34.5MB/s]\n",
      "Downloading data files:   0%|                             | 0/3 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                             | 0.00/3.11M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████| 3.11M/3.11M [00:00<00:00, 13.2MB/s]\u001b[A\n",
      "Downloading data files:  33%|███████              | 1/3 [00:00<00:00,  4.21it/s]\n",
      "Downloading data: 100%|████████████████████| 72.8k/72.8k [00:00<00:00, 1.10MB/s]\u001b[A\n",
      "\n",
      "Downloading data: 100%|██████████████████████| 148k/148k [00:00<00:00, 2.15MB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████| 3/3 [00:00<00:00,  7.95it/s]\n",
      "Extracting data files: 100%|████████████████████| 3/3 [00:00<00:00, 1902.75it/s]\n",
      "Generating train split: 100%|█| 67349/67349 [00:00<00:00, 1148633.06 examples/s]\n",
      "Generating validation split: 100%|█| 872/872 [00:00<00:00, 402313.62 examples/s]\n",
      "Generating test split: 100%|█████| 1821/1821 [00:00<00:00, 723484.66 examples/s]\n",
      "[WARNING|modeling_utils.py:3777] 2024-01-17 18:28:54,309 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/jupyter/.cache/sparsezoo/neuralmagic/bert-base-wikipedia_bookcorpus-base/training and are newly initialized: ['classifier.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-01-17 18:28:54 sparseml.transformers.utils.model INFO     Loaded model from /home/jupyter/.cache/sparsezoo/neuralmagic/bert-base-wikipedia_bookcorpus-base/training with 109483778 total params. Of those there are 85526016 prunable params which have 0.0 avg sparsity.\n",
      "2024-01-17 18:28:54 sparseml.transformers.utils.model INFO     dense model detected, all sparsification info: {\"params_summary\": {\"total\": 109483778, \"sparse\": 770, \"sparsity_percent\": 0.0007033005382770039, \"prunable\": 85526016, \"prunable_sparse\": 0, \"prunable_sparsity_percent\": 0.0, \"quantizable\": 85609730, \"quantized\": 0, \"quantized_percent\": 0.0}, \"params_info\": {\"bert.encoder.layer.0.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.pooler.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"classifier.weight\": {\"numel\": 1536, \"sparsity\": 0.0, \"quantized\": false}}}\n",
      "Running tokenizer on dataset: 100%|█| 67349/67349 [00:06<00:00, 10980.40 example\n",
      "Running tokenizer on dataset: 100%|█| 872/872 [00:00<00:00, 10960.97 examples/s]\n",
      "Running tokenizer on dataset: 100%|█| 1821/1821 [00:00<00:00, 11053.87 examples/\n",
      "/opt/conda/lib/python3.10/site-packages/sparseml/transformers/text_classification.py:464: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", data_args.task_name)\n",
      "Downloading builder script: 5.76kB [00:00, 14.1MB/s]                            \n",
      "2024-01-17 18:29:01 sparseml.core.logger INFO     Logging all SparseML modifier-level logs to sparse_logs/17-01-2024_18.29.01.log\n",
      "No default recipe recipe_transfer-text_classification found, falling back to first listed recipe recipe.md\n",
      "Downloading (…)orpus-base/recipe.md: 100%|█| 2.70k/2.70k [00:00<00:00, 1.01MB/s]\n",
      "2024-01-17 18:29:02 sparseml.transformers.sparsification.trainer INFO     Loaded SparseML recipe variable into manager for recipe: zoo:nlp/masked_language_modeling/bert-base/pytorch/huggingface/wikipedia_bookcorpus/base-none?recipe_type=transfer-text_classification, recipe_variables: {\"init_lr\":0.00003} and metadata {'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'fp16': True}\n",
      "2024-01-17 18:29:02 sparseml.transformers.sparsification.trainer WARNING  Overriding num_train_epochs from Recipe to 10\n",
      "2024-01-17 18:29:03 sparseml.transformers.sparsification.trainer INFO     Applied structure from SparseML recipe argument to model at epoch 0.0\n",
      "2024-01-17 18:29:03 sparseml.transformers.sparsification.trainer INFO     Modified the optimizer from the recipe for training with total_batch_size: 32 and steps_per_epoch: 2105\n",
      "2024-01-17 18:29:03 sparseml.transformers.sparsification.trainer WARNING  Overrode the lr_scheduler from SparseML recipe\n",
      "{'loss': 0.3256, 'learning_rate': 5e-05, 'epoch': 0.24}                         \n",
      "{'loss': 0.2306, 'learning_rate': 5e-05, 'epoch': 0.48}                         \n",
      "{'loss': 0.2016, 'learning_rate': 5e-05, 'epoch': 0.71}                         \n",
      "{'loss': 0.1875, 'learning_rate': 5e-05, 'epoch': 0.95}                         \n",
      " 10%|███▌                                | 2105/21050 [07:47<1:04:50,  4.87it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:01, 21.33it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:00<00:01, 16.75it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:00<00:01, 15.92it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:00<00:01, 15.46it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:00<00:01, 15.11it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:00<00:00, 14.93it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:01<00:00, 14.73it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:01<00:00, 14.72it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:01<00:00, 14.66it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:01<00:00, 14.55it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:01<00:00, 14.57it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.2322058379650116, 'eval_accuracy': 0.9277522935779816, 'eval_runtime': 1.9126, 'eval_samples_per_second': 455.928, 'eval_steps_per_second': 14.64, 'epoch': 1.0}\n",
      " 10%|███▌                                | 2105/21050 [07:49<1:04:50,  4.87it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:01<00:00, 14.54it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 18:36:54 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/teacher/checkpoint-2105/recipe.yaml\n",
      "{'loss': 0.1393, 'learning_rate': 5e-05, 'epoch': 1.19}                         \n",
      "{'loss': 0.1285, 'learning_rate': 5e-05, 'epoch': 1.43}                         \n",
      "{'loss': 0.1319, 'learning_rate': 5e-05, 'epoch': 1.66}                         \n",
      "{'loss': 0.1364, 'learning_rate': 5e-05, 'epoch': 1.9}                          \n",
      " 20%|███████▌                              | 4210/21050 [15:40<57:44,  4.86it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:01, 21.21it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:00<00:01, 16.59it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:00<00:01, 15.84it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:00<00:01, 15.41it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:00<00:01, 15.07it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:00<00:00, 14.91it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:01<00:00, 14.84it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:01<00:00, 14.82it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:01<00:00, 14.76it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:01<00:00, 14.66it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:01<00:00, 14.68it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.24315863847732544, 'eval_accuracy': 0.930045871559633, 'eval_runtime': 1.9024, 'eval_samples_per_second': 458.371, 'eval_steps_per_second': 14.718, 'epoch': 2.0}\n",
      " 20%|███████▌                              | 4210/21050 [15:42<57:44,  4.86it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:01<00:00, 14.64it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 18:44:46 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/teacher/checkpoint-4210/recipe.yaml\n",
      "{'loss': 0.1031, 'learning_rate': 5e-05, 'epoch': 2.14}                         \n",
      "{'loss': 0.0938, 'learning_rate': 5e-05, 'epoch': 2.38}                         \n",
      "{'loss': 0.0949, 'learning_rate': 5e-05, 'epoch': 2.61}                         \n",
      "{'loss': 0.1002, 'learning_rate': 5e-05, 'epoch': 2.85}                         \n",
      " 30%|███████████▍                          | 6315/21050 [23:32<50:31,  4.86it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:01, 21.76it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:00<00:01, 16.78it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:00<00:01, 15.87it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:00<00:01, 15.42it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:00<00:01, 15.10it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:00<00:00, 14.91it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:01<00:00, 14.77it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:01<00:00, 14.75it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:01<00:00, 14.71it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:01<00:00, 14.64it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:01<00:00, 14.61it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.2870825231075287, 'eval_accuracy': 0.9208715596330275, 'eval_runtime': 1.9017, 'eval_samples_per_second': 458.525, 'eval_steps_per_second': 14.723, 'epoch': 3.0}\n",
      " 30%|███████████▍                          | 6315/21050 [23:34<50:31,  4.86it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:01<00:00, 14.68it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 18:52:38 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/teacher/checkpoint-6315/recipe.yaml\n",
      "{'loss': 0.0921, 'learning_rate': 5e-05, 'epoch': 3.09}                         \n",
      "{'loss': 0.0683, 'learning_rate': 5e-05, 'epoch': 3.33}                         \n",
      "{'loss': 0.071, 'learning_rate': 5e-05, 'epoch': 3.56}                          \n",
      "{'loss': 0.0787, 'learning_rate': 5e-05, 'epoch': 3.8}                          \n",
      " 40%|███████████████▏                      | 8420/21050 [31:24<43:11,  4.87it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:01, 21.56it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:00<00:01, 16.95it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:00<00:01, 16.04it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:00<00:01, 15.49it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:00<00:01, 15.16it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:00<00:00, 14.93it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:01<00:00, 14.83it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:01<00:00, 14.78it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:01<00:00, 14.73it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:01<00:00, 14.77it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:01<00:00, 14.70it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.3631801903247833, 'eval_accuracy': 0.9128440366972477, 'eval_runtime': 1.8968, 'eval_samples_per_second': 459.721, 'eval_steps_per_second': 14.762, 'epoch': 4.0}\n",
      " 40%|███████████████▏                      | 8420/21050 [31:26<43:11,  4.87it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:01<00:00, 14.65it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 19:00:30 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/teacher/checkpoint-8420/recipe.yaml\n",
      "{'loss': 0.069, 'learning_rate': 5e-05, 'epoch': 4.04}                          \n",
      "{'loss': 0.0504, 'learning_rate': 5e-05, 'epoch': 4.28}                         \n",
      "{'loss': 0.0559, 'learning_rate': 5e-05, 'epoch': 4.51}                         \n",
      "{'loss': 0.0595, 'learning_rate': 5e-05, 'epoch': 4.75}                         \n",
      "{'loss': 0.0577, 'learning_rate': 5e-05, 'epoch': 4.99}                         \n",
      " 50%|██████████████████▌                  | 10525/21050 [39:16<36:02,  4.87it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:01, 22.20it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:00<00:01, 17.02it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:00<00:01, 16.19it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:00<00:01, 15.53it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:00<00:01, 15.20it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:00<00:00, 15.01it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:01<00:00, 14.87it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:01<00:00, 14.71it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:01<00:00, 14.65it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:01<00:00, 14.66it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:01<00:00, 14.65it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.3492595851421356, 'eval_accuracy': 0.9048165137614679, 'eval_runtime': 1.8956, 'eval_samples_per_second': 460.002, 'eval_steps_per_second': 14.771, 'epoch': 5.0}\n",
      " 50%|██████████████████▌                  | 10525/21050 [39:17<36:02,  4.87it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:01<00:00, 14.59it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 19:08:22 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/teacher/checkpoint-10525/recipe.yaml\n",
      "{'loss': 0.042, 'learning_rate': 5e-05, 'epoch': 5.23}                          \n",
      "{'loss': 0.039, 'learning_rate': 5e-05, 'epoch': 5.46}                          \n",
      "{'loss': 0.0473, 'learning_rate': 5e-05, 'epoch': 5.7}                          \n",
      "{'loss': 0.0439, 'learning_rate': 5e-05, 'epoch': 5.94}                         \n",
      " 60%|██████████████████████▏              | 12630/21050 [47:08<28:48,  4.87it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:01, 21.61it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:00<00:01, 16.94it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:00<00:01, 15.99it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:00<00:01, 15.49it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:00<00:01, 15.24it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:00<00:00, 15.10it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:01<00:00, 14.99it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:01<00:00, 14.89it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:01<00:00, 14.85it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:01<00:00, 14.77it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:01<00:00, 14.69it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.3251519203186035, 'eval_accuracy': 0.9162844036697247, 'eval_runtime': 1.8955, 'eval_samples_per_second': 460.043, 'eval_steps_per_second': 14.772, 'epoch': 6.0}\n",
      " 60%|██████████████████████▏              | 12630/21050 [47:10<28:48,  4.87it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:01<00:00, 14.60it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 19:16:14 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/teacher/checkpoint-12630/recipe.yaml\n",
      "{'loss': 0.034, 'learning_rate': 5e-05, 'epoch': 6.18}                          \n",
      "{'loss': 0.0304, 'learning_rate': 5e-05, 'epoch': 6.41}                         \n",
      "{'loss': 0.0336, 'learning_rate': 5e-05, 'epoch': 6.65}                         \n",
      "{'loss': 0.037, 'learning_rate': 5e-05, 'epoch': 6.89}                          \n",
      " 70%|█████████████████████████▉           | 14735/21050 [54:59<21:42,  4.85it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:01, 22.04it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:00<00:01, 16.95it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:00<00:01, 16.04it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:00<00:01, 15.49it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:00<00:01, 15.15it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:00<00:00, 15.02it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:01<00:00, 14.85it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:01<00:00, 14.74it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:01<00:00, 14.65it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:01<00:00, 14.65it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:01<00:00, 14.64it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.39463821053504944, 'eval_accuracy': 0.911697247706422, 'eval_runtime': 1.8977, 'eval_samples_per_second': 459.507, 'eval_steps_per_second': 14.755, 'epoch': 7.0}\n",
      " 70%|█████████████████████████▉           | 14735/21050 [55:01<21:42,  4.85it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:01<00:00, 14.61it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 19:24:06 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/teacher/checkpoint-14735/recipe.yaml\n",
      "{'loss': 0.0274, 'learning_rate': 5e-05, 'epoch': 7.13}                         \n",
      "{'loss': 0.0246, 'learning_rate': 5e-05, 'epoch': 7.36}                         \n",
      "{'loss': 0.0248, 'learning_rate': 5e-05, 'epoch': 7.6}                          \n",
      "{'loss': 0.0286, 'learning_rate': 5e-05, 'epoch': 7.84}                         \n",
      " 80%|████████████████████████████       | 16840/21050 [1:02:51<14:20,  4.90it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:01, 21.32it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:00<00:01, 16.77it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:00<00:01, 16.01it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:00<00:01, 15.52it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:00<00:01, 15.14it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:00<00:00, 14.91it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:01<00:00, 14.82it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:01<00:00, 14.73it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:01<00:00, 14.69it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:01<00:00, 14.64it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:01<00:00, 14.67it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.43581604957580566, 'eval_accuracy': 0.9174311926605505, 'eval_runtime': 1.8957, 'eval_samples_per_second': 459.995, 'eval_steps_per_second': 14.77, 'epoch': 8.0}\n",
      " 80%|████████████████████████████       | 16840/21050 [1:02:53<14:20,  4.90it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:01<00:00, 14.72it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 19:31:58 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/teacher/checkpoint-16840/recipe.yaml\n",
      "{'loss': 0.0207, 'learning_rate': 5e-05, 'epoch': 8.08}                         \n",
      "{'loss': 0.0166, 'learning_rate': 5e-05, 'epoch': 8.31}                         \n",
      "{'loss': 0.0207, 'learning_rate': 5e-05, 'epoch': 8.55}                         \n",
      "{'loss': 0.0197, 'learning_rate': 5e-05, 'epoch': 8.79}                         \n",
      " 90%|███████████████████████████████▌   | 18945/21050 [1:10:43<07:12,  4.87it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:01, 21.79it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:00<00:01, 16.71it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:00<00:01, 15.96it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:00<00:01, 15.48it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:00<00:01, 15.16it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:00<00:00, 14.97it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:01<00:00, 14.91it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:01<00:00, 14.76it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:01<00:00, 14.68it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:01<00:00, 14.69it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:01<00:00, 14.61it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.48899605870246887, 'eval_accuracy': 0.911697247706422, 'eval_runtime': 1.9028, 'eval_samples_per_second': 458.261, 'eval_steps_per_second': 14.715, 'epoch': 9.0}\n",
      " 90%|███████████████████████████████▌   | 18945/21050 [1:10:45<07:12,  4.87it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:01<00:00, 14.56it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 19:39:49 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/teacher/checkpoint-18945/recipe.yaml\n",
      "{'loss': 0.0171, 'learning_rate': 5e-05, 'epoch': 9.03}                         \n",
      "{'loss': 0.0126, 'learning_rate': 5e-05, 'epoch': 9.26}                         \n",
      "{'loss': 0.0141, 'learning_rate': 5e-05, 'epoch': 9.5}                          \n",
      "{'loss': 0.0128, 'learning_rate': 5e-05, 'epoch': 9.74}                         \n",
      "{'loss': 0.0105, 'learning_rate': 5e-05, 'epoch': 9.98}                         \n",
      "100%|███████████████████████████████████| 21050/21050 [1:18:34<00:00,  4.87it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:01, 21.68it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:00<00:01, 16.98it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:00<00:01, 15.98it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:00<00:01, 15.56it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:00<00:01, 15.33it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:00<00:00, 15.10it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:01<00:00, 14.93it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:01<00:00, 14.88it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:01<00:00, 14.85it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:01<00:00, 14.77it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:01<00:00, 14.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.5401017665863037, 'eval_accuracy': 0.9139908256880734, 'eval_runtime': 1.8812, 'eval_samples_per_second': 463.525, 'eval_steps_per_second': 14.884, 'epoch': 10.0}\n",
      "100%|███████████████████████████████████| 21050/21050 [1:18:36<00:00,  4.87it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:01<00:00, 14.87it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 19:47:41 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/teacher/checkpoint-21050/recipe.yaml\n",
      "{'train_runtime': 4717.4739, 'train_samples_per_second': 142.765, 'train_steps_per_second': 4.462, 'train_loss': 0.07211658114209028, 'epoch': 10.0}\n",
      "100%|███████████████████████████████████| 21050/21050 [1:18:37<00:00,  4.46it/s]\n",
      "2024-01-17 19:47:41 sparseml.transformers.sparsification.trainer INFO     Finalized SparseML recipe argument applied to the model\n",
      "2024-01-17 19:47:41 sparseml.transformers.sparsification.trainer INFO     Sparsification info for /home/jupyter/.cache/sparsezoo/neuralmagic/bert-base-wikipedia_bookcorpus-base/training: 109483778 total params. Of those there are 85526016 prunable params which have 0.0 avg sparsity.\n",
      "2024-01-17 19:47:41 sparseml.transformers.sparsification.trainer INFO     dense model detected, all sparsification info: {\"params_summary\": {\"total\": 109483778, \"sparse\": 0, \"sparsity_percent\": 0.0, \"prunable\": 85526016, \"prunable_sparse\": 0, \"prunable_sparsity_percent\": 0.0, \"quantizable\": 85609730, \"quantized\": 0, \"quantized_percent\": 0.0}, \"params_info\": {\"bert.encoder.layer.0.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.pooler.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"classifier.weight\": {\"numel\": 1536, \"sparsity\": 0.0, \"quantized\": false}}}\n",
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  train_loss               =     0.0721\n",
      "  train_runtime            = 1:18:37.47\n",
      "  train_samples            =      67349\n",
      "  train_samples_per_second =    142.765\n",
      "  train_steps_per_second   =      4.462\n",
      "2024-01-17 19:47:42 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/teacher/recipe.yaml\n",
      "100%|███████████████████████████████████████████| 28/28 [00:01<00:00, 15.50it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_accuracy           =      0.914\n",
      "  eval_loss               =     0.5401\n",
      "  eval_runtime            = 0:00:01.87\n",
      "  eval_samples            =        872\n",
      "  eval_samples_per_second =    466.019\n",
      "  eval_steps_per_second   =     14.964\n"
     ]
    }
   ],
   "source": [
    "!sparseml.transformers.text_classification \\\n",
    "    --output_dir models/teacher \\\n",
    "    --model_name_or_path \"zoo:nlp/masked_language_modeling/bert-base/pytorch/huggingface/wikipedia_bookcorpus/base-none\" \\\n",
    "    --recipe \"zoo:nlp/masked_language_modeling/bert-base/pytorch/huggingface/wikipedia_bookcorpus/base-none?recipe_type=transfer-text_classification\" \\\n",
    "    --recipe_args '{\"init_lr\":0.00003}' \\\n",
    "    --task_name sst2 \\\n",
    "    --max_seq_length 128 \\\n",
    "    --per_device_train_batch_size 32 --per_device_eval_batch_size 32 \\\n",
    "    --do_train --do_eval --evaluation_strategy epoch --fp16  \\\n",
    "    --save_strategy epoch --save_total_limit 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb73a3-ffd8-43cf-bb8d-6d8e61a7169f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a5ed516-f520-40b7-9cec-66c38280cf2c",
   "metadata": {},
   "source": [
    "### With the teacher model trained, it is ready to be distilled into a sparsified student model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59332c8f-114e-4f1c-aaf6-2fe5a081f047",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading (…)ng/training_args.bin: 100%|█| 3.23k/3.23k [00:00<00:00, 1.57MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████| 421/421 [00:00<00:00, 190kB/s]\n",
      "Downloading (…)d/training/vocab.txt: 100%|███| 226k/226k [00:00<00:00, 4.52MB/s]\n",
      "Downloading (…)ining/tokenizer.json: 100%|███| 695k/695k [00:00<00:00, 10.4MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|█████| 112/112 [00:00<00:00, 51.9kB/s]\n",
      "Downloading (…)ng/pytorch_model.bin: 100%|███| 256M/256M [00:03<00:00, 70.0MB/s]\n",
      "Downloading (…)ng/eval_results.json: 100%|██████| 269/269 [00:00<00:00, 127kB/s]\n",
      "Downloading (…)training/recipe.yaml: 100%|██| 1.55k/1.55k [00:00<00:00, 596kB/s]\n",
      "Downloading (…)g/train_results.json: 100%|█████| 199/199 [00:00<00:00, 92.7kB/s]\n",
      "Downloading (…)training/config.json: 100%|██████| 615/615 [00:00<00:00, 296kB/s]\n",
      "Downloading (…)lidation-metric.yaml: 100%|█████| 177/177 [00:00<00:00, 80.7kB/s]\n",
      "Downloading (…)ing/all_results.json: 100%|██████| 448/448 [00:00<00:00, 193kB/s]\n",
      "Downloading (…)g/trainer_state.json: 100%|█| 12.7k/12.7k [00:00<00:00, 5.22MB/s]\n",
      "2024-01-17 19:49:22 sparseml.transformers.text_classification WARNING  Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "[WARNING|modeling_utils.py:3777] 2024-01-17 19:49:25,452 >> Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /home/jupyter/.cache/sparsezoo/neuralmagic/distilbert-wikipedia_bookcorpus-pruned80.4block_quantized/training and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-01-17 19:49:25 sparseml.transformers.utils.model INFO     Loaded student from /home/jupyter/.cache/sparsezoo/neuralmagic/distilbert-wikipedia_bookcorpus-pruned80.4block_quantized/training with 66955010 total params. Of those there are 43058688 prunable params which have 78.90131719758855 avg sparsity.\n",
      "2024-01-17 19:49:25 sparseml.transformers.utils.model INFO     sparse model detected, all sparsification info: {\"params_summary\": {\"total\": 66955010, \"sparse\": 33974642, \"sparsity_percent\": 50.742494101636304, \"prunable\": 43058688, \"prunable_sparse\": 33973872, \"prunable_sparsity_percent\": 78.90131719758855, \"quantizable\": 43100930, \"quantized\": 0, \"quantized_percent\": 0.0}, \"params_info\": {\"distilbert.transformer.layer.0.attention.q_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.0.attention.k_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.0.attention.v_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.0.attention.out_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.0.ffn.lin1.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": false}, \"distilbert.transformer.layer.0.ffn.lin2.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": false}, \"distilbert.transformer.layer.1.attention.q_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.1.attention.k_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.1.attention.v_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.1.attention.out_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.1.ffn.lin1.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": false}, \"distilbert.transformer.layer.1.ffn.lin2.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": false}, \"distilbert.transformer.layer.2.attention.q_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.2.attention.k_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.2.attention.v_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.2.attention.out_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.2.ffn.lin1.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": false}, \"distilbert.transformer.layer.2.ffn.lin2.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": false}, \"distilbert.transformer.layer.3.attention.q_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.3.attention.k_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.3.attention.v_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.3.attention.out_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.3.ffn.lin1.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": false}, \"distilbert.transformer.layer.3.ffn.lin2.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": false}, \"distilbert.transformer.layer.4.attention.q_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.4.attention.k_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.4.attention.v_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.4.attention.out_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.4.ffn.lin1.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": false}, \"distilbert.transformer.layer.4.ffn.lin2.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": false}, \"distilbert.transformer.layer.5.attention.q_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.5.attention.k_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.5.attention.v_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.5.attention.out_lin.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": false}, \"distilbert.transformer.layer.5.ffn.lin1.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": false}, \"distilbert.transformer.layer.5.ffn.lin2.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": false}, \"pre_classifier.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"classifier.weight\": {\"numel\": 1536, \"sparsity\": 0.0, \"quantized\": false}}}\n",
      "2024-01-17 19:49:27 sparseml.transformers.utils.model INFO     Loaded teacher from models/teacher with 109483778 total params. Of those there are 85526016 prunable params which have 0.0 avg sparsity.\n",
      "2024-01-17 19:49:27 sparseml.transformers.utils.model INFO     dense model detected, all sparsification info: {\"params_summary\": {\"total\": 109483778, \"sparse\": 0, \"sparsity_percent\": 0.0, \"prunable\": 85526016, \"prunable_sparse\": 0, \"prunable_sparsity_percent\": 0.0, \"quantizable\": 85609730, \"quantized\": 0, \"quantized_percent\": 0.0}, \"params_info\": {\"bert.encoder.layer.0.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.0.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.1.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.2.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.3.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.4.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.5.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.6.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.7.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.8.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.9.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.10.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.query.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.key.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.self.value.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.attention.output.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.intermediate.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.encoder.layer.11.output.dense.weight\": {\"numel\": 2359296, \"sparsity\": 0.0, \"quantized\": false}, \"bert.pooler.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"classifier.weight\": {\"numel\": 1536, \"sparsity\": 0.0, \"quantized\": false}}}\n",
      "Running tokenizer on dataset: 100%|█| 67349/67349 [00:05<00:00, 11305.33 example\n",
      "Running tokenizer on dataset: 100%|█| 872/872 [00:00<00:00, 11498.47 examples/s]\n",
      "Running tokenizer on dataset: 100%|█| 1821/1821 [00:00<00:00, 11937.37 examples/\n",
      "/opt/conda/lib/python3.10/site-packages/sparseml/transformers/text_classification.py:464: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", data_args.task_name)\n",
      "2024-01-17 19:49:33 sparseml.core.logger INFO     Logging all SparseML modifier-level logs to sparse_logs/17-01-2024_19.49.33.log\n",
      "Downloading (…)_quantized/recipe.md: 100%|█| 4.61k/4.61k [00:00<00:00, 2.07MB/s]\n",
      "2024-01-17 19:49:34 sparseml.transformers.sparsification.trainer INFO     Loaded SparseML recipe variable into manager for recipe: zoo:nlp/sentiment_analysis/distilbert-none/pytorch/huggingface/sst2/pruned80_quant-none-vnni, recipe_variables: None and metadata {'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32, 'fp16': True}\n",
      "2024-01-17 19:49:34 sparseml.transformers.sparsification.trainer INFO     Loaded 1 SparseML checkpoint recipe stage(s) from /home/jupyter/.cache/sparsezoo/neuralmagic/distilbert-wikipedia_bookcorpus-pruned80.4block_quantized/training/recipe.yaml to replicate model sparse state\n",
      "2024-01-17 19:49:34 sparseml.transformers.sparsification.trainer WARNING  Overriding num_train_epochs from Recipe to 13\n",
      "2024-01-17 19:49:35 sparseml.transformers.sparsification.trainer INFO     Applied structure from 1 previous recipe stage(s) to model and finalized (recipes saved with model_path)\n",
      "2024-01-17 19:49:35 sparseml.transformers.sparsification.trainer INFO     Applied structure from SparseML recipe argument to model at epoch 0.0\n",
      "2024-01-17 19:49:36 sparseml.pytorch.sparsification.distillation.modifier_distillation_base INFO     distillation modifier using distillation_teacher object\n",
      "2024-01-17 19:49:36 sparseml.transformers.sparsification.trainer INFO     Modified the optimizer from the recipe for training with total_batch_size: 32 and steps_per_epoch: 2105\n",
      "2024-01-17 19:49:36 sparseml.transformers.sparsification.trainer WARNING  Overrode the lr_scheduler from SparseML recipe\n",
      "  0%|                                                 | 0/27365 [00:00<?, ?it/s]2024-01-17 19:49:37 sparseml.pytorch.sparsification.distillation.modifier_distillation_base INFO     Teacher device cpu does not match inputs device cuda:0, moving teacher to correct device\n",
      "{'loss': 1.4999, 'learning_rate': 5e-05, 'epoch': 0.24}                         \n",
      "{'loss': 0.9552, 'learning_rate': 5e-05, 'epoch': 0.48}                         \n",
      "{'loss': 0.7532, 'learning_rate': 5e-05, 'epoch': 0.71}                         \n",
      "{'loss': 0.65, 'learning_rate': 5e-05, 'epoch': 0.95}                           \n",
      "  8%|██▊                                 | 2105/27365 [12:27<2:18:14,  3.05it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███▏                                        | 2/28 [00:00<00:03,  8.21it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:04,  5.68it/s]\u001b[A\n",
      " 14%|██████▎                                     | 4/28 [00:00<00:04,  4.83it/s]\u001b[A\n",
      " 18%|███████▊                                    | 5/28 [00:00<00:05,  4.54it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:01<00:05,  4.38it/s]\u001b[A\n",
      " 25%|███████████                                 | 7/28 [00:01<00:04,  4.26it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:01<00:04,  4.14it/s]\u001b[A\n",
      " 32%|██████████████▏                             | 9/28 [00:01<00:04,  4.10it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:02<00:04,  4.09it/s]\u001b[A\n",
      " 39%|████████████████▉                          | 11/28 [00:02<00:04,  4.07it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:02<00:03,  4.04it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 13/28 [00:02<00:03,  4.02it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:03<00:03,  4.04it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 15/28 [00:03<00:03,  4.04it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:03<00:02,  4.03it/s]\u001b[A\n",
      " 61%|██████████████████████████                 | 17/28 [00:03<00:02,  4.01it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:04<00:02,  4.02it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 19/28 [00:04<00:02,  4.03it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:04<00:01,  4.02it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 21/28 [00:04<00:01,  4.01it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:05<00:01,  4.02it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 23/28 [00:05<00:01,  4.02it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:05<00:00,  4.01it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 25/28 [00:05<00:00,  4.01it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▉   | 26/28 [00:06<00:00,  4.00it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.6092345714569092, 'eval_accuracy': 0.8784403669724771, 'eval_runtime': 6.8216, 'eval_samples_per_second': 127.829, 'eval_steps_per_second': 4.105, 'epoch': 1.0}\n",
      "  8%|██▊                                 | 2105/27365 [12:34<2:18:14,  3.05it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:06<00:00,  4.02it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 20:02:10 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/sparsified/checkpoint-2105/recipe.yaml\n",
      "{'loss': 0.544, 'learning_rate': 5e-05, 'epoch': 1.19}                          \n",
      "{'loss': 0.477, 'learning_rate': 5e-05, 'epoch': 1.43}                          \n",
      "{'loss': 0.4621, 'learning_rate': 5e-05, 'epoch': 1.66}                         \n",
      "{'loss': 0.4531, 'learning_rate': 5e-05, 'epoch': 1.9}                          \n",
      " 15%|█████▌                              | 4210/27365 [25:02<2:06:43,  3.05it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███▏                                        | 2/28 [00:00<00:03,  8.15it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:04,  5.63it/s]\u001b[A\n",
      " 14%|██████▎                                     | 4/28 [00:00<00:04,  4.83it/s]\u001b[A\n",
      " 18%|███████▊                                    | 5/28 [00:00<00:05,  4.56it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:01<00:04,  4.40it/s]\u001b[A\n",
      " 25%|███████████                                 | 7/28 [00:01<00:04,  4.26it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:01<00:04,  4.15it/s]\u001b[A\n",
      " 32%|██████████████▏                             | 9/28 [00:01<00:04,  4.11it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:02<00:04,  4.10it/s]\u001b[A\n",
      " 39%|████████████████▉                          | 11/28 [00:02<00:04,  4.09it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:02<00:03,  4.04it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 13/28 [00:02<00:03,  4.04it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:03<00:03,  4.04it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 15/28 [00:03<00:03,  4.04it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:03<00:02,  4.01it/s]\u001b[A\n",
      " 61%|██████████████████████████                 | 17/28 [00:03<00:02,  4.02it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:04<00:02,  4.02it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 19/28 [00:04<00:02,  4.03it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:04<00:01,  4.01it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 21/28 [00:04<00:01,  4.00it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:05<00:01,  4.01it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 23/28 [00:05<00:01,  4.03it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:05<00:00,  4.02it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 25/28 [00:05<00:00,  4.01it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▉   | 26/28 [00:06<00:00,  4.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.7252407073974609, 'eval_accuracy': 0.8784403669724771, 'eval_runtime': 6.8158, 'eval_samples_per_second': 127.937, 'eval_steps_per_second': 4.108, 'epoch': 2.0}\n",
      " 15%|█████▌                              | 4210/27365 [25:09<2:06:43,  3.05it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:06<00:00,  4.03it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 20:14:45 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/sparsified/checkpoint-4210/recipe.yaml\n",
      "{'loss': 0.3771, 'learning_rate': 5e-05, 'epoch': 2.14}                         \n",
      "{'loss': 0.333, 'learning_rate': 5e-05, 'epoch': 2.38}                          \n",
      "{'loss': 0.329, 'learning_rate': 5e-05, 'epoch': 2.61}                          \n",
      "{'loss': 0.3354, 'learning_rate': 5e-05, 'epoch': 2.85}                         \n",
      " 23%|████████▎                           | 6315/27365 [37:36<1:55:33,  3.04it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███▏                                        | 2/28 [00:00<00:03,  8.09it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:04,  5.64it/s]\u001b[A\n",
      " 14%|██████▎                                     | 4/28 [00:00<00:04,  4.81it/s]\u001b[A\n",
      " 18%|███████▊                                    | 5/28 [00:01<00:05,  4.54it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:01<00:05,  4.37it/s]\u001b[A\n",
      " 25%|███████████                                 | 7/28 [00:01<00:04,  4.23it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:01<00:04,  4.13it/s]\u001b[A\n",
      " 32%|██████████████▏                             | 9/28 [00:02<00:04,  4.11it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:02<00:04,  4.10it/s]\u001b[A\n",
      " 39%|████████████████▉                          | 11/28 [00:02<00:04,  4.07it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:02<00:03,  4.03it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 13/28 [00:02<00:03,  4.03it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:03<00:03,  4.05it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 15/28 [00:03<00:03,  4.04it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:03<00:02,  4.02it/s]\u001b[A\n",
      " 61%|██████████████████████████                 | 17/28 [00:03<00:02,  4.02it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:04<00:02,  4.03it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 19/28 [00:04<00:02,  4.04it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:04<00:01,  4.02it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 21/28 [00:04<00:01,  4.03it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:05<00:01,  4.03it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 23/28 [00:05<00:01,  4.04it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:05<00:00,  4.02it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 25/28 [00:05<00:00,  4.02it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▉   | 26/28 [00:06<00:00,  4.03it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.6182897686958313, 'eval_accuracy': 0.9025229357798165, 'eval_runtime': 6.8143, 'eval_samples_per_second': 127.967, 'eval_steps_per_second': 4.109, 'epoch': 3.0}\n",
      " 23%|████████▎                           | 6315/27365 [37:43<1:55:33,  3.04it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:06<00:00,  4.04it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 20:27:20 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/sparsified/checkpoint-6315/recipe.yaml\n",
      "{'loss': 0.3083, 'learning_rate': 5e-05, 'epoch': 3.09}                         \n",
      "{'loss': 0.2553, 'learning_rate': 5e-05, 'epoch': 3.33}                         \n",
      "{'loss': 0.272, 'learning_rate': 5e-05, 'epoch': 3.56}                          \n",
      "{'loss': 0.2663, 'learning_rate': 5e-05, 'epoch': 3.8}                          \n",
      " 31%|███████████                         | 8420/27365 [50:11<1:43:27,  3.05it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███▏                                        | 2/28 [00:00<00:03,  8.13it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:04,  5.62it/s]\u001b[A\n",
      " 14%|██████▎                                     | 4/28 [00:00<00:04,  4.85it/s]\u001b[A\n",
      " 18%|███████▊                                    | 5/28 [00:00<00:05,  4.56it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:01<00:04,  4.40it/s]\u001b[A\n",
      " 25%|███████████                                 | 7/28 [00:01<00:04,  4.26it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:01<00:04,  4.16it/s]\u001b[A\n",
      " 32%|██████████████▏                             | 9/28 [00:01<00:04,  4.13it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:02<00:04,  4.13it/s]\u001b[A\n",
      " 39%|████████████████▉                          | 11/28 [00:02<00:04,  4.09it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:02<00:03,  4.06it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 13/28 [00:02<00:03,  4.05it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:03<00:03,  4.05it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 15/28 [00:03<00:03,  4.06it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:03<00:02,  4.04it/s]\u001b[A\n",
      " 61%|██████████████████████████                 | 17/28 [00:03<00:02,  4.02it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:04<00:02,  4.05it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 19/28 [00:04<00:02,  4.04it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:04<00:01,  4.04it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 21/28 [00:04<00:01,  4.02it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:05<00:01,  4.04it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 23/28 [00:05<00:01,  4.04it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:05<00:00,  4.03it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 25/28 [00:05<00:00,  4.00it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▉   | 26/28 [00:06<00:00,  4.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.6198960542678833, 'eval_accuracy': 0.8956422018348624, 'eval_runtime': 6.7966, 'eval_samples_per_second': 128.299, 'eval_steps_per_second': 4.12, 'epoch': 4.0}\n",
      " 31%|███████████                         | 8420/27365 [50:17<1:43:27,  3.05it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:06<00:00,  4.03it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 20:39:54 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/sparsified/checkpoint-8420/recipe.yaml\n",
      "{'loss': 0.2471, 'learning_rate': 5e-05, 'epoch': 4.04}                         \n",
      "{'loss': 0.2172, 'learning_rate': 5e-05, 'epoch': 4.28}                         \n",
      "{'loss': 0.2107, 'learning_rate': 5e-05, 'epoch': 4.51}                         \n",
      "{'loss': 0.2101, 'learning_rate': 5e-05, 'epoch': 4.75}                         \n",
      "{'loss': 0.2108, 'learning_rate': 5e-05, 'epoch': 4.99}                         \n",
      " 38%|████████████▋                    | 10525/27365 [1:02:45<1:32:09,  3.05it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███▏                                        | 2/28 [00:00<00:03,  8.22it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:04,  5.68it/s]\u001b[A\n",
      " 14%|██████▎                                     | 4/28 [00:00<00:04,  4.85it/s]\u001b[A\n",
      " 18%|███████▊                                    | 5/28 [00:00<00:05,  4.55it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:01<00:05,  4.37it/s]\u001b[A\n",
      " 25%|███████████                                 | 7/28 [00:01<00:04,  4.23it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:01<00:04,  4.13it/s]\u001b[A\n",
      " 32%|██████████████▏                             | 9/28 [00:01<00:04,  4.11it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:02<00:04,  4.10it/s]\u001b[A\n",
      " 39%|████████████████▉                          | 11/28 [00:02<00:04,  4.07it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:02<00:03,  4.05it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 13/28 [00:02<00:03,  4.04it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:03<00:03,  4.04it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 15/28 [00:03<00:03,  4.04it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:03<00:02,  4.03it/s]\u001b[A\n",
      " 61%|██████████████████████████                 | 17/28 [00:03<00:02,  4.03it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:04<00:02,  4.03it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 19/28 [00:04<00:02,  4.03it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:04<00:01,  4.02it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 21/28 [00:04<00:01,  4.01it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:05<00:01,  4.03it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 23/28 [00:05<00:01,  4.02it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:05<00:00,  4.02it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 25/28 [00:05<00:00,  4.02it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▉   | 26/28 [00:06<00:00,  4.03it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.6871667504310608, 'eval_accuracy': 0.8944954128440367, 'eval_runtime': 6.8123, 'eval_samples_per_second': 128.004, 'eval_steps_per_second': 4.11, 'epoch': 5.0}\n",
      " 38%|████████████▋                    | 10525/27365 [1:02:52<1:32:09,  3.05it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:06<00:00,  4.04it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 20:52:28 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/sparsified/checkpoint-10525/recipe.yaml\n",
      "{'loss': 0.174, 'learning_rate': 5e-05, 'epoch': 5.23}                          \n",
      "{'loss': 0.1801, 'learning_rate': 5e-05, 'epoch': 5.46}                         \n",
      "{'loss': 0.1756, 'learning_rate': 5e-05, 'epoch': 5.7}                          \n",
      "{'loss': 0.1876, 'learning_rate': 5e-05, 'epoch': 5.94}                         \n",
      " 46%|███████████████▏                 | 12630/27365 [1:15:19<1:20:39,  3.04it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███▏                                        | 2/28 [00:00<00:03,  8.19it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:04,  5.70it/s]\u001b[A\n",
      " 14%|██████▎                                     | 4/28 [00:00<00:04,  4.83it/s]\u001b[A\n",
      " 18%|███████▊                                    | 5/28 [00:00<00:05,  4.54it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:01<00:05,  4.37it/s]\u001b[A\n",
      " 25%|███████████                                 | 7/28 [00:01<00:04,  4.24it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:01<00:04,  4.13it/s]\u001b[A\n",
      " 32%|██████████████▏                             | 9/28 [00:02<00:04,  4.10it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:02<00:04,  4.10it/s]\u001b[A\n",
      " 39%|████████████████▉                          | 11/28 [00:02<00:04,  4.08it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:02<00:03,  4.05it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 13/28 [00:02<00:03,  4.03it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:03<00:03,  4.04it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 15/28 [00:03<00:03,  4.04it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:03<00:02,  4.02it/s]\u001b[A\n",
      " 61%|██████████████████████████                 | 17/28 [00:03<00:02,  4.00it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:04<00:02,  4.01it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 19/28 [00:04<00:02,  4.02it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:04<00:01,  4.02it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 21/28 [00:04<00:01,  4.02it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:05<00:01,  4.02it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 23/28 [00:05<00:01,  4.02it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:05<00:00,  4.02it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 25/28 [00:05<00:00,  4.02it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▉   | 26/28 [00:06<00:00,  4.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.6708552241325378, 'eval_accuracy': 0.8990825688073395, 'eval_runtime': 6.82, 'eval_samples_per_second': 127.859, 'eval_steps_per_second': 4.106, 'epoch': 6.0}\n",
      " 46%|███████████████▏                 | 12630/27365 [1:15:26<1:20:39,  3.04it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:06<00:00,  4.03it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 21:05:02 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/sparsified/checkpoint-12630/recipe.yaml\n",
      "{'loss': 0.1589, 'learning_rate': 5e-05, 'epoch': 6.18}                         \n",
      "{'loss': 0.157, 'learning_rate': 5e-05, 'epoch': 6.41}                          \n",
      "{'loss': 0.1509, 'learning_rate': 5e-05, 'epoch': 6.65}                         \n",
      "{'loss': 0.155, 'learning_rate': 5e-05, 'epoch': 6.89}                          \n",
      " 54%|█████████████████▊               | 14735/27365 [1:27:53<1:09:00,  3.05it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███▏                                        | 2/28 [00:00<00:03,  8.26it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:04,  5.70it/s]\u001b[A\n",
      " 14%|██████▎                                     | 4/28 [00:00<00:04,  4.85it/s]\u001b[A\n",
      " 18%|███████▊                                    | 5/28 [00:00<00:05,  4.56it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:01<00:05,  4.40it/s]\u001b[A\n",
      " 25%|███████████                                 | 7/28 [00:01<00:04,  4.27it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:01<00:04,  4.15it/s]\u001b[A\n",
      " 32%|██████████████▏                             | 9/28 [00:01<00:04,  4.12it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:02<00:04,  4.11it/s]\u001b[A\n",
      " 39%|████████████████▉                          | 11/28 [00:02<00:04,  4.09it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:02<00:03,  4.04it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 13/28 [00:02<00:03,  4.05it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:03<00:03,  4.06it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 15/28 [00:03<00:03,  4.05it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:03<00:02,  4.02it/s]\u001b[A\n",
      " 61%|██████████████████████████                 | 17/28 [00:03<00:02,  4.03it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:04<00:02,  4.03it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 19/28 [00:04<00:02,  4.03it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:04<00:01,  4.02it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 21/28 [00:04<00:01,  4.04it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:05<00:01,  4.04it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 23/28 [00:05<00:01,  4.04it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:05<00:00,  4.03it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 25/28 [00:05<00:00,  4.04it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▉   | 26/28 [00:06<00:00,  4.05it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.9249287843704224, 'eval_accuracy': 0.8738532110091743, 'eval_runtime': 6.7916, 'eval_samples_per_second': 128.394, 'eval_steps_per_second': 4.123, 'epoch': 7.0}\n",
      " 54%|█████████████████▊               | 14735/27365 [1:28:00<1:09:00,  3.05it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:06<00:00,  4.06it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 21:17:37 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/sparsified/checkpoint-14735/recipe.yaml\n",
      "{'loss': 0.1445, 'learning_rate': 5e-05, 'epoch': 7.13}                         \n",
      "{'loss': 0.1298, 'learning_rate': 5e-05, 'epoch': 7.36}                         \n",
      "{'loss': 0.1282, 'learning_rate': 5e-05, 'epoch': 7.6}                          \n",
      "{'loss': 0.13, 'learning_rate': 5e-05, 'epoch': 7.84}                           \n",
      " 62%|█████████████████████▌             | 16840/27365 [1:40:27<57:26,  3.05it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███▏                                        | 2/28 [00:00<00:03,  8.21it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:04,  5.65it/s]\u001b[A\n",
      " 14%|██████▎                                     | 4/28 [00:00<00:04,  4.84it/s]\u001b[A\n",
      " 18%|███████▊                                    | 5/28 [00:00<00:05,  4.55it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:01<00:05,  4.38it/s]\u001b[A\n",
      " 25%|███████████                                 | 7/28 [00:01<00:04,  4.24it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:01<00:04,  4.15it/s]\u001b[A\n",
      " 32%|██████████████▏                             | 9/28 [00:01<00:04,  4.13it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:02<00:04,  4.10it/s]\u001b[A\n",
      " 39%|████████████████▉                          | 11/28 [00:02<00:04,  4.07it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:02<00:03,  4.04it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 13/28 [00:02<00:03,  4.05it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:03<00:03,  4.06it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 15/28 [00:03<00:03,  4.05it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:03<00:02,  4.04it/s]\u001b[A\n",
      " 61%|██████████████████████████                 | 17/28 [00:03<00:02,  4.03it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:04<00:02,  4.03it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 19/28 [00:04<00:02,  4.03it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:04<00:01,  4.02it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 21/28 [00:04<00:01,  4.00it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:05<00:01,  4.02it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 23/28 [00:05<00:01,  4.01it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:05<00:00,  4.01it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 25/28 [00:05<00:00,  4.02it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▉   | 26/28 [00:06<00:00,  4.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.7729621529579163, 'eval_accuracy': 0.893348623853211, 'eval_runtime': 6.8133, 'eval_samples_per_second': 127.985, 'eval_steps_per_second': 4.11, 'epoch': 8.0}\n",
      " 62%|█████████████████████▌             | 16840/27365 [1:40:34<57:26,  3.05it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:06<00:00,  4.02it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 21:30:11 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/sparsified/checkpoint-16840/recipe.yaml\n",
      "{'loss': 0.1371, 'learning_rate': 5e-05, 'epoch': 8.08}                         \n",
      "{'loss': 0.1695, 'learning_rate': 5e-05, 'epoch': 8.31}                         \n",
      "{'loss': 0.1597, 'learning_rate': 5e-05, 'epoch': 8.55}                         \n",
      "{'loss': 0.1695, 'learning_rate': 5e-05, 'epoch': 8.79}                         \n",
      " 69%|████████████████████████▏          | 18945/27365 [1:56:00<57:23,  2.45it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███▏                                        | 2/28 [00:00<00:04,  6.05it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:05,  4.36it/s]\u001b[A\n",
      " 14%|██████▎                                     | 4/28 [00:00<00:06,  3.78it/s]\u001b[A\n",
      " 18%|███████▊                                    | 5/28 [00:01<00:06,  3.52it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:01<00:06,  3.37it/s]\u001b[A\n",
      " 25%|███████████                                 | 7/28 [00:01<00:06,  3.27it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:02<00:06,  3.21it/s]\u001b[A\n",
      " 32%|██████████████▏                             | 9/28 [00:02<00:05,  3.17it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:02<00:05,  3.17it/s]\u001b[A\n",
      " 39%|████████████████▉                          | 11/28 [00:03<00:05,  3.16it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:03<00:05,  3.16it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 13/28 [00:03<00:04,  3.14it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:04<00:04,  3.14it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 15/28 [00:04<00:04,  3.13it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:04<00:03,  3.13it/s]\u001b[A\n",
      " 61%|██████████████████████████                 | 17/28 [00:05<00:03,  3.11it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:05<00:03,  3.11it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 19/28 [00:05<00:02,  3.10it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:06<00:02,  3.12it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 21/28 [00:06<00:02,  3.13it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:06<00:01,  3.14it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 23/28 [00:07<00:01,  3.13it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:07<00:01,  3.13it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 25/28 [00:07<00:00,  3.12it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▉   | 26/28 [00:08<00:00,  3.11it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▍ | 27/28 [00:08<00:00,  3.12it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.7243310809135437, 'eval_accuracy': 0.9036697247706422, 'eval_runtime': 8.838, 'eval_samples_per_second': 98.665, 'eval_steps_per_second': 3.168, 'epoch': 9.0}\n",
      " 69%|████████████████████████▏          | 18945/27365 [1:56:09<57:23,  2.45it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:08<00:00,  3.83it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 21:45:45 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/sparsified/checkpoint-18945/recipe.yaml\n",
      "{'loss': 0.1565, 'learning_rate': 5e-05, 'epoch': 9.03}                         \n",
      "{'loss': 0.1376, 'learning_rate': 5e-05, 'epoch': 9.26}                         \n",
      "{'loss': 0.1343, 'learning_rate': 5e-05, 'epoch': 9.5}                          \n",
      "{'loss': 0.128, 'learning_rate': 5e-05, 'epoch': 9.74}                          \n",
      "{'loss': 0.1541, 'learning_rate': 5e-05, 'epoch': 9.98}                         \n",
      " 77%|██████████████████████████▉        | 21050/27365 [2:11:33<43:03,  2.44it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███▏                                        | 2/28 [00:00<00:04,  5.99it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:05,  4.29it/s]\u001b[A\n",
      " 14%|██████▎                                     | 4/28 [00:00<00:06,  3.77it/s]\u001b[A\n",
      " 18%|███████▊                                    | 5/28 [00:01<00:06,  3.50it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:01<00:06,  3.37it/s]\u001b[A\n",
      " 25%|███████████                                 | 7/28 [00:01<00:06,  3.28it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:02<00:06,  3.22it/s]\u001b[A\n",
      " 32%|██████████████▏                             | 9/28 [00:02<00:05,  3.19it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:02<00:05,  3.19it/s]\u001b[A\n",
      " 39%|████████████████▉                          | 11/28 [00:03<00:05,  3.18it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:03<00:05,  3.18it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 13/28 [00:03<00:04,  3.17it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:04<00:04,  3.16it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 15/28 [00:04<00:04,  3.15it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:04<00:03,  3.14it/s]\u001b[A\n",
      " 61%|██████████████████████████                 | 17/28 [00:05<00:03,  3.14it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:05<00:03,  3.13it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 19/28 [00:05<00:02,  3.13it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:06<00:02,  3.14it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 21/28 [00:06<00:02,  3.15it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:06<00:01,  3.15it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 23/28 [00:07<00:01,  3.15it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:07<00:01,  3.16it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 25/28 [00:07<00:00,  3.15it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▉   | 26/28 [00:07<00:00,  3.14it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▍ | 27/28 [00:08<00:00,  3.15it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.7628218531608582, 'eval_accuracy': 0.8967889908256881, 'eval_runtime': 8.7883, 'eval_samples_per_second': 99.223, 'eval_steps_per_second': 3.186, 'epoch': 10.0}\n",
      " 77%|██████████████████████████▉        | 21050/27365 [2:11:42<43:03,  2.44it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:08<00:00,  3.87it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 22:01:19 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/sparsified/checkpoint-21050/recipe.yaml\n",
      "{'loss': 0.1132, 'learning_rate': 5e-05, 'epoch': 10.21}                        \n",
      "{'loss': 0.1178, 'learning_rate': 5e-05, 'epoch': 10.45}                        \n",
      "{'loss': 0.1108, 'learning_rate': 5e-05, 'epoch': 10.69}                        \n",
      "{'loss': 0.1223, 'learning_rate': 5e-05, 'epoch': 10.93}                        \n",
      " 85%|█████████████████████████████▌     | 23155/27365 [2:27:05<28:41,  2.45it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███▏                                        | 2/28 [00:00<00:04,  6.11it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:05,  4.36it/s]\u001b[A\n",
      " 14%|██████▎                                     | 4/28 [00:00<00:06,  3.81it/s]\u001b[A\n",
      " 18%|███████▊                                    | 5/28 [00:01<00:06,  3.55it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:01<00:06,  3.39it/s]\u001b[A\n",
      " 25%|███████████                                 | 7/28 [00:01<00:06,  3.30it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:02<00:06,  3.22it/s]\u001b[A\n",
      " 32%|██████████████▏                             | 9/28 [00:02<00:05,  3.18it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:02<00:05,  3.14it/s]\u001b[A\n",
      " 39%|████████████████▉                          | 11/28 [00:03<00:05,  3.12it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:03<00:05,  3.10it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 13/28 [00:03<00:04,  3.10it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:04<00:04,  3.12it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 15/28 [00:04<00:04,  3.12it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:04<00:03,  3.12it/s]\u001b[A\n",
      " 61%|██████████████████████████                 | 17/28 [00:05<00:03,  3.14it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:05<00:03,  3.15it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 19/28 [00:05<00:02,  3.14it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:06<00:02,  3.13it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 21/28 [00:06<00:02,  3.12it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:06<00:01,  3.13it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 23/28 [00:07<00:01,  3.11it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:07<00:01,  3.12it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 25/28 [00:07<00:00,  3.14it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▉   | 26/28 [00:08<00:00,  3.13it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▍ | 27/28 [00:08<00:00,  3.14it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.7670502066612244, 'eval_accuracy': 0.9048165137614679, 'eval_runtime': 8.8338, 'eval_samples_per_second': 98.712, 'eval_steps_per_second': 3.17, 'epoch': 11.0}\n",
      " 85%|█████████████████████████████▌     | 23155/27365 [2:27:14<28:41,  2.45it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:08<00:00,  3.86it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 22:16:51 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/sparsified/checkpoint-23155/recipe.yaml\n",
      "{'loss': 0.1198, 'learning_rate': 5e-05, 'epoch': 11.16}                        \n",
      "{'loss': 0.1119, 'learning_rate': 5e-05, 'epoch': 11.4}                         \n",
      "{'loss': 0.1102, 'learning_rate': 5e-05, 'epoch': 11.64}                        \n",
      "{'loss': 0.1277, 'learning_rate': 5e-05, 'epoch': 11.88}                        \n",
      " 92%|████████████████████████████████▎  | 25260/27365 [2:42:43<14:22,  2.44it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███▏                                        | 2/28 [00:00<00:04,  6.13it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:05,  4.38it/s]\u001b[A\n",
      " 14%|██████▎                                     | 4/28 [00:00<00:06,  3.78it/s]\u001b[A\n",
      " 18%|███████▊                                    | 5/28 [00:01<00:06,  3.50it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:01<00:06,  3.35it/s]\u001b[A\n",
      " 25%|███████████                                 | 7/28 [00:01<00:06,  3.27it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:02<00:06,  3.22it/s]\u001b[A\n",
      " 32%|██████████████▏                             | 9/28 [00:02<00:06,  3.16it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:02<00:05,  3.14it/s]\u001b[A\n",
      " 39%|████████████████▉                          | 11/28 [00:03<00:05,  3.13it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:03<00:05,  3.12it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 13/28 [00:03<00:04,  3.10it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:04<00:04,  3.10it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 15/28 [00:04<00:04,  3.10it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:04<00:03,  3.09it/s]\u001b[A\n",
      " 61%|██████████████████████████                 | 17/28 [00:05<00:03,  3.08it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:05<00:03,  3.10it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 19/28 [00:05<00:02,  3.10it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:06<00:02,  3.09it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 21/28 [00:06<00:02,  3.09it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:06<00:01,  3.08it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 23/28 [00:07<00:01,  3.09it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:07<00:01,  3.10it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 25/28 [00:07<00:00,  3.09it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▉   | 26/28 [00:08<00:00,  3.09it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▍ | 27/28 [00:08<00:00,  3.09it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.7642903923988342, 'eval_accuracy': 0.9025229357798165, 'eval_runtime': 8.9126, 'eval_samples_per_second': 97.839, 'eval_steps_per_second': 3.142, 'epoch': 12.0}\n",
      " 92%|████████████████████████████████▎  | 25260/27365 [2:42:52<14:22,  2.44it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:08<00:00,  3.79it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 22:32:28 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/sparsified/checkpoint-25260/recipe.yaml\n",
      "{'loss': 0.1069, 'learning_rate': 5e-05, 'epoch': 12.11}                        \n",
      "{'loss': 0.0995, 'learning_rate': 5e-05, 'epoch': 12.35}                        \n",
      "{'loss': 0.0997, 'learning_rate': 5e-05, 'epoch': 12.59}                        \n",
      "{'loss': 0.1005, 'learning_rate': 5e-05, 'epoch': 12.83}                        \n",
      "100%|███████████████████████████████████| 27365/27365 [2:56:42<00:00,  2.73it/s]\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███▏                                        | 2/28 [00:00<00:03,  7.56it/s]\u001b[A\n",
      " 11%|████▋                                       | 3/28 [00:00<00:04,  5.33it/s]\u001b[A\n",
      " 14%|██████▎                                     | 4/28 [00:00<00:05,  4.54it/s]\u001b[A\n",
      " 18%|███████▊                                    | 5/28 [00:01<00:05,  4.19it/s]\u001b[A\n",
      " 21%|█████████▍                                  | 6/28 [00:01<00:05,  4.02it/s]\u001b[A\n",
      " 25%|███████████                                 | 7/28 [00:01<00:05,  3.91it/s]\u001b[A\n",
      " 29%|████████████▌                               | 8/28 [00:01<00:05,  3.84it/s]\u001b[A\n",
      " 32%|██████████████▏                             | 9/28 [00:02<00:05,  3.78it/s]\u001b[A\n",
      " 36%|███████████████▎                           | 10/28 [00:02<00:04,  3.76it/s]\u001b[A\n",
      " 39%|████████████████▉                          | 11/28 [00:02<00:04,  3.75it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 12/28 [00:02<00:04,  3.72it/s]\u001b[A\n",
      " 46%|███████████████████▉                       | 13/28 [00:03<00:04,  3.72it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 14/28 [00:03<00:03,  3.72it/s]\u001b[A\n",
      " 54%|███████████████████████                    | 15/28 [00:03<00:03,  3.71it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 16/28 [00:04<00:03,  3.71it/s]\u001b[A\n",
      " 61%|██████████████████████████                 | 17/28 [00:04<00:02,  3.72it/s]\u001b[A\n",
      " 64%|███████████████████████████▋               | 18/28 [00:04<00:02,  3.70it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 19/28 [00:04<00:02,  3.70it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 20/28 [00:05<00:02,  3.71it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 21/28 [00:05<00:01,  3.70it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 22/28 [00:05<00:01,  3.70it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 23/28 [00:05<00:01,  3.70it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 24/28 [00:06<00:01,  3.70it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 25/28 [00:06<00:00,  3.70it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████▉   | 26/28 [00:06<00:00,  3.69it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.7953541874885559, 'eval_accuracy': 0.9002293577981652, 'eval_runtime': 7.4079, 'eval_samples_per_second': 117.712, 'eval_steps_per_second': 3.78, 'epoch': 13.0}\n",
      "100%|███████████████████████████████████| 27365/27365 [2:56:49<00:00,  2.73it/s]\n",
      "100%|███████████████████████████████████████████| 28/28 [00:07<00:00,  3.71it/s]\u001b[A\n",
      "                                                                                \u001b[A2024-01-17 22:46:26 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/sparsified/checkpoint-27365/recipe.yaml\n",
      "{'train_runtime': 10610.167, 'train_samples_per_second': 82.519, 'train_steps_per_second': 2.579, 'train_loss': 0.25580873020645384, 'epoch': 13.0}\n",
      "100%|███████████████████████████████████| 27365/27365 [2:56:50<00:00,  2.58it/s]\n",
      "2024-01-17 22:46:26 sparseml.transformers.sparsification.trainer INFO     Finalized SparseML recipe argument applied to the model\n",
      "2024-01-17 22:46:26 sparseml.transformers.sparsification.trainer INFO     Sparsification info for /home/jupyter/.cache/sparsezoo/neuralmagic/distilbert-wikipedia_bookcorpus-pruned80.4block_quantized/training: 66955010 total params. Of those there are 43058688 prunable params which have 78.90131719758855 avg sparsity.\n",
      "2024-01-17 22:46:26 sparseml.transformers.sparsification.trainer INFO     sparse model detected, all sparsification info: {\"params_summary\": {\"total\": 66955010, \"sparse\": 33973872, \"sparsity_percent\": 50.741344075671115, \"prunable\": 43058688, \"prunable_sparse\": 33973872, \"prunable_sparsity_percent\": 78.90131719758855, \"quantizable\": 43100930, \"quantized\": 43100930, \"quantized_percent\": 100.0}, \"params_info\": {\"distilbert.transformer.layer.0.attention.q_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.0.attention.k_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.0.attention.v_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.0.attention.out_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.0.ffn.lin1.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.0.ffn.lin2.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.1.attention.q_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.1.attention.k_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.1.attention.v_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.1.attention.out_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.1.ffn.lin1.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.1.ffn.lin2.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.2.attention.q_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.2.attention.k_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.2.attention.v_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.2.attention.out_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.2.ffn.lin1.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.2.ffn.lin2.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.3.attention.q_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.3.attention.k_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.3.attention.v_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.3.attention.out_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.3.ffn.lin1.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.3.ffn.lin2.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.4.attention.q_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.4.attention.k_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.4.attention.v_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.4.attention.out_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.4.ffn.lin1.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.4.ffn.lin2.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.5.attention.q_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.5.attention.k_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.5.attention.v_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.5.attention.out_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.5.ffn.lin1.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.5.ffn.lin2.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"pre_classifier.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": true}, \"classifier.module.weight\": {\"numel\": 1536, \"sparsity\": 0.0, \"quantized\": true}}}\n",
      "***** train metrics *****\n",
      "  epoch                    =       13.0\n",
      "  train_loss               =     0.2558\n",
      "  train_runtime            = 2:56:50.16\n",
      "  train_samples            =      67349\n",
      "  train_samples_per_second =     82.519\n",
      "  train_steps_per_second   =      2.579\n",
      "2024-01-17 22:46:26 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to models/sparsified/recipe.yaml\n",
      "100%|███████████████████████████████████████████| 28/28 [00:01<00:00, 18.63it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =       13.0\n",
      "  eval_accuracy           =     0.9002\n",
      "  eval_loss               =     0.6592\n",
      "  eval_runtime            = 0:00:01.56\n",
      "  eval_samples            =        872\n",
      "  eval_samples_per_second =    555.596\n",
      "  eval_steps_per_second   =      17.84\n"
     ]
    }
   ],
   "source": [
    "!sparseml.transformers.text_classification \\\n",
    "    --output_dir models/sparsified \\\n",
    "    --model_name_or_path \"zoo:nlp/masked_language_modeling/distilbert-none/pytorch/huggingface/wikipedia_bookcorpus/pruned80_quant-none-vnni\" \\\n",
    "    --recipe \"zoo:nlp/sentiment_analysis/distilbert-none/pytorch/huggingface/sst2/pruned80_quant-none-vnni\" \\\n",
    "    --distill_teacher models/teacher \\\n",
    "    --task_name sst2 \\\n",
    "    --max_seq_length 128 \\\n",
    "    --per_device_train_batch_size 32 --per_device_eval_batch_size 32 \\\n",
    "    --do_train --do_eval --evaluation_strategy epoch --fp16 \\\n",
    "    --save_strategy epoch --save_total_limit 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1832aeef-8be7-41c3-9cf5-702497268480",
   "metadata": {},
   "source": [
    "#### Exporting for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d186dfe5-4761-4547-9953-259814311896",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-18 12:41:33 sparseml.transformers.export INFO     Attempting onnx export for model at /home/jupyter/neural-magic/models/sparsified for task text-classification\n",
      "2024-01-18 12:41:36 sparseml.transformers.utils.model WARNING  QAT state detected, ignore any loading errors, weights will reload after SparseML recipes have been applied /home/jupyter/neural-magic/models/sparsified\n",
      "Some weights of the model checkpoint at /home/jupyter/neural-magic/models/sparsified were not used when initializing DistilBertForSequenceClassification: ['distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.k_lin.module.weight', 'distilbert.transformer.layer.3.attention.q_lin.module.bias', 'distilbert.transformer.layer.1.attention.v_lin.module.weight', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.scale', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.ffn.lin1.module.bias', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.fake_quant_enabled', 'distilbert.embeddings.word_embeddings.weight_fake_quant.activation_post_process.eps', 'distilbert.embeddings.position_embeddings.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.2.attention.out_lin.module.bias', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin1.module.weight', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.fake_quant_enabled', 'pre_classifier.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'pre_classifier.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.fake_quant_enabled', 'pre_classifier.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.k_lin.module.bias', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.v_lin.module.bias', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.ffn.lin1.module.weight', 'distilbert.transformer.layer.1.ffn.lin2.module.bias', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.v_lin.module.bias', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.activation_post_process.eps', 'distilbert.embeddings.word_embeddings.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.3.attention.k_lin.module.weight', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.activation_post_process.max_val', 'pre_classifier.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'classifier.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.k_lin.module.bias', 'distilbert.transformer.layer.1.attention.q_lin.module.weight', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.out_lin.module.bias', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.scale', 'classifier.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.k_lin.module.weight', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin2.module.bias', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.ffn.lin2.module.weight', 'classifier.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.observer_enabled', 'classifier.module.weight', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.ffn.lin1.module.bias', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.ffn.lin1.module.weight', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.scale', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.observer_enabled', 'classifier.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.activation_post_process.eps', 'distilbert.embeddings.position_embeddings.weight_fake_quant.scale', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.observer_enabled', 'distilbert.embeddings.word_embeddings.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.embeddings.word_embeddings.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.ffn.lin1.module.weight', 'classifier.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.1.ffn.lin2.module.weight', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.scale', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.activation_post_process.min_val', 'pre_classifier.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.embeddings.word_embeddings.weight_fake_quant.scale', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.embeddings.position_embeddings.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.ffn.lin2.module.bias', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'classifier.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.scale', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.activation_post_process.min_val', 'classifier.module.bias', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.0.ffn.lin2.module.bias', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.out_lin.module.bias', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.q_lin.module.bias', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.out_lin.module.weight', 'pre_classifier.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.attention.q_lin.module.weight', 'distilbert.transformer.layer.0.attention.v_lin.module.weight', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.v_lin.module.weight', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.2.ffn.lin1.module.weight', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.q_lin.module.bias', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.ffn.lin1.module.bias', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.out_lin.module.weight', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.ffn.lin1.module.bias', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.k_lin.module.weight', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.scale', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.q_lin.module.weight', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'pre_classifier.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.activation_post_process.max_val', 'classifier.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.k_lin.module.bias', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.activation_post_process.min_val', 'pre_classifier.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.v_lin.module.weight', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'pre_classifier.module.weight', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.3.ffn.lin2.module.weight', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.4.ffn.lin2.module.weight', 'distilbert.transformer.layer.5.attention.k_lin.module.bias', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.v_lin.module.weight', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.v_lin.module.bias', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.q_lin.module.bias', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.out_lin.module.bias', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.embeddings.word_embeddings.weight_fake_quant.zero_point', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin2.module.bias', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.out_lin.module.weight', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.scale', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.ffn.lin1.module.bias', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.out_lin.module.bias', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.activation_post_process.eps', 'pre_classifier.quant.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.v_lin.module.weight', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.out_lin.module.bias', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.out_lin.module.weight', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.scale', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.scale', 'pre_classifier.module.bias', 'distilbert.transformer.layer.3.attention.v_lin.module.bias', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.activation_post_process.min_val', 'classifier.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'classifier.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.activation_post_process.eps', 'classifier.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.k_lin.module.weight', 'classifier.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.ffn.lin2.module.bias', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.embeddings.word_embeddings.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.ffn.lin2.module.weight', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.observer_enabled', 'classifier.module.weight_fake_quant.scale', 'distilbert.transformer.layer.5.attention.out_lin.module.weight', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.scale', 'pre_classifier.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.4.attention.q_lin.module.bias', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.zero_point', 'pre_classifier.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.scale', 'pre_classifier.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.embeddings.position_embeddings.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.ffn.lin1.module.bias', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.2.attention.v_lin.module.bias', 'pre_classifier.module.weight_fake_quant.zero_point', 'distilbert.embeddings.position_embeddings.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin1.module.weight', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.v_lin.module.bias', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.ffn.lin2.module.weight', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.activation_post_process.eps', 'classifier.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.q_lin.module.weight', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.k_lin.module.weight', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.scale', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.k_lin.module.bias', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.embeddings.position_embeddings.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.q_lin.module.bias', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.attention.q_lin.module.weight', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'classifier.module.weight_fake_quant.activation_post_process.eps', 'distilbert.embeddings.position_embeddings.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.k_lin.module.bias', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.out_lin.module.weight', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.zero_point', 'pre_classifier.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.2.attention.q_lin.module.weight', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.activation_post_process.eps']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /home/jupyter/neural-magic/models/sparsified and are newly initialized: ['distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'classifier.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'classifier.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-01-18 12:41:38 sparseml.transformers.utils.model INFO     Delayed load of model /home/jupyter/neural-magic/models/sparsified detected. Will print out model information once SparseML recipes have loaded\n",
      "2024-01-18 12:41:38 sparseml.transformers.export INFO     loaded model, config, and tokenizer from /home/jupyter/neural-magic/models/sparsified\n",
      "2024-01-18 12:41:53 sparseml.core.logger INFO     Logging all SparseML modifier-level logs to sparse_logs/18-01-2024_12.41.53.log\n",
      "2024-01-18 12:41:53 sparseml.transformers.sparsification.trainer INFO     Loaded 2 SparseML checkpoint recipe stage(s) from /home/jupyter/neural-magic/models/sparsified/recipe.yaml to replicate model sparse state\n",
      "2024-01-18 12:41:54 sparseml.transformers.sparsification.trainer INFO     Applied structure from 2 previous recipe stage(s) to model and finalized (recipes saved with model_path)\n",
      "2024-01-18 12:41:54 sparseml.transformers.sparsification.trainer INFO     Reloaded 860 model params for SparseML Recipe from /home/jupyter/neural-magic/models/sparsified\n",
      "2024-01-18 12:41:54 sparseml.transformers.utils.model INFO     Loaded model from /home/jupyter/neural-magic/models/sparsified with 66955010 total params. Of those there are 43058688 prunable params which have 78.90131719758855 avg sparsity.\n",
      "2024-01-18 12:41:55 sparseml.transformers.utils.model INFO     sparse model detected, all sparsification info: {\"params_summary\": {\"total\": 66955010, \"sparse\": 33973872, \"sparsity_percent\": 50.741344075671115, \"prunable\": 43058688, \"prunable_sparse\": 33973872, \"prunable_sparsity_percent\": 78.90131719758855, \"quantizable\": 43100930, \"quantized\": 43100930, \"quantized_percent\": 100.0}, \"params_info\": {\"distilbert.transformer.layer.0.attention.q_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.0.attention.k_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.0.attention.v_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.0.attention.out_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.0.ffn.lin1.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.0.ffn.lin2.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.1.attention.q_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.1.attention.k_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.1.attention.v_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.1.attention.out_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.1.ffn.lin1.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.1.ffn.lin2.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.2.attention.q_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.2.attention.k_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.2.attention.v_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.2.attention.out_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.2.ffn.lin1.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.2.ffn.lin2.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.3.attention.q_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.3.attention.k_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.3.attention.v_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.3.attention.out_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.3.ffn.lin1.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.3.ffn.lin2.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.4.attention.q_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.4.attention.k_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.4.attention.v_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.4.attention.out_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.4.ffn.lin1.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.4.ffn.lin2.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.5.attention.q_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.5.attention.k_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.5.attention.v_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.5.attention.out_lin.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8000013828277588, \"quantized\": true}, \"distilbert.transformer.layer.5.ffn.lin1.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"distilbert.transformer.layer.5.ffn.lin2.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.7999996542930603, \"quantized\": true}, \"pre_classifier.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": true}, \"classifier.module.weight\": {\"numel\": 1536, \"sparsity\": 0.0, \"quantized\": true}}}\n",
      "2024-01-18 12:41:55 sparseml.transformers.sparsification.trainer INFO     Reloaded model state after SparseML recipe structure modifications from /home/jupyter/neural-magic/models/sparsified\n",
      "2024-01-18 12:41:55 sparseml.transformers.export INFO     Applied a staged recipe with 2 stages to the model at /home/jupyter/neural-magic/models/sparsified\n",
      "2024-01-18 12:41:55 sparseml.transformers.export WARNING  The following inputs were not present in the model forward function and therefore dropped from ONNX export: ['token_type_ids']\n",
      "2024-01-18 12:41:55 sparseml.transformers.export INFO     Created sample inputs for the ONNX export process: {'input_ids': 'torch.int64: [1, 128]', 'attention_mask': 'torch.int64: [1, 128]'}\n",
      "/opt/conda/lib/python3.10/site-packages/torch/onnx/utils.py:823: UserWarning: It is recommended that constant folding be turned off ('do_constant_folding=False') when exporting the model in training-amenable mode, i.e. with 'training=TrainingMode.TRAIN' or 'training=TrainingMode.PRESERVE' (when model is in training mode). Otherwise, some learnable model parameters may not translate correctly in the exported ONNX model because constant folding mutates model parameters. Please consider turning off constant folding or setting the training=TrainingMode.EVAL.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:260: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n",
      "2024-01-18 12:42:04 sparseml.exporters.transforms.onnx_transform INFO     [ConstantsToInitializers] Transformed 190 matches\n",
      "2024-01-18 12:42:04 sparseml.exporters.transforms.onnx_transform INFO     [FoldIdentityInitializers] Transformed 0 matches\n",
      "2024-01-18 12:42:05 sparseml.exporters.transforms.onnx_transform INFO     [InitializersToUint8] Transformed 1 matches\n",
      "2024-01-18 12:42:05 sparseml.exporters.transforms.onnx_transform INFO     [FlattenQParams] Transformed 0 matches\n",
      "2024-01-18 12:42:06 sparseml.exporters.transforms.onnx_transform INFO     [FoldConvDivBn] Transformed 0 matches\n",
      "2024-01-18 12:42:06 sparseml.exporters.transforms.onnx_transform INFO     [DeleteRepeatedQdq] Transformed 0 matches\n",
      "2024-01-18 12:42:07 sparseml.exporters.transforms.onnx_transform INFO     [QuantizeQATEmbedding] Transformed 2 matches\n",
      "2024-01-18 12:42:08 sparseml.exporters.transforms.onnx_transform INFO     [PropagateEmbeddingQuantization] Transformed 0 matches\n",
      "2024-01-18 12:42:08 sparseml.exporters.transforms.onnx_transform INFO     [PropagateDequantThroughSplit] Transformed 0 matches\n",
      "2024-01-18 12:42:09 sparseml.exporters.transforms.onnx_transform INFO     [MatMulAddToMatMulIntegerAddCastMul] Transformed 36 matches\n",
      "2024-01-18 12:42:09 sparseml.exporters.transforms.onnx_transform INFO     [MatMulToMatMulIntegerCastMul] Transformed 12 matches\n",
      "2024-01-18 12:42:09 sparseml.exporters.transforms.onnx_transform INFO     [FoldReLUQuants] Transformed 1 matches\n",
      "2024-01-18 12:42:10 sparseml.exporters.transforms.onnx_transform INFO     [ConvToQLinearConv] Transformed 0 matches\n",
      "2024-01-18 12:42:10 sparseml.exporters.transforms.onnx_transform INFO     [GemmToQLinearMatMul] Transformed 0 matches\n",
      "2024-01-18 12:42:10 sparseml.exporters.transforms.onnx_transform INFO     [GemmToMatMulIntegerAddCastMul] Transformed 2 matches\n",
      "2024-01-18 12:42:10 sparseml.exporters.transforms.onnx_transform INFO     [QuantizeResiduals] Transformed 0 matches\n",
      "2024-01-18 12:42:10 sparseml.exporters.transforms.onnx_transform INFO     [RemoveDuplicateQConvWeights] Transformed 0 matches\n",
      "2024-01-18 12:42:10 sparseml.exporters.transforms.onnx_transform INFO     [RemoveDuplicateQuantizeOps] Transformed 0 matches\n",
      "2024-01-18 12:42:11 sparseml.transformers.export INFO     ONNX exported to /home/jupyter/neural-magic/models/sparsified/model.onnx\n",
      "2024-01-18 12:42:11 sparseml.transformers.export INFO     0 sample inputs/outputs exported\n",
      "2024-01-18 12:42:11 sparseml.transformers.export INFO     Saved model.onnx in the deployment folder at /home/jupyter/neural-magic/models/deployment/model.onnx\n",
      "2024-01-18 12:42:11 sparseml.transformers.export INFO     Saved tokenizer_config.json in the deployment folder at /home/jupyter/neural-magic/models/deployment/tokenizer_config.json\n",
      "2024-01-18 12:42:11 sparseml.transformers.export INFO     Saved config.json in the deployment folder at /home/jupyter/neural-magic/models/deployment/config.json\n",
      "2024-01-18 12:42:11 sparseml.transformers.export INFO     Saved tokenizer.json in the deployment folder at /home/jupyter/neural-magic/models/deployment/tokenizer.json\n",
      "2024-01-18 12:42:11 sparseml.transformers.export WARNING  Optional file model.data not found in /home/jupyter/neural-magic/models/sparsified. Skipping copying to deployment folder.\n",
      "2024-01-18 12:42:11 sparseml.transformers.export INFO     Saved special_tokens_map.json in the deployment folder at /home/jupyter/neural-magic/models/deployment/special_tokens_map.json\n",
      "2024-01-18 12:42:11 sparseml.transformers.export WARNING  Optional file vocab.json not found in /home/jupyter/neural-magic/models/sparsified. Skipping copying to deployment folder.\n",
      "2024-01-18 12:42:11 sparseml.transformers.export WARNING  Optional file merges.txt not found in /home/jupyter/neural-magic/models/sparsified. Skipping copying to deployment folder.\n",
      "2024-01-18 12:42:11 sparseml.transformers.export INFO     Created deployment folder at /home/jupyter/neural-magic/models/deployment with files: ['config.json', 'tokenizer_config.json', 'tokenizer.json', 'model.onnx', 'special_tokens_map.json']\n"
     ]
    }
   ],
   "source": [
    "!sparseml.transformers.export_onnx \\\n",
    "    --model_path models/sparsified \\\n",
    "    --task 'text-classification' --finetuning_task sst2 \\\n",
    "    --sequence_length 128"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu113.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu113:m115"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
