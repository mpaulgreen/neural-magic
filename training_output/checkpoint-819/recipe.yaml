version: 1.1.0

__metadata__:
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  fp16: False
  framework_metadata:
    python_version: 3.10.13
    sparseml_version: 1.6.1
    torch_version: 2.1.2+cu121

modifiers:
    - !EpochRangeModifier
        end_epoch: 13.0
        start_epoch: 0.0

    - !LearningRateFunctionModifier
        cycle_epochs: 1.0
        end_epoch: 13.0
        final_lr: 0
        init_lr: 0.00015
        lr_func: linear
        start_epoch: 0.0
        update_frequency: -1.0

    - !DistillationModifier
        distill_output_keys: ['logits']
        end_epoch: -1.0
        hardness: 1.0
        start_epoch: -1.0
        temperature: 2.0
        update_frequency: -1.0

    - !ConstantPruningModifier
        end_epoch: -1.0
        params: __ALL_PRUNABLE__
        start_epoch: 0.0
        update_frequency: -1

    - !QuantizationModifier
        activation_bits: 8
        disable_quantization_observer_epoch: 12.0
        end_epoch: -1.0
        exclude_batchnorm: True
        exclude_module_types: ['Softmax', 'LayerNorm', 'Tanh', 'BatchNorm1d', 'BatchNorm2d', 'BatchNorm3d']
        freeze_bn_stats_epoch: 12.0
        model_fuse_fn_name: conv_bn_relus
        quantize_conv_activations: True
        quantize_embedding_activations: True
        quantize_embeddings: 1
        quantize_linear_activations: 0
        reduce_range: False
        start_epoch: 8.0
        submodules: ['classifier', 'bert.encoder', 'bert.pooler', 'bert.embeddings']
        tensorrt: False
        weight_bits: 8
